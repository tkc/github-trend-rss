<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for typescript - TypeScript Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for typescript.</description>
        <lastBuildDate>Tue, 19 Aug 2025 00:04:43 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[immich-app/immich]]></title>
            <link>https://github.com/immich-app/immich</link>
            <guid>https://github.com/immich-app/immich</guid>
            <pubDate>Tue, 19 Aug 2025 00:04:43 GMT</pubDate>
            <description><![CDATA[High performance self-hosted photo and video management solution.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/immich-app/immich">immich-app/immich</a></h1>
            <p>High performance self-hosted photo and video management solution.</p>
            <p>Language: TypeScript</p>
            <p>Stars: 73,290</p>
            <p>Forks: 3,867</p>
            <p>Stars today: 114 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt; 
  &lt;br/&gt;
  &lt;a href=&quot;https://opensource.org/license/agpl-v3&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-AGPL_v3-blue.svg?color=3F51B5&amp;style=for-the-badge&amp;label=License&amp;logoColor=000000&amp;labelColor=ececec&quot; alt=&quot;License: AGPLv3&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.immich.app&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/discord/979116623879368755.svg?label=Discord&amp;logo=Discord&amp;style=for-the-badge&amp;logoColor=000000&amp;labelColor=ececec&quot; alt=&quot;Discord&quot;/&gt;
  &lt;/a&gt;
  &lt;br/&gt;
  &lt;br/&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;design/immich-logo-stacked-light.svg&quot; width=&quot;300&quot; title=&quot;Login With Custom URL&quot;&gt;
&lt;/p&gt;
&lt;h3 align=&quot;center&quot;&gt;High performance self-hosted photo and video management solution&lt;/h3&gt;
&lt;br/&gt;
&lt;a href=&quot;https://immich.app&quot;&gt;
&lt;img src=&quot;design/immich-screenshots.png&quot; title=&quot;Main Screenshot&quot;&gt;
&lt;/a&gt;
&lt;br/&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;readme_i18n/README_ca_ES.md&quot;&gt;Català&lt;/a&gt;
  &lt;a href=&quot;readme_i18n/README_es_ES.md&quot;&gt;Español&lt;/a&gt;
  &lt;a href=&quot;readme_i18n/README_fr_FR.md&quot;&gt;Français&lt;/a&gt;
  &lt;a href=&quot;readme_i18n/README_it_IT.md&quot;&gt;Italiano&lt;/a&gt;
  &lt;a href=&quot;readme_i18n/README_ja_JP.md&quot;&gt;日本語&lt;/a&gt;
  &lt;a href=&quot;readme_i18n/README_ko_KR.md&quot;&gt;한국어&lt;/a&gt;
  &lt;a href=&quot;readme_i18n/README_de_DE.md&quot;&gt;Deutsch&lt;/a&gt;
  &lt;a href=&quot;readme_i18n/README_nl_NL.md&quot;&gt;Nederlands&lt;/a&gt;
  &lt;a href=&quot;readme_i18n/README_tr_TR.md&quot;&gt;Türkçe&lt;/a&gt;
  &lt;a href=&quot;readme_i18n/README_zh_CN.md&quot;&gt;中文&lt;/a&gt;
  &lt;a href=&quot;readme_i18n/README_uk_UA.md&quot;&gt;Українська&lt;/a&gt;
  &lt;a href=&quot;readme_i18n/README_ru_RU.md&quot;&gt;Русский&lt;/a&gt;
  &lt;a href=&quot;readme_i18n/README_pt_BR.md&quot;&gt;Português Brasileiro&lt;/a&gt;
  &lt;a href=&quot;readme_i18n/README_sv_SE.md&quot;&gt;Svenska&lt;/a&gt;
  &lt;a href=&quot;readme_i18n/README_ar_JO.md&quot;&gt;العربية&lt;/a&gt;
  &lt;a href=&quot;readme_i18n/README_vi_VN.md&quot;&gt;Tiếng Việt&lt;/a&gt;
  &lt;a href=&quot;readme_i18n/README_th_TH.md&quot;&gt;ภาษาไทย&lt;/a&gt;
&lt;/p&gt;

## Disclaimer

- ⚠️ The project is under **very active** development.
- ⚠️ Expect bugs and breaking changes.
- ⚠️ **Do not use the app as the only way to store your photos and videos.**
- ⚠️ Always follow [3-2-1](https://www.backblaze.com/blog/the-3-2-1-backup-strategy/) backup plan for your precious photos and videos!

&gt; [!NOTE]
&gt; You can find the main documentation, including installation guides, at https://immich.app/.

## Links

- [Documentation](https://immich.app/docs)
- [About](https://immich.app/docs/overview/introduction)
- [Installation](https://immich.app/docs/install/requirements)
- [Roadmap](https://immich.app/roadmap)
- [Demo](#demo)
- [Features](#features)
- [Translations](https://immich.app/docs/developer/translations)
- [Contributing](https://immich.app/docs/overview/support-the-project)

## Demo

Access the demo [here](https://demo.immich.app). For the mobile app, you can use `https://demo.immich.app` for the `Server Endpoint URL`.

### Login credentials

| Email           | Password |
| --------------- | -------- |
| demo@immich.app | demo     |

## Features

| Features                                     | Mobile | Web |
| :------------------------------------------- | ------ | --- |
| Upload and view videos and photos            | Yes    | Yes |
| Auto backup when the app is opened           | Yes    | N/A |
| Prevent duplication of assets                | Yes    | Yes |
| Selective album(s) for backup                | Yes    | N/A |
| Download photos and videos to local device   | Yes    | Yes |
| Multi-user support                           | Yes    | Yes |
| Album and Shared albums                      | Yes    | Yes |
| Scrubbable/draggable scrollbar               | Yes    | Yes |
| Support raw formats                          | Yes    | Yes |
| Metadata view (EXIF, map)                    | Yes    | Yes |
| Search by metadata, objects, faces, and CLIP | Yes    | Yes |
| Administrative functions (user management)   | No     | Yes |
| Background backup                            | Yes    | N/A |
| Virtual scroll                               | Yes    | Yes |
| OAuth support                                | Yes    | Yes |
| API Keys                                     | N/A    | Yes |
| LivePhoto/MotionPhoto backup and playback    | Yes    | Yes |
| Support 360 degree image display             | No     | Yes |
| User-defined storage structure               | Yes    | Yes |
| Public Sharing                               | Yes    | Yes |
| Archive and Favorites                        | Yes    | Yes |
| Global Map                                   | Yes    | Yes |
| Partner Sharing                              | Yes    | Yes |
| Facial recognition and clustering            | Yes    | Yes |
| Memories (x years ago)                       | Yes    | Yes |
| Offline support                              | Yes    | No  |
| Read-only gallery                            | Yes    | Yes |
| Stacked Photos                               | Yes    | Yes |
| Tags                                         | No     | Yes |
| Folder View                                  | Yes    | Yes |

## Translations

Read more about translations [here](https://immich.app/docs/developer/translations).

&lt;a href=&quot;https://hosted.weblate.org/engage/immich/&quot;&gt;
&lt;img src=&quot;https://hosted.weblate.org/widget/immich/immich/multi-auto.svg&quot; alt=&quot;Translation status&quot; /&gt;
&lt;/a&gt;

## Repository activity

![Activities](https://repobeats.axiom.co/api/embed/9e86d9dc3ddd137161f2f6d2e758d7863b1789cb.svg &quot;Repobeats analytics image&quot;)

## Star history

&lt;a href=&quot;https://star-history.com/#immich-app/immich&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=immich-app/immich&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=immich-app/immich&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=immich-app/immich&amp;type=Date&quot; width=&quot;100%&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;

## Contributors

&lt;a href=&quot;https://github.com/alextran1502/immich/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=immich-app/immich&quot; width=&quot;100%&quot;/&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[MotiaDev/motia]]></title>
            <link>https://github.com/MotiaDev/motia</link>
            <guid>https://github.com/MotiaDev/motia</guid>
            <pubDate>Tue, 19 Aug 2025 00:04:42 GMT</pubDate>
            <description><![CDATA[Modern Backend Framework that unifies APIs, background jobs, workflows, and AI agents into a single cohesive system with built-in observability and state management.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/MotiaDev/motia">MotiaDev/motia</a></h1>
            <p>Modern Backend Framework that unifies APIs, background jobs, workflows, and AI agents into a single cohesive system with built-in observability and state management.</p>
            <p>Language: TypeScript</p>
            <p>Stars: 6,471</p>
            <p>Forks: 502</p>
            <p>Stars today: 310 stars today</p>
            <h2>README</h2><pre>&lt;a href=&quot;https://motia.dev&quot;&gt;
  &lt;img src=&quot;assets/github-readme-banner.png&quot; alt=&quot;Motia Banner&quot; width=&quot;100%&quot;&gt;
&lt;/a&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/14032&quot; style=&quot;margin-right:8px;&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/14032&quot; alt=&quot;Motia&quot; style=&quot;width: 250px; height: 55px; margin-right:8px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://vercel.com/blog/summer-2025-oss-program#motia&quot; target=&quot;_blank&quot; style=&quot;margin-left:8px;&quot;&gt;
    &lt;img alt=&quot;Vercel OSS Program&quot; src=&quot;https://vercel.com/oss/program-badge.svg&quot; style=&quot;width: 250px; height: 55px; margin-left:8px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;🔥 The Unified Backend Framework That Eliminates Runtime Fragmentation 🔥&lt;/strong&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;em&gt;APIs, background jobs, workflows, and AI agents in one system. JavaScript, TypeScript, Python, and more in one codebase.&lt;/em&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.npmjs.com/package/motia&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/npm/v/motia?style=flat&amp;logo=npm&amp;logoColor=white&amp;color=CB3837&amp;labelColor=000000&quot; alt=&quot;npm version&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/MotiaDev/motia/blob/main/LICENSE&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/license-MIT-green?style=flat&amp;logo=opensourceinitiative&amp;logoColor=white&amp;labelColor=000000&quot; alt=&quot;license&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/MotiaDev/motia&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/MotiaDev/motia?style=flat&amp;logo=github&amp;logoColor=white&amp;color=yellow&amp;labelColor=000000&quot; alt=&quot;GitHub stars&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/motiadev&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Follow-@motiadev-1DA1F2?style=flat&amp;logo=twitter&amp;logoColor=white&amp;labelColor=000000&quot; alt=&quot;Twitter Follow&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/motia&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/discord/1322278831184281721?style=flat&amp;logo=discord&amp;logoColor=white&amp;color=5865F2&amp;label=Discord&amp;labelColor=000000&quot; alt=&quot;Discord&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.motia.dev/manifesto&quot;&gt;💡 Motia Manifesto&lt;/a&gt; •
  &lt;a href=&quot;https://www.motia.dev/docs/getting-started/quick-start&quot;&gt;🚀 Quick Start&lt;/a&gt; •
  &lt;a href=&quot;https://www.motia.dev/docs/concepts/steps/defining-steps&quot;&gt;📋 Defining Steps&lt;/a&gt; •
  &lt;a href=&quot;https://www.motia.dev/docs&quot;&gt;📚 Docs&lt;/a&gt;
&lt;/p&gt;

---

## 🎯 What is Motia?

**Motia solves backend fragmentation.** 

Modern software engineering is splintered – APIs live in one framework, background jobs in another, queues have their own tooling, and AI agents are springing up in yet more isolated runtimes. **This fragmentation demands a unified system.**

Motia unifies APIs, background jobs, workflows, and AI agents into a **single coherent system** with shared observability and developer experience. Similar to how React simplified frontend development where everything is a component, **Motia simplifies backend development where everything is a Step**.

Write **JavaScript, TypeScript, Python, and more** in the same workflow. **What used to take 5 frameworks to build now comes out of the box with Motia.**

[![Motia combines APIs, background queues, and AI agents into one system](assets/Motia_Github_Repository_GIF.gif)](https://motia.dev)

## 🚀 Quickstart

Get Motia project up and running in **under 60 seconds**:

### 1. Bootstrap a New Motia Project

```bash
npx motia@latest create -i   # runs the interactive terminal
```

Follow the prompts to pick a template, project name, and language.

### 2. Start the Workbench

Inside your new project folder, launch the dev server:

```bash
npx motia dev # ➜ http://localhost:3000
```

This spins up the Motia Workbench – a local UI for building, testing &amp; observing your backend in real-time.

![motia-terminal](assets/motia-terminal.gif)

### 3. Hit Your First Endpoint

Open a new terminal tab and run:

```bash
curl http://localhost:3000/default
```

You should see the JSON response:

```json
{ &quot;message&quot;: &quot;Hello World from Motia!&quot; }
```

### 4. Explore the Workbench UI

![new-workbench](assets/new-workbench.png)
The Workbench is your command centre:

- **🌊 Flows** – Visualise how your Steps connect.
- **🔌 Endpoints** – Test APIs with one click and stream results live.
- **👁️ Traces** – Inspect end-to-end traces of every execution.
- **📊 Logs** – View structured logs grouped by trace.
- **🏪 State** – Inspect the key-value store across Steps.

---

🎉 **That&#039;s it!** You now have a production-ready backend with everything you need:

- ✅ **REST API endpoints** with automatic validation and error handling
- ✅ **Visual debugger** with real-time flow inspection and tracing  
- ✅ **Built-in observability** - logs, traces, and state visualization
- ✅ **Hot-reload** for instant feedback during development
- ✅ **Event-driven architecture** ready for complex workflows
- ✅ **Multi-language support** - add Python, Javascript, or other languages anytime
- ✅ **Zero configuration** - no infrastructure setup required


&gt; 💡 **Want a more detailed walkthrough?**  
&gt; Check out the [Quick Start guide in our docs](https://www.motia.dev/docs/getting-started/quick-start) for step-by-step instructions and more examples.

### 🧱 The Step Philosophy

**Everything is a Step** – similar to how React made everything a component, Motia makes every backend pattern a Step:

- **🎯 Steps Represent Distinct Entry Points**: APIs, background jobs, scheduled tasks, and AI agents – all unified under a single primitive
- **🌍 Any Language, One Workflow**: Write **JavaScript, TypeScript, Python, and more** in the same project. Use Python for AI agents, TypeScript for APIs, JavaScript for workflows – all sharing state effortlessly  
- **⚡ Enterprise-Grade, Out of the Box**: Get **event-driven architecture, fault tolerance, observability, and real-time streaming** without complex infrastructure setup
- **👁️ Automatic Observability**: Complete end-to-end tracing, structured logging, and state visualization. **No setup required** – works in both local development and production
- **🌊 Composable Workflows**: Connect Steps by emitting and subscribing to events. Build complex, multi-stage processes with simple, declarative code
- **🏪 Unified State Management**: All Steps share a traced key-value store. Every `get`, `set`, and `delete` is automatically tracked across your entire workflow
- **🔄 Built-in Fault Tolerance**: Retry mechanisms, error handling, and queue infrastructure abstracted away – focus on business logic, not infrastructure

---

## 🚧 The Fragmentation Problem

Today, backend engineers face several recurring challenges:

- **🧩 Fragmented Systems**: APIs in Express, background jobs in Celery/BullMQ, AI agents in LangChain – each with different deployment, debugging, and scaling patterns
- **🌐 Multi-Language Barriers**: AI tools in Python, business logic in TypeScript – forcing teams to choose between cutting-edge tech and their existing skillset  
- **🔍 Observability Gaps**: Tracing requests across multiple frameworks and runtimes is complex and often incomplete
- **⚖️ Scalability vs. Velocity**: Choose between fast development (monolith) or proper scaling (microservices complexity)
- **🚀 Deployment Complexity**: Multiple runtimes mean multiple deploy targets, configs, and failure points

**The rapid advancement of AI has made this worse** – many cutting-edge AI tools are only available in specific languages, forcing companies to abandon their existing tech stack or miss out on breakthrough technologies.

---

## ✅ The Motia Solution

**Motia removes this limitation** by unifying your entire backend into a single runtime where everything is a **Step**:

### 🎯 **Unified vs. Fragmented**
- **Before**: APIs in Express, jobs in BullMQ, AI agents in LangChain
- **After**: All backend patterns as composable Steps with shared state and observability

### 🌐 **True Multi-Language Support**  
- **Before**: Choose between Python AI tools OR your existing TypeScript stack
- **After**: Each Step can be written in any language while sharing common state – use Python for AI, TypeScript for APIs, JavaScript for workflows

### 🔍 **Built-in Observability**
- **Before**: Complex tracing setups across multiple frameworks
- **After**: Complete observability toolkit available in both cloud and local environments out of the box

### ⚖️ **Scalability Without Complexity**
- **Before**: Choose between monolith simplicity or microservice complexity  
- **After**: Each Step scales independently, avoiding bottlenecks while maintaining development velocity

### 🚀 **One-Click Everything**
- **Before**: Multiple deployment pipelines, configs, and failure points
- **After**: Single deployable with atomic blue/green deployments and instant rollbacks

| **Traditional Stack**       | **Motia**                               |
| --------------------------- | --------------------------------------- |
| Multiple deployment targets | **Single unified deployment**           |
| Fragmented observability    | **End-to-end tracing**                  |
| Language silos              | **JavaScript, TypeScript, Python, etc** |
| Context-switching overhead  | **Single intuitive model**              |
| Manual error handling       | **Automatic retries &amp; fault tolerance** |
| Complex infrastructure      | **Zero-config queue &amp; streaming**       |

---

## 🔧 Supported Step Types

| Type        | Trigger               | Use Case                              |
| ----------- | --------------------- | ------------------------------------- |
| **`api`**   | HTTP Request          | Expose REST endpoints                 |
| **`event`** | Emitted Topics        | React to internal or external events  |
| **`cron`**  | Scheduled Time (cron) | Automate recurring jobs               |
| **`noop`**  | None                  | Placeholder for manual/external tasks |

---

### 🤔 How it Works

**One framework. All backend patterns.** Motia replaces your entire backend stack with a single, event-driven system:

**🚀 Replace Multiple Frameworks:**
- **Instead of**: Express/Nest.js + BullMQ + Temporal + LangChain + custom observability
- **Use**: Motia Steps with automatic observability, queuing, and multi-language support

**⚡ Simple but Powerful:**
- **Need a REST API?** Create an `api` step → instant HTTP endpoint with validation, tracing, and error handling
- **Need background processing?** Emit an event → `event` steps pick it up asynchronously with built-in retries and fault tolerance  
- **Need scheduled jobs?** Use a `cron` step → automatic scheduling with full observability
- **Need AI agents?** Write Python steps with access to the entire ecosystem (PyTorch, transformers, etc.) while sharing state with TypeScript APIs

**🔄 Event-Driven by Design:**
Each Step can emit events that trigger other Steps, creating powerful workflows that automatically handle:
- **Parallel processing** across multiple languages
- **Fault tolerance** with automatic retries  
- **Real-time updates** streamed to clients
- **Complete traceability** of every operation

**The result?** What used to require 5+ frameworks, complex deployment pipelines, and weeks of infrastructure setup now works out of the box with Motia.

## ⚡ Core Concepts

The **Step** is Motia&#039;s core primitive. The following concepts are deeply integrated with Steps to help you build powerful, complex, and scalable backends:

### 🔑 Steps &amp; Step Types

Understand the three ways Steps are triggered:

- **HTTP (`api`)** – Build REST/GraphQL endpoints with zero boilerplate.
- **Events (`event`)** – React to internal or external events emitted by other steps.
- **Cron (`cron`)** – Schedule recurring jobs with a familiar cron syntax.

### 📣 Emit &amp; Subscribe (Event-Driven Workflows)

Steps talk to each other by **emitting** and **subscribing** to topics. This decouples producers from consumers and lets you compose complex workflows with simple, declarative code.

### 🏪 State Management

All steps share a unified key-value state store. Every `get`, `set`, and `delete` is automatically traced so you always know when and where your data changed.

### 📊 Structured Logging

Motia provides structured, JSON logs correlated with trace IDs and step names. Search and filter your logs without regex hassle.

### 📡 Streams: Real-time Messaging

Push live updates from long-running or asynchronous workflows to clients without polling. Perfect for dashboards, progress indicators, and interactive AI agents.

### 👁️ End-to-End Observability with Traces

Every execution generates a full trace, capturing step timelines, state operations, emits, stream calls, and logs. Visualise everything in the Workbench&#039;s Traces UI and debug faster.

---

## 🗂 Production-Ready Examples

**⭐ Explore 20+ sophisticated examples** demonstrating real-world use cases from AI agents to enterprise workflows: **[View All Examples →](https://github.com/MotiaDev/motia-examples)**

### 🤖 **AI Agents &amp; Workflows**

| **AI Deep Research Agent** | **Finance Analysis Agent** | **PDF RAG System** |
|----------------------------|----------------------------|-------------------|
| Comprehensive web research with iterative analysis and synthesis | Real-time financial data + AI insights for investment analysis | Document processing with Docling, Weaviate, and OpenAI RAG |
| [View Example →](https://github.com/MotiaDev/motia-examples/tree/main/examples/ai-deep-research-agent) | [View Example →](https://github.com/MotiaDev/motia-examples/tree/main/examples/finance-agent) | [View Example →](https://github.com/MotiaDev/motia-examples/tree/main/examples/rag-docling-weaviate) |

| **GitHub PR Manager** | **Gmail Intelligence** | **Vision Analysis** |
|-----------------------|------------------------|-------------------|
| AI-powered PR classification, labeling, and reviewer assignment | Smart email analysis, auto-responses, and Discord summaries | Multi-modal conversation analysis with visual understanding |
| [View Example →](https://github.com/MotiaDev/motia-examples/tree/main/examples/github-integration-workflow) | [View Example →](https://github.com/MotiaDev/motia-examples/tree/main/examples/gmail-workflow) | [View Example →](https://github.com/MotiaDev/motia-examples/tree/main/examples/conversation-analyzer-vision) |

### 🌊 **Real-Time Streaming &amp; Chat**

| **Streaming AI Chatbot** | **Real-Time Chat App** | **Live Health Monitor** |
|--------------------------|------------------------|------------------------|
| Token-by-token AI responses with WebSocket streaming | Multi-user chat with real-time message processing and moderation | Production uptime monitoring with intelligent Discord alerts |
| [View Example →](https://github.com/MotiaDev/motia-examples/tree/main/examples/streaming-ai-chatbot) | [View Example →](https://github.com/MotiaDev/motia-examples/tree/main/examples/realtime-chat-application) | [View Example →](https://github.com/MotiaDev/motia-examples/tree/main/examples/realtime-uptime-monitor) |

### ⚡ **Parallel Processing &amp; Workflows** 

| **Parallel Execution Demo** | **Content Automation** | **Task Management** |
|-----------------------------|------------------------|-------------------|
| Concurrent task processing with workload distribution | Blog-to-Tweet automation with AI content optimization | Trello workflow automation with AI task validation |
| [View Example →](https://github.com/MotiaDev/motia-examples/tree/main/examples/motia-parallel-execution) | [View Example →](https://github.com/MotiaDev/motia-examples/tree/main/examples/blog-to-tweet-automation) | [View Example →](https://github.com/MotiaDev/motia-examples/tree/main/examples/trello-flow) |

### 🎯 **Key Features Demonstrated:**
- ✅ **Multi-Language Workflows** - JavaScript, TypeScript, Python working together
- ✅ **Real-Time Streaming** - WebSocket integration with live updates  
- ✅ **AI Integration** - OpenAI, Claude, vision models, and custom AI workflows
- ✅ **Event-Driven Architecture** - Complex workflows with automatic retry and fault tolerance
- ✅ **Production Monitoring** - Health checks, uptime monitoring, and intelligent alerting
- ✅ **Parallel Processing** - Concurrent execution and workload distribution
- ✅ **Enterprise Integration** - GitHub, Gmail, Trello, Discord, and social media APIs

**🚀 Each example includes:** Complete source code • Step-by-step tutorials • Production deployment guides • Docker configurations

---

## 🌐 Language Support

Write steps in your preferred language:

| Language       | Status         | Example           |
| -------------- | -------------- | ----------------- |
| **JavaScript** | ✅ Stable      | `handler.step.js` |
| **TypeScript** | ✅ Stable      | `handler.step.ts` |
| **Python**     | ✅ Stable      | `handler.step.py` |
| **Ruby**       | 🚧 Beta        | `handler.step.rb` |
| **Go**         | 🔄 Coming Soon | `handler.step.go` |
| **Rust**       | 🔄 Coming Soon | `handler.step.rs` |

---

### 💬 **Get Help**

- **📋 Questions**: Use our [Discord community](https://discord.gg/motia)
- **🐛 Bug Reports**: [GitHub Issues](https://github.com/MotiaDev/motia/issues)
- **📖 Documentation**: [Official Docs](https://motia.dev/docs)
- **📰 Blog**: [Motia Blog](https://blog.motia.dev)
- **🎥 Youtube**: [Motia Youtube](https://www.youtube.com/@motiadev)

### 🤝 **Contributing**

We welcome contributions! Whether it&#039;s:

- 🐛 Bug fixes and improvements
- ✨ New features and step types
- 📚 Documentation and examples
- 🌍 Language support additions
- 🎨 Workbench UI enhancements

Check out our [Contributing Guide](https://github.com/MotiaDev/motia/blob/main/CONTRIBUTING.md) to get started.

---

&lt;div align=&quot;center&quot;&gt;

**🌟 Ready to unify your backend?**

[🚀 **Get Started Now**](https://motia.dev) • [📖 **Read the Docs**](https://motia.dev/docs) • [💬 **Join Discord**](https://discord.com/invite/nJFfsH5d6v)

&lt;/div&gt;

---

&lt;div align=&quot;center&quot;&gt;

[![Star History Chart](https://api.star-history.com/svg?repos=motiadev/motia&amp;type=Date)](https://www.star-history.com/#motiadev/motia&amp;Date)

&lt;sub&gt;Built with ❤️ by the Motia team • **Star us if you find [Motia](https://github.com/orgs/MotiaDev/motia) useful!** ⭐&lt;/sub&gt;

&lt;/div&gt;

### 🚧 Roadmap

We have a public roadmap for Motia, you can view it [here](https://github.com/orgs/MotiaDev/projects/2/views/4).

Feel free to add comments to the issues, or create a new issue if you have a feature request.

| Feature                               | Status  | Link                                                 | Description                            |
| ------------------------------------- | ------- | ---------------------------------------------------- | -------------------------------------- |
| Python Types                          | Planned | [#485](https://github.com/MotiaDev/motia/issues/485) | Add support for Python types           |
| Streams: RBAC                         | Planned | [#495](https://github.com/MotiaDev/motia/issues/495) | Add support for RBAC                   |
| Streams: Workbench UI                 | Planned | [#497](https://github.com/MotiaDev/motia/issues/497) | Add support for Workbench UI           |
| Queue Strategies                      | Planned | [#476](https://github.com/MotiaDev/motia/issues/476) | Add support for Queue Strategies       |
| Reactive Steps                        | Planned | [#477](https://github.com/MotiaDev/motia/issues/477) | Add support for Reactive Steps         |
| Allow cloud configuration             | Planned | [#478](https://github.com/MotiaDev/motia/issues/478) | Add support for cloud configuration    |
| BYOC: Bring your own Cloud: AWS       | Planned | [#479](https://github.com/MotiaDev/motia/issues/479) | Add support for AWS                    |
| Point in time triggers                | Planned | [#480](https://github.com/MotiaDev/motia/issues/480) | Add support for Point in time triggers |
| Workbench plugins                     | Planned | [#481](https://github.com/MotiaDev/motia/issues/481) | 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[bytebot-ai/bytebot]]></title>
            <link>https://github.com/bytebot-ai/bytebot</link>
            <guid>https://github.com/bytebot-ai/bytebot</guid>
            <pubDate>Tue, 19 Aug 2025 00:04:41 GMT</pubDate>
            <description><![CDATA[Bytebot is a self-hosted AI desktop agent that automates computer tasks through natural language commands, operating within a containerized Linux desktop environment.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/bytebot-ai/bytebot">bytebot-ai/bytebot</a></h1>
            <p>Bytebot is a self-hosted AI desktop agent that automates computer tasks through natural language commands, operating within a containerized Linux desktop environment.</p>
            <p>Language: TypeScript</p>
            <p>Stars: 1,643</p>
            <p>Forks: 128</p>
            <p>Stars today: 196 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;img src=&quot;docs/images/bytebot-logo.png&quot; width=&quot;500&quot; alt=&quot;Bytebot Logo&quot;&gt;

# Bytebot: Open-Source AI Desktop Agent

**An AI that has its own computer to complete tasks for you**

[![Deploy on Railway](https://railway.com/button.svg)](https://railway.com/deploy/bytebot?referralCode=L9lKXQ)
[![Docker](https://img.shields.io/badge/docker-ready-blue.svg)](https://github.com/bytebot-ai/bytebot/tree/main/docker)
[![License](https://img.shields.io/badge/license-Apache%202.0-green.svg)](LICENSE)
[![Discord](https://img.shields.io/discord/1234567890?color=7289da&amp;label=discord)](https://discord.com/invite/d9ewZkWPTP)

[🌐 Website](https://bytebot.ai) • [📚 Documentation](https://docs.bytebot.ai) • [💬 Discord](https://discord.com/invite/d9ewZkWPTP) • [𝕏 Twitter](https://x.com/bytebot_ai)

&lt;/div&gt;

---

## What is a Desktop Agent?

A desktop agent is an AI that has its own computer. Unlike browser-only agents or traditional RPA tools, Bytebot comes with a full virtual desktop where it can:

- Use any application (browsers, email clients, office tools, IDEs)
- Download and organize files with its own file system
- Log into websites and applications using password managers
- Read and process documents, PDFs, and spreadsheets
- Complete complex multi-step workflows across different programs

Think of it as a virtual employee with their own computer who can see the screen, move the mouse, type on the keyboard, and complete tasks just like a human would.

## Why Give AI Its Own Computer?

When AI has access to a complete desktop environment, it unlocks capabilities that aren&#039;t possible with browser-only agents or API integrations:

### Complete Task Autonomy

Give Bytebot a task like &quot;Download all invoices from our vendor portals and organize them by date&quot; and it will:

- Open the browser
- Navigate to each portal
- Handle authentication (including 2FA via password managers)
- Download the files to its local file system
- Organize them into folders
- Generate reports or summaries as needed

### Process Any Document

Upload files directly to Bytebot&#039;s desktop and it can:

- Read entire PDFs into its context
- Extract data from complex documents
- Cross-reference information across multiple files
- Create new documents based on analysis
- Handle formats that APIs can&#039;t access

### Use Real Applications

Bytebot isn&#039;t limited to web interfaces. It can:

- Use desktop applications like text editors, VS Code, or email clients
- Run scripts and command-line tools
- Install new software as needed
- Configure applications for specific workflows

## Quick Start

### Deploy in 2 Minutes

**Option 1: Railway (Easiest)**
[![Deploy on Railway](https://railway.com/button.svg)](https://railway.com/deploy/bytebot?referralCode=L9lKXQ)

Just click and add your AI provider API key.

**Option 2: Docker Compose**

```bash
git clone https://github.com/bytebot-ai/bytebot.git
cd bytebot

# Add your AI provider key (choose one)
echo &quot;ANTHROPIC_API_KEY=sk-ant-...&quot; &gt; docker/.env
# Or: echo &quot;OPENAI_API_KEY=sk-...&quot; &gt; docker/.env
# Or: echo &quot;GEMINI_API_KEY=...&quot; &gt; docker/.env

docker-compose -f docker/docker-compose.yml up -d

# Open http://localhost:9992
```

[Full deployment guide →](https://docs.bytebot.ai/quickstart)

## How It Works

Bytebot consists of four integrated components:

1. **Virtual Desktop**: A complete Ubuntu Linux environment with pre-installed applications
2. **AI Agent**: Understands your tasks and controls the desktop to complete them
3. **Task Interface**: Web UI where you create tasks and watch Bytebot work
4. **APIs**: REST endpoints for programmatic task creation and desktop control

### Key Features

- **Natural Language Tasks**: Just describe what you need done
- **File Uploads**: Drop files onto tasks for Bytebot to process
- **Live Desktop View**: Watch Bytebot work in real-time
- **Takeover Mode**: Take control when you need to help or configure something
- **Password Manager Support**: Install 1Password, Bitwarden, etc. for automatic authentication
- **Persistent Environment**: Install programs and they stay available for future tasks

## Example Tasks

### Basic Examples

```
&quot;Go to Wikipedia and create a summary of quantum computing&quot;
&quot;Research flights from NYC to London and create a comparison document&quot;
&quot;Take screenshots of the top 5 news websites&quot;
```

### Document Processing

```
&quot;Read the uploaded contracts.pdf and extract all payment terms and deadlines&quot;
&quot;Process these 50 invoice PDFs and create a summary report&quot;
&quot;Analyze this financial report and answer: What were the key risks mentioned?&quot;
```

### Multi-Application Workflows

```
&quot;Download last month&#039;s bank statements from our three banks and consolidate them&quot;
&quot;Check all our vendor portals for new invoices and create a summary report&quot;
&quot;Log into our CRM, export the customer list, and update records in the ERP system&quot;
```

## Programmatic Control

### Create Tasks via API

```python
import requests

# Simple task
response = requests.post(&#039;http://localhost:9991/tasks&#039;, json={
    &#039;description&#039;: &#039;Download the latest sales report and create a summary&#039;
})

# Task with file upload
files = {&#039;files&#039;: open(&#039;contracts.pdf&#039;, &#039;rb&#039;)}
response = requests.post(&#039;http://localhost:9991/tasks&#039;,
    data={&#039;description&#039;: &#039;Review these contracts for important dates&#039;},
    files=files
)
```

### Direct Desktop Control

```bash
# Take a screenshot
curl -X POST http://localhost:9990/computer-use \
  -H &quot;Content-Type: application/json&quot; \
  -d &#039;{&quot;action&quot;: &quot;screenshot&quot;}&#039;

# Click at specific coordinates
curl -X POST http://localhost:9990/computer-use \
  -H &quot;Content-Type: application/json&quot; \
  -d &#039;{&quot;action&quot;: &quot;click_mouse&quot;, &quot;coordinate&quot;: [500, 300]}&#039;
```

[Full API documentation →](https://docs.bytebot.ai/api-reference/introduction)

## Setting Up Your Desktop Agent

### 1. Deploy Bytebot

Use one of the deployment methods above to get Bytebot running.

### 2. Configure the Desktop

Use the Desktop tab in the UI to:

- Install additional programs you need
- Set up password managers for authentication
- Configure applications with your preferences
- Log into websites you want Bytebot to access

### 3. Start Giving Tasks

Create tasks in natural language and watch Bytebot complete them using the configured desktop.

## Use Cases

### Business Process Automation

- Invoice processing and data extraction
- Multi-system data synchronization
- Report generation from multiple sources
- Compliance checking across platforms

### Development &amp; Testing

- Automated UI testing
- Cross-browser compatibility checks
- Documentation generation with screenshots
- Code deployment verification

### Research &amp; Analysis

- Competitive analysis across websites
- Data gathering from multiple sources
- Document analysis and summarization
- Market research compilation

## Architecture

Bytebot is built with:

- **Desktop**: Ubuntu 22.04 with XFCE, Firefox, VS Code, and other tools
- **Agent**: NestJS service that coordinates AI and desktop actions
- **UI**: Next.js application for task management
- **AI Support**: Works with Anthropic Claude, OpenAI GPT, Google Gemini
- **Deployment**: Docker containers for easy self-hosting

## Why Self-Host?

- **Data Privacy**: Everything runs on your infrastructure
- **Full Control**: Customize the desktop environment as needed
- **No Limits**: Use your own AI API keys without platform restrictions
- **Flexibility**: Install any software, access any systems

## Advanced Features

### Multiple AI Providers

Use any AI provider through our [LiteLLM integration](https://docs.bytebot.ai/deployment/litellm):

- Azure OpenAI
- AWS Bedrock
- Local models via Ollama
- 100+ other providers

### Enterprise Deployment

Deploy on Kubernetes with Helm:

```bash
# Clone the repository
git clone https://github.com/bytebot-ai/bytebot.git
cd bytebot

# Install with Helm
helm install bytebot ./helm \
  --set agent.env.ANTHROPIC_API_KEY=sk-ant-...
```

[Enterprise deployment guide →](https://docs.bytebot.ai/deployment/helm)

## Community &amp; Support

- **Discord**: [Join our community](https://discord.com/invite/d9ewZkWPTP) for help and discussions
- **Documentation**: Comprehensive guides at [docs.bytebot.ai](https://docs.bytebot.ai)
- **GitHub Issues**: Report bugs and request features

## Contributing

We welcome contributions! Whether it&#039;s:

- 🐛 Bug fixes
- ✨ New features
- 📚 Documentation improvements
- 🌐 Translations

Please:

1. Check existing [issues](https://github.com/bytebot-ai/bytebot/issues) first
2. Open an issue to discuss major changes
3. Submit PRs with clear descriptions
4. Join our [Discord](https://discord.com/invite/d9ewZkWPTP) to discuss ideas

## License

Bytebot is open source under the Apache 2.0 license.

---

&lt;div align=&quot;center&quot;&gt;

**Give your AI its own computer. See what it can do.**

[![Deploy on Railway](https://railway.com/button.svg)](https://railway.com/deploy/bytebot?referralCode=L9lKXQ)

&lt;sub&gt;Built by [Tantl Labs](https://tantl.com) and the open source community&lt;/sub&gt;

&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[mfts/papermark]]></title>
            <link>https://github.com/mfts/papermark</link>
            <guid>https://github.com/mfts/papermark</guid>
            <pubDate>Tue, 19 Aug 2025 00:04:40 GMT</pubDate>
            <description><![CDATA[Papermark is the open-source DocSend alternative with built-in analytics and custom domains.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mfts/papermark">mfts/papermark</a></h1>
            <p>Papermark is the open-source DocSend alternative with built-in analytics and custom domains.</p>
            <p>Language: TypeScript</p>
            <p>Stars: 7,190</p>
            <p>Forks: 987</p>
            <p>Stars today: 77 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;h1 align=&quot;center&quot;&gt;Papermark&lt;/h1&gt;
  &lt;h3&gt;The open-source DocSend alternative.&lt;/h3&gt;

&lt;a target=&quot;_blank&quot; href=&quot;https://www.producthunt.com/posts/papermark-3?utm_source=badge-top-post-badge&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-papermark&quot;&gt;&lt;img src=&quot;https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=411605&amp;amp;theme=light&amp;amp;period=daily&quot; alt=&quot;Papermark - The open-source DocSend alternative | Product Hunt&quot; style=&quot;width:250px;height:40px&quot;&gt;&lt;/a&gt;

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.papermark.com&quot;&gt;papermark.com&lt;/a&gt;
&lt;/div&gt;

&lt;br/&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/mfts/papermark/stargazers&quot;&gt;&lt;img alt=&quot;GitHub Repo stars&quot; src=&quot;https://img.shields.io/github/stars/mfts/papermark&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/papermarkio&quot;&gt;&lt;img alt=&quot;Twitter Follow&quot; src=&quot;https://img.shields.io/twitter/follow/papermarkio&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/mfts/papermark/blob/main/LICENSE&quot;&gt;&lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/badge/license-AGPLv3-purple&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;br/&gt;

Papermark is the open-source document-sharing alternative to DocSend, featuring built-in analytics and custom domains.

## Features

- **Shareable Links:** Share your documents securely by sending a custom link.
- **Custom Branding:** Add a custom domain and your own branding.
- **Analytics:** Gain insights through document tracking and soon page-by-page analytics.
- **Self-hosted, Open-source:** Host it yourself and customize it as needed.

## Demo

![Papermark Welcome GIF](.github/images/papermark-welcome.gif)

## Tech Stack

- [Next.js](https://nextjs.org/) – Framework
- [TypeScript](https://www.typescriptlang.org/) – Language
- [Tailwind](https://tailwindcss.com/) – CSS
- [shadcn/ui](https://ui.shadcn.com) - UI Components
- [Prisma](https://prisma.io) - ORM [![Made with Prisma](https://made-with.prisma.io/dark.svg)](https://prisma.io)
- [PostgreSQL](https://www.postgresql.org/) - Database
- [NextAuth.js](https://next-auth.js.org/) – Authentication
- [Tinybird](https://tinybird.co) – Analytics
- [Resend](https://resend.com) – Email
- [Stripe](https://stripe.com) – Payments
- [Vercel](https://vercel.com/) – Hosting

## Getting Started

### Prerequisites

Here&#039;s what you need to run Papermark:

- Node.js (version &gt;= 18.17.0)
- PostgreSQL Database
- Blob storage (currently [AWS S3](https://aws.amazon.com/s3/) or [Vercel Blob](https://vercel.com/storage/blob))
- [Resend](https://resend.com) (for sending emails)

### 1. Clone the repository

```shell
git clone https://github.com/mfts/papermark.git
cd papermark
```

### 2. Install npm dependencies

```shell
npm install
```

### 3. Copy the environment variables to `.env` and change the values

```shell
cp .env.example .env
```

### 4. Initialize the database

```shell
npm run dev:prisma
```

### 5. Run the dev server

```shell
npm run dev
```

### 6. Open the app in your browser

Visit [http://localhost:3000](http://localhost:3000) in your browser.

## Tinybird Instructions

To prepare the Tinybird database, follow these steps:

0. We use `pipenv` to manage our Python dependencies. If you don&#039;t have it installed, you can install it using the following command:
   ```sh
   pkgx pipenv
   ```
1. Download the Tinybird CLI from [here](https://www.tinybird.co/docs/cli.html) and install it on your system.
2. After authenticating with the Tinybird CLI, navigate to the `lib/tinybird` directory:
   ```sh
   cd lib/tinybird
   ```
3. Push the necessary data sources using the following command:
   ```sh
   tb push datasources/*
   tb push endpoints/get_*
   ```
4. Don&#039;t forget to set the `TINYBIRD_TOKEN` with the appropriate rights in your `.env` file.

#### Updating Tinybird

```sh
pipenv shell
## start: pkgx-specific
cd ..
cd papermark
## end: pkgx-specific
pipenv update tinybird-cli
```

## Contributing

Papermark is an open-source project, and we welcome contributions from the community.

If you&#039;d like to contribute, please fork the repository and make any changes you&#039;d like. Pull requests are warmly welcome.

### Our Contributors ✨

&lt;a href=&quot;https://github.com/mfts/papermark/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=mfts/papermark&quot; /&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[microsoft/data-formulator]]></title>
            <link>https://github.com/microsoft/data-formulator</link>
            <guid>https://github.com/microsoft/data-formulator</guid>
            <pubDate>Tue, 19 Aug 2025 00:04:39 GMT</pubDate>
            <description><![CDATA[🪄 Create rich visualizations with AI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/data-formulator">microsoft/data-formulator</a></h1>
            <p>🪄 Create rich visualizations with AI</p>
            <p>Language: TypeScript</p>
            <p>Stars: 13,214</p>
            <p>Forks: 1,122</p>
            <p>Stars today: 102 stars today</p>
            <h2>README</h2><pre>&lt;h1&gt;
    &lt;img src=&quot;./public/favicon.ico&quot; alt=&quot;Data Formulator icon&quot; width=&quot;28&quot;&gt; &lt;b&gt;Data Formulator: Create Rich Visualizations with AI&lt;/b&gt;
&lt;/h1&gt;

&lt;div&gt;
    
[![arxiv](https://img.shields.io/badge/Paper-arXiv:2408.16119-b31b1b.svg)](https://arxiv.org/abs/2408.16119)&amp;ensp;
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)&amp;ensp;
[![YouTube](https://img.shields.io/badge/YouTube-white?logo=youtube&amp;logoColor=%23FF0000)](https://youtu.be/3ndlwt0Wi3c)&amp;ensp;
[![build](https://github.com/microsoft/data-formulator/actions/workflows/python-build.yml/badge.svg)](https://github.com/microsoft/data-formulator/actions/workflows/python-build.yml)
[![Discord](https://img.shields.io/badge/discord-chat-green?logo=discord)](https://discord.gg/mYCZMQKYZb)

&lt;/div&gt;

Transform data and create rich visualizations iteratively with AI 🪄. Try Data Formulator now!

Any questions? Ask on the Discord channel! [![Discord](https://img.shields.io/badge/discord-chat-green?logo=discord)](https://discord.gg/mYCZMQKYZb)

&lt;!-- [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/data-formulator?quickstart=1) --&gt;

&lt;kbd&gt;
  &lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://codespaces.new/microsoft/data-formulator?quickstart=1&quot; title=&quot;open Data Formulator in GitHub Codespaces&quot;&gt;&lt;img src=&quot;public/data-formulator-screenshot.png&quot;&gt;&lt;/a&gt;
&lt;/kbd&gt;



## News 🔥🔥🔥
- [07-10-2025] Data Formulator 0.2.2: Start with an analysis goal
  - Some key frontend performance updates. 
  - You can start your exploration with a goal, or, tab and see if the agent can recommend some good exploration ideas for you. [Demo](https://github.com/microsoft/data-formulator/pull/176)

- [05-13-2025] Data Formulator 0.2.1.3/4: External Data Loader 
  - We introduced external data loader class to make import data easier. [Readme](https://github.com/microsoft/data-formulator/tree/main/py-src/data_formulator/data_loader) and [Demo](https://github.com/microsoft/data-formulator/pull/155)
    - Current data loaders: MySQL, Azure Data Explorer (Kusto), Azure Blob and Amazon S3 (json, parquet, csv).
    - [07-01-2025] Updated with: Postgresql, mssql.
  - Call for action [link](https://github.com/microsoft/data-formulator/issues/156):
    - Users: let us know which data source you&#039;d like to load data from.
    - Developers: let&#039;s build more data loaders.

- [04-23-2025] Data Formulator 0.2: working with *large* data 📦📦📦
  - Explore large data by:
    1. Upload large data file to the local database (powered by [DuckDB](https://github.com/duckdb/duckdb)).
    2. Use drag-and-drop to specify charts, and Data Formulator dynamically fetches data from the database to create visualizations (with ⚡️⚡️⚡️ speeds).
    3. Work with AI agents: they generate SQL queries to transform the data to create rich visualizations!
    4. Anchor the result / follow up / create a new branch / join tables; let&#039;s dive deeper. 
  - Checkout the demos at [[https://github.com/microsoft/data-formulator/releases/tag/0.2]](https://github.com/microsoft/data-formulator/releases/tag/0.2)
  - Improved overall system performance, and enjoy the updated derive concept functionality.

- [03-20-2025] Data Formulator 0.1.7: Anchoring ⚓︎
  - Anchor an intermediate dataset, so that followup data analysis are built on top of the anchored data, not the original one.
  - Clean a data and work with only the cleaned data; create a subset from the original data or join multiple data, and then go from there. AI agents will be less likely to get confused and work faster. ⚡️⚡️
  - Check out the demos at [[https://github.com/microsoft/data-formulator/releases/tag/0.1.7]](https://github.com/microsoft/data-formulator/releases/tag/0.1.7)
  - Don&#039;t forget to update Data Formulator to test it out!

- [02-20-2025] Data Formulator 0.1.6 released! 
  - Now supports working with multiple datasets at once! Tell Data Formulator which data tables you would like to use in the encoding shelf, and it will figure out how to join the tables to create a visualization to answer your question. 🪄
  - Checkout the demo at [[https://github.com/microsoft/data-formulator/releases/tag/0.1.6]](https://github.com/microsoft/data-formulator/releases/tag/0.1.6).
  - Update your Data Formulator to the latest version to play with the new features.

- [02-12-2025] More models supported now!
  - Now supports OpenAI, Azure, Ollama, and Anthropic models (and more powered by [LiteLLM](https://github.com/BerriAI/litellm));
  - Models with strong code generation and instruction following capabilities are recommended (gpt-4o, claude-3-5-sonnet etc.);
  - You can store API keys in `api-keys.env` to avoid typing them every time (see template `api-keys.env.template`).
  - Let us know which models you have good/bad experiences with, and what models you would like to see supported! [[comment here]](https://github.com/microsoft/data-formulator/issues/49)

- [11-07-2024] Minor fun update: data visualization challenges!
  - We added a few visualization challenges with the sample datasets. Can you complete them all? [[try them out!]](https://github.com/microsoft/data-formulator/issues/53#issue-2641841252)
  - Comment in the issue when you did, or share your results/questions with others! [[comment here]](https://github.com/microsoft/data-formulator/issues/53)

- [10-11-2024] Data Formulator python package released! 
  - You can now install Data Formulator using Python and run it locally, easily. [[check it out]](#get-started).
  - Our Codespaces configuration is also updated for fast start up ⚡️. [[try it now!]](https://codespaces.new/microsoft/data-formulator?quickstart=1)
  - New experimental feature: load an image or a messy text, and ask AI to parse and clean it for you(!). [[demo]](https://github.com/microsoft/data-formulator/pull/31#issuecomment-2403652717)
  
- [10-01-2024] Initial release of Data Formulator, check out our [[blog]](https://www.microsoft.com/en-us/research/blog/data-formulator-exploring-how-ai-can-help-analysts-create-rich-data-visualizations/) and [[video]](https://youtu.be/3ndlwt0Wi3c)!

## Overview

**Data Formulator** is an application from Microsoft Research that uses large language models to transform data, expediting the practice of data visualization.

Data Formulator is an AI-powered tool for analysts to iteratively create rich visualizations. Unlike most chat-based AI tools where users need to describe everything in natural language, Data Formulator combines *user interface interactions (UI)* and *natural language (NL) inputs* for easier interaction. This blended approach makes it easier for users to describe their chart designs while delegating data transformation to AI. 

## Get Started

Play with Data Formulator with one of the following options:

- **Option 1: Install via Python PIP**
  
  Use Python PIP for an easy setup experience, running locally (recommend: install it in a virtual environment).
  
  ```bash
  # install data_formulator
  pip install data_formulator

  # start data_formulator
  data_formulator 
  
  # alternatively, you can run data formulator with this command
  python -m data_formulator
  ```

  Data Formulator will be automatically opened in the browser at [http://localhost:5000](http://localhost:5000).

  *Update: you can specify the port number (e.g., 8080) by `python -m data_formulator --port 8080` if the default port is occupied.*

- **Option 2: Codespaces (5 minutes)**
  
  You can also run Data Formulator in Codespaces; we have everything pre-configured. For more details, see [CODESPACES.md](CODESPACES.md).
  
  [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/data-formulator?quickstart=1)

- **Option 3: Working in the developer mode**
  
  You can build Data Formulator locally if you prefer full control over your development environment and the ability to customize the setup to your specific needs. For detailed instructions, refer to [DEVELOPMENT.md](DEVELOPMENT.md).


## Using Data Formulator

Once you&#039;ve completed the setup using either option, follow these steps to start using Data Formulator:

### The basics of data visualization
* Provide OpenAI keys and select a model (GPT-4o suggested) and choose a dataset.
* Choose a chart type, and then drag-and-drop data fields to chart properties (x, y, color, ...) to specify visual encodings.

https://github.com/user-attachments/assets/0fbea012-1d2d-46c3-a923-b1fc5eb5e5b8


### Create visualization beyond the initial dataset (powered by 🤖)
* You can type names of **fields that do not exist in current data** in the encoding shelf:
    - this tells Data Formulator that you want to create visualizations that require computation or transformation from existing data,
    - you can optionally provide a natural language prompt to explain and clarify your intent (not necessary when field names are self-explanatory).
* Click the **Formulate** button.
    - Data Formulator will transform data and instantiate the visualization based on the encoding and prompt.
* Inspect the data, chart and code.
* To create a new chart based on existing ones, follow up in natural language:
    - provide a follow up prompt (e.g., *``show only top 5!&#039;&#039;*),
    - you may also update visual encodings for the new chart.

https://github.com/user-attachments/assets/160c69d2-f42d-435c-9ff3-b1229b5bddba

https://github.com/user-attachments/assets/c93b3e84-8ca8-49ae-80ea-f91ceef34acb

Repeat this process as needed to explore and understand your data. Your explorations are trackable in the **Data Threads** panel. 

## Developers&#039; Guide

Follow the [developers&#039; instructions](DEVELOPMENT.md) to build your new data analysis tools on top of Data Formulator.

## Research Papers
* [Data Formulator 2: Iteratively Creating Rich Visualizations with AI](https://arxiv.org/abs/2408.16119)

```
@article{wang2024dataformulator2iteratively,
      title={Data Formulator 2: Iteratively Creating Rich Visualizations with AI}, 
      author={Chenglong Wang and Bongshin Lee and Steven Drucker and Dan Marshall and Jianfeng Gao},
      year={2024},
      booktitle={ArXiv preprint arXiv:2408.16119},
}
```

* [Data Formulator: AI-powered Concept-driven Visualization Authoring](https://arxiv.org/abs/2309.10094)

```
@article{wang2023data,
  title={Data Formulator: AI-powered Concept-driven Visualization Authoring},
  author={Wang, Chenglong and Thompson, John and Lee, Bongshin},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  year={2023},
  publisher={IEEE}
}
```


## Contributing

This project welcomes contributions and suggestions. Most contributions require you to
agree to a Contributor License Agreement (CLA) declaring that you have the right to,
and actually do, grant us the rights to use your contribution. For details, visit
https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need
to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the
instructions provided by the bot. You will only need to do this once across all repositories using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)
or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft 
trademarks or logos is subject to and must follow 
[Microsoft&#039;s Trademark &amp; Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party&#039;s policies.
</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[bytedance/UI-TARS-desktop]]></title>
            <link>https://github.com/bytedance/UI-TARS-desktop</link>
            <guid>https://github.com/bytedance/UI-TARS-desktop</guid>
            <pubDate>Tue, 19 Aug 2025 00:04:38 GMT</pubDate>
            <description><![CDATA[The Open-sourced Multimodal AI Agent Stack connecting Cutting-edge AI Models and Agent Infra.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/bytedance/UI-TARS-desktop">bytedance/UI-TARS-desktop</a></h1>
            <p>The Open-sourced Multimodal AI Agent Stack connecting Cutting-edge AI Models and Agent Infra.</p>
            <p>Language: TypeScript</p>
            <p>Stars: 17,439</p>
            <p>Forks: 1,584</p>
            <p>Stars today: 184 stars today</p>
            <h2>README</h2><pre>&lt;picture&gt;
  &lt;img alt=&quot;Agent TARS Banner&quot; src=&quot;./images/tars.png&quot;&gt;
&lt;/picture&gt;

&lt;br/&gt;

## Introduction

English | [简体中文](./README.zh-CN.md)

[![](https://trendshift.io/api/badge/repositories/13584)](https://trendshift.io/repositories/13584)

&lt;b&gt;TARS&lt;sup&gt;\*&lt;/sup&gt;&lt;/b&gt; is a Multimodal AI Agent stack, currently shipping two projects: [Agent TARS](#agent-tars) and [UI-TARS-desktop](#ui-tars-desktop):

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th width=&quot;50%&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;#agent-tars&quot;&gt;Agent TARS&lt;/a&gt;&lt;/th&gt;
      &lt;th width=&quot;50%&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;#ui-tars-desktop&quot;&gt;UI-TARS-desktop&lt;/a&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;
        &lt;video src=&quot;https://github.com/user-attachments/assets/c9489936-afdc-4d12-adda-d4b90d2a869d&quot; width=&quot;50%&quot;&gt;&lt;/video&gt;
      &lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;
        &lt;video src=&quot;https://github.com/user-attachments/assets/e0914ce9-ad33-494b-bdec-0c25c1b01a27&quot; width=&quot;50%&quot;&gt;&lt;/video&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;left&quot;&gt;
        &lt;b&gt;Agent TARS&lt;/b&gt; is a general multimodal AI Agent stack, it brings the power of GUI Agent and Vision into your terminal, computer, browser and product.
        &lt;br&gt;
        &lt;br&gt;
        It primarily ships with a &lt;a href=&quot;https://agent-tars.com/guide/basic/cli.html&quot; target=&quot;_blank&quot;&gt;CLI&lt;/a&gt; and &lt;a href=&quot;https://agent-tars.com/guide/basic/web-ui.html&quot; target=&quot;_blank&quot;&gt;Web UI&lt;/a&gt; for usage.
        It aims to provide a workflow that is closer to human-like task completion through cutting-edge multimodal LLMs and seamless integration with various real-world &lt;a href=&quot;https://agent-tars.com/guide/basic/mcp.html&quot; target=&quot;_blank&quot;&gt;MCP&lt;/a&gt; tools.
      &lt;/td&gt;
      &lt;td align=&quot;left&quot;&gt;
        &lt;b&gt;UI-TARS Desktop&lt;/b&gt; is a desktop application that provides a native GUI Agent based on the &lt;a href=&quot;https://github.com/bytedance/UI-TARS&quot; target=&quot;_blank&quot;&gt;UI-TARS&lt;/a&gt; model.
        &lt;br&gt;
        &lt;br&gt;
        It primarily ships a
        &lt;a href=&quot;https://github.com/bytedance/UI-TARS-desktop/blob/main/docs/quick-start.md#get-model-and-run-local-operator&quot; target=&quot;_blank&quot;&gt;local&lt;/a&gt; and 
        &lt;a href=&quot;https://github.com/bytedance/UI-TARS-desktop/blob/main/docs/quick-start.md#run-remote-operator&quot; target=&quot;_blank&quot;&gt;remote&lt;/a&gt; computer as well as browser operators.
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

## Table of Contents

&lt;!-- START doctoc generated TOC please keep comment here to allow auto update --&gt;
&lt;!-- DON&#039;T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE --&gt;

- [News](#news)
- [Agent TARS](#agent-tars)
  - [Showcase](#showcase)
  - [Core Features](#core-features)
  - [Quick Start](#quick-start)
  - [Documentation](#documentation)
- [UI-TARS Desktop](#ui-tars-desktop)
  - [Showcase](#showcase-1)
  - [Features](#features)
  - [Quick Start](#quick-start-1)
- [Contributing](#contributing)
- [License](#license)
- [Citation](#citation)

&lt;!-- END doctoc generated TOC please keep comment here to allow auto update --&gt;

## News

- **\[2025-06-25\]** We released a Agent TARS Beta and Agent TARS CLI - [Introducing Agent TARS Beta](https://agent-tars.com/blog/2025-06-25-introducing-agent-tars-beta.html), a multimodal AI agent that aims to explore a work form that is closer to human-like task completion through rich multimodal capabilities (such as GUI Agent, Vision) and seamless integration with various real-world tools.
- **\[2025-06-12\]** - 🎁 We are thrilled to announce the release of UI-TARS Desktop v0.2.0! This update introduces two powerful new features: **Remote Computer Operator** and **Remote Browser Operator**—both completely free. No configuration required: simply click to remotely control any computer or browser, and experience a new level of convenience and intelligence.
- **\[2025-04-17\]** - 🎉 We&#039;re thrilled to announce the release of new UI-TARS Desktop application v0.1.0, featuring a redesigned Agent UI. The application enhances the computer using experience, introduces new browser operation features, and supports [the advanced UI-TARS-1.5 model](https://seed-tars.com/1.5) for improved performance and precise control.
- **\[2025-02-20\]** - 📦 Introduced [UI TARS SDK](./docs/sdk.md), is a powerful cross-platform toolkit for building GUI automation agents.
- **\[2025-01-23\]** - 🚀 We updated the **[Cloud Deployment](./docs/deployment.md#cloud-deployment)** section in the 中文版: [GUI模型部署教程](https://bytedance.sg.larkoffice.com/docx/TCcudYwyIox5vyxiSDLlgIsTgWf#U94rdCxzBoJMLex38NPlHL21gNb) with new information related to the ModelScope platform. You can now use the ModelScope platform for deployment.

&lt;br&gt;

## Agent TARS

&lt;p&gt;
    &lt;a href=&quot;https://npmjs.com/package/@agent-tars/cli?activeTab=readme&quot;&gt;&lt;img src=&quot;https://img.shields.io/npm/v/@agent-tars/cli?style=for-the-badge&amp;colorA=1a1a2e&amp;colorB=3B82F6&amp;logo=npm&amp;logoColor=white&quot; alt=&quot;npm version&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;https://npmcharts.com/compare/@agent-tars/cli?minimal=true&quot;&gt;&lt;img src=&quot;https://img.shields.io/npm/dm/@agent-tars/cli.svg?style=for-the-badge&amp;colorA=1a1a2e&amp;colorB=0EA5E9&amp;logo=npm&amp;logoColor=white&quot; alt=&quot;downloads&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;https://nodejs.org/en/about/previous-releases&quot;&gt;&lt;img src=&quot;https://img.shields.io/node/v/@agent-tars/cli.svg?style=for-the-badge&amp;colorA=1a1a2e&amp;colorB=06B6D4&amp;logo=node.js&amp;logoColor=white&quot; alt=&quot;node version&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://discord.gg/HnKcSBgTVx&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20Community-5865F2?style=for-the-badge&amp;logo=discord&amp;logoColor=white&quot; alt=&quot;Discord Community&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;https://twitter.com/agent_tars&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Twitter-Follow%20%40agent__tars-1DA1F2?style=for-the-badge&amp;logo=twitter&amp;logoColor=white&quot; alt=&quot;Official Twitter&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;https://applink.larkoffice.com/client/chat/chatter/add_by_link?link_token=279h3365-b0fa-407f-89f3-0f96f36cd4d8&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/飞书群-加入交流群-00D4AA?style=for-the-badge&amp;logo=lark&amp;logoColor=white&quot; alt=&quot;飞书交流群&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;https://deepwiki.com/bytedance/UI-TARS-desktop&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/DeepWiki-Ask%20AI-8B5CF6?style=for-the-badge&amp;logo=gitbook&amp;logoColor=white&quot; alt=&quot;Ask DeepWiki&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;b&gt;Agent TARS&lt;/b&gt; is a general multimodal AI Agent stack, it brings the power of GUI Agent and Vision into your terminal, computer, browser and product. &lt;br&gt; &lt;br&gt;
It primarily ships with a &lt;a href=&quot;https://agent-tars.com/guide/basic/cli.html&quot; target=&quot;_blank&quot;&gt;CLI&lt;/a&gt; and &lt;a href=&quot;https://agent-tars.com/guide/basic/web-ui.html&quot; target=&quot;_blank&quot;&gt;Web UI&lt;/a&gt; for usage.
It aims to provide a workflow that is closer to human-like task completion through cutting-edge multimodal LLMs and seamless integration with various real-world &lt;a href=&quot;https://agent-tars.com/guide/basic/mcp.html&quot; target=&quot;_blank&quot;&gt;MCP&lt;/a&gt; tools.


### Showcase

```
Please help me book the earliest flight from San Jose to New York on September 1st and the last return flight on September 6th on Priceline
```

https://github.com/user-attachments/assets/772b0eef-aef7-4ab9-8cb0-9611820539d8

&lt;br&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th width=&quot;50%&quot; align=&quot;center&quot;&gt;Booking Hotel&lt;/th&gt;
      &lt;th width=&quot;50%&quot; align=&quot;center&quot;&gt;Generate Chart with extra MCP Servers&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;
        &lt;video src=&quot;https://github.com/user-attachments/assets/c9489936-afdc-4d12-adda-d4b90d2a869d&quot; width=&quot;50%&quot;&gt;&lt;/video&gt;
      &lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;
        &lt;video src=&quot;https://github.com/user-attachments/assets/a9fd72d0-01bb-4233-aa27-ca95194bbce9&quot; width=&quot;50%&quot;&gt;&lt;/video&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;left&quot;&gt;
        &lt;b&gt;Instruction:&lt;/b&gt; &lt;i&gt;I am in Los Angeles from September 1st to September 6th, with a budget of $5,000. Please help me book a Ritz-Carlton hotel closest to the airport on booking.com and compile a transportation guide for me&lt;/i&gt;
      &lt;/td&gt;
      &lt;td align=&quot;left&quot;&gt;
        &lt;b&gt;Instruction:&lt;/b&gt; &lt;i&gt;Draw me a chart of Hangzhou&#039;s weather for one month&lt;/i&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

For more use cases, please check out [#842](https://github.com/bytedance/UI-TARS-desktop/issues/842).

### Core Features

- 🖱️ **One-Click Out-of-the-box CLI** - Supports both **headful** [Web UI](https://agent-tars.com/guide/basic/web-ui.html) and **headless** [server](https://agent-tars.com/guide/advanced/server.html)) [execution](https://agent-tars.com/guide/basic/cli.html).
- 🌐 **Hybrid Browser Agent** - Control browsers using [GUI Agent](https://agent-tars.com/guide/basic/browser.html#visual-grounding), [DOM](https://agent-tars.com/guide/basic/browser.html#dom), or a hybrid strategy.
- 🔄 **Event Stream** - Protocol-driven Event Stream drives [Context Engineering](https://agent-tars.com/beta#context-engineering) and [Agent UI](https://agent-tars.com/blog/2025-06-25-introducing-agent-tars-beta.html#easy-to-build-applications).
- 🧰 **MCP Integration** - The kernel is built on MCP and also supports mounting [MCP Servers](https://agent-tars.com/guide/basic/mcp.html) to connect to real-world tools.

### Quick Start

&lt;img alt=&quot;Agent TARS CLI&quot; src=&quot;https://agent-tars.com/agent-tars-cli.png&quot;&gt;

```bash
# Luanch with `npx`.
npx @agent-tars/cli@latest

# Install globally, required Node.js &gt;= 22
npm install @agent-tars/cli@latest -g

# Run with your preferred model provider
agent-tars --provider volcengine --model doubao-1-5-thinking-vision-pro-250428 --apiKey your-api-key
agent-tars --provider anthropic --model claude-3-7-sonnet-latest --apiKey your-api-key
```

Visit the comprehensive [Quick Start](https://agent-tars.com/guide/get-started/quick-start.html) guide for detailed setup instructions.

### Documentation

&gt; 🌟 **Explore Agent TARS Universe** 🌟

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th width=&quot;20%&quot; align=&quot;center&quot;&gt;Category&lt;/th&gt;
      &lt;th width=&quot;30%&quot; align=&quot;center&quot;&gt;Resource Link&lt;/th&gt;
      &lt;th width=&quot;50%&quot; align=&quot;left&quot;&gt;Description&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;🏠 &lt;strong&gt;Central Hub&lt;/strong&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://agent-tars.com&quot;&gt;
          &lt;img src=&quot;https://img.shields.io/badge/Visit-Website-4F46E5?style=for-the-badge&amp;logo=globe&amp;logoColor=white&quot; alt=&quot;Website&quot; /&gt;
        &lt;/a&gt;
      &lt;/td&gt;
      &lt;td align=&quot;left&quot;&gt;Your gateway to Agent TARS ecosystem&lt;/td&gt;
    &lt;/tr&gt;
      &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;📚 &lt;strong&gt;Quick Start&lt;/strong&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://agent-tars.com/guide/get-started/quick-start.html&quot;&gt;
          &lt;img src=&quot;https://img.shields.io/badge/Get-Started-06B6D4?style=for-the-badge&amp;logo=rocket&amp;logoColor=white&quot; alt=&quot;Quick Start&quot; /&gt;
        &lt;/a&gt;
      &lt;/td&gt;
      &lt;td align=&quot;left&quot;&gt;Zero to hero in 5 minutes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;🚀 &lt;strong&gt;What&#039;s New&lt;/strong&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://agent-tars.com/beta&quot;&gt;
          &lt;img src=&quot;https://img.shields.io/badge/Read-Blog-F59E0B?style=for-the-badge&amp;logo=rss&amp;logoColor=white&quot; alt=&quot;Blog&quot; /&gt;
        &lt;/a&gt;
      &lt;/td&gt;
      &lt;td align=&quot;left&quot;&gt;Discover cutting-edge features &amp; vision&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;🛠️ &lt;strong&gt;Developer Zone&lt;/strong&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://agent-tars.com/guide/get-started/introduction.html&quot;&gt;
          &lt;img src=&quot;https://img.shields.io/badge/View-Docs-10B981?style=for-the-badge&amp;logo=gitbook&amp;logoColor=white&quot; alt=&quot;Docs&quot; /&gt;
        &lt;/a&gt;
      &lt;/td&gt;
      &lt;td align=&quot;left&quot;&gt;Master every command &amp; features&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;🎯 &lt;strong&gt;Showcase&lt;/strong&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://github.com/bytedance/UI-TARS-desktop/issues/842&quot;&gt;
          &lt;img src=&quot;https://img.shields.io/badge/View-Examples-8B5CF6?style=for-the-badge&amp;logo=github&amp;logoColor=white&quot; alt=&quot;Examples&quot; /&gt;
        &lt;/a&gt;
      &lt;/td&gt;
      &lt;td align=&quot;left&quot;&gt;View use cases built by the official and community&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;🔧 &lt;strong&gt;Reference&lt;/strong&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://agent-tars.com/api/&quot;&gt;
          &lt;img src=&quot;https://img.shields.io/badge/API-Reference-EF4444?style=for-the-badge&amp;logo=book&amp;logoColor=white&quot; alt=&quot;API&quot; /&gt;
        &lt;/a&gt;
      &lt;/td&gt;
      &lt;td align=&quot;left&quot;&gt;Complete technical reference&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;br/&gt;
&lt;br/&gt;
&lt;br/&gt;

## UI-TARS Desktop

&lt;p align=&quot;center&quot;&gt;
  &lt;img alt=&quot;UI-TARS&quot; width=&quot;260&quot; src=&quot;./apps/ui-tars/resources/icon.png&quot;&gt;
&lt;/p&gt;

UI-TARS Desktop is a native GUI agent driven by [UI-TARS](https://github.com/bytedance/UI-TARS) and Seed-1.5-VL/1.6 series models, available on your local computer and remote VM sandbox on cloud.

&lt;div align=&quot;center&quot;&gt;
&lt;p&gt;
        &amp;nbsp&amp;nbsp 📑 &lt;a href=&quot;https://arxiv.org/abs/2501.12326&quot;&gt;Paper&lt;/a&gt; &amp;nbsp&amp;nbsp
        | 🤗 &lt;a href=&quot;https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B&quot;&gt;Hugging Face Models&lt;/a&gt;&amp;nbsp&amp;nbsp
        | &amp;nbsp&amp;nbsp🫨 &lt;a href=&quot;https://discord.gg/pTXwYVjfcs&quot;&gt;Discord&lt;/a&gt;&amp;nbsp&amp;nbsp
        | &amp;nbsp&amp;nbsp🤖 &lt;a href=&quot;https://www.modelscope.cn/collections/UI-TARS-bccb56fa1ef640&quot;&gt;ModelScope&lt;/a&gt;&amp;nbsp&amp;nbsp
&lt;br&gt;
🖥️ Desktop Application &amp;nbsp&amp;nbsp
| &amp;nbsp&amp;nbsp 👓 &lt;a href=&quot;https://github.com/web-infra-dev/midscene&quot;&gt;Midscene (use in browser)&lt;/a&gt; &amp;nbsp&amp;nbsp
&lt;/p&gt;

&lt;/div&gt;

### Showcase

&lt;!-- // FIXME: Choose only two demo, one local computer and one remote computer showcase. --&gt;

|                                                          Instruction                                                           |                                                Local Operator                                                |                                               Remote Operator                                                |
| :----------------------------------------------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------------: |
| Please help me open the autosave feature of VS Code and delay AutoSave operations for 500 milliseconds in the VS Code setting. | &lt;video src=&quot;https://github.com/user-attachments/assets/e0914ce9-ad33-494b-bdec-0c25c1b01a27&quot; height=&quot;300&quot; /&gt; | &lt;video src=&quot;https://github.com/user-attachments/assets/01e49b69-7070-46c8-b3e3-2aaaaec71800&quot; height=&quot;300&quot; /&gt; |
|                    Could you help me check the latest open issue of the UI-TARS-Desktop project on GitHub?                     | &lt;video src=&quot;https://github.com/user-attachments/assets/3d159f54-d24a-4268-96c0-e149607e9199&quot; height=&quot;300&quot; /&gt; | &lt;video src=&quot;https://github.com/user-attachments/assets/072fb72d-7394-4bfa-95f5-4736e29f7e58&quot; height=&quot;300&quot; /&gt; |

### Features

- 🤖 Natural language control powered by Vision-Language Model
- 🖥️ Screenshot and visual recognition support
- 🎯 Precise mouse and keyboard control
- 💻 Cross-platform support (Windows/MacOS/Browser)
- 🔄 Real-time feedback and status display
- 🔐 Private and secure - fully local processing
- 🛠️ Effortless setup and intuitive remote operators

### Quick Start

See [Quick Start](./docs/quick-start.md)

## Contributing

See [CONTRIBUTING.md](./CONTRIBUTING.md).

## License

This project is licensed under the Apache License 2.0.

## Citation

If you find our paper and code useful in your research, please consider giving a star :star: and citation :pencil:

```BibTeX
@article{qin2025ui,
  title={UI-TARS: Pioneering Automated GUI Interaction with Native Agents},
  author={Qin, Yujia and Ye, Yining and Fang, Junjie and Wang, Haoming and Liang, Shihao and Tian, Shizuo and Zhang, Junda and Li, Jiahao and Li, Yunxin and Huang, Shijue and others},
  journal={arXiv preprint arXiv:2501.12326},
  year={2025}
}
```</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[hcengineering/platform]]></title>
            <link>https://github.com/hcengineering/platform</link>
            <guid>https://github.com/hcengineering/platform</guid>
            <pubDate>Tue, 19 Aug 2025 00:04:37 GMT</pubDate>
            <description><![CDATA[Huly — All-in-One Project Management Platform (alternative to Linear, Jira, Slack, Notion, Motion)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hcengineering/platform">hcengineering/platform</a></h1>
            <p>Huly — All-in-One Project Management Platform (alternative to Linear, Jira, Slack, Notion, Motion)</p>
            <p>Language: TypeScript</p>
            <p>Stars: 22,692</p>
            <p>Forks: 1,539</p>
            <p>Stars today: 97 stars today</p>
            <h2>README</h2><pre># Huly Platform

[![X (formerly Twitter) Follow](https://img.shields.io/twitter/follow/huly_io?style=for-the-badge)](https://x.com/huly_io)
![GitHub License](https://img.shields.io/github/license/hcengineering/platform?style=for-the-badge)

⭐️ Your star shines on us. Star us on GitHub!

## About

The Huly Platform is a robust framework designed to accelerate the development of business applications, such as CRM systems. 
This repository includes several applications, such as Chat, Project Management, CRM, HRM, and ATS. 
Various teams are building products on top of the Platform, including [Huly](https://huly.io) and [TraceX](https://tracex.co).

![Huly](https://repository-images.githubusercontent.com/392073243/6d27d5cc-38cd-4d88-affe-bb88b393180c)

## Self-Hosting

If you&#039;re primarily interested in self-hosting Huly without the intention to modify or contribute to its development, please use [huly-selfhost](https://github.com/hcengineering/huly-selfhost). 
This project offers a convenient method to host Huly using `docker`, designed for ease of use and quick setup. Explore this option to effortlessly enjoy Huly on your own server.

## Activity

![Alt](https://repobeats.axiom.co/api/embed/c42c99e21691fa60ea61b5cdf11c2e0647621534.svg &quot;Repobeats analytics image&quot;)

## Table of Content

- [Huly Platform](#huly-platform)
  - [About](#about)
  - [Self-Hosting](#self-hosting)
  - [Activity](#activity)
  - [Table of Content](#table-of-content)
  - [Pre-requisites](#pre-requisites)
  - [Verification](#verification)
  - [Installation](#installation)
  - [Build and run](#build-and-run)
  - [Run in development mode](#run-in-development-mode)
  - [Update project structure and database](#update-project-structure-and-database)
  - [Troubleshooting](#troubleshooting)
  - [Build \&amp; Watch](#build--watch)
  - [Tests](#tests)
    - [Unit tests](#unit-tests)
    - [UI tests](#ui-tests)
  - [Package publishing](#package-publishing)
  - [Additional testing](#additional-testing)

## Pre-requisites

- Before proceeding, ensure that your system meets the following requirements:
  - [Node.js](https://nodejs.org/en/download/) (v20.11.0 is required)
  - [Docker](https://docs.docker.com/get-docker/)
  - [Docker Compose](https://docs.docker.com/compose/install/)

## Verification

To verify the installation, perform the following checks in your terminal:

- Ensure that the `docker` commands are available:
  ```bash
  docker --version
  docker compose version
## Fast start

```bash
sh ./scripts/fast-start.sh
```

## Installation

You need Microsoft&#039;s [rush](https://rushjs.io) to install application.

1. Install Rush globally using the command:
   ```bash
   npm install -g @microsoft/rush
2. Navigate to the repository root and run the following commands:
   ```bash
   rush install
   rush build
Alternatively, you can just execute:

```bash
sh ./scripts/presetup-rush.sh
```

## Build and run

Development environment setup requires Docker to be installed on system.

Support is available for both amd64 and arm64 containers on Linux and macOS.

```bash
cd ./dev/
rush build    # Will build all the required packages. 
# rush rebuild  # could be used to omit build cache.
rush bundle   # Will prepare bundles.
rush package  # Will build all webpack packages.
rush validate # Will validate all sources with typescript and generate d.ts files required for ts-node execution.
rush svelte-check # Optional. svelte files validation using svelte-check.
rush docker:build   # Will build Docker containers for all applications in the local Docker environment.
rush docker:up # Will set up all the containers
```

Be aware `rush docker:build` will automatically execute all required phases like build, bundle, package.

Alternatively, you can just execute:

```bash
sh ./scripts/build.sh
```

By default, Docker volumes named dev_db, dev_elastic, and dev_files will be created for the MongoDB, Elasticsearch, and MinIO instances.

Before you can begin, you need to create a workspace and an account and associate it with the workspace.

```bash
cd ./tool # dev/tool in the repository root
rushx run-local create-workspace ws1 -w DevWorkspace # Create workspace
rushx run-local create-account user1 -p 1234 -f John -l Appleseed # Create account
rushx run-local configure ws1 --list --enable &#039;*&#039; # Enable all modules, even if they are not yet intended to be used by a wide audience.
rushx run-local assign-workspace user1 ws1 # Assign workspace to user.
rushx run-local confirm-email user1 # To allow the creation of additional test workspaces.

```

Alternatively, you can just execute:

```bash
sh ./scripts/create-workspace.sh
```

Add the following line to your /etc/hosts file

```
127.0.0.1 host.docker.internal
```

Accessing the URL http://host.docker.internal:8087 will lead you to the app in development mode.

Limitations:

- Local installation does not support sending emails, thus disabling functionalities such as password recovery and email notifications.

## Run in development mode

Development mode allows for live reloading and a smoother development process.

```bash
cd dev/prod
rush validate
rushx dev-server
```

Then go to http://localhost:8080

Click on &quot;Login with password&quot; link on the bottom of the right panel and use the following login credentials:

```plain
Email: user1
Password: 1234
Workspace: ws1
```

## Update project structure and database

If the project&#039;s structure is updated, it may be necessary to relink and rebuild the projects.

```bash
rush update
rush build
```

It may also be necessary to upgrade the running database.

```bash
cd ./dev/tool
rushx upgrade -f
```

## Troubleshooting

If a build fails, but the code is correct, try to delete the [build cache](https://rushjs.io/pages/maintainer/build_cache/) and retry.

```bash
# from the project root
rm -rf common/temp/build-cache
```

## Build &amp; Watch

For development purpose `rush build:watch` action could be used.

It includes build and validate phases in watch mode.

## Tests

### Unit tests

```bash
rush test # To execute all tests

rushx test # For individual test execution inside a package directory
```

### UI tests

```bash
cd ./tests
rush build
rush bundle
rush docker:build
## creates test Docker containers and sets up test database
./prepare.sh
## runs UI tests
rushx uitest
```

To execute tests in the development environment, please follow these steps:

```bash
cd ./tests
./create-local.sh ## use ./restore-local.sh if you only want to restore the workspace to a predefined initial state for sanity.
cd ./sanity
rushx dev-uitest # To execute all tests against the development environment.
rushx dev-debug -g &#039;pattern&#039; # To execute tests in debug mode with only the matching test pattern.
```

## Package publishing

```bash
node ./common/scripts/bump.js -p projectName
```

## Additional testing

This project is tested with BrowserStack.

&lt;sub&gt;&lt;sup&gt;&amp;copy; 2025 &lt;a href=&quot;https://hardcoreeng.com&quot;&gt;Hardcore Engineering Inc&lt;/a&gt;.&lt;/sup&gt;&lt;/sub&gt;

</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[huggingface/aisheets]]></title>
            <link>https://github.com/huggingface/aisheets</link>
            <guid>https://github.com/huggingface/aisheets</guid>
            <pubDate>Tue, 19 Aug 2025 00:04:36 GMT</pubDate>
            <description><![CDATA[Build, enrich, and transform datasets using AI models with no code]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/huggingface/aisheets">huggingface/aisheets</a></h1>
            <p>Build, enrich, and transform datasets using AI models with no code</p>
            <p>Language: TypeScript</p>
            <p>Stars: 736</p>
            <p>Forks: 61</p>
            <p>Stars today: 128 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# 🤗 Hugging Face AI Sheets

*Build, enrich, and transform datasets using AI models with no code. Deploy locally or on the Hub with access to thousands of open models.*

[Introduction](https://huggingface.co/blog/aisheets) • [Try it out](https://huggingface.co/spaces/aisheets/sheets)

&lt;video width=&quot;400&quot; src=&quot;https://github.com/user-attachments/assets/a284e4d4-3c11-4885-96cc-2f6f0314f2a1&quot;&gt;&lt;/video&gt;

&lt;/div&gt;

## What&#039;s AI Sheets?

Hugging Face AI Sheets is an open-source tool for building, enriching, and transforming datasets using AI models with no code. The tool can be deployed locally or on the Hub. It lets you use thousands of open models from the Hugging Face Hub via Inference Providers or local models, including `gpt-oss` from OpenAI!

## Quick Start

### Using the AI Sheets Space

Try it instantly at &lt;https://huggingface.co/spaces/aisheets/sheets&gt;

### Using Docker

First, get your Hugging Face token from &lt;https://huggingface.co/settings/tokens&gt;

```bash
export HF_TOKEN=your_token_here
docker run -p 3000:3000 \
-e HF_TOKEN=HF_TOKEN \
AI Sheets/sheets
```

Open `http://localhost:3000` in your browser.

### Using pnpm

First, [install pnpm](https://pnpm.io/installation) if you haven&#039;t already.

```bash
git clone https://github.com/huggingface/sheets.git
cd sheets
export HF_TOKEN=your_token_here
pnpm install
pnpm dev
```

Open `http://localhost:5173` in your browser.

#### Building for production

To build the production application, run:

```bash
pnpm build
```

This will create a production build in the `dist` directory.

Then, you can launch the built-in Express server to serve the production build:

```bash
export HF_TOKEN=your_token_here
pnpm serve
```

## Running data generation scripts using HF Jobs

If you want to generate a larger dataset, you can use the above-mentioned config and script, like this:

```bash
hf jobs uv run \
-s HF_TOKEN=$HF_TOKEN \
https://github.com/huggingface/aisheets/raw/refs/heads/main/scripts/extend_dataset/with_inference_client.py \ # script for running the pipeline
nvidia/Nemotron-Personas dvilasuero/nemotron-kimi-qa-distilled \
--config https://huggingface.co/datasets/dvilasuero/nemotron-personas-kimi-questions/raw/main/config.yml \ # config with prompts
--num-rows 100 # limit to 100 rows, leave empty for the full dataset
```

Alternatively, you can use a script that utilizes vllm inference instead of the inference client. This script helps you to save on inference costs, but it requires you to set up a vllm-compatible flavor when running the job:

```bash
hf jobs uv run --flavor l4x1 \
-s HF_TOKEN=$HF_TOKEN \
https://github.com/huggingface/aisheets/raw/refs/heads/main/scripts/extend_dataset/with_vllm.py \ # script for running the pipeline
nvidia/Nemotron-Personas dvilasuero/nemotron-kimi-qa-distilled \
--config https://huggingface.co/datasets/dvilasuero/nemotron-personas-kimi-questions/raw/main/config.yml \ # config with prompts
--num-rows 100 \ # limit to 100 rows, leave empty for the full dataset
--vllm-model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
```

## Running AI Sheets with custom (and local) LLMs

By default, AI Sheets is configured to use the Huggingface Inference Providers API to run inference on the latest open-source models. However, you can also run Sheets with own custom LLMs, such as those hosted on your own infrastructure or other cloud providers. The only requirement is that your LLMs must support the [OpenAI API specification](https://platform.openai.com/docs/api-reference/introduction).

### Steps

When running AI Sheets with custom LLMs, you need to set some environment variables to point the inference calls to your custom LLMs. Here are the steps:

1. **Set the `MODEL_ENDPOINT_URL` environment variable**: This variable should point to the base URL of your custom LLM&#039;s API endpoint. For example, if you are using Ollama to run your LLM locally, you would set it like this:

```sh
export MODEL_ENDPOINT_URL=http://localhost:11434
```

Since Ollama starts a local server on port `11434` by default, this URL will point to your local Ollama instance.

2. **Set the `MODEL_ENDPOINT_NAME` environment variable**: This variable should specify the name of the model you want to use. For example, if you are using the `llama3` model, you would set it like this:

```sh
export MODEL_ENDPOINT_NAME=llama3
```

This is a crucial step to conform to the OpenAI API specification. The model name is a required parameter in the [OpenAI API](https://platform.openai.com/docs/api-reference/responses/create#responses-create-model), and it is used to identify which model to use for inference.

3. **Run the AI Sheets app**: After setting the environment variables, you can run the Sheets app as usual. The app will now use your custom LLM for inference instead of the default Huggingface Inference Providers API as the default behavior. Anyway, all the models provided by the Huggingface Inference Providers API will still be available when selecting a model in the column settings.

* Note: The text-to-image generation feature cannot be customized yet. It will always utilize the Hugging Face Inference Providers API to generate images. Take this into account when running AI Sheets with custom LLMs.

### Example of running AI Sheets with Ollama

To run AI Sheets with Ollama, you can follow these steps:

1. Start the Ollama server, and run the model of your choice

```sh
export OLLAMA_NOHISTORY=1
ollama serve
```

```sh
ollama run llama3
```

(Visit the Ollama [FAQ](https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-specify-the-context-window-size) page to know more about Ollama server configuration)

2. Set the environment variables:

```sh
export MODEL_ENDPOINT_URL=http://localhost:11434
export MODEL_ENDPOINT_NAME=llama3
```

3. Run the AI Sheets app:

```sh
pnpm serve
```

This will start the AI Sheets app and use the `llama3` model running on your local Ollama instance for inference.

## Advanced configuration

AI Sheets defines some environment variables that can be used to customize the behavior of the application. In the following sections, we will describe the available environment variables and their usage.

###  Authentication

* `OAUTH_CLIENT_ID`: The Hugging Face OAuth client ID for the application. This is used to authenticate users via the Hugging Face OAuth. If this variable is defined, it will be used to authenticate users. (See how to setup the Hugging Face OAuth [here](https://huggingface.co/blog/frascuchon/running-sheets-locally#oauth-authentication)).

* `HF_TOKEN`: A Hugging Face token to use for authentication. If this variable is defined, it will be used for authenticated inference calls, instead of the OAuth token.

* `OAUTH_SCOPES`: The scopes to request during the OAuth authentication. The default value is `openid profile inference-api manage-repos`. This variable is used to request the necessary permissions for the application to function correctly, and normally does not need to be changed.

###  Inference

* `DEFAULT_MODEL`: The default model id to use when calling the inference API for text generation. The default value is `meta-llama/Llama-3.3-70B-Instruct`. This variable can be used to change the default model used for text generation and must be a valid model id from the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=text-generation&amp;inference_provider=all&amp;sort=trending),

* `DEFAULT_MODEL_PROVIDER`: The default model provider to use when calling the inference API for text generation. The default value is `nebius`. This variable can be used to change the default model provider used for text generation and must be a valid provider from the [Hugging Face Inference Providers](https://huggingface.co/docs/inference-providers/en/index).

* `ORG_BILLING`: The organization billing to use for inference calls. If this variable is defined, the inference calls will be billed to the specified organization. This is useful for organizations that want to manage their inference costs and usage. Remember that users must be part of the organization to use this feature, or an `HF_TOKEN` of a user that is part of the organization must be defined.

* `MODEL_ENDPOINT_URL`:  The URL of a custom inference endpoint to use for text generation. If this variable is defined, it will be used instead of the default Hugging Face Inference API. This is useful for using custom inference endpoints that are not hosted on the Hugging Face Hub, such as Ollama or LLM Studio. The URL must be a valid endpoint that supports the [OpenAI API format](https://platform.openai.com/docs/api-reference/chat/create).

* `MODEL_ENDPOINT_NAME`: The model id to use when calling the custom inference endpoint defined by `MODEL_ENDPOINT_URL`. This variable is required if `MODEL_ENDPOINT_URL` is defined for custom inference endpoints that require a model id, such as Ollama or LLM Studio. The model id must correspond to the model deployed on the custom inference endpoint.

* `NUM_CONCURRENT_REQUESTS`: The number of concurrent requests to allow when calling the inference API in the column cells generation process. The default value is `5`, and the maximum value is `10`. This is useful to control the number of concurrent requests made to the inference API and avoid hitting rate limits defined by the provider.

### Miscellaneous

* `DATA_DIR`: The directory where the application will store all its data. The default value is `./data`. This variable can be used to change the data directory used by the application. The directory must be writable by the application.

* `SERPER_API_KEY`: The API key to use for the Serper web search API. If this variable is defined, it will be used to authenticate web search requests. If this variable is not defined, web search will be disabled. The Serper API key can be obtained from the [Serper website](https://serper.dev/).

* `TELEMETRY_ENABLED`: A boolean value that indicates whether telemetry is enabled or not. The default value is `1`. This variable can be used to disable telemetry if desired. Telemetry is used to collect anonymous usage data to help improve the application.

* `EXAMPLES_PROMPT_MAX_CONTEXT_SIZE`: The maximum context size (in characters) for the examples section in the prompt for text generation. The default value is `8192`. If the examples section exceeds this size, it will be truncated. This variable can be used when the examples section is too large and needs to be reduced to fit within the context size limits of the model.

* `SOURCES_PROMPT_MAX_CONTEXT_SIZE`: The maximum context size (in characters) for the sources section in the prompt for text generation. The default value is `61440`. If the sources section exceeds this size, it will be truncated. This variable can be used when the sources section is too large and needs to be reduced to fit within the context size limits of the model.

## Developer docs

### Dev dependencies on your vscode

#### vitest runner

&lt;https://marketplace.visualstudio.com/items?itemName=rluvaton.vscode-vitest&gt;

#### biome

&lt;https://marketplace.visualstudio.com/items?itemName=biomejs.biome&gt;

### Project Structure

This project is using Qwik with [QwikCity](https://qwik.dev/qwikcity/overview/). QwikCity is just an extra set of tools on top of Qwik to make it easier to build a full site, including directory-based routing, layouts, and more.

Inside your project, you&#039;ll see the following directory structure:

```
├── public/
│   └── ...
└── src/
    ├── components/ --&gt; Stateless components
    │   └── ...
    ├── features/ --&gt; Components with business logic
    │   └── ...
    └── routes/
        └── ...
```

* `src/routes`: Provides the directory-based routing, which can include a hierarchy of `layout.tsx` layout files, and an `index.tsx` file as the page. Additionally, `index.ts` files are endpoints. Please see the [routing docs](https://qwik.dev/qwikcity/routing/overview/) for more info.

* `src/components`: Recommended directory for components.

* `public`: Any static assets, like images, can be placed in the public directory. Please see the [Vite public directory](https://vitejs.dev/guide/assets.html#the-public-directory) for more info.

### Development

Run this on your root folder

```sh
touch .env
```

Add in your `.env` file the following variable:

```
HF_TOKEN=your_hugging_face_token
```

Development mode uses [Vite&#039;s development server](https://vitejs.dev/). The `dev` command will server-side render (SSR) the output during development.

```shell
pnpm dev
```

&gt; Note: during dev mode, Vite may request a significant number of `.js` files. This does not represent a Qwik production build.

### Preview

The preview command will create a production build of the client modules, a production build of `src/entry.preview.tsx`, and run a local server. The preview server is only for convenience to preview a production build locally and should not be used as a production server.

```shell
pnpm preview
```

### Production

The production build will generate client and server modules by running both client and server build commands. The build command will use Typescript to run a type check on the source code.

```shell
pnpm build
```

### Express Server

This app has a minimal [Express server](https://expressjs.com/) implementation. After running a full build, you can preview the build using the command:

```
pnpm serve
```

Then visit [http://localhost:3000/](http://localhost:3000/)
</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[blakeblackshear/frigate]]></title>
            <link>https://github.com/blakeblackshear/frigate</link>
            <guid>https://github.com/blakeblackshear/frigate</guid>
            <pubDate>Tue, 19 Aug 2025 00:04:35 GMT</pubDate>
            <description><![CDATA[NVR with realtime local object detection for IP cameras]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/blakeblackshear/frigate">blakeblackshear/frigate</a></h1>
            <p>NVR with realtime local object detection for IP cameras</p>
            <p>Language: TypeScript</p>
            <p>Stars: 25,232</p>
            <p>Forks: 2,315</p>
            <p>Stars today: 65 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img align=&quot;center&quot; alt=&quot;logo&quot; src=&quot;docs/static/img/frigate.png&quot;&gt;
&lt;/p&gt;

# Frigate - NVR With Realtime Object Detection for IP Cameras

&lt;a href=&quot;https://hosted.weblate.org/engage/frigate-nvr/&quot;&gt;
&lt;img src=&quot;https://hosted.weblate.org/widget/frigate-nvr/language-badge.svg&quot; alt=&quot;Translation status&quot; /&gt;
&lt;/a&gt;

\[English\] | [简体中文](https://github.com/blakeblackshear/frigate/blob/dev/README_CN.md)

A complete and local NVR designed for [Home Assistant](https://www.home-assistant.io) with AI object detection. Uses OpenCV and Tensorflow to perform realtime object detection locally for IP cameras.

Use of a GPU or AI accelerator such as a [Google Coral](https://coral.ai/products/) or [Hailo](https://hailo.ai/) is highly recommended. AI accelerators will outperform even the best CPUs with very little overhead.

- Tight integration with Home Assistant via a [custom component](https://github.com/blakeblackshear/frigate-hass-integration)
- Designed to minimize resource use and maximize performance by only looking for objects when and where it is necessary
- Leverages multiprocessing heavily with an emphasis on realtime over processing every frame
- Uses a very low overhead motion detection to determine where to run object detection
- Object detection with TensorFlow runs in separate processes for maximum FPS
- Communicates over MQTT for easy integration into other systems
- Records video with retention settings based on detected objects
- 24/7 recording
- Re-streaming via RTSP to reduce the number of connections to your camera
- WebRTC &amp; MSE support for low-latency live view

## Documentation

View the documentation at https://docs.frigate.video

## Donations

If you would like to make a donation to support development, please use [Github Sponsors](https://github.com/sponsors/blakeblackshear).

## Screenshots

### Live dashboard

&lt;div&gt;
&lt;img width=&quot;800&quot; alt=&quot;Live dashboard&quot; src=&quot;https://github.com/blakeblackshear/frigate/assets/569905/5e713cb9-9db5-41dc-947a-6937c3bc376e&quot;&gt;
&lt;/div&gt;

### Streamlined review workflow

&lt;div&gt;
&lt;img width=&quot;800&quot; alt=&quot;Streamlined review workflow&quot; src=&quot;https://github.com/blakeblackshear/frigate/assets/569905/6fed96e8-3b18-40e5-9ddc-31e6f3c9f2ff&quot;&gt;
&lt;/div&gt;

### Multi-camera scrubbing

&lt;div&gt;
&lt;img width=&quot;800&quot; alt=&quot;Multi-camera scrubbing&quot; src=&quot;https://github.com/blakeblackshear/frigate/assets/569905/d6788a15-0eeb-4427-a8d4-80b93cae3d74&quot;&gt;
&lt;/div&gt;

### Built-in mask and zone editor

&lt;div&gt;
&lt;img width=&quot;800&quot; alt=&quot;Multi-camera scrubbing&quot; src=&quot;https://github.com/blakeblackshear/frigate/assets/569905/d7885fc3-bfe6-452f-b7d0-d957cb3e31f5&quot;&gt;
&lt;/div&gt;

## Translations

We use [Weblate](https://hosted.weblate.org/projects/frigate-nvr/) to support language translations. Contributions are always welcome.

&lt;a href=&quot;https://hosted.weblate.org/engage/frigate-nvr/&quot;&gt;
&lt;img src=&quot;https://hosted.weblate.org/widget/frigate-nvr/multi-auto.svg&quot; alt=&quot;Translation status&quot; /&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[supermemoryai/supermemory]]></title>
            <link>https://github.com/supermemoryai/supermemory</link>
            <guid>https://github.com/supermemoryai/supermemory</guid>
            <pubDate>Tue, 19 Aug 2025 00:04:34 GMT</pubDate>
            <description><![CDATA[Build your own second brain with supermemory. Extremely fast, scalable, memory API for the AI era.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/supermemoryai/supermemory">supermemoryai/supermemory</a></h1>
            <p>Build your own second brain with supermemory. Extremely fast, scalable, memory API for the AI era.</p>
            <p>Language: TypeScript</p>
            <p>Stars: 10,686</p>
            <p>Forks: 1,084</p>
            <p>Stars today: 151 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; style=&quot;padding-bottom:20px;padding-top:20px&quot;&gt;
  &lt;img src=&quot;logo.svg&quot; alt=&quot;supermemory Logo&quot; width=&quot;400&quot; /&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot; style=&quot;padding-bottom:10px;padding-top:10px&quot;&gt;
  &lt;img src=&quot;apps/web/public/landing-page.jpeg&quot; alt=&quot;supermemory&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

## Features

### Core Functionality

- **[Add Memories from Any Content](#add-memory)**: Easily add memories from URLs, PDFs, and plain text—just paste, upload, or link.
- **[Chat with Your Memories](#chat-memories)**: Converse with your stored content using natural language chat.
- **[Supermemory MCP Integration](#mcp-integration)**: Seamlessly connect with all major AI tools (Claude, Cursor, etc.) via Supermemory MCP.

## How do i use this?

Go to [app.supermemory.ai](https://app.supermemory.ai) and sign into with your account

1. &lt;a id=&quot;add-memory&quot;&gt;&lt;/a&gt;Start Adding Memory with your choose of format (Note, Link, File)
&lt;div align=&quot;center&quot; style=&quot;padding-bottom:10px;padding-top:10px&quot;&gt;
  &lt;img src=&quot;apps/web/public/add-memory.png&quot; alt=&quot;supermemory&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

2. You can also Connect to your favourite services (Notion, Google Drive, OneDrive)
&lt;div align=&quot;center&quot; style=&quot;padding-bottom:10px;padding-top:10px&quot;&gt;
  &lt;img src=&quot;apps/web/public/add-connections.png&quot; alt=&quot;supermemory&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

3. &lt;a id=&quot;chat-memories&quot;&gt;&lt;/a&gt;Once Memories are added, you can chat with Supermemory by clicking on &quot;Open Chat&quot; and retrieve info from your saved memories
&lt;div align=&quot;center&quot; style=&quot;padding-bottom:10px;padding-top:10px&quot;&gt;
  &lt;img src=&quot;apps/web/public/chat.png&quot; alt=&quot;supermemory&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

4. &lt;a id=&quot;mcp-integration&quot;&gt;&lt;/a&gt;Add MCP to your AI Tools (by clicking on &quot;Connect to your AI&quot; and select the AI tool you are trying to integrate)
&lt;div align=&quot;center&quot; style=&quot;padding-bottom:10px;padding-top:10px&quot;&gt;
  &lt;img src=&quot;apps/web/public/mcp.png&quot; alt=&quot;supermemory&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

## Support

Have questions or feedback? We&#039;re here to help:

- Email: [dhravya@supermemory.com](mailto:dhravya@supermemory.com)
- Documentation: [docs.supermemory.ai](https://docs.supermemory.ai)

## Contributing

We welcome contributions from developers of all skill levels! Whether you&#039;re fixing bugs, adding features, or improving documentation, your help makes supermemory better for everyone.

### Quick Start for Contributors

1. **Fork and clone** the repository
2. **Install dependencies** with `bun install`
3. **Set up your environment** by copying `.env.example` to `.env.local`
4. **Start developing** with `bun run dev`

For detailed guidelines, development setup, coding standards, and the complete contribution workflow, please see our [**Contributing Guide**](CONTRIBUTE.md).

### Ways to Contribute

- 🐛 **Bug fixes** - Help us squash those pesky issues
- ✨ **New features** - Add functionality that users will love
- 🎨 **UI/UX improvements** - Make the interface more intuitive
- ⚡ **Performance optimizations** - Help us make supermemory faster

Check out our [Issues](https://github.com/supermemoryai/supermemory/issues) page for `good first issue` and `help wanted` labels to get started!

## Updates &amp; Roadmap

Stay up to date with the latest improvements:

- [Changelog](https://docs.supermemory.ai/changelog/overview)
- [X](https://x.com/supermemoryai)
</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[kortix-ai/suna]]></title>
            <link>https://github.com/kortix-ai/suna</link>
            <guid>https://github.com/kortix-ai/suna</guid>
            <pubDate>Tue, 19 Aug 2025 00:04:33 GMT</pubDate>
            <description><![CDATA[Suna - Open Source Generalist AI Agent]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/kortix-ai/suna">kortix-ai/suna</a></h1>
            <p>Suna - Open Source Generalist AI Agent</p>
            <p>Language: TypeScript</p>
            <p>Stars: 17,528</p>
            <p>Forks: 2,868</p>
            <p>Stars today: 25 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# Kortix – Open Source Platform to Build, Manage and Train AI Agents

![Kortix Screenshot](frontend/public/banner.png)

**The complete platform for creating autonomous AI agents that work for you**

Kortix is a comprehensive open source platform that empowers you to build, manage, and train sophisticated AI agents for any use case. Create powerful agents that act autonomously on your behalf, from general-purpose assistants to specialized automation tools.

[![License](https://img.shields.io/badge/License-Apache--2.0-blue)](./license)
[![Discord Follow](https://dcbadge.limes.pink/api/server/Py6pCBUUPw?style=flat)](https://discord.gg/Py6pCBUUPw)
[![Twitter Follow](https://img.shields.io/twitter/follow/kortixai)](https://x.com/kortixai)
[![GitHub Repo stars](https://img.shields.io/github/stars/kortix-ai/suna)](https://github.com/kortix-ai/suna)
[![Issues](https://img.shields.io/github/issues/kortix-ai/suna)](https://github.com/kortix-ai/suna/labels/bug)

&lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
[Deutsch](https://www.readme-i18n.com/kortix-ai/suna?lang=de) | 
[Español](https://www.readme-i18n.com/kortix-ai/suna?lang=es) | 
[français](https://www.readme-i18n.com/kortix-ai/suna?lang=fr) | 
[日本語](https://www.readme-i18n.com/kortix-ai/suna?lang=ja) | 
[한국어](https://www.readme-i18n.com/kortix-ai/suna?lang=ko) | 
[Português](https://www.readme-i18n.com/kortix-ai/suna?lang=pt) | 
[Русский](https://www.readme-i18n.com/kortix-ai/suna?lang=ru) | 
[中文](https://www.readme-i18n.com/kortix-ai/suna?lang=zh)

&lt;/div&gt;

## 🌟 What Makes Kortix Special

### 🤖 Includes Suna – Flagship Generalist AI Worker
Meet Suna, our showcase agent that demonstrates the full power of the Kortix platform. Through natural conversation, Suna handles research, data analysis, browser automation, file management, and complex workflows – showing you what&#039;s possible when you build with Kortix.

### 🔧 Build Custom Suna-Type Agents
Create your own specialized agents tailored to specific domains, workflows, or business needs. Whether you need agents for customer service, data processing, content creation, or industry-specific tasks, Kortix provides the infrastructure and tools to build, deploy, and scale them.

### 🚀 Complete Platform Capabilities
- **Browser Automation**: Navigate websites, extract data, fill forms, automate web workflows
- **File Management**: Create, edit, and organize documents, spreadsheets, presentations, code
- **Web Intelligence**: Crawling, search capabilities, data extraction and synthesis
- **System Operations**: Command-line execution, system administration, DevOps tasks
- **API Integrations**: Connect with external services and automate cross-platform workflows
- **Agent Builder**: Visual tools to configure, customize, and deploy agents

## 📋 Table of Contents

- [🌟 What Makes Kortix Special](#-what-makes-kortix-special)
- [🎯 Agent Examples &amp; Use Cases](#-agent-examples--use-cases)
- [🏗️ Platform Architecture](#️-platform-architecture)
- [🚀 Quick Start](#-quick-start)
- [🏠 Self-Hosting](#-self-hosting)
- [🤝 Contributing](#-contributing)
- [📄 License](#-license)

## 🎯 Agent Examples &amp; Use Cases

### Suna - Your Generalist AI Worker

Suna demonstrates the full capabilities of the Kortix platform as a versatile AI worker that can:

**🔍 Research &amp; Analysis**
- Conduct comprehensive web research across multiple sources
- Analyze documents, reports, and datasets
- Synthesize information and create detailed summaries
- Market research and competitive intelligence

**🌐 Browser Automation**
- Navigate complex websites and web applications
- Extract data from multiple pages automatically
- Fill forms and submit information
- Automate repetitive web-based workflows

**📁 File &amp; Document Management**
- Create and edit documents, spreadsheets, presentations
- Organize and structure file systems
- Convert between different file formats
- Generate reports and documentation

**📊 Data Processing &amp; Analysis**
- Clean and transform datasets from various sources
- Perform statistical analysis and create visualizations
- Monitor KPIs and generate insights
- Integrate data from multiple APIs and databases

**⚙️ System Administration**
- Execute command-line operations safely
- Manage system configurations and deployments
- Automate DevOps workflows
- Monitor system health and performance

### Build Your Own Specialized Agents

The Kortix platform enables you to create agents tailored to specific needs:

**🎧 Customer Service Agents**
- Handle support tickets and FAQ responses
- Manage user onboarding and training
- Escalate complex issues to human agents
- Track customer satisfaction and feedback

**✍️ Content Creation Agents**
- Generate marketing copy and social media posts
- Create technical documentation and tutorials
- Develop educational content and training materials
- Maintain content calendars and publishing schedules

**📈 Sales &amp; Marketing Agents**
- Qualify leads and manage CRM systems
- Schedule meetings and follow up with prospects
- Create personalized outreach campaigns
- Generate sales reports and forecasts

**🔬 Research &amp; Development Agents**
- Conduct academic and scientific research
- Monitor industry trends and innovations
- Analyze patents and competitive landscapes
- Generate research reports and recommendations

**🏭 Industry-Specific Agents**
- Healthcare: Patient data analysis, appointment scheduling
- Finance: Risk assessment, compliance monitoring
- Legal: Document review, case research
- Education: Curriculum development, student assessment

Each agent can be configured with custom tools, workflows, knowledge bases, and integrations specific to your requirements.

## 🏗️ Platform Architecture

![Architecture Diagram](docs/images/diagram.png)

Kortix consists of four main components that work together to provide a complete AI agent development platform:

### 🔧 Backend API
Python/FastAPI service that powers the agent platform with REST endpoints, thread management, agent orchestration, and LLM integration with Anthropic, OpenAI, and others via LiteLLM. Includes agent builder tools, workflow management, and extensible tool system.

### 🖥️ Frontend Dashboard
Next.js/React application providing a comprehensive agent management interface with chat interfaces, agent configuration dashboards, workflow builders, monitoring tools, and deployment controls.

### 🐳 Agent Runtime
Isolated Docker execution environments for each agent instance featuring browser automation, code interpreter, file system access, tool integration, security sandboxing, and scalable agent deployment.

### 🗄️ Database &amp; Storage
Supabase-powered data layer handling authentication, user management, agent configurations, conversation history, file storage, workflow state, analytics, and real-time subscriptions for live agent monitoring.

## 🚀 Quick Start

Get your Kortix platform running in minutes with our automated setup wizard:

### 1️⃣ Clone the Repository
```bash
git clone https://github.com/kortix-ai/suna.git
cd suna
```

### 2️⃣ Run the Setup Wizard
```bash
python setup.py
```
The wizard will guide you through 14 steps with progress saving, so you can resume if interrupted.

### 3️⃣ Start the Platform
```bash
python start.py
```

That&#039;s it! Your Kortix platform will be running with Suna ready to assist you.

## 🏠 Self-Hosting

Kortix can be self-hosted on your own infrastructure using our comprehensive setup wizard, giving you complete control over your AI agent platform. For a complete guide to self-hosting Kortix, please refer to our [Self-Hosting Guide](./docs/SELF-HOSTING.md).

### 🔧 Setup Process Includes

- **🏗️ Infrastructure**: Supabase project setup for database and authentication
- **⚡ Performance**: Redis configuration for caching and session management
- **🛡️ Security**: Daytona setup for secure agent execution environments
- **🤖 AI Integration**: LLM providers (Anthropic, OpenAI, OpenRouter, etc.)
- **🌐 Web Capabilities**: Search and scraping (Tavily, Firecrawl)
- **📋 Workflows**: QStash for background job processing
- **🔗 Automation**: Webhook handling for automated tasks
- **📊 Data Sources**: Optional RapidAPI integrations

### 📚 Manual Setup

For advanced users who prefer manual configuration, see the [Self-Hosting Guide](./docs/SELF-HOSTING.md) for detailed manual setup instructions.

The wizard will guide you through all necessary steps to get your Kortix platform up and running. For detailed instructions, troubleshooting tips, and advanced configuration options, see the [Self-Hosting Guide](./docs/SELF-HOSTING.md).

## 🤝 Contributing

We welcome contributions from the community! Whether you&#039;re fixing bugs, adding features, or improving documentation, your help makes Kortix better for everyone.

Please see our [Contributing Guide](./CONTRIBUTING.md) for more details on:
- How to set up your development environment
- Code style and standards
- Pull request process
- Community guidelines

## 📄 License

Kortix is licensed under the Apache License, Version 2.0. See [LICENSE](./LICENSE) for the full license text.

---

&lt;div align=&quot;center&quot;&gt;

**Ready to build your first AI agent?** 

[Get Started](./docs/SELF-HOSTING.md) • [Join Discord](https://discord.gg/Py6pCBUUPw) • [Follow on Twitter](https://x.com/kortixai)

&lt;/div&gt;</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[ZuodaoTech/everyone-can-use-english]]></title>
            <link>https://github.com/ZuodaoTech/everyone-can-use-english</link>
            <guid>https://github.com/ZuodaoTech/everyone-can-use-english</guid>
            <pubDate>Tue, 19 Aug 2025 00:04:32 GMT</pubDate>
            <description><![CDATA[人人都能用英语]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ZuodaoTech/everyone-can-use-english">ZuodaoTech/everyone-can-use-english</a></h1>
            <p>人人都能用英语</p>
            <p>Language: TypeScript</p>
            <p>Stars: 26,605</p>
            <p>Forks: 3,931</p>
            <p>Stars today: 90 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;./enjoy/assets/icon.png&quot; alt=&quot;Clash&quot; width=&quot;128&quot; /&gt;
&lt;/div&gt;

&lt;h3 align=&quot;center&quot;&gt;
AI 是当今世界上最好的外语老师，Enjoy 做 AI 最好的助教。
&lt;/h3&gt;

[![Deploy 1000h website](https://github.com/ZuodaoTech/everyone-can-use-english/actions/workflows/deploy-1000h.yml/badge.svg)](https://github.com/ZuodaoTech/everyone-can-use-english/actions/workflows/deploy-1000h.yml)
[![Test Enjoy App](https://github.com/ZuodaoTech/everyone-can-use-english/actions/workflows/test-enjoy-app.yml/badge.svg)](https://github.com/ZuodaoTech/everyone-can-use-english/actions/workflows/test-enjoy-app.yml)
[![Release Enjoy App](https://github.com/ZuodaoTech/everyone-can-use-english/actions/workflows/release-enjoy-app.yml/badge.svg)](https://github.com/ZuodaoTech/everyone-can-use-english/actions/workflows/release-enjoy-app.yml)
![Latest Version](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fenjoy.bot%2Fapi%2Fconfig%2Fapp_version&amp;query=%24.version&amp;label=Latest&amp;link=https%3A%2F%2F1000h.org%2Fenjoy-app%2Finstall.html)
![Recording Duration](https://img.shields.io/endpoint?url=https%3A%2F%2Fenjoy.bot%2Fapi%2Fbadges%2Frecordings)

---

## 网页版

Enjoy 网页版已经上线，可访问 [https://enjoy.bot](https://enjoy.bot) 直接使用。

&lt;div align=&quot;center&quot; style=&quot;display:flex;overflow:auto;gap:10px;&quot;&gt;
  &lt;img src=&quot;./enjoy/snapshots/web-audios.jpg&quot; alt=&quot;Audios&quot; width=&quot;300&quot; /&gt;
  &lt;img src=&quot;./enjoy/snapshots/web-add-audio.jpg&quot; alt=&quot;Add Audio&quot; width=&quot;300&quot; /&gt;
  &lt;img src=&quot;./enjoy/snapshots/web-audio-shadow.jpg&quot; alt=&quot;Shadow&quot; width=&quot;300&quot; /&gt;
  &lt;img src=&quot;./enjoy/snapshots/web-audio-assessment.jpg&quot; alt=&quot;Assessment&quot; width=&quot;300&quot; /&gt;
  &lt;img src=&quot;./enjoy/snapshots/web-new-chat.jpg&quot; alt=&quot;New Chat&quot; width=&quot;300&quot; /&gt;
  &lt;img src=&quot;./enjoy/snapshots/web-chat.jpg&quot; alt=&quot;Chat&quot; width=&quot;300&quot; /&gt;
&lt;/div&gt;

---

## 桌面版安装及使用

下载及使用相关说明，请参阅 [文档](https://1000h.org/enjoy-app/)。

## 预览

&lt;div align=&quot;center&quot; style=&quot;display:flex;overflow:auto;&quot;&gt;
  &lt;img src=&quot;./enjoy/snapshots/home.png&quot; alt=&quot;Home&quot; width=&quot;800&quot; /&gt;

  &lt;img src=&quot;./enjoy/snapshots/shadow.png&quot; alt=&quot;Home&quot; width=&quot;800&quot; /&gt;

  &lt;img src=&quot;./enjoy/snapshots/assessment.png&quot; alt=&quot;Home&quot; width=&quot;800&quot; /&gt;

  &lt;img src=&quot;./enjoy/snapshots/document.png&quot; alt=&quot;Home&quot; width=&quot;800&quot; /&gt;

  &lt;img src=&quot;./enjoy/snapshots/chat.png&quot; alt=&quot;Home&quot; width=&quot;800&quot; /&gt;
&lt;/div&gt;

## 桌面版开发

```bash
yarn install
yarn enjoy:start
```

## 相关阅读

### 一千小时（2024）

- [简要说明](https://1000h.org/intro.html)
- [训练任务](https://1000h.org/training-tasks/kick-off.html)
- [语音塑造](https://1000h.org/sounds-of-american-english/0-intro.html)
- [大脑内部](https://1000h.org/in-the-brain/01-inifinite.html)
- [自我训练](https://1000h.org/self-training/00-intro.html)

### 人人都能用英语（2010）

- [简介](./book/README.md)
- [第一章：起点](./book/chapter1.md)
- [第二章：口语](./book/chapter2.md)
- [第三章：语音](./book/chapter3.md)
- [第四章：朗读](./book/chapter4.md)
- [第五章：词典](./book/chapter5.md)
- [第六章：语法](./book/chapter6.md)
- [第七章：精读](./book/chapter7.md)
- [第八章：叮嘱](./book/chapter8.md)
- [后记](./book/end.md)

## 常见问题

请查询 [文档 FAQ](https://1000h.org/enjoy-app/faq.html)。
</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[actualbudget/actual]]></title>
            <link>https://github.com/actualbudget/actual</link>
            <guid>https://github.com/actualbudget/actual</guid>
            <pubDate>Tue, 19 Aug 2025 00:04:31 GMT</pubDate>
            <description><![CDATA[A local-first personal finance app]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/actualbudget/actual">actualbudget/actual</a></h1>
            <p>A local-first personal finance app</p>
            <p>Language: TypeScript</p>
            <p>Stars: 21,733</p>
            <p>Forks: 1,741</p>
            <p>Stars today: 41 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/demo.png&quot; alt=&quot;Actualbudget&quot; /&gt;
&lt;/p&gt;

## Getting Started

Actual is a local-first personal finance tool. It is 100% free and open-source, written in NodeJS, it has a synchronization element so that all your changes can move between devices without any heavy lifting.

If you are interested in contributing, or want to know how development works, see our [contributing](https://actualbudget.org/docs/contributing/) document we would love to have you.

Want to say thanks? Click the ⭐ at the top of the page.

## Key Links

- Actual [discord](https://discord.gg/pRYNYr4W5A) community.
- Actual [Community Documentation](https://actualbudget.org/docs)
- [Frequently asked questions](https://actualbudget.org/docs/faq)

## Installation

There are four ways to deploy Actual:

1. One-click deployment [via PikaPods](https://www.pikapods.com/pods?run=actual) (~1.40 $/month) - recommended for non-technical users
1. Managed hosting [via Fly.io](https://actualbudget.org/docs/install/fly) (~1.50 $/month)
1. Self-hosted by using [a Docker image](https://actualbudget.org/docs/install/docker)
1. Local-only apps - [downloadable Windows, Mac and Linux apps](https://actualbudget.org/download/) you can run on your device

Learn more in the [installation instructions docs](https://actualbudget.org/docs/install/).

## Ready to Start Budgeting?

Read about [Envelope budgeting](https://actualbudget.org/docs/getting-started/envelope-budgeting) to know more about the idea behind Actual Budget.

### Are you new to budgeting or want to start fresh?

Check out the community&#039;s [Starting Fresh](https://actualbudget.org/docs/getting-started/starting-fresh) guide so you can quickly get up and running!

### Are you migrating from other budgeting apps?

Check out the community&#039;s [Migration](https://actualbudget.org/docs/migration/) guide to start jumping on the Actual Budget train!

## Documentation

We have a wide range of documentation on how to use Actual, this is all available in our [Community Documentation](https://actualbudget.org/docs), this includes topics on Budgeting, Account Management, Tips &amp; Tricks and some documentation for developers.

## Contributing

Actual is a community driven product. Learn more about [contributing to Actual](https://actualbudget.org/docs/contributing/).

### Code structure

The Actual app is split up into a few packages:

- loot-core - The core application that runs on any platform
- desktop-client - The desktop UI
- desktop-electron - The desktop app

More information on the project structure is available in our [community documentation](https://actualbudget.org/docs/contributing/project-details).

### Feature Requests

Current feature requests can be seen [here](https://github.com/actualbudget/actual/issues?q=is%3Aissue+label%3A%22needs+votes%22+sort%3Areactions-%2B1-desc).
Vote for your favorite requests by reacting :+1: to the top comment of the request.

To add new feature requests, open a new Issue of the &quot;Feature Request&quot; type.

### Translation

Make Actual Budget accessible to more people by helping with the [Internationalization](https://actualbudget.org/docs/contributing/i18n/) of Actual. We are using a crowd sourcing tool to manage the translations, see our [Weblate Project](https://hosted.weblate.org/projects/actualbudget/). Weblate proudly supports open-source software projects through their [Libre plan](https://weblate.org/en/hosting/#libre).

&lt;a href=&quot;https://hosted.weblate.org/engage/actualbudget/&quot;&gt;
&lt;img src=&quot;https://hosted.weblate.org/widget/actualbudget/actual/287x66-grey.png&quot; alt=&quot;Translation status&quot; /&gt;
&lt;/a&gt;

## Repo Activity

![Alt](https://repobeats.axiom.co/api/embed/e20537dd8b74956f86736726ccfbc6f0565bec22.svg &#039;Repobeats analytics image&#039;)

## Sponsors

Thanks to our wonderful sponsors who make Actual Budget possible!

&lt;a href=&quot;https://www.netlify.com&quot;&gt; &lt;img src=&quot;https://www.netlify.com/v3/img/components/netlify-color-accent.svg&quot; alt=&quot;Deploys by Netlify&quot; /&gt; &lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[linkwarden/linkwarden]]></title>
            <link>https://github.com/linkwarden/linkwarden</link>
            <guid>https://github.com/linkwarden/linkwarden</guid>
            <pubDate>Tue, 19 Aug 2025 00:04:30 GMT</pubDate>
            <description><![CDATA[⚡️⚡️⚡️ Self-hosted collaborative bookmark manager to collect, read, annotate, and fully preserve what matters, all in one place.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/linkwarden/linkwarden">linkwarden/linkwarden</a></h1>
            <p>⚡️⚡️⚡️ Self-hosted collaborative bookmark manager to collect, read, annotate, and fully preserve what matters, all in one place.</p>
            <p>Language: TypeScript</p>
            <p>Stars: 14,308</p>
            <p>Forks: 557</p>
            <p>Stars today: 21 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;./assets/logo.png&quot; width=&quot;100px&quot; /&gt;
  &lt;h1&gt;Linkwarden&lt;/h1&gt;
  &lt;h3&gt;Bookmarks, Evolved&lt;/h3&gt;

&lt;a href=&quot;https://discord.com/invite/CtuYV47nuJ&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/1117993124669702164?logo=discord&amp;style=flat&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://twitter.com/LinkwardenHQ&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/linkwarden&quot; alt=&quot;Twitter&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://news.ycombinator.com/item?id=36942308&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Hacker%20News-280-%23FF6600&quot;&gt;&lt;/img&gt;&lt;/a&gt;

&lt;a href=&quot;https://github.com/linkwarden/linkwarden/releases&quot;&gt;&lt;img alt=&quot;GitHub release&quot; src=&quot;https://img.shields.io/github/v/release/linkwarden/linkwarden&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://crowdin.com/project/linkwarden&quot;&gt;
&lt;img src=&quot;https://badges.crowdin.net/linkwarden/localized.svg&quot; alt=&quot;Crowdin&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://opencollective.com/linkwarden&quot;&gt;&lt;img src=&quot;https://img.shields.io/opencollective/all/linkwarden&quot; alt=&quot;Open Collective&quot;&gt;&lt;/a&gt;

&lt;/div&gt;

&lt;div align=&#039;center&#039;&gt;

[« LAUNCH DEMO »](https://demo.linkwarden.app)

[Cloud](https://cloud.linkwarden.app) · [Website](https://linkwarden.app) · [Features](https://github.com/linkwarden/linkwarden#features) · [Docs](https://docs.linkwarden.app)

&lt;img src=&quot;./assets/home.png&quot; /&gt;

&lt;/div&gt;

## Intro &amp; motivation

**Linkwarden is a self-hosted, open-source collaborative bookmark manager to collect, read, annotate, and fully preserve what matters, all in one place.**

The objective is to organize useful webpages and articles you find across the web in one place, and since useful webpages can go away (see the inevitability of [Link Rot](https://en.wikipedia.org/wiki/Link_rot)), Linkwarden also saves a copy of each webpage as a Screenshot and PDF, ensuring accessibility even if the original content is no longer available.

In addition to preservation, Linkwarden provides a user-friendly reading and annotation experience that blends the simplicity of a “read-it-later” tool with the reliability of a web archive. Whether you’re highlighting key ideas, jotting down thoughts, or revisiting content long after it’s disappeared from the web, Linkwarden keeps your knowledge accessible and organized.

Linkwarden is also designed with collaboration in mind, enabling you to share links with the public and/or collaborate seamlessly with multiple users.

&gt; [!TIP]  
&gt; Our official [Cloud](https://linkwarden.app/#pricing) offering provides the simplest way to begin using Linkwarden and it&#039;s the preferred choice for many due to its time-saving benefits. &lt;br&gt; Your subscription supports our hosting infrastructure and ongoing development. &lt;br&gt; Alternatively, if you prefer self-hosting Linkwarden, you can do so by following our [Installation documentation](https://docs.linkwarden.app/self-hosting/installation).

## Features

- 📸 Auto capture a screenshot, PDF, and single html file of each webpage.
- 📖 Reader view of the webpage, with the ability to highlight and annotate text.
- 🏛️ Send your webpage to Wayback Machine ([archive.org](https://archive.org)) for a snapshot. (Optional)
- ✨ Local AI Tagging to automatically tag your links based on their content (Optional).
- 📂 Organize links by collection, sub-collection, name, description and multiple tags.
- 👥 Collaborate on gathering links in a collection.
- 🎛️ Customize the permissions of each member.
- 🌐 Share your collected links and preserved formats with the world.
- 📌 Pin your favorite links to dashboard.
- 🔍 Full text search, filter and sort for easy retrieval.
- 📱 Responsive design and supports most modern browsers.
- 🌓 Dark/Light mode support.
- 🧩 Browser extension. [Star it here!](https://github.com/linkwarden/browser-extension)
- 🔄 Browser Synchronization (using [Floccus](https://floccus.org)!)
- ⬇️ Import and export your bookmarks.
- 🔐 SSO integration. (Enterprise and Self-hosted users only)
- 📦 Installable Progressive Web App (PWA).
- 🍎 iOS Shortcut to save Links to Linkwarden.
- 🔑 API keys.
- ✅ Bulk actions.
- 👥 User administration.
- 🌐 Support for Other Languages (i18n).
- 📁 Image and PDF Uploads.
- 🎨 Custom Icons for Links and Collections.
- 🔔 RSS Feed Subscription.
- ✨ And many more features. (Literally!)

## Like what we&#039;re doing? Give us a Star ⭐

![Star Us](https://raw.githubusercontent.com/linkwarden/linkwarden/main/assets/star_repo.gif)

## We&#039;re building our Community 🌐

Join and follow us in the following platforms to stay up to date about the most recent features and for support:

&lt;a href=&quot;https://discord.com/invite/CtuYV47nuJ&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/1117993124669702164?logo=discord&amp;style=flat&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt;

&lt;a href=&quot;https://twitter.com/LinkwardenHQ&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/linkwarden&quot; alt=&quot;Twitter&quot;&gt;&lt;/a&gt;

&lt;a href=&quot;https://fosstodon.org/@linkwarden&quot;&gt;&lt;img src=&quot;https://img.shields.io/mastodon/follow/110748840237143200?domain=https%3A%2F%2Ffosstodon.org&quot; alt=&quot;Mastodon&quot;&gt;&lt;/a&gt;

## Suggestions

We _usually_ go after the [popular suggestions](https://github.com/linkwarden/linkwarden/issues?q=is%3Aissue%20is%3Aopen%20sort%3Areactions-%2B1-desc). Feel free to open a [new issue](https://github.com/linkwarden/linkwarden/issues/new?assignees=&amp;labels=enhancement&amp;projects=&amp;template=feature_request.md&amp;title=) to suggest one - others might be interested too! :)

## Roadmap

Make sure to check out our [public roadmap](https://github.com/orgs/linkwarden/projects/1).

## Community Projects

Here are some community-maintained projects that are built around Linkwarden:

- [My Links](https://apps.apple.com/ca/app/my-links-for-linkwarden/id6504573402) - iOS and MacOS Apps, maintained by [JGeek00](https://github.com/JGeek00).
- [LinkDroid](https://fossdroid.com/a/linkdroid-for-linkwarden.html) - Android App with share sheet integration, [source code](https://github.com/Dacid99/LinkDroid-for-Linkwarden).
- [LinkGuardian](https://github.com/Elbullazul/LinkGuardian) - An Android client for Linkwarden. Built with Kotlin and Jetpack compose.
- [StarWarden](https://github.com/rtuszik/starwarden) - A browser extension to save your starred GitHub repositories to Linkwarden.

## Development

If you want to contribute, Thanks! Start by choosing one of our [popular suggestions](https://github.com/linkwarden/linkwarden/issues?q=is%3Aissue%20is%3Aopen%20sort%3Areactions-%2B1-desc), just please stay in touch with [@daniel31x13](https://github.com/daniel31x13) before starting.

# Translations

If you want to help us translate Linkwarden to your language, please check out our [Crowdin page](https://crowdin.com/project/linkwarden) and start translating. We would love to have your help!

To start translating a new language, please create an issue so we can set it up for you. New languages will be added once they reach at least 50% translation completion.

&lt;a href=&quot;https://crowdin.com/project/linkwarden&quot;&gt;
&lt;img src=&quot;https://badges.crowdin.net/linkwarden/localized.svg&quot; alt=&quot;Crowdin&quot; /&gt;&lt;/a&gt;

## Security

If you found a security vulnerability, please do **not** create a public issue, instead send an email to [security@linkwarden.app](mailto:security@linkwarden.app) stating the vulnerability. Thanks!

## Support &lt;3

Other than using our official [Cloud](https://linkwarden.app/#pricing) offering, any [donations](https://opencollective.com/linkwarden) are highly appreciated as well!

Here are the other ways to support/cheer this project:

- Starring this repository.
- Joining us on [Discord](https://discord.com/invite/CtuYV47nuJ).
- Referring Linkwarden to a friend.

If you did any of the above, Thanksss! Otherwise thanks.

## Thanks to All the Contributors 💪

Huge thanks to these guys for spending their time helping Linkwarden grow. They rock! ⚡️

&lt;img src=&quot;https://contributors-img.web.app/image?repo=linkwarden/linkwarden&quot; alt=&quot;Contributors&quot;/&gt;
</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
    </channel>
</rss>