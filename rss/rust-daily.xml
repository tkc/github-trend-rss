<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for rust - Rust Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for rust.</description>
        <lastBuildDate>Thu, 07 Aug 2025 00:06:13 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[nautechsystems/nautilus_trader]]></title>
            <link>https://github.com/nautechsystems/nautilus_trader</link>
            <guid>https://github.com/nautechsystems/nautilus_trader</guid>
            <pubDate>Thu, 07 Aug 2025 00:06:13 GMT</pubDate>
            <description><![CDATA[A high-performance algorithmic trading platform and event-driven backtester]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/nautechsystems/nautilus_trader">nautechsystems/nautilus_trader</a></h1>
            <p>A high-performance algorithmic trading platform and event-driven backtester</p>
            <p>Language: Rust</p>
            <p>Stars: 11,567</p>
            <p>Forks: 1,328</p>
            <p>Stars today: 218 stars today</p>
            <h2>README</h2><pre># &lt;img src=&quot;https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-trader-logo.png&quot; width=&quot;500&quot;&gt;

[![codecov](https://codecov.io/gh/nautechsystems/nautilus_trader/branch/master/graph/badge.svg?token=DXO9QQI40H)](https://codecov.io/gh/nautechsystems/nautilus_trader)
[![codspeed](https://img.shields.io/endpoint?url=https://codspeed.io/badge.json)](https://codspeed.io/nautechsystems/nautilus_trader)
![pythons](https://img.shields.io/pypi/pyversions/nautilus_trader)
![pypi-version](https://img.shields.io/pypi/v/nautilus_trader)
![pypi-format](https://img.shields.io/pypi/format/nautilus_trader?color=blue)
[![Downloads](https://pepy.tech/badge/nautilus-trader)](https://pepy.tech/project/nautilus-trader)
[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;logoColor=white)](https://discord.gg/NautilusTrader)

| Branch    | Version                                                                                                                                                                                                                     | Status                                                                                                                                                                                            |
| :-------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `master`  | [![version](https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fmaster%2Fversion.json)](https://packages.nautechsystems.io/simple/nautilus-trader/index.html)  | [![build](https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=nightly)](https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml) |
| `nightly` | [![version](https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fnightly%2Fversion.json)](https://packages.nautechsystems.io/simple/nautilus-trader/index.html) | [![build](https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=nightly)](https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml) |
| `develop` | [![version](https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fdevelop%2Fversion.json)](https://packages.nautechsystems.io/simple/nautilus-trader/index.html) | [![build](https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=develop)](https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml) |

| Platform           | Rust   | Python     |
| :----------------- | :----- | :--------- |
| `Linux (x86_64)`   | 1.88.0 | 3.11-3.13  |
| `Linux (ARM64)`    | 1.88.0 | 3.11-3.13  |
| `macOS (ARM64)`    | 1.88.0 | 3.11-3.13  |
| `Windows (x86_64)` | 1.88.0 | 3.11-3.13* |

\* Windows builds are currently pinned to CPython 3.13.2, see [installation guide](https://github.com/nautechsystems/nautilus_trader/blob/develop/docs/getting_started/installation.md).

- **Docs**: &lt;https://nautilustrader.io/docs/&gt;
- **Website**: &lt;https://nautilustrader.io&gt;
- **Support**: [support@nautilustrader.io](mailto:support@nautilustrader.io)

## Introduction

NautilusTrader is an open-source, high-performance, production-grade algorithmic trading platform,
providing quantitative traders with the ability to backtest portfolios of automated trading strategies
on historical data with an event-driven engine, and also deploy those same strategies live, with no code changes.

The platform is *AI-first*, designed to develop and deploy algorithmic trading strategies within a highly performant
and robust Python-native environment. This helps to address the parity challenge of keeping the Python research/backtest
environment consistent with the production live trading environment.

NautilusTrader&#039;s design, architecture, and implementation philosophy prioritizes software correctness and safety at the
highest level, with the aim of supporting Python-native, mission-critical, trading system backtesting
and live deployment workloads.

The platform is also universal, and asset-class-agnostic ‚Äî  with any REST API or WebSocket feed able to be integrated via modular
adapters. It supports high-frequency trading across a wide range of asset classes and instrument types
including FX, Equities, Futures, Options, Crypto and Betting, enabling seamless operations across multiple venues simultaneously.

![nautilus-trader](https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-trader.png &quot;nautilus-trader&quot;)

## Features

- **Fast**: Core is written in Rust with asynchronous networking using [tokio](https://crates.io/crates/tokio).
- **Reliable**: Rust-powered type- and thread-safety, with optional Redis-backed state persistence.
- **Portable**: OS independent, runs on Linux, macOS, and Windows. Deploy using Docker.
- **Flexible**: Modular adapters mean any REST API or WebSocket feed can be integrated.
- **Advanced**: Time in force `IOC`, `FOK`, `GTC`, `GTD`, `DAY`, `AT_THE_OPEN`, `AT_THE_CLOSE`, advanced order types and conditional triggers. Execution instructions `post-only`, `reduce-only`, and icebergs. Contingency orders including `OCO`, `OUO`, `OTO`.
- **Customizable**: Add user-defined custom components, or assemble entire systems from scratch leveraging the [cache](https://nautilustrader.io/docs/latest/concepts/cache) and [message bus](https://nautilustrader.io/docs/latest/concepts/message_bus).
- **Backtesting**: Run with multiple venues, instruments and strategies simultaneously using historical quote tick, trade tick, bar, order book and custom data with nanosecond resolution.
- **Live**: Use identical strategy implementations between backtesting and live deployments.
- **Multi-venue**: Multiple venue capabilities facilitate market-making and statistical arbitrage strategies.
- **AI Training**: Backtest engine fast enough to be used to train AI trading agents (RL/ES).

![Alt text](https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-art.png &quot;nautilus&quot;)

&gt; *nautilus - from ancient Greek &#039;sailor&#039; and naus &#039;ship&#039;.*
&gt;
&gt; *The nautilus shell consists of modular chambers with a growth factor which approximates a logarithmic spiral.
&gt; The idea is that this can be translated to the aesthetics of design and architecture.*

## Why NautilusTrader?

- **Highly performant event-driven Python**: Native binary core components.
- **Parity between backtesting and live trading**: Identical strategy code.
- **Reduced operational risk**: Enhanced risk management functionality, logical accuracy, and type safety.
- **Highly extendable**: Message bus, custom components and actors, custom data, custom adapters.

Traditionally, trading strategy research and backtesting might be conducted in Python
using vectorized methods, with the strategy then needing to be reimplemented in a more event-driven way
using C++, C#, Java or other statically typed language(s). The reasoning here is that vectorized backtesting code cannot
express the granular time and event dependent complexity of real-time trading, where compiled languages have
proven to be more suitable due to their inherently higher performance, and type safety.

One of the key advantages of NautilusTrader here, is that this reimplementation step is now circumvented - as the critical core components of the platform
have all been written entirely in [Rust](https://www.rust-lang.org/) or [Cython](https://cython.org/).
This means we&#039;re using the right tools for the job, where systems programming languages compile performant binaries,
with CPython C extension modules then able to offer a Python-native environment, suitable for professional quantitative traders and trading firms.

## Why Python?

Python was originally created decades ago as a simple scripting language with a clean straightforward syntax.
It has since evolved into a fully fledged general purpose object-oriented programming language.
Based on the TIOBE index, Python is currently the most popular programming language in the world.
Not only that, Python has become the *de facto lingua franca* of data science, machine learning, and artificial intelligence.

developer/user communities.
However, Python has performance and typing limitations for large-scale, latency-sensitive systems. Cython addresses many of these issues by introducing static typing into Python&#039;s rich ecosystem of libraries and communities.

## Why Rust?

[Rust](https://www.rust-lang.org/) is a multi-paradigm programming language designed for performance and safety, especially safe
concurrency. Rust is &quot;blazingly fast&quot; and memory-efficient (comparable to C and C++) with no garbage collector.
It can power mission-critical systems, run on embedded devices, and easily integrates with other languages.

Rust‚Äôs rich type system and ownership model guarantees memory-safety and thread-safety deterministically ‚Äî
eliminating many classes of bugs at compile-time.

The project increasingly utilizes Rust for core performance-critical components. Python bindings are implemented via Cython and [PyO3](https://pyo3.rs)‚Äîno Rust toolchain is required at install time.

This project makes the [Soundness Pledge](https://raphlinus.github.io/rust/2020/01/18/soundness-pledge.html):

&gt; ‚ÄúThe intent of this project is to be free of soundness bugs.
&gt; The developers will do their best to avoid them, and welcome help in analyzing and fixing them.‚Äù

&gt; [!NOTE]
&gt;
&gt; **MSRV:** NautilusTrader relies heavily on improvements in the Rust language and compiler.
&gt; As a result, the Minimum Supported Rust Version (MSRV) is generally equal to the latest stable release of Rust.

## Integrations

NautilusTrader is modularly designed to work with *adapters*, enabling connectivity to trading venues
and data providers by translating their raw APIs into a unified interface and normalized domain model.

The following integrations are currently supported; see [docs/integrations/](https://nautilustrader.io/docs/latest/integrations/) for details:

| Name                                                                         | ID                    | Type                    | Status                                                  | Docs                                        |
| :--------------------------------------------------------------------------- | :-------------------- | :---------------------- | :------------------------------------------------------ | :------------------------------------------ |
| [Betfair](https://betfair.com)                                               | `BETFAIR`             | Sports Betting Exchange | ![status](https://img.shields.io/badge/stable-green)    | [Guide](docs/integrations/betfair.md)       |
| [Binance](https://binance.com)                                               | `BINANCE`             | Crypto Exchange (CEX)   | ![status](https://img.shields.io/badge/stable-green)    | [Guide](docs/integrations/binance.md)       |
| [Binance US](https://binance.us)                                             | `BINANCE`             | Crypto Exchange (CEX)   | ![status](https://img.shields.io/badge/stable-green)    | [Guide](docs/integrations/binance.md)       |
| [Binance Futures](https://www.binance.com/en/futures)                        | `BINANCE`             | Crypto Exchange (CEX)   | ![status](https://img.shields.io/badge/stable-green)    | [Guide](docs/integrations/binance.md)       |
| [Bybit](https://www.bybit.com)                                               | `BYBIT`               | Crypto Exchange (CEX)   | ![status](https://img.shields.io/badge/stable-green)    | [Guide](docs/integrations/bybit.md)         |
| [Coinbase International](https://www.coinbase.com/en/international-exchange) | `COINBASE_INTX`       | Crypto Exchange (CEX)   | ![status](https://img.shields.io/badge/stable-green)    | [Guide](docs/integrations/coinbase_intx.md) |
| [Databento](https://databento.com)                                           | `DATABENTO`           | Data Provider           | ![status](https://img.shields.io/badge/stable-green)    | [Guide](docs/integrations/databento.md)     |
| [dYdX](https://dydx.exchange/)                                               | `DYDX`                | Crypto Exchange (DEX)   | ![status](https://img.shields.io/badge/stable-green)    | [Guide](docs/integrations/dydx.md)          |
| [Interactive Brokers](https://www.interactivebrokers.com)                    | `INTERACTIVE_BROKERS` | Brokerage (multi-venue) | ![status](https://img.shields.io/badge/stable-green)    | [Guide](docs/integrations/ib.md)            |
| [OKX](https://okx.com)                                                       | `OKX`                 | Crypto Exchange (CEX)   | ![status](https://img.shields.io/badge/building-orange) | [Guide](docs/integrations/okx.md)           |
| [Polymarket](https://polymarket.com)                                         | `POLYMARKET`          | Prediction Market (DEX) | ![status](https://img.shields.io/badge/stable-green)    | [Guide](docs/integrations/polymarket.md)    |
| [Tardis](https://tardis.dev)                                                 | `TARDIS`              | Crypto Data Provider    | ![status](https://img.shields.io/badge/stable-green)    | [Guide](docs/integrations/tardis.md)        |

- **ID**: The default client ID for the integrations adapter clients.
- **Type**: The type of integration (often the venue type).

### Status

- `building`: Under construction and likely not in a usable state.
- `beta`: Completed to a minimally working state and in a beta testing phase.
- `stable`: Stabilized feature set and API, the integration has been tested by both developers and users to a reasonable level (some bugs may still remain).

See the [Integrations](https://nautilustrader.io/docs/latest/integrations/index.html) documentation for further details.

## Versioning and releases

**NautilusTrader is still under active development**. Some features may be incomplete, and while
the API is becoming more stable, breaking changes can occur between releases.
We strive to document these changes in the release notes on a **best-effort basis**.

We aim to follow a **bi-weekly release schedule**, though experimental or larger features may cause delays.

### Branches

We aim to maintain a stable, passing build across all branches.

- `master`: Reflects the source code for the latest released version; recommended for production use.
- `nightly`: Daily snapshots of the `develop` branch for early testing; merged at **14:00 UTC** or on demand.
- `develop`: Active development branch for contributors and feature work.

&gt; [!NOTE]
&gt;
&gt; Our [roadmap](/ROADMAP.md) aims to achieve a **stable API for version 2.x** (likely after the Rust port).
&gt; Once this milestone is reached, we plan to implement a formal deprecation process for any API changes.
&gt; This approach allows us to maintain a rapid development pace for now.

## Precision mode

NautilusTrader supports two precision modes for its core value types (`Price`, `Quantity`, `Money`),
which differ in their internal bit-width and maximum decimal precision.

- **High-precision**: 128-bit integers with up to 16 decimals of precision, and a larger value range.
- **Standard-precision**: 64-bit integers with up to 9 decimals of precision, and a smaller value range.

&gt; [!NOTE]
&gt;
&gt; By default, the official Python wheels **ship** in high-precision (128-bit) mode on Linux and macOS.
&gt; On Windows, only standard-precision (64-bit) is available due to the lack of native 128-bit integer support.
&gt; For the Rust crates, the default is standard-precision unless you explicitly enable the `high-precision` feature flag.

See the [Installation Guide](https://nautilustrader.io/docs/latest/getting_started/installation) for further details.

**Rust feature flag**: To enable high-precision mode in Rust, add the `high-precision` feature to your Cargo.toml:

```toml
[dependencies]
nautilus_model = { version = &quot;*&quot;, features = [&quot;high-precision&quot;] }
```

## Installation

We recommend using the latest supported version of Python and installing [nautilus_trader](https://pypi.org/project/nautilus_trader/) inside a virtual environment to isolate dependencies.

**There are two supported ways to install**:

1. Pre-built binary wheel from PyPI *or* the Nautech Systems package index.
2. Build from source.

&gt; [!TIP]
&gt;
&gt; We highly recommend installing using the [uv](https://docs.astral.sh/uv) package manager with a &quot;vanilla&quot; CPython.
&gt;
&gt; Conda and other Python distributions *may* work but aren‚Äôt officially supported.

### From PyPI

To install the latest binary wheel (or sdist package) from PyPI using Python&#039;s pip package manager:

```bash
pip install -U nautilus_trader
```

### From the Nautech Systems package index

The Nautech Systems package index (`packages.nautechsystems.io`) is [PEP-503](https://peps.python.org/pep-0503/) compliant and hosts both stable and development binary wheels for `nautilus_trader`.
This enables users to install either the latest stable release or pre-release versions for testing.

#### Stable wheels

Stable wheels correspond to official releases of `nautilus_trader` on PyPI, and use standard versioning.

To install the latest stable release:

```bash
pip install -U nautilus_trader --index-url=https://packages.nautechsystems.io/simple
```

#### Development wheels

Development wheels are published from both the `nightly` and `develop` branches,
allowing users to test features and fixes ahead of stable releases.

**Note**: Wheels from the `develop` branch are only built for the Linux x86_64 platform to save time
and compute resources, while `nightly` wheels support additional platforms as shown below.

| Platform           | Nightly | Develop |
| :----------------- | :------ | :------ |
| `Linux (x86_64)`   | ‚úì       | ‚úì       |
| `Linux (ARM64)`    | ‚úì       | -       |
| `macOS (ARM64)`    | ‚úì       | -       |
| `Windows (x86_64)` | ‚úì       | -       |

This process also helps preserve compute resources and ensures easy access to the exact binaries tested in CI pipelines,
while adhering to [PEP-440](https://peps.python.org/pep-0440/) versioning standards:

- `develop` wheels use the version format `dev{date}+{build_number}` (e.g., `1.208.0.dev20241212+7001`).
- `nightly` wheels use the version format `a{date}` (alpha) (e.g., `1.208.0a20241212`).

&gt; [!WARNING]
&gt;
&gt; We don&#039;t recommend using development wheels in production environments, such as live trading controlling real capital.

#### Installation commands

By default, pip installs the latest stable release. Adding the `--pre` flag ensures that pre-release versions, including development wheels, are considered.

To install the latest available pre-release (including development wheels):

```bash
pip install -U nautilus_trader --pre --index-url=https://packages.nautechsystems.io/simple
```

To install a specific development wheel (e.g., `1.208.0a20241212` for December 12, 2024):

```bash
pip install nautilus_trader==1.208.0a20241212 --index-url=https://packages.nautechsystems.io/simple
```

#### Available versions

You can view all available versions of `nautilus_trader` on the [package index](https://packages.nautechsystems.io/simple/nautilus-trader/index.html).

To programmatically fetch and list available versions:

```bash
curl -s https://packages.nautechsystems.io/simple/nautilus-trader/index.html | grep -oP &#039;(?&lt;=&lt;a href=&quot;)[^&quot;]+(?=&quot;)&#039; | awk -F&#039;#&#039; &#039;{print $1}&#039; | sort
```

#### Branch updates

- `develop` bran

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[category-labs/monad-bft]]></title>
            <link>https://github.com/category-labs/monad-bft</link>
            <guid>https://github.com/category-labs/monad-bft</guid>
            <pubDate>Thu, 07 Aug 2025 00:06:12 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/category-labs/monad-bft">category-labs/monad-bft</a></h1>
            <p></p>
            <p>Language: Rust</p>
            <p>Stars: 203</p>
            <p>Forks: 60</p>
            <p>Stars today: 21 stars today</p>
            <h2>README</h2><pre># Monad BFT

![Nightly Tests][tests-badge]

## Overview

This repository contains implementation for the Monad consensus client and JsonRpc server. Monad consensus collects transactions and produces blocks which are written to a ledger filestream. These blocks are consumed by Monad execution, which then updates the state of the blockchain. The [triedb](monad-triedb/README.md) is a database which stores block information and the blockchain state.

## Getting Started

```sh
git submodule update --init --recursive
```

### Using Docker

The most straightforward way to start a consensus client + an execution client + a JsonRpc server. Run the following:
1. `cd docker/single-node`
2. `nets/run.sh`

### Using Cargo

To run a Monad consensus client, follow instructions [here](monad-node/README.md).
 
To run a JsonRpc server, follow instructions [here](monad-rpc/README.md).

## Architecture

```mermaid
sequenceDiagram
autonumber
    participant D as Driver
    box Purple Executor
    participant S as impl Stream
    participant E as impl Executor
    end
    participant State
    participant PersistenceLogger
    loop
    D -&gt;&gt;+ S: CALL next()
    Note over S: blocks until event ready
    S --&gt;&gt;- D: RETURN Event
    D -&gt;&gt; PersistenceLogger: CALL push(Event)
    D -&gt;&gt;+ State: CALL update(Event)
    Note over State: mutate state
    State --&gt;&gt;- D: RETURN Vec&lt;Command&gt;
    D -&gt;&gt; E: CALL exec(Vec&lt;Command&gt;)
    Note over E: apply side effects
    end
```

[tests-badge]: https://github.com/monad-crypto/monad-bft/actions/workflows/randomized.yml/badge.svg?branch=master
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[zellij-org/zellij]]></title>
            <link>https://github.com/zellij-org/zellij</link>
            <guid>https://github.com/zellij-org/zellij</guid>
            <pubDate>Thu, 07 Aug 2025 00:06:11 GMT</pubDate>
            <description><![CDATA[A terminal workspace with batteries included]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/zellij-org/zellij">zellij-org/zellij</a></h1>
            <p>A terminal workspace with batteries included</p>
            <p>Language: Rust</p>
            <p>Stars: 25,557</p>
            <p>Forks: 782</p>
            <p>Stars today: 35 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
  &lt;br&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/zellij-org/zellij/main/assets/logo.png&quot; alt=&quot;logo&quot; width=&quot;200&quot;&gt;
  &lt;br&gt;
  Zellij
  &lt;br&gt;
  &lt;br&gt;
&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://discord.gg/CrUAFH3&quot;&gt;&lt;img alt=&quot;Discord Chat&quot; src=&quot;https://img.shields.io/discord/771367133715628073?color=5865F2&amp;label=discord&amp;style=flat-square&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://matrix.to/#/#zellij_general:matrix.org&quot;&gt;&lt;img alt=&quot;Matrix Chat&quot; src=&quot;https://img.shields.io/matrix/zellij_general:matrix.org?color=1d7e64&amp;label=matrix%20chat&amp;style=flat-square&amp;logo=matrix&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://zellij.dev/documentation/&quot;&gt;&lt;img alt=&quot;Zellij documentation&quot; src=&quot;https://img.shields.io/badge/zellij-documentation-fc0060?style=flat-square&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/zellij-org/zellij/main/assets/demo.gif&quot; alt=&quot;demo&quot;&gt;
&lt;/p&gt;

&lt;h4 align=&quot;center&quot;&gt;
  [&lt;a href=&quot;https://zellij.dev/documentation/installation&quot;&gt;Installation&lt;/a&gt;]
  [&lt;a href=&quot;https://zellij.dev/screencasts/&quot;&gt;Screencasts &amp; Tutorials&lt;/a&gt;]
  [&lt;a href=&quot;https://zellij.dev/documentation/configuration&quot;&gt;Configuration&lt;/a&gt;]
  [&lt;a href=&quot;https://zellij.dev/documentation/layouts&quot;&gt;Layouts&lt;/a&gt;]
  [&lt;a href=&quot;https://zellij.dev/documentation/faq&quot;&gt;FAQ&lt;/a&gt;]
&lt;/h4&gt;

# What is this?

[Zellij](#origin-of-the-name) is a workspace aimed at developers, ops-oriented people and anyone who loves the terminal. Similar programs are sometimes called &quot;Terminal Multiplexers&quot;.

Zellij is designed around the philosophy that one must not sacrifice simplicity for power, taking pride in its great experience out of the box as well as the advanced features it places at its users&#039; fingertips.

Zellij is geared toward beginner and power users alike - allowing deep customizability, personal automation through [layouts](https://zellij.dev/documentation/layouts.html), true multiplayer collaboration, unique UX features such as floating and stacked panes, and a [plugin system](https://zellij.dev/documentation/plugins.html) allowing one to create plugins in any language that compiles to WebAssembly.

You can get started by [installing](https://zellij.dev/documentation/installation.html) Zellij and checking out the [Screencasts &amp; Tutorials](https://zellij.dev/screencasts/).

For more details about our future plans, read about upcoming features in our [roadmap](#roadmap).

## How do I install it?

The easiest way to install Zellij is through a [package for your OS](./docs/THIRD_PARTY_INSTALL.md).

If one is not available for your OS, you could download a prebuilt binary from the [latest release](https://github.com/zellij-org/zellij/releases/latest) and place it in your `$PATH`. If you&#039;d like, we could [automatically choose one for you](#try-zellij-without-installing).

You can also install (compile) with `cargo`:

```
cargo install --locked zellij
```

#### Try Zellij without installing

bash/zsh:
```bash
bash &lt;(curl -L https://zellij.dev/launch)
```
fish/xonsh:
```bash
bash -c &#039;bash &lt;(curl -L https://zellij.dev/launch)&#039;
```

#### Installing from `main`
Installing Zellij from the `main` branch is not recommended. This branch represents pre-release code, is constantly being worked on and may contain broken or unusable features. In addition, using it may corrupt the cache for future versions, forcing users to clear it before they can use the officially released version.

That being said - no-one will stop you from using it (and bug reports involving new features are greatly appreciated), but please consider using the latest release instead as detailed at the top of this section.

## How do I start a development environment?

* Clone the project
* In the project folder, for debug builds run: `cargo xtask run`
* To run all tests: `cargo xtask test`

For more build commands, see [CONTRIBUTING.md](CONTRIBUTING.md).

## Configuration
For configuring Zellij, please see the [Configuration Documentation](https://zellij.dev/documentation/configuration.html).

## About issues in this repository
Issues in this repository, whether open or closed, do not necessarily indicate a problem or a bug in the software. They only indicate that the reporter wanted to communicate their experiences or thoughts to the maintainers. The Zellij maintainers do their best to go over and reply to all issue reports, but unfortunately cannot promise these will always be dealt with or even read. Your understanding is appreciated.

## Roadmap
Presented here is the project roadmap, divided into three main sections.

These are issues that are either being actively worked on or are planned for the near future.

***If you&#039;ll click on the image, you&#039;ll be led to an SVG version of it on the website where you can directly click on every issue***

[![roadmap](https://github.com/zellij-org/zellij/assets/795598/9c5b573b-20f5-41c6-908b-6b21c5fd456e)](https://zellij.dev/roadmap)

## Origin of the Name
[From Wikipedia, the free encyclopedia](https://en.wikipedia.org/wiki/Zellij)

Zellij (Arabic: ÿßŸÑÿ≤ŸÑŸäÿ¨, romanized: zillƒ´j; also spelled zillij or zellige) is a style of mosaic tilework made from individually hand-chiseled tile pieces. The pieces were typically of different colours and fitted together to form various patterns on the basis of tessellations, most notably elaborate Islamic geometric motifs such as radiating star patterns composed of various polygons. This form of Islamic art is one of the main characteristics of architecture in the western Islamic world. It is found in the architecture of Morocco, the architecture of Algeria, early Islamic sites in Tunisia, and in the historic monuments of al-Andalus (in the Iberian Peninsula).

## License

MIT

## Sponsored by
&lt;a href=&quot;https://terminaltrove.com/&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/121595180?s=200&amp;v=4&quot; width=&quot;80px&quot;&gt;&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[lumina-ai-inc/chunkr]]></title>
            <link>https://github.com/lumina-ai-inc/chunkr</link>
            <guid>https://github.com/lumina-ai-inc/chunkr</guid>
            <pubDate>Thu, 07 Aug 2025 00:06:10 GMT</pubDate>
            <description><![CDATA[Vision infrastructure to turn complex documents into RAG/LLM-ready data]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lumina-ai-inc/chunkr">lumina-ai-inc/chunkr</a></h1>
            <p>Vision infrastructure to turn complex documents into RAG/LLM-ready data</p>
            <p>Language: Rust</p>
            <p>Stars: 2,699</p>
            <p>Forks: 162</p>
            <p>Stars today: 45 stars today</p>
            <h2>README</h2><pre>&lt;br /&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/lumina-ai-inc/chunkr&quot;&gt;
    &lt;img src=&quot;images/logo.svg&quot; alt=&quot;Logo&quot; width=&quot;80&quot; height=&quot;80&quot;&gt;
  &lt;/a&gt;

&lt;h3 align=&quot;center&quot;&gt;Chunkr | Open Source Document Intelligence API&lt;/h3&gt;

  &lt;p align=&quot;center&quot;&gt;
    Production-ready API service for document layout analysis, OCR, and semantic chunking.&lt;br /&gt;Convert PDFs, PPTs, Word docs &amp; images into RAG/LLM-ready chunks.
    &lt;br /&gt;&lt;br /&gt;
    &lt;b&gt;Layout Analysis&lt;/b&gt; | &lt;b&gt;OCR + Bounding Boxes&lt;/b&gt; | &lt;b&gt;Structured HTML and markdown&lt;/b&gt; | &lt;b&gt;VLM Processing controls&lt;/b&gt;
    &lt;br /&gt;
    &lt;br /&gt;
    &lt;a href=&quot;https://www.chunkr.ai&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Try_it_out-chunkr.ai-blue?style=flat&amp;logo=rocket&amp;height=20&quot; alt=&quot;Try it out&quot; height=&quot;20&quot;&gt;&lt;/a&gt;
    &amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://github.com/lumina-ai-inc/chunkr/issues/new&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Report_Bug-GitHub_Issues-red?style=flat&amp;logo=github&amp;height=20&quot; alt=&quot;Report Bug&quot; height=&quot;20&quot;&gt;&lt;/a&gt;
    &amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;#connect-with-us&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Contact-Get_in_Touch-green?style=flat&amp;logo=mail&amp;height=20&quot; alt=&quot;Contact&quot; height=&quot;20&quot;&gt;&lt;/a&gt;
    &amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://discord.gg/XzKWFByKzW&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join_Community-5865F2?style=flat&amp;logo=discord&amp;logoColor=white&amp;height=20&quot; alt=&quot;Discord&quot; height=&quot;20&quot;&gt;&lt;/a&gt;
    &amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://deepwiki.com/lumina-ai-inc/chunkr&quot;&gt;&lt;img src=&quot;https://deepwiki.com/badge.svg&quot; alt=&quot;Ask DeepWiki&quot;&gt;&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.chunkr.ai&quot; width=&quot;1200&quot; height=&quot;630&quot;&gt;
    &lt;img src=&quot;https://chunkr.ai/og-image.png&quot; style=&quot;bor&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

## Table of Contents
- [Table of Contents](#table-of-contents)
- [(Super) Quick Start](#super-quick-start)
- [Documentation](#documentation)
- [OpenSource vs Commercial API vs Enterprise](#opensource-vs-commercial-api-vs-enterprise)
- [Quick Start with Docker Compose](#quick-start-with-docker-compose)
- [LLM Configuration](#llm-configuration)
  - [Using models.yaml (Recommended)](#using-modelsyaml-recommended)
  - [Using environment variables (Basic)](#using-environment-variables-basic)
  - [Common LLM API Providers](#common-llm-api-providers)
- [Licensing](#licensing)
- [Connect With Us](#connect-with-us)

## (Super) Quick Start

1. Go to [chunkr.ai](https://www.chunkr.ai) 
2. Make an account and copy your API key
3. Install our Python SDK:
```bash
pip install chunkr-ai
```
4. Use the SDK to process your documents:
```python
from chunkr_ai import Chunkr

# Initialize with your API key from chunkr.ai
chunkr = Chunkr(api_key=&quot;your_api_key&quot;)

# Upload a document (URL or local file path)
url = &quot;https://chunkr-web.s3.us-east-1.amazonaws.com/landing_page/input/science.pdf&quot;
task = chunkr.upload(url)

# Export results in various formats
html = task.html(output_file=&quot;output.html&quot;)
markdown = task.markdown(output_file=&quot;output.md&quot;)
content = task.content(output_file=&quot;output.txt&quot;)
task.json(output_file=&quot;output.json&quot;)

# Clean up
chunkr.close()
```

## Documentation

Visit our [docs](https://docs.chunkr.ai) for more information and examples.

## OpenSource vs Commercial API vs Enterprise

| Feature | Open Source | Commercial API | Enterprise |
|---------|-------------|----------------|------------|
| **Perfect for** | Development &amp; testing | Production applications | Large-scale/High security deployments|
| **Layout Analysis** | Basic models | Advanced models | Advanced + custom-tuned |
| **OCR Accuracy** | Standard models | Premium models | Premium + domain-tuned |
| **VLM Processing** | Basic vision models | Enhanced VLM models | Enhanced + custom fine-tunes |
| **Excel Support** | ‚ùå | ‚úÖ Native parser | ‚úÖ Native parser |
| **Document Types** | PDF, PPT, Word, Images | PDF, PPT, Word, Images, Excel | PDF, PPT, Word, Images, Excel |
| **Infrastructure** | Self-hosted | Fully managed | Fully managed (On-prem or Chunkr-hosted) |
| **Support** | Discord community | Priority email + community | 24/7 dedicated founing team support |
| **Migration Support** | Community resources | Documentation + email | Dedicated migration team |

## Quick Start with Docker Compose

1. Prerequisites:
   - [Docker and Docker Compose](https://docs.docker.com/get-docker/)
   - [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html) (for GPU support, optional)

2. Clone the repo:
```bash
git clone https://github.com/lumina-ai-inc/chunkr
cd chunkr
```

3. Set up environment variables:
```bash
# Copy the example environment file
cp .env.example .env

# Configure your llm models
cp models.example.yaml models.yaml
```

For more information on how to set up LLMs, see [here](#llm-configuration).

4. Start the services:
```bash
# For GPU deployment:
docker compose up -d

# For CPU-only deployment:
docker compose -f compose.yaml -f compose.cpu.yaml up -d

# For Mac ARM architecture (M1, M2, M3, etc.):
docker compose -f compose.yaml -f compose.cpu.yaml -f compose.mac.yaml up -d
```

5. Access the services:
   - Web UI: `http://localhost:5173`
   - API: `http://localhost:8000`

6. Stop the services when done:
```bash
# For GPU deployment:
docker compose down

# For CPU-only deployment:
docker compose -f compose.yaml -f compose.cpu.yaml down

# For Mac ARM architecture (M1, M2, M3, etc.):
docker compose -f compose.yaml -f compose.cpu.yaml -f compose.mac.yaml down
```
## LLM Configuration

Chunkr supports two ways to configure LLMs:

1. **models.yaml file**: Advanced configuration for multiple LLMs with additional options
2. **Environment variables**: Simple configuration for a single LLM

### Using models.yaml (Recommended)

For more flexible configuration with multiple models, default/fallback options, and rate limits:

1. Copy the example file to create your configuration:
```bash
cp models.example.yaml models.yaml
```

2. Edit the models.yaml file with your configuration. Example:
```yaml
models:
  - id: gpt-4o
    model: gpt-4o
    provider_url: https://api.openai.com/v1/chat/completions
    api_key: &quot;your_openai_api_key_here&quot;
    default: true
    rate-limit: 200 # requests per minute - optional
```

Benefits of using models.yaml:
- Configure multiple LLM providers simultaneously
- Set default and fallback models
- Add distributed rate limits per model
- Reference models by ID in API requests (see docs for more info)

&gt;Read the `models.example.yaml` file for more information on the available options.

### Using environment variables (Basic)

You can use any OpenAI API compatible endpoint by setting the following variables in your .env file:
``` 
LLM__KEY:
LLM__MODEL:
LLM__URL:
```

### Common LLM API Providers

Below is a table of common LLM providers and their configuration details to get you started:

| Provider         | API URL                                                                  | Documentation                                                                                                                          |
| ---------------- | ------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI           | https://api.openai.com/v1/chat/completions                               | [OpenAI Docs](https://platform.openai.com/docs)                                                                                        |
| Google AI Studio | https://generativelanguage.googleapis.com/v1beta/openai/chat/completions | [Google AI Docs](https://ai.google.dev/gemini-api/docs/openai)                                                                         |
| OpenRouter       | https://openrouter.ai/api/v1/chat/completions                            | [OpenRouter Models](https://openrouter.ai/models)                                                                                      |
| Self-Hosted      | http://localhost:8000/v1                                                 | [VLLM](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html) or [Ollama](https://ollama.com/blog/openai-compatibility) |

## Licensing

The core of this project is dual-licensed:

1. [GNU Affero General Public License v3.0 (AGPL-3.0)](LICENSE)
2. Commercial License

To use Chunkr without complying with the AGPL-3.0 license terms you can [contact us](mailto:mehul@chunkr.ai) or visit our [website](https://chunkr.ai).

## Connect With Us
- üìß Email: [mehul@chunkr.ai](mailto:mehul@chunkr.ai)
- üìÖ Schedule a call: [Book a 30-minute meeting](https://cal.com/mehulc/30min)
- üåê Visit our website: [chunkr.ai](https://chunkr.ai)
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[typst/typst]]></title>
            <link>https://github.com/typst/typst</link>
            <guid>https://github.com/typst/typst</guid>
            <pubDate>Thu, 07 Aug 2025 00:06:09 GMT</pubDate>
            <description><![CDATA[A new markup-based typesetting system that is powerful and easy to learn.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/typst/typst">typst/typst</a></h1>
            <p>A new markup-based typesetting system that is powerful and easy to learn.</p>
            <p>Language: Rust</p>
            <p>Stars: 44,656</p>
            <p>Forks: 1,192</p>
            <p>Stars today: 63 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
  &lt;img alt=&quot;Typst&quot; src=&quot;https://user-images.githubusercontent.com/17899797/226108480-722b770e-6313-40d7-84f2-26bebb55a281.png&quot;&gt;
&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://typst.app/docs/&quot;&gt;
    &lt;img alt=&quot;Documentation&quot; src=&quot;https://img.shields.io/website?down_message=offline&amp;label=docs&amp;up_color=007aff&amp;up_message=online&amp;url=https%3A%2F%2Ftypst.app%2Fdocs&quot;
  &gt;&lt;/a&gt;
  &lt;a href=&quot;https://typst.app/&quot;&gt;
    &lt;img alt=&quot;Typst App&quot; src=&quot;https://img.shields.io/website?down_message=offline&amp;label=typst.app&amp;up_color=239dad&amp;up_message=online&amp;url=https%3A%2F%2Ftypst.app&quot;
  &gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/2uDybryKPe&quot;&gt;
    &lt;img alt=&quot;Discord Server&quot; src=&quot;https://img.shields.io/discord/1054443721975922748?color=5865F2&amp;label=discord&amp;labelColor=555&quot;
  &gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/typst/typst/blob/main/LICENSE&quot;&gt;
    &lt;img alt=&quot;Apache-2 License&quot; src=&quot;https://img.shields.io/badge/license-Apache%202-brightgreen&quot;
  &gt;&lt;/a&gt;
  &lt;a href=&quot;https://typst.app/jobs/&quot;&gt;
    &lt;img alt=&quot;Jobs at Typst&quot; src=&quot;https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Ftypst.app%2Fassets%2Fdata%2Fshields.json&amp;query=%24.jobs.text&amp;label=jobs&amp;color=%23A561FF&amp;cacheSeconds=1800&quot;
  &gt;&lt;/a&gt;
&lt;/p&gt;

Typst is a new markup-based typesetting system that is designed to be as powerful
as LaTeX while being much easier to learn and use. Typst has:

- Built-in markup for the most common formatting tasks
- Flexible functions for everything else
- A tightly integrated scripting system
- Math typesetting, bibliography management, and more
- Fast compile times thanks to incremental compilation
- Friendly error messages in case something goes wrong

This repository contains the Typst compiler and its CLI, which is everything you
need to compile Typst documents locally. For the best writing experience,
consider signing up to our [collaborative online editor][app] for free.

## Example
A [gentle introduction][tutorial] to Typst is available in our documentation.
However, if you want to see the power of Typst encapsulated in one image, here
it is:
&lt;p align=&quot;center&quot;&gt;
 &lt;img alt=&quot;Example&quot; width=&quot;900&quot; src=&quot;https://user-images.githubusercontent.com/17899797/228031796-ced0e452-fcee-4ae9-92da-b9287764ff25.png&quot;&gt;
&lt;/p&gt;


Let&#039;s dissect what&#039;s going on:

- We use _set rules_ to configure element properties like the size of pages or
  the numbering of headings. By setting the page height to `auto`, it scales to
  fit the content. Set rules accommodate the most common configurations. If you
  need full control, you can also use [show rules][show] to completely redefine
  the appearance of an element.

- We insert a heading with the `= Heading` syntax. One equals sign creates a top
  level heading, two create a subheading and so on. Typst has more lightweight
  markup like this, see the [syntax] reference for a full list.

- [Mathematical equations][math] are enclosed in dollar signs. By adding extra
  spaces around the contents of an equation, we can put it into a separate block.
  Multi-letter identifiers are interpreted as Typst definitions and functions
  unless put into quotes. This way, we don&#039;t need backslashes for things like
  `floor` and `sqrt`. And `phi.alt` applies the `alt` modifier to the `phi` to
  select a particular symbol variant.

- Now, we get to some [scripting]. To input code into a Typst document, we can
  write a hash followed by an expression. We define two variables and a
  recursive function to compute the n-th fibonacci number. Then, we display the
  results in a center-aligned table. The table function takes its cells
  row-by-row. Therefore, we first pass the formulas `$F_1$` to `$F_8$` and then
  the computed fibonacci numbers. We apply the spreading operator (`..`) to both
  because they are arrays and we want to pass the arrays&#039; items as individual
  arguments.

&lt;details&gt;
  &lt;summary&gt;Text version of the code example.&lt;/summary&gt;

  ```typst
  #set page(width: 10cm, height: auto)
  #set heading(numbering: &quot;1.&quot;)

  = Fibonacci sequence
  The Fibonacci sequence is defined through the
  recurrence relation $F_n = F_(n-1) + F_(n-2)$.
  It can also be expressed in _closed form:_

  $ F_n = round(1 / sqrt(5) phi.alt^n), quad
    phi.alt = (1 + sqrt(5)) / 2 $

  #let count = 8
  #let nums = range(1, count + 1)
  #let fib(n) = (
    if n &lt;= 2 { 1 }
    else { fib(n - 1) + fib(n - 2) }
  )

  The first #count numbers of the sequence are:

  #align(center, table(
    columns: count,
    ..nums.map(n =&gt; $F_#n$),
    ..nums.map(n =&gt; str(fib(n))),
  ))
  ```
&lt;/details&gt;

## Installation
Typst&#039;s CLI is available from different sources:

- You can get sources and pre-built binaries for the latest release of Typst
  from the [releases page][releases]. Download the archive for your platform and
  place it in a directory that is in your `PATH`. To stay up to date with future
  releases, you can simply run `typst update`.

- You can install Typst through different package managers. Note that the
  versions in the package managers might lag behind the latest release.
  - Linux:
      - View [Typst on Repology][repology]
      - View [Typst&#039;s Snap][snap]
  - macOS: `brew install typst`
  - Windows: `winget install --id Typst.Typst`

- If you have a [Rust][rust] toolchain installed, you can install
  - the latest released Typst version with
    `cargo install --locked typst-cli`
  - a development version with
    `cargo install --git https://github.com/typst/typst --locked typst-cli`

- Nix users can
  - use the `typst` package with `nix-shell -p typst`
  - build and run a development version with
    `nix run github:typst/typst -- --version`.

- Docker users can run a prebuilt image with
  `docker run ghcr.io/typst/typst:latest --help`.

## Usage
Once you have installed Typst, you can use it like this:
```sh
# Creates `file.pdf` in working directory.
typst compile file.typ

# Creates PDF file at the desired path.
typst compile path/to/source.typ path/to/output.pdf
```

You can also watch source files and automatically recompile on changes. This is
faster than compiling from scratch each time because Typst has incremental
compilation.
```sh
# Watches source files and recompiles on changes.
typst watch file.typ
```

Typst further allows you to add custom font paths for your project and list all
of the fonts it discovered:
```sh
# Adds additional directories to search for fonts.
typst compile --font-path path/to/fonts file.typ

# Lists all of the discovered fonts in the system and the given directory.
typst fonts --font-path path/to/fonts

# Or via environment variable (Linux syntax).
TYPST_FONT_PATHS=path/to/fonts typst fonts
```

For other CLI subcommands and options, see below:
```sh
# Prints available subcommands and options.
typst help

# Prints detailed usage of a subcommand.
typst help watch
```

If you prefer an integrated IDE-like experience with autocompletion and instant 
preview, you can also check out our [free web app][app]. Alternatively, there is 
a community-created language server called 
[Tinymist](https://myriad-dreamin.github.io/tinymist/) which is integrated into 
various editor extensions.

## Community
The main places where the community gathers are our [Forum][forum] and our
[Discord server][discord]. The Forum is a great place to ask questions, help
others, and share cool things you created with Typst. The Discord server is more
suitable for quicker questions, discussions about contributing, or just to chat.
We&#039;d be happy to see you there!

[Typst Universe][universe] is where the community shares templates and packages.
If you want to share your own creations, you can submit them to our
[package repository][packages].

If you had a bad experience in our community, please [reach out to us][contact].

## Contributing
We love to see contributions from the community. If you experience bugs, feel
free to open an issue. If you would like to implement a new feature or bug fix,
please follow the steps outlined in the [contribution guide][contributing].

To build Typst yourself, first ensure that you have the
[latest stable Rust][rust] installed. Then, clone this repository and build the
CLI with the following commands:

```sh
git clone https://github.com/typst/typst
cd typst
cargo build --release
```

The optimized binary will be stored in `target/release/`.

Another good way to contribute is by [sharing packages][packages] with the
community.

## Pronunciation and Spelling
IPA: /ta…™pst/. &quot;Ty&quot; like in **Ty**pesetting and &quot;pst&quot; like in Hi**pst**er. When
writing about Typst, capitalize its name as a proper noun, with a capital &quot;T&quot;.

## Design Principles
All of Typst has been designed with three key goals in mind: Power,
simplicity, and performance. We think it&#039;s time for a system that matches the
power of LaTeX, is easy to learn and use, all while being fast enough to realize
instant preview. To achieve these goals, we follow three core design principles:

- **Simplicity through Consistency:**
  If you know how to do one thing in Typst, you should be able to transfer that
  knowledge to other things. If there are multiple ways to do the same thing,
  one of them should be at a different level of abstraction than the other. E.g.
  it&#039;s okay that `= Introduction` and `#heading[Introduction]` do the same thing
  because the former is just syntax sugar for the latter.

- **Power through Composability:**
  There are two ways to make something flexible: Have a knob for everything or
  have a few knobs that you can combine in many ways. Typst is designed with the
  second way in mind. We provide systems that you can compose in ways we&#039;ve
  never even thought of. TeX is also in the second category, but it&#039;s a bit
  low-level and therefore people use LaTeX instead. But there, we don&#039;t really
  have that much composability. Instead, there&#039;s a package for everything
  (`\usepackage{knob}`).

- **Performance through Incrementality:**
  All Typst language features must accommodate for incremental compilation.
  Luckily we have [`comemo`], a system for incremental compilation which does
  most of the hard work in the background.

## Acknowledgements

We&#039;d like to thank everyone who is supporting Typst&#039;s development, be it via
[GitHub sponsors] or elsewhere. In particular, special thanks[^1] go to:

- [Posit](https://posit.co/blog/posit-and-typst/) for financing a full-time
  compiler engineer
- [NLnet](https://nlnet.nl/) for supporting work on Typst via multiple grants
  through the [NGI Zero Core](https://nlnet.nl/core) fund:
  - Work on [HTML export](https://nlnet.nl/project/Typst-HTML/)
  - Work on [PDF accessibility](https://nlnet.nl/project/Typst-Accessibility/)
- [Science &amp; Startups](https://www.science-startups.berlin/) for having financed
  Typst development from January through June 2023 via the Berlin Startup
  Scholarship
- [Zerodha](https://zerodha.tech/blog/1-5-million-pdfs-in-25-minutes/) for their
  generous one-time sponsorship

[^1]: This list only includes contributions for our open-source work that exceed
    or are expected to exceed ‚Ç¨10K.

[docs]: https://typst.app/docs/
[app]: https://typst.app/
[discord]: https://discord.gg/2uDybryKPe
[forum]: https://forum.typst.app/
[universe]: https://typst.app/universe/
[tutorial]: https://typst.app/docs/tutorial/
[show]: https://typst.app/docs/reference/styling/#show-rules
[math]: https://typst.app/docs/reference/math/
[syntax]: https://typst.app/docs/reference/syntax/
[scripting]: https://typst.app/docs/reference/scripting/
[rust]: https://rustup.rs/
[releases]: https://github.com/typst/typst/releases/
[repology]: https://repology.org/project/typst/versions
[contact]: https://typst.app/contact
[architecture]: https://github.com/typst/typst/blob/main/docs/dev/architecture.md
[contributing]: https://github.com/typst/typst/blob/main/CONTRIBUTING.md
[packages]: https://github.com/typst/packages/
[`comemo`]: https://github.com/typst/comemo/
[snap]: https://snapcraft.io/typst
[GitHub sponsors]: https://github.com/sponsors/typst/
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[tensorzero/tensorzero]]></title>
            <link>https://github.com/tensorzero/tensorzero</link>
            <guid>https://github.com/tensorzero/tensorzero</guid>
            <pubDate>Thu, 07 Aug 2025 00:06:08 GMT</pubDate>
            <description><![CDATA[TensorZero is an open-source stack for industrial-grade LLM applications. It unifies an LLM gateway, observability, optimization, evaluation, and experimentation.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/tensorzero/tensorzero">tensorzero/tensorzero</a></h1>
            <p>TensorZero is an open-source stack for industrial-grade LLM applications. It unifies an LLM gateway, observability, optimization, evaluation, and experimentation.</p>
            <p>Language: Rust</p>
            <p>Stars: 9,354</p>
            <p>Forks: 610</p>
            <p>Stars today: 31 stars today</p>
            <h2>README</h2><pre>&lt;p&gt;&lt;picture&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/47d67430-386d-4675-82ad-d4734d3262d9&quot; alt=&quot;TensorZero Logo&quot; width=128 height=128&gt;&lt;/picture&gt;&lt;/p&gt;

# TensorZero

&lt;p&gt;&lt;picture&gt;&lt;img src=&quot;https://www.tensorzero.com/github-trending-badge.svg&quot; alt=&quot;#1 Repository Of The Day&quot;&gt;&lt;/picture&gt;&lt;/p&gt;

**TensorZero is an open-source stack for _industrial-grade LLM applications_:**

- **Gateway:** access every LLM provider through a unified API, built for performance (&lt;1ms p99 latency)
- **Observability:** store inferences and feedback in your database, available programmatically or in the UI
- **Optimization:** collect metrics and human feedback to optimize prompts, models, and inference strategies
- **Evaluation:** benchmark individual inferences or end-to-end workflows using heuristics, LLM judges, etc.
- **Experimentation:** ship with confidence with built-in A/B testing, routing, fallbacks, retries, etc.

Take what you need, adopt incrementally, and complement with other tools.

---

&lt;p align=&quot;center&quot;&gt;
  &lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/&quot; target=&quot;_blank&quot;&gt;Website&lt;/a&gt;&lt;/b&gt;
  ¬∑
  &lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs&quot; target=&quot;_blank&quot;&gt;Docs&lt;/a&gt;&lt;/b&gt;
  ¬∑
  &lt;b&gt;&lt;a href=&quot;https://www.x.com/tensorzero&quot; target=&quot;_blank&quot;&gt;Twitter&lt;/a&gt;&lt;/b&gt;
  ¬∑
  &lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/slack&quot; target=&quot;_blank&quot;&gt;Slack&lt;/a&gt;&lt;/b&gt;
  ¬∑
  &lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/discord&quot; target=&quot;_blank&quot;&gt;Discord&lt;/a&gt;&lt;/b&gt;
  &lt;br&gt;
  &lt;br&gt;
  &lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/quickstart&quot; target=&quot;_blank&quot;&gt;Quick Start (5min)&lt;/a&gt;&lt;/b&gt;
  ¬∑
  &lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/deployment&quot; target=&quot;_blank&quot;&gt;Deployment Guide&lt;/a&gt;&lt;/b&gt;
  ¬∑
  &lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/api-reference&quot; target=&quot;_blank&quot;&gt;API Reference&lt;/a&gt;&lt;/b&gt;
  ¬∑
  &lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/deployment&quot; target=&quot;_blank&quot;&gt;Configuration Reference&lt;/a&gt;&lt;/b&gt;
&lt;/p&gt;

---

&lt;table&gt;
  &lt;tr&gt;
    &lt;td width=&quot;30%&quot; valign=&quot;top&quot;&gt;&lt;b&gt;What is TensorZero?&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;70%&quot; valign=&quot;top&quot;&gt;TensorZero is an open-source stack for industrial-grade LLM applications. It unifies an LLM gateway, observability, optimization, evaluation, and experimentation.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;30%&quot; valign=&quot;top&quot;&gt;&lt;b&gt;How is TensorZero different from other LLM frameworks?&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;70%&quot; valign=&quot;top&quot;&gt;
      1. TensorZero enables you to optimize complex LLM applications based on production metrics and human feedback.&lt;br&gt;
      2. TensorZero supports the needs of industrial-grade LLM applications: low latency, high throughput, type safety, self-hosted, GitOps, customizability, etc.&lt;br&gt;
      3. TensorZero unifies the entire LLMOps stack, creating compounding benefits. For example, LLM evaluations can be used for fine-tuning models alongside AI judges.
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;30%&quot; valign=&quot;top&quot;&gt;&lt;b&gt;Can I use TensorZero with ___?&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;70%&quot; valign=&quot;top&quot;&gt;Yes. Every major programming language is supported. You can use TensorZero with our Python client, any OpenAI SDK or OpenAI-compatible client, or our HTTP API.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;30%&quot; valign=&quot;top&quot;&gt;&lt;b&gt;Is TensorZero production-ready?&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;70%&quot; valign=&quot;top&quot;&gt;Yes. Here&#039;s a case study: &lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/blog/case-study-automating-code-changelogs-at-a-large-bank-with-llms&quot;&gt;Automating Code Changelogs at a Large Bank with LLMs&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;30%&quot; valign=&quot;top&quot;&gt;&lt;b&gt;How much does TensorZero cost?&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;70%&quot; valign=&quot;top&quot;&gt;Nothing. TensorZero is 100% self-hosted and open-source. There are no paid features.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;30%&quot; valign=&quot;top&quot;&gt;&lt;b&gt;Who is building TensorZero?&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;70%&quot; valign=&quot;top&quot;&gt;Our technical team includes a former Rust compiler maintainer, machine learning researchers (Stanford, CMU, Oxford, Columbia) with thousands of citations, and the chief product officer of a decacorn startup. We&#039;re backed by the same investors as leading open-source projects (e.g. ClickHouse, CockroachDB) and AI labs (e.g. OpenAI, Anthropic).&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;30%&quot; valign=&quot;top&quot;&gt;&lt;b&gt;How do I get started?&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;70%&quot; valign=&quot;top&quot;&gt;You can adopt TensorZero incrementally. Our &lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/quickstart&quot;&gt;Quick Start&lt;/a&gt;&lt;/b&gt; goes from a vanilla OpenAI wrapper to a production-ready LLM application with observability and fine-tuning in just 5 minutes.&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

---

## Features

### üåê LLM Gateway

&gt; **Integrate with TensorZero once and access every major LLM provider.**

- [x] Access every major LLM provider (API or self-hosted) through a single unified API
- [x] Infer with streaming, tool use, structured generation (JSON mode), batch, multimodal (VLMs), file inputs, caching, etc.
- [x] Define prompt templates and schemas to enforce a consistent, typed interface between your application and the LLMs
- [x] Satisfy extreme throughput and latency needs, thanks to ü¶Ä Rust: &lt;1ms p99 latency overhead at 10k+ QPS
- [x] Integrate using our Python client, any OpenAI SDK or OpenAI-compatible client, or our HTTP API (use any programming language)
- [x] Ensure high availability with routing, retries, fallbacks, load balancing, granular timeouts, etc.
- [ ] Soon: embeddings; real-time voice

&lt;table&gt;
  &lt;tr&gt;&lt;/tr&gt; &lt;!-- flip highlight order --&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;Model Providers&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;Features&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;left&quot; valign=&quot;top&quot;&gt;
      &lt;p&gt;
        The TensorZero Gateway natively supports:
      &lt;/p&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/anthropic&quot;&gt;Anthropic&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/aws-bedrock&quot;&gt;AWS Bedrock&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/aws-sagemaker&quot;&gt;AWS SageMaker&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/azure&quot;&gt;Azure OpenAI Service&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/deepseek&quot;&gt;DeepSeek&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/fireworks&quot;&gt;Fireworks&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/gcp-vertex-ai-anthropic&quot;&gt;GCP Vertex AI Anthropic&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/gcp-vertex-ai-gemini&quot;&gt;GCP Vertex AI Gemini&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/google-ai-studio-gemini&quot;&gt;Google AI Studio (Gemini API)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/groq&quot;&gt;Groq&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/hyperbolic&quot;&gt;Hyperbolic&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/mistral&quot;&gt;Mistral&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/openai&quot;&gt;OpenAI&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/openrouter&quot;&gt;OpenRouter&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/sglang&quot;&gt;SGLang&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/tgi&quot;&gt;TGI&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/together&quot;&gt;Together AI&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/vllm&quot;&gt;vLLM&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/xai&quot;&gt;xAI (Grok)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
      &lt;/ul&gt;
      &lt;p&gt;
        &lt;em&gt;
          Need something else?
          Your provider is most likely supported because TensorZero integrates with &lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/openai-compatible&quot;&gt;any OpenAI-compatible API (e.g. Ollama)&lt;/a&gt;&lt;/b&gt;.
          &lt;/em&gt;
      &lt;/p&gt;
    &lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;left&quot; valign=&quot;top&quot;&gt;
      &lt;p&gt;
        The TensorZero Gateway supports advanced features like:
      &lt;/p&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/retries-fallbacks&quot;&gt;Retries &amp; Fallbacks&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations&quot;&gt;Inference-Time Optimizations&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/prompt-templates-schemas&quot;&gt;Prompt Templates &amp; Schemas&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/experimentation/&quot;&gt;Experimentation (A/B Testing)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/configuration-reference&quot;&gt;Configuration-as-Code (GitOps)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/batch-inference&quot;&gt;Batch Inference&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/multimodal-inference&quot;&gt;Multimodal Inference (VLMs)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/inference-caching&quot;&gt;Inference Caching&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/metrics-feedback&quot;&gt;Metrics &amp; Feedback&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/episodes&quot;&gt;Multi-Step LLM Workflows (Episodes)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;em&gt;&amp; a lot more...&lt;/em&gt;&lt;/li&gt;
      &lt;/ul&gt;
      &lt;p&gt;
        The TensorZero Gateway is written in Rust ü¶Ä with &lt;b&gt;performance&lt;/b&gt; in mind (&amp;lt;1ms p99 latency overhead @ 10k QPS).
        See &lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/benchmarks&quot;&gt;Benchmarks&lt;/a&gt;&lt;/b&gt;.&lt;br&gt;
      &lt;/p&gt;
      &lt;p&gt;
        You can run inference using the &lt;b&gt;TensorZero client&lt;/b&gt; (recommended), the &lt;b&gt;OpenAI client&lt;/b&gt;, or the &lt;b&gt;HTTP API&lt;/b&gt;.
      &lt;/p&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;br&gt;

&lt;details open&gt;
&lt;summary&gt;&lt;b&gt;Usage: Python &amp;mdash; TensorZero Client (Recommended)&lt;/b&gt;&lt;/summary&gt;

You can access any provider using the TensorZero Python client.

1. `pip install tensorzero`
2. Optional: Set up the TensorZero configuration.
3. Run inference:

```python
from tensorzero import TensorZeroGateway  # or AsyncTensorZeroGateway


with TensorZeroGateway.build_embedded(clickhouse_url=&quot;...&quot;, config_file=&quot;...&quot;) as client:
    response = client.inference(
        model_name=&quot;openai::gpt-4o-mini&quot;,
        # Try other providers easily: &quot;anthropic::claude-3-7-sonnet-20250219&quot;
        input={
            &quot;messages&quot;: [
                {
                    &quot;role&quot;: &quot;user&quot;,
                    &quot;content&quot;: &quot;Write a haiku about artificial intelligence.&quot;,
                }
            ]
        },
    )
```

See **[Quick Start](https://www.tensorzero.com/docs/quickstart)** for more information.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Usage: Python &amp;mdash; OpenAI Client&lt;/b&gt;&lt;/summary&gt;

You can access any provider using the OpenAI Python client with TensorZero.

1. `pip install tensorzero`
2. Optional: Set up the TensorZero configuration.
3. Run inference:

```python
from openai import OpenAI  # or AsyncOpenAI
from tensorzero import patch_openai_client

client = OpenAI()

patch_openai_client(
    client,
    clickhouse_url=&quot;http://chuser:chpassword@localhost:8123/tensorzero&quot;,
    config_file=&quot;config/tensorzero.toml&quot;,
    async_setup=False,
)

response = client.chat.completions.create(
    model=&quot;tensorzero::model_name::openai::gpt-4o-mini&quot;,
    # Try other providers easily: &quot;tensorzero::model_name::anthropic::claude-3-7-sonnet-20250219&quot;
    messages=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;Write a haiku about artificial intelligence.&quot;,
        }
    ],
)
```

See **[Quick Start](https://www.tensorzero.com/docs/quickstart)** for more information.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Usage: JavaScript / TypeScript (Node) &amp;mdash; OpenAI Client&lt;/b&gt;&lt;/summary&gt;

You can access any provider using the OpenAI Node client with TensorZero.

1. Deploy `tensorzero/gateway` using Docker.
   **[Detailed instructions ‚Üí](https://www.tensorzero.com/docs/gateway/deployment)**
2. Set up the TensorZero configuration.
3. Run inference:

```ts
import OpenAI from &quot;openai&quot;;

const client = new OpenAI({
  baseURL: &quot;http://localhost:3000/openai/v1&quot;,
});

const response = await client.chat.completions.create({
  model: &quot;tensorzero::model_name::openai::gpt-4o-mini&quot;,
  // Try other providers easily: &quot;tensorzero::model_name::anthropic::claude-3-7-sonnet-20250219&quot;
  messages: [
    {
      role: &quot;user&quot;,
      content: &quot;Write a haiku about artificial intelligence.&quot;,
    },
  ],
});
```

See **[Quick Start](https://www.tensorzero.com/docs/quickstart)** for more information.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Usage: Other Languages &amp; Platforms &amp;mdash; HTTP API&lt;/b&gt;&lt;/summary&gt;

TensorZero supports virtually any programming language or platform via its HTTP API.

1. Deploy `tensorzero/gateway` using Docker.
   **[Detailed instructions ‚Üí](https://www.tensorzero.com/docs/gateway/deployment)**
2. Optional: Set up the TensorZero configuration.
3. Run inference:

```bash
curl -X POST &quot;http://localhost:3000/inference&quot; \
  -H &quot;Content-Type: application/json&quot; \
  -d &#039;{
    &quot;model_name&quot;: &quot;openai::gpt-4o-mini&quot;,
    &quot;input&quot;: {
      &quot;messages&quot;: [
        {
          &quot;role&quot;: &quot;user&quot;,
          &quot;content&quot;: &quot;Write a haiku about artificial intelligence.&quot;
        }
      ]
    }
  }&#039;
```

See **[Quick Start](https://www.tensorzero.com/docs/quickstart)** for more information.

&lt;/details&gt;

&lt;br&gt;

### üîç LLM Observability

&gt; **Zoom in to debug individual API calls, or zoom out to monitor metrics across models and prompts over time &amp;mdash; all using the open-source TensorZero UI.**

- [x] Store inferences and feedback (metrics, human edits, etc.) in your own database
- [x] Dive into individual inferences or high-level aggregate patterns using the TensorZero UI or programmatically
- [x] Build datasets for optimization, evaluation, and other workflows
- [x] Replay historical inferences with new prompts, models, inference strategies, etc.
- [x] Export OpenTelemetry (OTLP) traces to your favorite general-purpose observability tool
- [ ] Soon: AI-assisted debugging and root cause analysis; AI-assisted data labeling

&lt;table&gt;
  &lt;tr&gt;&lt;/tr&gt; &lt;!-- flip highlight order --&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;Observability ¬ª Inference&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;Observability ¬ª Function&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/2cc3cc9a-f33f-4e94-b8de-07522326f80a&quot;&gt;&lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/00ae6605-8fa0-4efd-8238-ae8ea589860f&quot;&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;br&gt;

### üìà LLM Optimization

&gt; **Send production metrics and human feedback to easily optimize your prompts, models, and inference strategies &amp;mdash; using the UI or programmatically.**

- [x] Optimize your models with supervised fine-tuning, RLHF, and other techniques
- [x] Optimize your prompts with automated prompt engineering algorithms like MIPROv2
- [x] Optimize your inference strategy with dynamic in-context learning, chain of thought, best/mixture-of-N sampling, etc.
- [x] Enable a feedback loop for your LLMs: a data &amp; learning flywheel turning production data into smarter, faster, and cheaper models
- [ ] Soon: programmatic optimization; synthetic data generation

#### Model Optimization

Optimize closed-source and open-source models using supervised fine-tuning (SFT) and preference fine-tuning (DPO).

&lt;table&gt;
  &lt;tr&gt;&lt;/tr&gt; &lt;!-- flip highlight order --&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;Supervised Fine-tuning &amp;mdash; UI&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;Preference Fine-tuning (DPO) &amp;mdash; Jupyter Notebook&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/cf7acf66-732b-43b3-af2a-5eba1ce40f6f&quot;&gt;&lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/a67a0634-04a7-42b0-b934-9130cb7cdf51&quot;&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

#### Inference-Time Optimization

Boost performance by dynamically updating your prompts with relevant examples, combining responses from multiple inferences, and more.

&lt;table&gt;
  &lt;tr&gt;&lt;/tr&gt; &lt;!-- flip highlight order --&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#best-of-n-sampling&quot;&gt;Best-of-N Sampling&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#mixture-of-n-sampling&quot;&gt;Mixture-of-N Sampling&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/c0edfa4c-713c-4996-9964-50c0d26e6970&quot;&gt;&lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/75b5bf05-4c1f-43c4-b158-d69d1b8d05be&quot;&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#dynamic-in-context-learning-dicl&quot;&gt;Dynamic In-Context Learning (DICL)&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#chain-of-thought-cot&quot;&gt;Chain-of-Thought (CoT)&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/d8489e92-ce93-46ac-9aab-289ce19bb67d&quot;&gt;&lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/ea13d73c-76a4-4e0c-a35b-0c648f898311&quot; height=&quot;320&quot;&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

_More coming soon..._

&lt;br&gt;

#### Prompt Optimization

Optimize your prompts programmatically using research-driven optimization techniques.

&lt;table&gt;
  &lt;tr&gt;&lt;/tr&gt; &lt;!-- flip highlight order --&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#best-of-n-sampling&quot;&gt;MIPROv2&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;&lt;a href=&quot;https://github.com/tensorzero/tensorzero/tree/main/examples/gsm8k-custom-recipe-dspy&quot;&gt;DSPy Integration&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/d81a7c37-382f-4c46-840f-e6c2593301db&quot; alt=&quot;MIPROv2 diagram&quot;&gt;&lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;
      TensorZero comes with several optimization recipes, but you can also easily create your own.
      This example shows how to optimize a TensorZero function using an arbitrary tool ‚Äî here, DSPy, a popular library for automated prompt engineering.
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

_More coming soon..._

&lt;br&gt;

### üìä LLM Evaluation

&gt; **Compare prompts, models, and inference strategies using evaluations powered by heuristics and LLM judges.**

- [x] Evaluate individual inferences with _static evaluations_ powered by heuristics or LLM judges (&amp;approx; unit tests for LLMs)
- [x] Evaluate end-to-end workflows with _dynamic evaluations_ with complete flexibility (&amp;approx; integration tests for LLMs)
- [x] Optimize LLM judges just like any other TensorZero function to align them to human preferences
- [ ] Soon: more built-in evaluators; headless evaluations

&lt;table&gt;
  &lt;tr&gt;&lt;/tr&gt; &lt;!-- flip highlight order --&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;Evaluation ¬ª UI&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;mi

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[astral-sh/uv]]></title>
            <link>https://github.com/astral-sh/uv</link>
            <guid>https://github.com/astral-sh/uv</guid>
            <pubDate>Thu, 07 Aug 2025 00:06:07 GMT</pubDate>
            <description><![CDATA[An extremely fast Python package and project manager, written in Rust.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/astral-sh/uv">astral-sh/uv</a></h1>
            <p>An extremely fast Python package and project manager, written in Rust.</p>
            <p>Language: Rust</p>
            <p>Stars: 63,903</p>
            <p>Forks: 1,852</p>
            <p>Stars today: 117 stars today</p>
            <h2>README</h2><pre># uv

[![uv](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/uv/main/assets/badge/v0.json)](https://github.com/astral-sh/uv)
[![image](https://img.shields.io/pypi/v/uv.svg)](https://pypi.python.org/pypi/uv)
[![image](https://img.shields.io/pypi/l/uv.svg)](https://pypi.python.org/pypi/uv)
[![image](https://img.shields.io/pypi/pyversions/uv.svg)](https://pypi.python.org/pypi/uv)
[![Actions status](https://github.com/astral-sh/uv/actions/workflows/ci.yml/badge.svg)](https://github.com/astral-sh/uv/actions)
[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;logoColor=white)](https://discord.gg/astral-sh)

An extremely fast Python package and project manager, written in Rust.

&lt;p align=&quot;center&quot;&gt;
  &lt;picture align=&quot;center&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/astral-sh/uv/assets/1309177/03aa9163-1c79-4a87-a31d-7a9311ed9310&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/astral-sh/uv/assets/1309177/629e59c0-9c6e-4013-9ad4-adb2bcf5080d&quot;&gt;
    &lt;img alt=&quot;Shows a bar chart with benchmark results.&quot; src=&quot;https://github.com/astral-sh/uv/assets/1309177/629e59c0-9c6e-4013-9ad4-adb2bcf5080d&quot;&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;i&gt;Installing &lt;a href=&quot;https://trio.readthedocs.io/&quot;&gt;Trio&lt;/a&gt;&#039;s dependencies with a warm cache.&lt;/i&gt;
&lt;/p&gt;

## Highlights

- üöÄ A single tool to replace `pip`, `pip-tools`, `pipx`, `poetry`, `pyenv`, `twine`, `virtualenv`,
  and more.
- ‚ö°Ô∏è [10-100x faster](https://github.com/astral-sh/uv/blob/main/BENCHMARKS.md) than `pip`.
- üóÇÔ∏è Provides [comprehensive project management](#projects), with a
  [universal lockfile](https://docs.astral.sh/uv/concepts/projects/layout#the-lockfile).
- ‚ùáÔ∏è [Runs scripts](#scripts), with support for
  [inline dependency metadata](https://docs.astral.sh/uv/guides/scripts#declaring-script-dependencies).
- üêç [Installs and manages](#python-versions) Python versions.
- üõ†Ô∏è [Runs and installs](#tools) tools published as Python packages.
- üî© Includes a [pip-compatible interface](#the-pip-interface) for a performance boost with a
  familiar CLI.
- üè¢ Supports Cargo-style [workspaces](https://docs.astral.sh/uv/concepts/projects/workspaces) for
  scalable projects.
- üíæ Disk-space efficient, with a [global cache](https://docs.astral.sh/uv/concepts/cache) for
  dependency deduplication.
- ‚è¨ Installable without Rust or Python via `curl` or `pip`.
- üñ•Ô∏è Supports macOS, Linux, and Windows.

uv is backed by [Astral](https://astral.sh), the creators of
[Ruff](https://github.com/astral-sh/ruff).

## Installation

Install uv with our standalone installers:

```bash
# On macOS and Linux.
curl -LsSf https://astral.sh/uv/install.sh | sh
```

```bash
# On Windows.
powershell -ExecutionPolicy ByPass -c &quot;irm https://astral.sh/uv/install.ps1 | iex&quot;
```

Or, from [PyPI](https://pypi.org/project/uv/):

```bash
# With pip.
pip install uv
```

```bash
# Or pipx.
pipx install uv
```

If installed via the standalone installer, uv can update itself to the latest version:

```bash
uv self update
```

See the [installation documentation](https://docs.astral.sh/uv/getting-started/installation/) for
details and alternative installation methods.

## Documentation

uv&#039;s documentation is available at [docs.astral.sh/uv](https://docs.astral.sh/uv).

Additionally, the command line reference documentation can be viewed with `uv help`.

## Features

### Projects

uv manages project dependencies and environments, with support for lockfiles, workspaces, and more,
similar to `rye` or `poetry`:

```console
$ uv init example
Initialized project `example` at `/home/user/example`

$ cd example

$ uv add ruff
Creating virtual environment at: .venv
Resolved 2 packages in 170ms
   Built example @ file:///home/user/example
Prepared 2 packages in 627ms
Installed 2 packages in 1ms
 + example==0.1.0 (from file:///home/user/example)
 + ruff==0.5.0

$ uv run ruff check
All checks passed!

$ uv lock
Resolved 2 packages in 0.33ms

$ uv sync
Resolved 2 packages in 0.70ms
Audited 1 package in 0.02ms
```

See the [project documentation](https://docs.astral.sh/uv/guides/projects/) to get started.

uv also supports building and publishing projects, even if they&#039;re not managed with uv. See the
[publish guide](https://docs.astral.sh/uv/guides/publish/) to learn more.

### Scripts

uv manages dependencies and environments for single-file scripts.

Create a new script and add inline metadata declaring its dependencies:

```console
$ echo &#039;import requests; print(requests.get(&quot;https://astral.sh&quot;))&#039; &gt; example.py

$ uv add --script example.py requests
Updated `example.py`
```

Then, run the script in an isolated virtual environment:

```console
$ uv run example.py
Reading inline script metadata from: example.py
Installed 5 packages in 12ms
&lt;Response [200]&gt;
```

See the [scripts documentation](https://docs.astral.sh/uv/guides/scripts/) to get started.

### Tools

uv executes and installs command-line tools provided by Python packages, similar to `pipx`.

Run a tool in an ephemeral environment using `uvx` (an alias for `uv tool run`):

```console
$ uvx pycowsay &#039;hello world!&#039;
Resolved 1 package in 167ms
Installed 1 package in 9ms
 + pycowsay==0.0.0.2
  &quot;&quot;&quot;

  ------------
&lt; hello world! &gt;
  ------------
   \   ^__^
    \  (oo)\_______
       (__)\       )\/\
           ||----w |
           ||     ||
```

Install a tool with `uv tool install`:

```console
$ uv tool install ruff
Resolved 1 package in 6ms
Installed 1 package in 2ms
 + ruff==0.5.0
Installed 1 executable: ruff

$ ruff --version
ruff 0.5.0
```

See the [tools documentation](https://docs.astral.sh/uv/guides/tools/) to get started.

### Python versions

uv installs Python and allows quickly switching between versions.

Install multiple Python versions:

```console
$ uv python install 3.10 3.11 3.12
Searching for Python versions matching: Python 3.10
Searching for Python versions matching: Python 3.11
Searching for Python versions matching: Python 3.12
Installed 3 versions in 3.42s
 + cpython-3.10.14-macos-aarch64-none
 + cpython-3.11.9-macos-aarch64-none
 + cpython-3.12.4-macos-aarch64-none
```

Download Python versions as needed:

```console
$ uv venv --python 3.12.0
Using Python 3.12.0
Creating virtual environment at: .venv
Activate with: source .venv/bin/activate

$ uv run --python pypy@3.8 -- python --version
Python 3.8.16 (a9dbdca6fc3286b0addd2240f11d97d8e8de187a, Dec 29 2022, 11:45:30)
[PyPy 7.3.11 with GCC Apple LLVM 13.1.6 (clang-1316.0.21.2.5)] on darwin
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt;&gt;
```

Use a specific Python version in the current directory:

```console
$ uv python pin 3.11
Pinned `.python-version` to `3.11`
```

See the [Python installation documentation](https://docs.astral.sh/uv/guides/install-python/) to get
started.

### The pip interface

uv provides a drop-in replacement for common `pip`, `pip-tools`, and `virtualenv` commands.

uv extends their interfaces with advanced features, such as dependency version overrides,
platform-independent resolutions, reproducible resolutions, alternative resolution strategies, and
more.

Migrate to uv without changing your existing workflows ‚Äî and experience a 10-100x speedup ‚Äî with the
`uv pip` interface.

Compile requirements into a platform-independent requirements file:

```console
$ uv pip compile docs/requirements.in \
   --universal \
   --output-file docs/requirements.txt
Resolved 43 packages in 12ms
```

Create a virtual environment:

```console
$ uv venv
Using Python 3.12.3
Creating virtual environment at: .venv
Activate with: source .venv/bin/activate
```

Install the locked requirements:

```console
$ uv pip sync docs/requirements.txt
Resolved 43 packages in 11ms
Installed 43 packages in 208ms
 + babel==2.15.0
 + black==24.4.2
 + certifi==2024.7.4
 ...
```

See the [pip interface documentation](https://docs.astral.sh/uv/pip/index/) to get started.

## Platform support

See uv&#039;s [platform support](https://docs.astral.sh/uv/reference/platforms/) document.

## Versioning policy

See uv&#039;s [versioning policy](https://docs.astral.sh/uv/reference/versioning/) document.

## Contributing

We are passionate about supporting contributors of all levels of experience and would love to see
you get involved in the project. See the
[contributing guide](https://github.com/astral-sh/uv/blob/main/CONTRIBUTING.md) to get started.

## FAQ

#### How do you pronounce uv?

It&#039;s pronounced as &quot;you - vee&quot; ([`/juÀê viÀê/`](https://en.wikipedia.org/wiki/Help:IPA/English#Key))

#### How should I stylize uv?

Just &quot;uv&quot;, please. See the [style guide](./STYLE.md#styling-uv) for details.

## Acknowledgements

uv&#039;s dependency resolver uses [PubGrub](https://github.com/pubgrub-rs/pubgrub) under the hood. We&#039;re
grateful to the PubGrub maintainers, especially [Jacob Finkelman](https://github.com/Eh2406), for
their support.

uv&#039;s Git implementation is based on [Cargo](https://github.com/rust-lang/cargo).

Some of uv&#039;s optimizations are inspired by the great work we&#039;ve seen in [pnpm](https://pnpm.io/),
[Orogene](https://github.com/orogene/orogene), and [Bun](https://github.com/oven-sh/bun). We&#039;ve also
learned a lot from Nathaniel J. Smith&#039;s [Posy](https://github.com/njsmith/posy) and adapted its
[trampoline](https://github.com/njsmith/posy/tree/main/src/trampolines/windows-trampolines/posy-trampoline)
for Windows support.

## License

uv is licensed under either of

- Apache License, Version 2.0, ([LICENSE-APACHE](LICENSE-APACHE) or
  &lt;https://www.apache.org/licenses/LICENSE-2.0&gt;)
- MIT license ([LICENSE-MIT](LICENSE-MIT) or &lt;https://opensource.org/licenses/MIT&gt;)

at your option.

Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in uv
by you, as defined in the Apache-2.0 license, shall be dually licensed as above, without any
additional terms or conditions.

&lt;div align=&quot;center&quot;&gt;
  &lt;a target=&quot;_blank&quot; href=&quot;https://astral.sh&quot; style=&quot;background:none&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/astral-sh/uv/main/assets/svg/Astral.svg&quot; alt=&quot;Made by Astral&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[biomejs/biome]]></title>
            <link>https://github.com/biomejs/biome</link>
            <guid>https://github.com/biomejs/biome</guid>
            <pubDate>Thu, 07 Aug 2025 00:06:06 GMT</pubDate>
            <description><![CDATA[A toolchain for web projects, aimed to provide functionalities to maintain them. Biome offers formatter and linter, usable via CLI and LSP.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/biomejs/biome">biomejs/biome</a></h1>
            <p>A toolchain for web projects, aimed to provide functionalities to maintain them. Biome offers formatter and linter, usable via CLI and LSP.</p>
            <p>Language: Rust</p>
            <p>Stars: 20,433</p>
            <p>Forks: 661</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre>./packages/@biomejs/biome/README.md</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[tauri-apps/tauri]]></title>
            <link>https://github.com/tauri-apps/tauri</link>
            <guid>https://github.com/tauri-apps/tauri</guid>
            <pubDate>Thu, 07 Aug 2025 00:06:05 GMT</pubDate>
            <description><![CDATA[Build smaller, faster, and more secure desktop and mobile applications with a web frontend.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/tauri-apps/tauri">tauri-apps/tauri</a></h1>
            <p>Build smaller, faster, and more secure desktop and mobile applications with a web frontend.</p>
            <p>Language: Rust</p>
            <p>Stars: 95,301</p>
            <p>Forks: 3,008</p>
            <p>Stars today: 40 stars today</p>
            <h2>README</h2><pre>&lt;img src=&quot;.github/splash.png&quot; alt=&quot;Tauri&quot; /&gt;

[![status](https://img.shields.io/badge/status-stable-blue.svg)](https://github.com/tauri-apps/tauri/tree/dev)
[![License](https://img.shields.io/badge/License-MIT%20or%20Apache%202-green.svg)](https://opencollective.com/tauri)
[![test core](https://img.shields.io/github/actions/workflow/status/tauri-apps/tauri/test-core.yml?label=test%20core&amp;logo=github)](https://github.com/tauri-apps/tauri/actions/workflows/test-core.yml)
[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Ftauri-apps%2Ftauri.svg?type=shield)](https://app.fossa.com/projects/git%2Bgithub.com%2Ftauri-apps%2Ftauri?ref=badge_shield)
[![Chat Server](https://img.shields.io/badge/chat-discord-7289da.svg)](https://discord.com/invite/tauri)
[![website](https://img.shields.io/badge/website-tauri.app-purple.svg)](https://tauri.app)
[![https://good-labs.github.io/greater-good-affirmation/assets/images/badge.svg](https://good-labs.github.io/greater-good-affirmation/assets/images/badge.svg)](https://good-labs.github.io/greater-good-affirmation)
[![support](https://img.shields.io/badge/sponsor-Open%20Collective-blue.svg)](https://opencollective.com/tauri)

## Introduction

Tauri is a framework for building tiny, blazingly fast binaries for all major desktop platforms. Developers can integrate any front-end framework that compiles to HTML, JS and CSS for building their user interface. The backend of the application is a rust-sourced binary with an API that the front-end can interact with.

The user interface in Tauri apps currently leverages [`tao`](https://docs.rs/tao) as a window handling library on macOS, Windows, Linux, Android and iOS. To render your application, Tauri uses [WRY](https://github.com/tauri-apps/wry), a library which provides a unified interface to the system webview, leveraging WKWebView on macOS &amp; iOS, WebView2 on Windows, WebKitGTK on Linux and Android System WebView on Android.

To learn more about the details of how all of these pieces fit together, please consult this [ARCHITECTURE.md](https://github.com/tauri-apps/tauri/blob/dev/ARCHITECTURE.md) document.

## Getting Started

If you are interested in making a tauri app, please visit the [documentation website](https://tauri.app).

The quickest way to get started is to install the [prerequisites](https://v2.tauri.app/start/prerequisites/) for your system and create a new project with [`create-tauri-app`](https://github.com/tauri-apps/create-tauri-app/#usage). For example with `npm`:

```sh
npm create tauri-app@latest
```

## Features

The list of Tauri&#039;s features includes, but is not limited to:

- Built-in app bundler to create app bundles in formats like `.app`, `.dmg`, `.deb`, `.rpm`, `.AppImage` and Windows installers like `.exe` (via NSIS) and `.msi` (via WiX).
- Built-in self updater (desktop only)
- System tray icons
- Native notifications
- Native WebView Protocol (tauri doesn&#039;t create a localhost http(s) server to serve the WebView contents)
- GitHub action for streamlined CI
- VS Code extension

### Platforms

Tauri currently supports development and distribution on the following platforms:

| Platform   | Versions                                                                                                        |
| :--------- | :-------------------------------------------------------------------------------------------------------------- |
| Windows    | 7 and above                                                                                                     |
| macOS      | 10.15 and above                                                                                                 |
| Linux      | webkit2gtk 4.0 for Tauri v1 (for example Ubuntu 18.04). webkit2gtk 4.1 for Tauri v2 (for example Ubuntu 22.04). |
| iOS/iPadOS | 9 and above                                                                                                     |
| Android    | 7 and above (currently 8 and above)                                                                             |

## Contributing

Before you start working on something, it&#039;s best to check if there is an existing issue first. It&#039;s also a good idea to stop by the Discord server and confirm with the team if it makes sense or if someone else is already working on it.

Please make sure to read the [Contributing Guide](./.github/CONTRIBUTING.md) before making a pull request.

Thank you to everyone contributing to Tauri!

### Documentation

Documentation in a polyglot system is a tricky proposition. To this end, we prefer to use inline documentation in the Rust &amp; JS source code as much as possible. Check out the hosting repository for the documentation site for further information: &lt;https://github.com/tauri-apps/tauri-docs&gt;

## Partners

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;middle&quot;&gt;
        &lt;a href=&quot;https://crabnebula.dev&quot; target=&quot;_blank&quot;&gt;
          &lt;img src=&quot;.github/sponsors/crabnebula.svg&quot; alt=&quot;CrabNebula&quot; width=&quot;283&quot;&gt;
        &lt;/a&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

For the complete list of sponsors please visit our [website](https://tauri.app#sponsors) and [Open Collective](https://opencollective.com/tauri).

## Organization

Tauri aims to be a sustainable collective based on principles that guide [sustainable free and open software communities](https://sfosc.org). To this end it has become a Programme within the [Commons Conservancy](https://commonsconservancy.org/), and you can contribute financially via [Open Collective](https://opencollective.com/tauri).

## Licenses

Code: (c) 2015 - Present - The Tauri Programme within The Commons Conservancy.

MIT or MIT/Apache 2.0 where applicable.

Logo: CC-BY-NC-ND

- Original Tauri Logo Designs by [Alve Larsson](https://alve.io/), [Daniel Thompson-Yvetot](https://github.com/nothingismagick) and [Guillaume Chau](https://github.com/akryum)

[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Ftauri-apps%2Ftauri.svg?type=large)](https://app.fossa.com/projects/git%2Bgithub.com%2Ftauri-apps%2Ftauri?ref=badge_large)
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[openobserve/openobserve]]></title>
            <link>https://github.com/openobserve/openobserve</link>
            <guid>https://github.com/openobserve/openobserve</guid>
            <pubDate>Thu, 07 Aug 2025 00:06:04 GMT</pubDate>
            <description><![CDATA[üöÄ 10x easier, üöÄ 140x lower storage cost, üöÄ high performance, üöÄ petabyte scale - Elasticsearch/Splunk/Datadog alternative for üöÄ (logs, metrics, traces, RUM, Error tracking, Session replay).]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/openobserve/openobserve">openobserve/openobserve</a></h1>
            <p>üöÄ 10x easier, üöÄ 140x lower storage cost, üöÄ high performance, üöÄ petabyte scale - Elasticsearch/Splunk/Datadog alternative for üöÄ (logs, metrics, traces, RUM, Error tracking, Session replay).</p>
            <p>Language: Rust</p>
            <p>Stars: 16,101</p>
            <p>Forks: 623</p>
            <p>Stars today: 25 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://openobserve.ai&quot;&gt;&lt;img src=&quot;https://openobserve.ai/img/logo/o2-logo-readme.svg&quot; alt=&quot;OpenObserve&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;em&gt;üöÄ 10x easier, üöÄ 140x lower storage cost, üöÄ high performance, üöÄ petabyte scale - Elasticsearch/Splunk/Datadog alternative for üöÄ (logs, metrics, traces).&lt;/em&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/openobserve/openobserve&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/last-commit/openobserve/openobserve&quot; alt=&quot;Last Commit&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/openobserve/openobserve/stargazers&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/openobserve/openobserve&quot; alt=&quot;GitHub Stars&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/openobserve/openobserve/issues&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/issues/openobserve/openobserve&quot; alt=&quot;GitHub Issues&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/openobserve/openobserve/graphs/contributors&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/contributors/openobserve/openobserve&quot; alt=&quot;Contributors&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/openobserve/openobserve/releases&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/v/release/openobserve/openobserve&quot; alt=&quot;GitHub Release&quot;&gt;
&lt;/a&gt;
&lt;/p&gt;

OpenObserve (O2 for short) is a cloud-native observability platform built specifically for logs, metrics, traces, analytics, RUM (Real User Monitoring - Performance, Errors, Session Replay) designed to work at petabyte scale.

It is straightforward and easy to operate, in contrast to Elasticsearch, which requires understanding and tuning numerous settings. Get OpenObserve up and running in under 2 minutes.

OpenObserve serves as a seamless replacement for Elasticsearch for users who ingest data using APIs and perform searches. OpenObserve comes with its own user interface, eliminating the need for separate installation.

You can reduce your log storage costs by ~140x compared to Elasticsearch by using OpenObserve. Below, we present the results from pushing logs from our production Kubernetes cluster to both Elasticsearch and OpenObserve using Fluent Bit.

![OpenObserve Vs Elasticsearch](./screenshots/zo_vs_es.png)

## üé• Introduction Video

[![OpenObserve Introduction](./screenshots/o2_intro.webp)](https://www.youtube.com/watch?v=4VwuC1tpRP4)

## üåü Features:

- **Logs, Metrics, Traces**: Comprehensive support for various data types.
- **OpenTelemetry Support**: Full compatibility with OTLP for logs, metrics, and traces.
- **Real User Monitoring (RUM)**: Includes performance tracking, error logging, and session replay.
- **Dashboards, Reports, Alerts**: Features over 18 different chart types for comprehensive data visualization for on-the-fly analysis and reporting along with alerting.
- **Pipelines**: Enrich, redact, reduce, normalize data on the fly. Stream processing for logs to metrics and more.
- **Advanced Embedded GUI**: Intuitive and user-friendly interface.
- **SQL and PromQL Support**: Query logs and traces with SQL, and metrics with SQL and PromQL.
- **Single Binary or HA Installation**: Install using a single binary for small deployments or in HA mode for large deployments.
- **Versatile Storage Options**: Supports local disk, S3, MinIO, GCS, Azure Blob Storage.
- **High Availability and Clustering**: Ensures reliable and scalable performance.
- **Dynamic Schema**: Adapts to your data structure seamlessly.
- **Built-in Authentication**: Secure and ready to use.
- **Ease of Operation**: Designed for simplicity and efficiency.
- **Seamless Upgrades**: Hassle-free updates.
- **Multilingual UI**: Supports 11 languages, including English, Spanish, German, French, Chinese, and more.

For a full list of features, check the [documentation](https://openobserve.ai/docs/#project-status-features-and-roadmap).

## ‚ö°Ô∏è Quick start

### üê≥ Docker:
```bash
docker run -d \
      --name openobserve \
      -v $PWD/data:/data \
      -p 5080:5080 \
      -e ZO_ROOT_USER_EMAIL=&quot;root@example.com&quot; \
      -e ZO_ROOT_USER_PASSWORD=&quot;Complexpass#123&quot; \
      public.ecr.aws/zinclabs/openobserve:latest
```

### üêô Docker Compose:
```yaml
services:
  openobserve:
    image: public.ecr.aws/zinclabs/openobserve:latest
    restart: unless-stopped
    environment:
      ZO_ROOT_USER_EMAIL: &quot;root@example.com&quot;
      ZO_ROOT_USER_PASSWORD: &quot;Complexpass#123&quot;
    ports:
      - &quot;5080:5080&quot;
    volumes:
      - data:/data
volumes:
  data:
```

For other ways to quickly install OpenObserve or use OpenObserve cloud, check [quickstart documentation](https://openobserve.ai/docs/quickstart).

For installing OpenObserve in HA mode, check [HA deployment documentation](https://openobserve.ai/docs/ha_deployment/).

&lt;!-- ## Enterprise Vs Open source Vs Cloud edition

OpenObserve is available in three different editions:


| Feature | Open Source (Self hosted) | Enterprise (Self hosted) | Cloud |
| --- | --- | --- | --- | 
| Logs | ‚úÖ | ‚úÖ | ‚úÖ |
| Metrics | ‚úÖ | ‚úÖ | ‚úÖ |
| Traces | ‚úÖ | ‚úÖ | ‚úÖ |
| RUM | ‚úÖ | ‚úÖ | ‚úÖ |
| Alerts | ‚úÖ | ‚úÖ | ‚úÖ |
| Dashboards | ‚úÖ | ‚úÖ | ‚úÖ |
| Reports | ‚úÖ | ‚úÖ | ‚úÖ |
| VRL functions | ‚úÖ | ‚úÖ | ‚úÖ |
| Pipelines | ‚úÖ | ‚úÖ | ‚úÖ |
| High Availability | ‚úÖ | ‚úÖ | ‚úÖ |
| Multitenancy (Organizations) | ‚úÖ | ‚úÖ | ‚úÖ |
| Dynamic schema and schema evolution | ‚úÖ | ‚úÖ | ‚úÖ |
| Advanced multilingual GUI | ‚úÖ | ‚úÖ | ‚úÖ |
| Single Sign On | ‚ùå | ‚úÖ | ‚úÖ |
| Role Based Access Control (RBAC) | ‚ùå | ‚úÖ | ‚úÖ |
| Federated search / Super cluster | ‚ùå | ‚úÖ | ‚ùå |
| Query management | ‚ùå | ‚úÖ | ‚ùå |
| Workload management (QoS) | ‚ùå | ‚úÖ | ‚ùå |
| Audit trail | ‚ùå | ‚úÖ | ‚ùå |
| Ability to influence roadmap | ‚ùå | ‚úÖ | ‚úÖ on enterprise plan |
| License | AGPL | Enterprise | Cloud |
| Support | Community | Enterprise | Cloud |
| Cost | Free | If self hosted, free for up to 200 GB/Day data ingested &lt;br&gt; Paid thereafter  | Free 200 GB/Month data ingested &lt;br&gt; Paid thereafter | --&gt;


## üì∑ Screenshots

### Home

![Home](./screenshots/zo_home.png)

### Logs

![Logs](./screenshots/logs.png)

### Traces (OpenTelemetry)

Trace details page
![Traces using OpenTelemetry](./screenshots/traces.png)

Golden metrics based on traces
![Traces golden metrics](./screenshots/traces-overall.png)

### Visualizations and Dashboards

![Dashboard](./screenshots/dashboard.png)
![Dashboard](./screenshots/dashboard2.png)
![Create panel](./screenshots/create-panel.png)
![Map](./screenshots/map.png)

### Front end monitoring

Performance analytics
![Performance](./screenshots/performance.png)

Session replay
![Session replay](./screenshots/session-replay.png)

Error tracking
![Error tracking](./screenshots/error-tracking.png)


### Alerts

![Alerts](./screenshots/alerts.png)


### Streams

![Streams](./screenshots/streams.png)

### Ingestion

![Ingestion](./screenshots/ingestion1.png)

### Pipeline

Pipeline
![Pipeline](./screenshots/pipeline.png)

Function
![Function](./screenshots/function.png)


### IAM

SSO (Single Sign On)
![SSO](./screenshots/sso.png)

RBAC (Role Based Access Control)
![RBAC](./screenshots/iam_rbac.png)


### SBOM

Software Bill of Materials for OpenObserve

#### Rust

SBOM can be found [here](./openobserve.cdx.xml). You can analyze it using [dependency track](https://dependencytrack.org/).

In order to generate the SBOM, you can use the following commands:

Install cargo-cyclonedx:

````bash
cargo install cargo-cyclonedx
````

Generate the SBOM:
```bash
cargo-cyclonedx cyclonedx
```

#### JavaScript

SBOM can be found [here](./web/sbom.json ). You can analyze it using [dependency track](https://dependencytrack.org/).

In order to generate the SBOM, you can use the following commands:

Install cyclonedx-npm:

````bash
npm install --global @cyclonedx/cyclonedx-npm
````

Generate the SBOM:
```bash
cd web
cyclonedx-npm &gt; sbom.json         
```


## ‚öñÔ∏è License

OpenObserve is licensed under the AGPL-3.0 license. For more details, see the [LICENSE](https://github.com/openobserve/openobserve/blob/main/LICENSE).

## üåç Community

### üîó Join OpenObserve community on Slack

[![Slack](./screenshots/slack.png)](https://short.openobserve.ai/community)

Easiest way to get support is to join the [Slack channel](https://short.openobserve.ai/community).

### üì± Join OpenObserve community on WeChat

&lt;img src=&quot;./screenshots/wechat_qr.jpg&quot; width=&quot;300&quot;&gt;
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[aome510/spotify-player]]></title>
            <link>https://github.com/aome510/spotify-player</link>
            <guid>https://github.com/aome510/spotify-player</guid>
            <pubDate>Thu, 07 Aug 2025 00:06:03 GMT</pubDate>
            <description><![CDATA[A Spotify player in the terminal with full feature parity]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/aome510/spotify-player">aome510/spotify-player</a></h1>
            <p>A Spotify player in the terminal with full feature parity</p>
            <p>Language: Rust</p>
            <p>Stars: 5,109</p>
            <p>Forks: 237</p>
            <p>Stars today: 40 stars today</p>
            <h2>README</h2><pre># spotify_player

## Table of Contents

- [Introduction](#introduction)
- [Examples](#examples)
- [Installation](#installation)
- [Features](#features)
  - [Spotify Connect](#spotify-connect)
  - [Streaming](#streaming)
  - [Media Control](#media-control)
  - [Image](#image)
  - [Notify](#notify)
  - [Mouse support](#mouse-support)
  - [Daemon](#daemon)
  - [Fuzzy search](#fuzzy-search)
  - [CLI commands](#cli-commands)
- [Commands](#commands)
- [Configurations](#configurations)
- [Caches](#caches)
  - [Logging](#logging)
- [Acknowledgement](#acknowledgement)

## Introduction

`spotify_player` is a fast, easy to use, and configurable terminal music player.

**Features**

- Minimalist UI with an intuitive paging and popup system.
- Highly [configurable](docs/config.md)
- Feature parity with the official Spotify application.
- Support remote control with [Spotify Connect](#spotify-connect).
- Support [streaming](#streaming) songs directly from the terminal.
- Support synced lyrics.
- Support [cross-platform media control](#media-control).
- Support [image rendering](#image).
- Support [desktop notification](#notify).
- Support running the application as [a daemon](#daemon)
- Offer a wide range of [CLI commands](#cli-commands)

## Examples

A demo of `spotify_player` `v0.5.0-pre-release` on [youtube](https://www.youtube.com/watch/Jbfe9GLNWbA) or on [asciicast](https://asciinema.org/a/446913):

Checkout [examples/README.md](./examples/README.md) for more examples.

## Installation

By default, the application&#039;s installed binary is `spotify_player`.

### Requirements

A Spotify Premium account is **required**.

#### Dependencies

##### Windows and MacOS

- [Rust and cargo](https://www.rust-lang.org/tools/install) as the build dependencies

##### Linux

- [Rust and cargo](https://www.rust-lang.org/tools/install) as the build dependencies
- install `openssl`, `alsa-lib` (`streaming` feature), `libdbus` (`media-control` feature).

  - For example, on Debian based systems, run the below command to install application&#039;s dependencies:

    ```shell
    sudo apt install libssl-dev libasound2-dev libdbus-1-dev
    ```
  - On RHEL/Fedora based systems, run the below command to install application&#039;s dependencies :

    ```shell
    sudo dnf install openssl-devel alsa-lib-devel dbus-devel
    ```
    or if you&#039;re using `yum`:
    ```shell
    sudo yum install openssl-devel alsa-lib-devel dbus-devel
    ```

### Binaries

Application&#039;s prebuilt binaries can be found in the [Releases Page](https://github.com/aome510/spotify-player/releases).

**Note**: to run the application, Linux systems need to install additional dependencies as specified in the [Dependencies section](#linux).

### Homebrew

Run `brew install spotify_player` to install the application.

### Scoop

Run `scoop install spotify-player` to install the application.

### Cargo

Run `cargo install spotify_player --locked` to install the application from [crates.io](https://crates.io/crates/spotify_player).

### Arch Linux

Run `pacman -S spotify-player` to install the application.

**Note**: Defaults to PulseAudio / Pipewire audio backend. For a different one, please consider modifying the [official PKGBUILD](https://gitlab.archlinux.org/archlinux/packaging/packages/spotify-player) and rebuilding it manually. See [Audio Backends](#audio-backend) for a list of options.


### Void Linux

Run `xbps-install -S spotify-player` to install the application.

### FreeBSD

Run `pkg install spotify-player` to install the `spotify_player` binary from FreeBSD ports.

### NetBSD

Using the package manager, run `pkgin install spotify-player` to install from the official repositories.

Building from source,

```
cd /usr/pkgsrc/audio/spotify-player
make install
```

### NixOS

[spotify-player](https://search.nixos.org/packages?channel=unstable&amp;show=spotify-player&amp;from=0&amp;size=50&amp;sort=relevance&amp;type=packages&amp;query=spotify-player) is available as a Nix package and can be installed via `nix-shell -p spotify-player` or as part of your system configuration.

If you want to build the source locally you can run `nix-shell` in the root of a checkout of the source code. The provided `shell.nix` file will install the build prerequisites.

### Docker

**Note**: [streaming](#streaming) feature is disabled when using the docker image.

You can download the binary image of the latest build from the `master` branch by running

```
docker pull aome510/spotify_player:latest
```

then run

```
docker run --rm -it aome510/spotify_player:latest
```

to run the application.

You can also use your local config folder to configure the application or your local cache folder to store the application&#039;s cache data when running the docker image:

```
docker run --rm \
-v $APP_CONFIG_FOLDER:/app/config/ \
-v $APP_CACHE_FOLDER:/app/cache/ \
-it aome510/spotify_player:latest
```

## Features

### Spotify Connect

To enable a full [Spotify connect](https://www.spotify.com/us/connect/) support, user will need to register a Spotify application and specify the application&#039;s `client_id` in the general configuration file as described in the [configuration documentation](docs/config.md#general).

More details about registering a Spotify application can be found in the [official Spotify documentation](https://developer.spotify.com/documentation/general/guides/authorization/app-settings/).

When `spotify_player` runs with your own `client_id`, press **D** (default shortcut for `SwitchDevice` command) to get the list of available devices, then press **enter** (default shortcut for `ChooseSelected` command) to connect to the selected device.

### Streaming

`spotify_player` supports streaming, which needs to be built/installed with `streaming` feature (**enabled** by default) **and** with an audio backend (`rodio-backend` by default). The streaming feature allows to `spotify_player` to play music directly from terminal.

The application uses [librespot](https://github.com/librespot-org/librespot) library to create an integrated Spotify client while running. The integrated client will register a Spotify speaker device under the `spotify-player` name, which is accessible on the [Spotify connect](#spotify-connect) device list.

#### Audio backend

`spotify_player` uses [rodio](https://github.com/RustAudio/rodio) as the default [audio backend](https://github.com/librespot-org/librespot/wiki/Audio-Backends). List of available audio backends:

- `alsa-backend`
- `pulseaudio-backend`
- `rodio-backend`
- `portaudio-backend`
- `jackaudio-backend`
- `rodiojack-backend`
- `sdl-backend`
- `gstreamer-backend`

User can change the audio backend when building/installing the application by specifying the `--features` option. For example, to install `spotify_player` with `pulseaudio-backend`, run

```shell
cargo install spotify_player --no-default-features --features pulseaudio-backend
```

**Note**:

- needs to specify `--no-default-features` here because `rodio-backend` is one of the default features.
- user will need to install additional dependencies depending on the selected audio backend. More details can be found in the [Librespot documentation](https://github.com/librespot-org/librespot/wiki/Compiling#general-dependencies).

The `streaming` feature can be also disabled upon installing by running

```shell
cargo install spotify_player --no-default-features
```

### Media Control

To enable media control support, `spotify_player` needs to be built/installed with `media-control` feature (**enabled** by default) and set the `enable_media_control` config option to `true` in the [general configuration file](docs/config.md#media-control).

Media control support is implemented using [MPRIS DBus](https://wiki.archlinux.org/title/MPRIS) on Linux and OS window event listener on Windows and MacOS.

### Image

To enable image rendering support, `spotify_player` needs to be built/installed with `image` feature (**disabled** by default). To install the application with `image` feature included, run:

```shell
cargo install spotify_player --features image
```

`spotify_player` supports rendering image in a full resolution if the application is run on either [Kitty](https://sw.kovidgoyal.net/kitty/graphics-protocol/) or [iTerm2](https://iterm2.com/documentation-images.html). Otherwise, the image will be displayed as [block characters](https://en.wikipedia.org/wiki/Block_Elements).

`spotify_player` also supports rendering images with `sixel` behind `sixel` feature flag, which also enables `image` feature:

```shell
cargo install spotify_player --features sixel
```

**Notes**:

- Not all terminals supported by [libsixel](https://github.com/saitoha/libsixel) are supported by `spotify_player` as it relies on a [third-party library](https://github.com/atanunq/viuer) for image rendering. A possible list of supported terminals can be found in [here](https://github.com/atanunq/viuer/blob/dc81f44a97727e04be0b000712e9233c92116ff8/src/printer/sixel.rs#L83-L95).
- Images rendered by `sixel` can have a _weird_ scale. It&#039;s recommended to tweak the `cover_img_scale` config option to get the best result as the scaling works differently with different terminals and fonts.

Examples of image rendering:

- iTerm2:

![iTerm2](https://user-images.githubusercontent.com/40011582/172966798-0aadc431-b0c3-4433-adf3-7526684fc2a0.png)

- Kitty:

![kitty](https://user-images.githubusercontent.com/40011582/172967028-8cfb2daa-1642-499a-a5bf-8ed77f2b3fac.png)

- Sixel (`foot` terminal, `cover_img_scale=1.8`):

![sixel](https://user-images.githubusercontent.com/40011582/219880331-58ac1c30-bbb0-4c99-a6cc-e5b7c9c81455.png)

- Others:

![others](https://user-images.githubusercontent.com/40011582/172967325-d2098037-e19e-440a-a38a-5b076253ecb1.png)

#### Pixelate

If your terminal supports high-res images, but you like the pixelated look you can enable the `pixelate` feature, which also enables the `image` feature:

```shell
cargo install spotify_player --features pixelate
```

The amount of pixels can be tweaked via the `cover_img_pixels` config option.

| `cover_img_pixels` | `8` | `16` | `32` | `64` |
|--------------------|-----|------|------|------|
|       example      | &lt;img width=&quot;100&quot; alt=&quot;8x8&quot; src=&quot;https://github.com/user-attachments/assets/4137aaea-ce28-4019-8cd5-2d14327e72e4&quot; /&gt; | &lt;img width=&quot;100&quot; alt=&quot;16x16&quot; src=&quot;https://github.com/user-attachments/assets/0ca94748-093a-468c-8fb3-1f5639666eb6&quot; /&gt; | &lt;img width=&quot;100&quot; alt=&quot;32x32&quot; src=&quot;https://github.com/user-attachments/assets/f5d0f2da-0439-47e4-91c9-3a2aa73ac90c&quot; /&gt; | &lt;img width=&quot;100&quot; alt=&quot;64x64&quot; src=&quot;https://github.com/user-attachments/assets/d06ef731-38fa-424d-9672-313f56c193d0&quot; /&gt; |

To temporarily disable the `pixelate` feature just set `cover_img_pixels` to a high value like `512`.

### Notify

To enable desktop notification support, `spotify_player` needs to be built/installed with `notify` feature (**disabled** by default). To install the application with `notify` feature included, run:

```shell
cargo install spotify_player --features notify
```

**Note**: the notification support in `MacOS` and `Windows` are quite restricted compared to `Linux`.

### Mouse support

Currently, the only supported use case for mouse is to seek to a position of the current playback by left-clicking to such position in the playback&#039;s progress bar.

### Daemon

To enable a [daemon](&lt;https://en.wikipedia.org/wiki/Daemon_(computing)&gt;) support, `spotify_player` needs to be built/installed with `daemon` feature (**disabled** by default). To install the application with `daemon` feature included, run:

```shell
cargo install spotify_player --features daemon
```

You can run the application as a daemon by specifying the `-d` or `--daemon` option: `spotify_player -d`.

**Notes**:

- `daemon` feature is not supported on Windows
- `daemon` feature requires the `streaming` feature to be enabled and the application to be built with [an audio backend](#audio-backend)
- because of the OS&#039;s restrictions, `daemon` feature doesn&#039;t work with the `media-control` feature on MacOS, which is **enabled by default**. In other words, if you want to use the `daemon` feature on MacOS, you must install the application with `media-control` feature **disabled**:

  ```shell
  cargo install spotify_player --no-default-features --features daemon,rodio-backend
  ```

### Fuzzy search

To enable [fuzzy search](https://en.wikipedia.org/wiki/Approximate_string_matching) support, `spotify_player` needs to be built/installed with `fzf` feature (**disabled** by default).

### CLI Commands

`spotify_player` offers several CLI commands to interact with Spotify:

- `get`: Get Spotify data (playlist/album/artist data, user&#039;s data, etc)
- `playback`: Interact with the playback (start a playback, play-pause, next, etc)
- `search`: Search spotify
- `connect`: Connect to a Spotify device
- `like`: Like currently playing track
- `authenticate`: Authenticate the application
- `playlist`: Playlist editing (new, delete, import, fork, etc)

For more details, run `spotify_player -h` or `spotify_player {command} -h`, in which `{command}` is a CLI command.

**Notes**

- When using the CLI for the first time, you&#039;ll need to run `spotify_player authenticate` to authenticate the application beforehand.
- Under the hood, CLI command is handled by sending requests to a `spotify_player` client socket running on port `client_port`, [a general application configuration](https://github.com/aome510/spotify-player/blob/master/docs/config.md#general) with a default value of `8080`. If there is no running application&#039;s instance, a new client will be created upon handling the CLI commands, which increases the latency of the command.

#### Scripting

The `spotify_player` command-line interface makes scripting easy.
With the `search` subcommand, you can search Spotify and retrieve data in JSON format, enabling queries with tools like [jq](https://jqlang.github.io/jq/).

Here‚Äôs an example of starting playback for the first track from a search query:

```sh
read -p &quot;Search spotify: &quot; query
spotify_player playback start track --id $(spotify_player search &quot;$query&quot; | jq &#039;.tracks.[0].id&#039; | xargs)
```

## Commands

To go to the shortcut help page, press `?` or `C-h` (default shortcuts for `OpenCommandHelp` command).

**Tips**:

- you can search in the shortcut help page (and some other pages) using `Search` command
- `RefreshPlayback` can be used to manually update the playback status.
- `RestartIntegratedClient` is useful when user wants to switch to another audio device (headphone, earphone, etc) without restarting the application, as the integrated client will be re-initialized with the new device.

List of supported commands:

| Command                        | Description                                                                                        | Default shortcuts  |
| ------------------------------ | -------------------------------------------------------------------------------------------------- | ------------------ |
| `NextTrack`                    | next track                                                                                         | `n`                |
| `PreviousTrack`                | previous track                                                                                     | `p`                |
| `ResumePause`                  | resume/pause based on the current playback                                                         | `space`            |
| `PlayRandom`                   | play a random track in the current context                                                         | `.`                |
| `Repeat`                       | cycle the repeat mode                                                                              | `C-r`              |
| `ToggleFakeTrackRepeatMode`    | toggle fake track repeat mode                                                                      | `M-r`              |
| `Shuffle`                      | toggle the shuffle mode                                                                            | `C-s`              |
| `VolumeChange`                 | change playback volume by an offset (default shortcuts use 5%)                                     | `+`, `-`           |
| `Mute`                         | toggle playback volume between 0% and previous level                                               | `_`                |
| `SeekForward`                  | seek forward by 5s                                                                                 | `&gt;`                |
| `SeekBackward`                 | seek backward by 5s                                                                                | `&lt;`                |
| `Quit`                         | quit the application                                                                               | `C-c`, `q`         |
| `ClosePopup`                   | close a popup                                                                                      | `esc`              |
| `SelectNextOrScrollDown`       | select the next item in a list/table or scroll down (supports vim-style count: 5j)                 | `j`, `C-n`, `down` |
| `SelectPreviousOrScrollUp`     | select the previous item in a list/table or scroll up (supports vim-style count: 10k)              | `k`, `C-p`, `up`   |
| `PageSelectNextOrScrollDown`   | select the next page item in a list/table or scroll a page down (supports vim-style count: 3C-f)   | `page_down`, `C-f` |
| `PageSelectPreviousOrScrollUp` | select the previous page item in a list/table or scroll a page up (supports vim-style count: 2C-b) | `page_up`, `C-b`   |
| `SelectFirstOrScrollToTop`     | select the first item in a list/table or scroll to the top                                         | `g g`, `home`      |
| `SelectLastOrScrollToBottom`   | select the last item in a list/table or scroll to the bottom                                       | `G`, `end`         |
| `ChooseSelected`               | choose the selected item                                                                           | `enter`            |
| `RefreshPlayback`              | manually refresh the current playback                                                              | `r`                |
| `RestartIntegratedClient`      | restart the integrated client (`streaming` feature only)                                           | `R`                |
| `ShowActionsOnSelectedItem`    | open a popup showing actions on a selected item                                                    | `g a`, `C-space`   |
| `ShowActionsOnCurrentTrack`    | open a popup showing actions on the current track                                                  | `a`                |
| `AddSelectedItemToQueue`       | add the selected item to queue                                                                     | `Z`, `C-z`         |
| `FocusNextWindow`              | focus the next focusable window (if any)                                                           | `tab`              |
| `FocusPreviousWindow`          | focus the previous focusable window (if any)                                                       | `backtab`          |
| `SwitchTheme`                  | open a popup for switching theme                                                                   | `T`                |
| `SwitchDevice`                 | open a popup for switching device                                                                  | `D`                |
| `Search`                       | open a popup for searching in the current page                                                     | `/`                |
| `BrowseUserPlaylists`          | open a popup for browsing user&#039;s playlists                                                         | `u p`              |
| `BrowseUserFollowedArtists`    | open a popup for browsing user&#039;s followed artists                           

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[rolldown/rolldown]]></title>
            <link>https://github.com/rolldown/rolldown</link>
            <guid>https://github.com/rolldown/rolldown</guid>
            <pubDate>Thu, 07 Aug 2025 00:06:02 GMT</pubDate>
            <description><![CDATA[Fast Rust bundler for JavaScript/TypeScript with Rollup-compatible API.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/rolldown/rolldown">rolldown/rolldown</a></h1>
            <p>Fast Rust bundler for JavaScript/TypeScript with Rollup-compatible API.</p>
            <p>Language: Rust</p>
            <p>Stars: 11,747</p>
            <p>Forks: 629</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://rolldown.rs&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;
    &lt;img width=&quot;180&quot; src=&quot;https://rolldown.rs/rolldown-round.svg&quot; alt=&quot;Rolldown logo&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;

[![MIT licensed][badge-license]][url-license]
[![NPM version][badge-npm-version]][url-npm]
[![CodSpeed Badge](https://img.shields.io/endpoint?url=https://codspeed.io/badge.json)](https://codspeed.io/rolldown/rolldown)
[![Discord chat][badge-discord]][discord-url]
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/rolldown/rolldown)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

[![NPM Unpacked Size (with version)](https://img.shields.io/npm/unpacked-size/rolldown/latest?label=npm)][url-npm]
[![NPM Unpacked Size darwin-arm64](https://img.shields.io/npm/unpacked-size/%40rolldown%2Fbinding-darwin-arm64/latest?label=darwin-arm64)](https://www.npmjs.com/package/@rolldown/binding-darwin-arm64)
[![NPM Unpacked Size darwin-x64](https://img.shields.io/npm/unpacked-size/%40rolldown%2Fbinding-darwin-x64/latest?label=darwin-x64)](https://www.npmjs.com/package/@rolldown/binding-darwin-x64)
[![NPM Unpacked Size linux-x64-gnu](https://img.shields.io/npm/unpacked-size/%40rolldown%2Fbinding-linux-x64-gnu/latest?label=linux-x64-gnu)](https://www.npmjs.com/package/@rolldown/binding-linux-x64-gnu)
[![NPM Unpacked Size win32-x64](https://img.shields.io/npm/unpacked-size/%40rolldown%2Fbinding-win32-x64-msvc/latest?label=win32-x64)](https://www.npmjs.com/package/@rolldown/binding-win32-x64-msvc)
[![NPM Unpacked Size wasm32-wasi](https://img.shields.io/npm/unpacked-size/%40rolldown%2Fbinding-wasm32-wasi/latest?label=wasm32-wasi)](https://www.npmjs.com/package/@rolldown/binding-wasm32-wasi)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

[![pkg.pr.new](https://pkg.pr.new/badge/pkg.pr.new/pkg.pr.new?style=flat&amp;color=000&amp;logoSize=auto)](https://pkg.pr.new/~/rolldown/rolldown)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

[![rolldown-starter-stackblitz](https://developer.stackblitz.com/img/open_in_stackblitz.svg)](https://stackblitz.com/fork/github/rolldown/rolldown-starter-stackblitz)

&lt;/div&gt;

&gt; üöß **Beta Software**
&gt;
&gt; Rolldown is currently in beta status. While it can already handle most production use cases, there may still be bugs and rough edges. Most notably, the built-in minification feature is still in alpha status.

# Rolldown

Rolldown is a JavaScript/TypeScript bundler written in Rust intended to serve as the future bundler used in [Vite](https://vitejs.dev/). It provides Rollup-compatible APIs and plugin interface, but will be more similar to esbuild in scope.

For more information, please check out the documentation at [rolldown.rs](https://rolldown.rs/about).

## VoidZero Inc.

Rolldown is a project of [VoidZero](https://voidzero.dev/), see our announcement [Announcing VoidZero - Next Generation Toolchain for JavaScript](https://voidzero.dev/posts/announcing-voidzero-inc).

If you have requirements for JavaScript tools at scale, please [get in touch](https://forms.gle/WQgjyzYJpwurpxWKA)!

## Contributing

We would love to have more contributors involved!

To get started, please read our [Contributing Guide](https://rolldown.rs/contrib-guide/).

## Credits

The Rolldown project is heavily inspired by:

- [Rollup](https://github.com/rollup/rollup), created by [Rich Harris](https://github.com/Rich-Harris) and maintained by [Lukas Taegert-Atkinson](https://github.com/lukastaegert).
- [esbuild](https://github.com/evanw/esbuild), created by [Evan Wallace](https://github.com/evanw).

And supported by:

- [napi-rs](https://github.com/napi-rs/napi-rs) for Node.js add-ons in Rust via Node-API.
- [oxc](https://github.com/oxc-project/oxc) for the underlying parser, resolver, and sourcemap support.

## Licenses

This project is licensed under the [MIT License](LICENSE).

This project also partially contains code derived or copied from the following projects:

- [rollup(MIT)](https://github.com/rollup/rollup/blob/680912e2ceb42c8d5e571e01c6ece0e4889aecbb/LICENSE-CORE.md)
- [esbuild(MIT)](https://github.com/evanw/esbuild/blob/0c8a0a901d9a6c7bbff9b4dd347c8a3f65f6c6dd/LICENSE.md)

Licenses of these projects are listed in [THIRD-PARTY-LICENSE](/THIRD-PARTY-LICENSE)

[badge-discord]: https://img.shields.io/discord/1079625926024900739?logo=discord&amp;label=Discord
[discord-url]: https://chat.rolldown.rs
[badge-license]: https://img.shields.io/badge/license-MIT-blue.svg
[url-license]: https://github.com/rolldown/rolldown/blob/main/LICENSE
[badge-npm-version]: https://img.shields.io/npm/v/rolldown/latest?color=brightgreen
[url-npm]: https://www.npmjs.com/package/rolldown/v/latest
[badge-binary-size-windows]: [https://img.shields.io/npm/unpacked-size/%40rolldown%2Fbinding-win32-x64-msvc/latest]
[badge-binary-size-macos]: [https://img.shields.io/npm/unpacked-size/%40rolldown%2Fbinding-darwin-arm64/latest]
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[seanmonstar/warp]]></title>
            <link>https://github.com/seanmonstar/warp</link>
            <guid>https://github.com/seanmonstar/warp</guid>
            <pubDate>Thu, 07 Aug 2025 00:06:01 GMT</pubDate>
            <description><![CDATA[A super-easy, composable, web server framework for warp speeds.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/seanmonstar/warp">seanmonstar/warp</a></h1>
            <p>A super-easy, composable, web server framework for warp speeds.</p>
            <p>Language: Rust</p>
            <p>Stars: 9,959</p>
            <p>Forks: 736</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre># warp

[![crates.io](https://img.shields.io/crates/v/warp.svg)](https://crates.io/crates/warp)
[![Released API docs](https://docs.rs/warp/badge.svg)](https://docs.rs/warp)
[![MIT licensed](https://img.shields.io/badge/license-MIT-blue.svg)](./LICENSE)
[![GHA Build Status](https://github.com/seanmonstar/warp/workflows/CI/badge.svg)](https://github.com/seanmonstar/warp/actions?query=workflow%3ACI)
[![Discord chat][discord-badge]][discord-url]

A super-easy, composable, web server framework for warp speeds.

The fundamental building block of `warp` is the `Filter`: they can be combined
and composed to express rich requirements on requests.

Thanks to its `Filter` system, warp provides these out of the box:

* Path routing and parameter extraction
* Header requirements and extraction
* Query string deserialization
* JSON and Form bodies
* Multipart form data
* Static Files and Directories
* Websockets
* Access logging
* Gzip, Deflate, and Brotli compression

Since it builds on top of [hyper](https://hyper.rs), you automatically get:

- HTTP/1
- HTTP/2
- Asynchronous
- One of the fastest HTTP implementations
- Tested and **correct**

## Example

Add warp and Tokio to your dependencies:

```toml
tokio = { version = &quot;1&quot;, features = [&quot;full&quot;] }
warp = { version = &quot;0.4&quot;, features = [&quot;server&quot;] }
```

And then get started in your `main.rs`:

```rust
use warp::Filter;

#[tokio::main]
async fn main() {
    // GET /hello/warp =&gt; 200 OK with body &quot;Hello, warp!&quot;
    let hello = warp::path!(&quot;hello&quot; / String)
        .map(|name| format!(&quot;Hello, {}!&quot;, name));

    warp::serve(hello)
        .run(([127, 0, 0, 1], 3030))
        .await;
}
```

For more information you can check the [docs](https://docs.rs/warp) or the [examples](https://github.com/seanmonstar/warp/tree/master/examples).

[discord-badge]: https://img.shields.io/discord/500028886025895936.svg?logo=discord
[discord-url]: https://discord.gg/RFsPjyt
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[eythaann/Seelen-UI]]></title>
            <link>https://github.com/eythaann/Seelen-UI</link>
            <guid>https://github.com/eythaann/Seelen-UI</guid>
            <pubDate>Thu, 07 Aug 2025 00:06:00 GMT</pubDate>
            <description><![CDATA[The Fully Customizable Desktop Environment for Windows 10/11.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/eythaann/Seelen-UI">eythaann/Seelen-UI</a></h1>
            <p>The Fully Customizable Desktop Environment for Windows 10/11.</p>
            <p>Language: Rust</p>
            <p>Stars: 10,109</p>
            <p>Forks: 312</p>
            <p>Stars today: 19 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[vectordotdev/vector]]></title>
            <link>https://github.com/vectordotdev/vector</link>
            <guid>https://github.com/vectordotdev/vector</guid>
            <pubDate>Thu, 07 Aug 2025 00:05:59 GMT</pubDate>
            <description><![CDATA[A high-performance observability data pipeline.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/vectordotdev/vector">vectordotdev/vector</a></h1>
            <p>A high-performance observability data pipeline.</p>
            <p>Language: Rust</p>
            <p>Stars: 20,076</p>
            <p>Forks: 1,818</p>
            <p>Stars today: 12 stars today</p>
            <h2>README</h2><pre>[![Nightly](https://github.com/vectordotdev/vector/actions/workflows/nightly.yml/badge.svg)](https://github.com/vectordotdev/vector/actions/workflows/nightly.yml)
[![E2E Test Suite](https://github.com/vectordotdev/vector/actions/workflows/e2e.yml/badge.svg)](https://github.com/vectordotdev/vector/actions/workflows/e2e.yml)
[![Component Features](https://github.com/vectordotdev/vector/actions/workflows/component_features.yml/badge.svg)](https://github.com/vectordotdev/vector/actions/workflows/component_features.yml)

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;website/static/img/diagram.svg&quot; alt=&quot;Vector&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;
    &lt;a href=&quot;https://vector.dev/docs/setup/quickstart/&quot;&gt;Quickstart&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://vector.dev/docs/&quot;&gt;Docs&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://vector.dev/guides/&quot;&gt;Guides&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://vector.dev/components/&quot;&gt;Integrations&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://chat.vector.dev&quot;&gt;Chat&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://vector.dev/releases/latest/download/&quot;&gt;Download&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://rust-doc.vector.dev/&quot;&gt;Rust Crate Docs&lt;/a&gt;
  &lt;/strong&gt;
&lt;/p&gt;

## What is Vector?

Vector is a high-performance, end-to-end (agent &amp; aggregator) observability data
pipeline that puts you in control of your observability data.
[Collect][docs.sources], [transform][docs.transforms], and [route][docs.sinks]
all your logs and metrics to any vendors you want today and any other
vendors you may want tomorrow. Vector enables dramatic cost reduction, novel
data enrichment, and data security where you need it, not where it is most
convenient for your vendors. Additionally, it is open source and up to 10x
faster than every alternative in the space.

To get started, follow our [**quickstart guide**][docs.quickstart] or [**install
Vector**][docs.installation].

Vector is maintained by the Datadog&#039;s [Community Open Source Engineering team](https://opensource.datadoghq.com/about/#the-community-open-source-engineering-team).

### Principles

* **Reliable** - Built in [Rust][urls.rust], Vector&#039;s primary design goal is reliability.
* **End-to-end** - Deploys as an [agent][docs.roles#agent] or [aggregator][docs.roles#aggregator]. Vector is a complete platform.
* **Unified** - [Logs][docs.data-model.log], [metrics][docs.data-model.metric] (beta), and traces (coming soon). One tool for all of your data.

### Use cases

* Reduce total observability costs.
* Transition vendors without disrupting workflows.
* Enhance data quality and improve insights.
* Consolidate agents and eliminate agent fatigue.
* Improve overall observability performance and reliability.

### Community

* Vector is relied on by startups and enterprises like **Atlassian**, **T-Mobile**,
  **Comcast**, **Zendesk**, **Discord**, **Fastly**, **CVS**, **Trivago**,
  **Tuple**, **Douban**, **Visa**, **Mambu**, **Blockfi**, **Claranet**,
  **Instacart**, **Forcepoint**, and [many more][urls.production_users].
* Vector is **downloaded over 100,000 times per day**.
* Vector&#039;s largest user **processes over 500TB daily**.
* Vector has **over 500 contributors** and growing.

## Documentation

All user documentation is available at **[vector.dev/docs](https://vector.dev/docs)**.

Other Resources:

* [**Vector Calendar**][urls.vector_calendar]
* **Policies**:
  * [**Code of Conduct**][urls.vector_code_of_conduct]
  * [**Contributing**][urls.vector_contributing_policy]
  * [**Privacy**][urls.vector_privacy_policy]
  * [**Releases**][urls.vector_releases_policy]
  * [**Versioning**][urls.vector_versioning_policy]
  * [**Security**][urls.vector_security_policy]

## Comparisons

### Performance

The following performance tests demonstrate baseline performance between
common protocols with the exception of the Regex Parsing test.

| Test                                                                                                                   | Vector          | Filebeat | FluentBit       | FluentD   | Logstash  | SplunkUF        | SplunkHF |
| ---------------------------------------------------------------------------------------------------------------------- | --------------- | -------- | --------------- | --------- | --------- | --------------- | -------- |
| [TCP to Blackhole](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/tcp_to_blackhole_performance) | _**86mib/s**_   | n/a      | 64.4mib/s       | 27.7mib/s | 40.6mib/s | n/a             | n/a      |
| [File to TCP](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_to_tcp_performance)           | _**76.7mib/s**_ | 7.8mib/s | 35mib/s         | 26.1mib/s | 3.1mib/s  | 40.1mib/s       | 39mib/s  |
| [Regex Parsing](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/regex_parsing_performance)       | 13.2mib/s       | n/a      | _**20.5mib/s**_ | 2.6mib/s  | 4.6mib/s  | n/a             | 7.8mib/s |
| [TCP to HTTP](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/tcp_to_http_performance)           | _**26.7mib/s**_ | n/a      | 19.6mib/s       | &lt;1mib/s   | 2.7mib/s  | n/a             | n/a      |
| [TCP to TCP](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/tcp_to_tcp_performance)             | 69.9mib/s       | 5mib/s   | 67.1mib/s       | 3.9mib/s  | 10mib/s   | _**70.4mib/s**_ | 7.6mib/s |

To learn more about our performance tests, please see the [Vector test harness][urls.vector_test_harness].

### Correctness

The following correctness tests are not exhaustive, but they demonstrate
fundamental differences in quality and attention to detail:

| Test                                                                                                                                 | Vector | Filebeat | FluentBit | FluentD | Logstash | Splunk UF | Splunk HF |
| ------------------------------------------------------------------------------------------------------------------------------------ | ------ | -------- | --------- | ------- | -------- | --------- | --------- |
| [Disk Buffer Persistence](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/disk_buffer_persistence_correctness) | **‚úì**  | ‚úì        |           |         | ‚ö†        | ‚úì         | ‚úì         |
| [File Rotate (create)](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_rotate_create_correctness)         | **‚úì**  | ‚úì        | ‚úì         | ‚úì       | ‚úì        | ‚úì         | ‚úì         |
| [File Rotate (copytruncate)](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_rotate_truncate_correctness) | **‚úì**  |          |           |         |          | ‚úì         | ‚úì         |
| [File Truncation](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_truncate_correctness)                   | **‚úì**  | ‚úì        | ‚úì         | ‚úì       | ‚úì        | ‚úì         | ‚úì         |
| [Process (SIGHUP)](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/sighup_correctness)                         | **‚úì**  |          |           |         | ‚ö†        | ‚úì         | ‚úì         |
| [JSON (wrapped)](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/wrapped_json_correctness)                     | **‚úì**  | ‚úì        | ‚úì         | ‚úì       | ‚úì        | ‚úì         | ‚úì         |

To learn more about our correctness tests, please see the [Vector test harness][urls.vector_test_harness].

### Features

Vector is an end-to-end, unified, open data platform.

|                     | **Vector** | Beats | Fluentbit | Fluentd | Logstash | Splunk UF | Splunk HF | Telegraf |
| ------------------- | ---------- | ----- | --------- | ------- | -------- | --------- | --------- | -------- |
| **End-to-end**      | **‚úì**      |       |           |         |          |           |           | ‚úì        |
| Agent               | **‚úì**      | ‚úì     | ‚úì         |         |          | ‚úì         |           | ‚úì        |
| Aggregator          | **‚úì**      |       |           | ‚úì       | ‚úì        |           | ‚úì         | ‚úì        |
| **Unified**         | **‚úì**      |       |           |         |          |           |           | ‚úì        |
| Logs                | **‚úì**      | ‚úì     | ‚úì         | ‚úì       | ‚úì        | ‚úì         | ‚úì         | ‚úì        |
| Metrics             | **‚úì**      | ‚ö†     | ‚ö†         | ‚ö†       | ‚ö†        | ‚ö†         | ‚ö†         | ‚úì        |
| Traces              | üöß         |       |           |         |          |           |           |          |
| **Open**            | **‚úì**      |       | ‚úì         | ‚úì       |          |           |           | ‚úì        |
| Open-source         | **‚úì**      | ‚úì     | ‚úì         | ‚úì       | ‚úì        |           |           | ‚úì        |
| Vendor-neutral      | **‚úì**      |       | ‚úì         | ‚úì       |          |           |           | ‚úì        |
| **Reliability**     | **‚úì**      |       |           |         |          |           |           |          |
| Memory-safe         | **‚úì**      |       |           |         |          |           |           | ‚úì        |
| Delivery guarantees | **‚úì**      |       |           |         |          | ‚úì         | ‚úì         |          |
| Multi-core          | **‚úì**      | ‚úì     | ‚úì         | ‚úì       | ‚úì        | ‚úì         | ‚úì         | ‚úì        |


‚ö† = Not interoperable, metrics are represented as structured logs

---

&lt;p align=&quot;center&quot;&gt;
  Developed with ‚ù§Ô∏è by &lt;strong&gt;&lt;a href=&quot;https://datadoghq.com&quot;&gt;Datadog&lt;/a&gt;&lt;/strong&gt; - &lt;a href=&quot;https://github.com/vectordotdev/vector/security/policy&quot;&gt;Security Policy&lt;/a&gt; - &lt;a href=&quot;https://github.com/vectordotdev/vector/blob/master/PRIVACY.md&quot;&gt;Privacy Policy&lt;/a&gt;
&lt;/p&gt;

[docs.about.concepts]: https://vector.dev/docs/introduction/concepts/
[docs.about.introduction]: https://vector.dev/docs/introduction/
[docs.administration.monitoring]: https://vector.dev/docs/administration/monitoring/
[docs.administration.management]: https://vector.dev/docs/administration/management/
[docs.administration.upgrading]: https://vector.dev/docs/administration/upgrading/
[docs.administration.validating]: https://vector.dev/docs/administration/validating/
[docs.architecture.concurrency-model]: https://vector.dev/docs/architecture/concurrency-model/
[docs.architecture.data-model]: https://vector.dev/docs/architecture/data-model/
[docs.architecture.pipeline-model]: https://vector.dev/docs/architecture/pipeline-model/
[docs.architecture.runtime-model]: https://vector.dev/docs/architecture/runtime-model/
[docs.configuration.sinks]: https://vector.dev/docs/reference/configuration/sinks/
[docs.configuration.sources]: https://vector.dev/docs/reference/configuration/sources/
[docs.configuration.tests]: https://vector.dev/docs/reference/configuration/tests/
[docs.configuration.transforms]: https://vector.dev/docs/reference/configuration/transforms/
[docs.configuration.enrichment_tables]: https://vector.dev/docs/reference/configuration/global-options/#enrichment_tables
[docs.data-model.log]: https://vector.dev/docs/architecture/data-model/log/
[docs.data-model.metric]: https://vector.dev/docs/architecture/data-model/metric/
[docs.deployment.roles]: https://vector.dev/docs/setup/deployment/roles/
[docs.deployment.topologies]: https://vector.dev/docs/setup/deployment/topologies/
[docs.deployment]: https://vector.dev/docs/setup/deployment/
[docs.installation.manual]: https://vector.dev/docs/setup/installation/manual/
[docs.installation.operating_systems]: https://vector.dev/docs/setup/installation/operating-systems/
[docs.installation.package_managers]: https://vector.dev/docs/setup/installation/package-managers/
[docs.installation.platforms]: https://vector.dev/docs/setup/installation/platforms/
[docs.installation]: https://vector.dev/docs/setup/installation/
[docs.architecture.adaptive-request-concurrency]: https://vector.dev/docs/architecture/arc/
[docs.platforms.kubernetes]: https://vector.dev/docs/setup/installation/platforms/kubernetes/
[docs.quickstart]: https://vector.dev/docs/setup/quickstart/
[docs.reference.api]: https://vector.dev/docs/reference/api/
[docs.reference.cli]: https://vector.dev/docs/reference/cli/
[docs.reference.vrl]: https://vector.dev/docs/reference/vrl/
[docs.roles#agent]: https://vector.dev/docs/setup/deployment/roles/#agent
[docs.roles#aggregator]: https://vector.dev/docs/setup/deployment/roles/#aggregator
[docs.setup.installation]: https://vector.dev/docs/setup/installation/
[docs.setup.quickstart]: https://vector.dev/docs/setup/quickstart/
[docs.sinks.aws_cloudwatch_logs]: https://vector.dev/docs/reference/configuration/sinks/aws_cloudwatch_logs/
[docs.sinks.aws_s3]: https://vector.dev/docs/reference/configuration/sinks/aws_s3/
[docs.sinks.clickhouse]: https://vector.dev/docs/reference/configuration/sinks/clickhouse/
[docs.sinks.elasticsearch]: https://vector.dev/docs/reference/configuration/sinks/elasticsearch/
[docs.sinks.gcp_cloud_storage]: https://vector.dev/docs/reference/configuration/sinks/gcp_cloud_storage/
[docs.sinks]: https://vector.dev/docs/reference/configuration/sinks/
[docs.sources.docker_logs]: https://vector.dev/docs/reference/configuration/sources/docker_logs/
[docs.sources.file]: https://vector.dev/docs/reference/configuration/sources/file/
[docs.sources.http]: https://vector.dev/docs/reference/configuration/sources/http/
[docs.sources.journald]: https://vector.dev/docs/reference/configuration/sources/journald/
[docs.sources.kafka]: https://vector.dev/docs/reference/configuration/sources/kafka/
[docs.sources.socket]: https://vector.dev/docs/reference/configuration/sources/socket/
[docs.sources]: https://vector.dev/docs/reference/configuration/sources/
[docs.transforms.dedupe]: https://vector.dev/docs/reference/configuration/transforms/dedupe/
[docs.transforms.filter]: https://vector.dev/docs/reference/configuration/transforms/filter/
[docs.transforms.log_to_metric]: https://vector.dev/docs/reference/configuration/transforms/log_to_metric/
[docs.transforms.lua]: https://vector.dev/docs/reference/configuration/transforms/lua/
[docs.transforms.remap]: https://vector.dev/docs/reference/configuration/transforms/remap/
[docs.transforms]: https://vector.dev/docs/reference/configuration/transforms/
[docs.introduction.architecture]: https://vector.dev/docs/architecture/
[docs.introduction.guarantees]: https://vector.dev/docs/introduction/guarantees/
[docs.introduction.architecture]: https://vector.dev/docs/architecture/
[urls.production_users]: https://github.com/vectordotdev/vector/issues/790
[urls.rust]: https://www.rust-lang.org/
[urls.vector_calendar]: https://calendar.vector.dev
[urls.vector_chat]: https://chat.vector.dev
[urls.vector_code_of_conduct]: https://github.com/vectordotdev/vector/blob/master/CODE_OF_CONDUCT.md
[urls.vector_contributing_policy]: https://github.com/vectordotdev/vector/blob/master/CONTRIBUTING.md
[urls.vector_community]: https://vector.dev/community/
[urls.vector_privacy_policy]: https://github.com/vectordotdev/vector/blob/master/PRIVACY.md
[urls.vector_release_policy]: https://github.com/vectordotdev/vector/blob/master/RELEASING.md
[urls.vector_releases]: https://vector.dev/releases/
[urls.vector_releases_policy]: https://github.com/vectordotdev/vector/blob/master/RELEASES.md
[urls.vector_security_policy]: https://github.com/vectordotdev/vector/security/policy
[urls.vector_test_harness]: https://github.com/vectordotdev/vector-test-harness/
[urls.vector_twitter]: https://twitter.com/vectordotdev
[urls.vector_versioning_policy]: https://github.com/vectordotdev/vector/blob/master/VERSIONING.md
[urls.vote_feature]: https://github.com/vectordotdev/vector/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc+label%3A%22type%3A+feature%22
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[cocoindex-io/cocoindex]]></title>
            <link>https://github.com/cocoindex-io/cocoindex</link>
            <guid>https://github.com/cocoindex-io/cocoindex</guid>
            <pubDate>Thu, 07 Aug 2025 00:05:58 GMT</pubDate>
            <description><![CDATA[Data transformation framework for AI. Ultra performant, with incremental processing.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/cocoindex-io/cocoindex">cocoindex-io/cocoindex</a></h1>
            <p>Data transformation framework for AI. Ultra performant, with incremental processing.</p>
            <p>Language: Rust</p>
            <p>Stars: 2,374</p>
            <p>Forks: 162</p>
            <p>Stars today: 15 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://cocoindex.io/images/github.svg&quot; alt=&quot;CocoIndex&quot;&gt;
&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt;Data transformation for AI&lt;/h1&gt;

&lt;div align=&quot;center&quot;&gt;

[![GitHub](https://img.shields.io/github/stars/cocoindex-io/cocoindex?color=5B5BD6)](https://github.com/cocoindex-io/cocoindex)
[![Documentation](https://img.shields.io/badge/Documentation-394e79?logo=readthedocs&amp;logoColor=00B9FF)](https://cocoindex.io/docs/getting_started/quickstart)
[![License](https://img.shields.io/badge/license-Apache%202.0-5B5BD6?logoColor=white)](https://opensource.org/licenses/Apache-2.0)
[![PyPI version](https://img.shields.io/pypi/v/cocoindex?color=5B5BD6)](https://pypi.org/project/cocoindex/)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/cocoindex)](https://pypistats.org/packages/cocoindex)
[![CI](https://github.com/cocoindex-io/cocoindex/actions/workflows/CI.yml/badge.svg?event=push&amp;color=5B5BD6)](https://github.com/cocoindex-io/cocoindex/actions/workflows/CI.yml)
[![release](https://github.com/cocoindex-io/cocoindex/actions/workflows/release.yml/badge.svg?event=push&amp;color=5B5BD6)](https://github.com/cocoindex-io/cocoindex/actions/workflows/release.yml)
[![Discord](https://img.shields.io/discord/1314801574169673738?logo=discord&amp;color=5B5BD6&amp;logoColor=white)](https://discord.com/invite/zpA9S2DR7s)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://trendshift.io/repositories/13939&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13939&quot; alt=&quot;cocoindex-io%2Fcocoindex | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;


Ultra performant data transformation framework for AI, with core engine written in Rust. Support incremental processing and data lineage out-of-box.  Exceptional developer velocity. Production-ready at day 0.

‚≠ê Drop a star to help us grow!

&lt;div align=&quot;center&quot;&gt;

&lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
[Deutsch](https://readme-i18n.com/cocoindex-io/cocoindex?lang=de) |
[English](https://readme-i18n.com/cocoindex-io/cocoindex?lang=en) |
[Espa√±ol](https://readme-i18n.com/cocoindex-io/cocoindex?lang=es) |
[fran√ßais](https://readme-i18n.com/cocoindex-io/cocoindex?lang=fr) |
[Êó•Êú¨Ë™û](https://readme-i18n.com/cocoindex-io/cocoindex?lang=ja) |
[ÌïúÍµ≠Ïñ¥](https://readme-i18n.com/cocoindex-io/cocoindex?lang=ko) |
[Portugu√™s](https://readme-i18n.com/cocoindex-io/cocoindex?lang=pt) |
[–†—É—Å—Å–∫–∏–π](https://readme-i18n.com/cocoindex-io/cocoindex?lang=ru) |
[‰∏≠Êñá](https://readme-i18n.com/cocoindex-io/cocoindex?lang=zh)

&lt;/div&gt;

&lt;/br&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://cocoindex.io/images/transformation.svg&quot; alt=&quot;CocoIndex Transformation&quot;&gt;
&lt;/p&gt;

&lt;/br&gt;

CocoIndex makes it super easy to transform data with AI workloads, and keep source data and target in sync effortlessly.

&lt;/br&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://cocoindex.io/images/venn-features.png&quot; alt=&quot;CocoIndex Features&quot; width=&#039;400&#039;&gt;
&lt;/p&gt;

&lt;/br&gt;

Either creating embedding, building knowledge graphs, or any data transformations - beyond traditional SQL.

## Exceptional velocity
Just declare transformation in dataflow with ~100 lines of python

```python
# import
data[&#039;content&#039;] = flow_builder.add_source(...)

# transform
data[&#039;out&#039;] = data[&#039;content&#039;]
    .transform(...)
    .transform(...)

# collect data
collector.collect(...)

# export to db, vector db, graph db ...
collector.export(...)
```

CocoIndex follows the idea of [Dataflow](https://en.wikipedia.org/wiki/Dataflow_programming) programming model. Each transformation creates a new field solely based on input fields, without hidden states and value mutation. All data before/after each transformation is observable, with lineage out of the box.

**Particularly**, developers don&#039;t explicitly mutate data by creating, updating and deleting. They just need to define transformation/formula for a set of source data.

## Build like LEGO
Native builtins for different source, targets and transformations. Standardize interface, make it 1-line code switch between different components.

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://cocoindex.io/images/components.svg&quot; alt=&quot;CocoIndex Features&quot;&gt;
&lt;/p&gt;

## Data Freshness
CocoIndex keep source data and target in sync effortlessly.

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/f4eb29b3-84ee-4fa0-a1e2-80eedeeabde6&quot; alt=&quot;Incremental Processing&quot; width=&quot;700&quot;&gt;
&lt;/p&gt;

It has out-of-box support for incremental indexing:
- minimal recomputation on source or logic change.
- (re-)processing necessary portions; reuse cache when possible

## Quick Start:
If you&#039;re new to CocoIndex, we recommend checking out
- üìñ [Documentation](https://cocoindex.io/docs)
- ‚ö°  [Quick Start Guide](https://cocoindex.io/docs/getting_started/quickstart)
- üé¨ [Quick Start Video Tutorial](https://youtu.be/gv5R8nOXsWU?si=9ioeKYkMEnYevTXT)

### Setup

1. Install CocoIndex Python library

```bash
pip install -U cocoindex
```

2. [Install Postgres](https://cocoindex.io/docs/getting_started/installation#-install-postgres) if you don&#039;t have one. CocoIndex uses it for incremental processing.


## Define data flow

Follow [Quick Start Guide](https://cocoindex.io/docs/getting_started/quickstart) to define your first indexing flow. An example flow looks like:

```python
@cocoindex.flow_def(name=&quot;TextEmbedding&quot;)
def text_embedding_flow(flow_builder: cocoindex.FlowBuilder, data_scope: cocoindex.DataScope):
    # Add a data source to read files from a directory
    data_scope[&quot;documents&quot;] = flow_builder.add_source(cocoindex.sources.LocalFile(path=&quot;markdown_files&quot;))

    # Add a collector for data to be exported to the vector index
    doc_embeddings = data_scope.add_collector()

    # Transform data of each document
    with data_scope[&quot;documents&quot;].row() as doc:
        # Split the document into chunks, put into `chunks` field
        doc[&quot;chunks&quot;] = doc[&quot;content&quot;].transform(
            cocoindex.functions.SplitRecursively(),
            language=&quot;markdown&quot;, chunk_size=2000, chunk_overlap=500)

        # Transform data of each chunk
        with doc[&quot;chunks&quot;].row() as chunk:
            # Embed the chunk, put into `embedding` field
            chunk[&quot;embedding&quot;] = chunk[&quot;text&quot;].transform(
                cocoindex.functions.SentenceTransformerEmbed(
                    model=&quot;sentence-transformers/all-MiniLM-L6-v2&quot;))

            # Collect the chunk into the collector.
            doc_embeddings.collect(filename=doc[&quot;filename&quot;], location=chunk[&quot;location&quot;],
                                   text=chunk[&quot;text&quot;], embedding=chunk[&quot;embedding&quot;])

    # Export collected data to a vector index.
    doc_embeddings.export(
        &quot;doc_embeddings&quot;,
        cocoindex.targets.Postgres(),
        primary_key_fields=[&quot;filename&quot;, &quot;location&quot;],
        vector_indexes=[
            cocoindex.VectorIndexDef(
                field_name=&quot;embedding&quot;,
                metric=cocoindex.VectorSimilarityMetric.COSINE_SIMILARITY)])
```

It defines an index flow like this:

&lt;p align=&quot;center&quot;&gt;
    &lt;img width=&quot;400&quot; alt=&quot;Data Flow&quot; src=&quot;https://github.com/user-attachments/assets/2ea7be6d-3d94-42b1-b2bd-22515577e463&quot; /&gt;
&lt;/p&gt;

## üöÄ Examples and demo

| Example | Description |
|---------|-------------|
| [Text Embedding](examples/text_embedding) | Index text documents with embeddings for semantic search |
| [Code Embedding](examples/code_embedding) | Index code embeddings for semantic search |
| [PDF Embedding](examples/pdf_embedding) | Parse PDF and index text embeddings for semantic search |
| [Manuals LLM Extraction](examples/manuals_llm_extraction) | Extract structured information from a manual using LLM |
| [Amazon S3 Embedding](examples/amazon_s3_embedding) | Index text documents from Amazon S3 |
| [Azure Blob Storage Embedding](examples/azure_blob_embedding) | Index text documents from Azure Blob Storage |
| [Google Drive Text Embedding](examples/gdrive_text_embedding) | Index text documents from Google Drive |
| [Docs to Knowledge Graph](examples/docs_to_knowledge_graph) | Extract relationships from Markdown documents and build a knowledge graph |
| [Embeddings to Qdrant](examples/text_embedding_qdrant) | Index documents in a Qdrant collection for semantic search |
| [FastAPI Server with Docker](examples/fastapi_server_docker) | Run the semantic search server in a Dockerized FastAPI setup |
| [Product Recommendation](examples/product_recommendation) | Build real-time product recommendations with LLM and graph database|
| [Image Search with Vision API](examples/image_search) | Generates detailed captions for images using a vision model, embeds them, enables live-updating semantic search via FastAPI and served on a React frontend|
| [Face Recognition](examples/face_recognition) | Recognize faces in images and build embedding index |
| [Paper Metadata](examples/paper_metadata) | Index papers in PDF files, and build metadata tables for each paper |
| [Multi Format Indexing](examples/multi_format_indexing) | Build visual document index from PDFs and images with ColPali for semantic search |
| [Custom Output Files](examples/custom_output_files) | Convert markdown files to HTML files and save them to a local directory, using *CocoIndex Custom Targets* |

More coming and stay tuned üëÄ!

## üìñ Documentation
For detailed documentation, visit [CocoIndex Documentation](https://cocoindex.io/docs), including a [Quickstart guide](https://cocoindex.io/docs/getting_started/quickstart).

## ü§ù Contributing
We love contributions from our community ‚ù§Ô∏è. For details on contributing or running the project for development, check out our [contributing guide](https://cocoindex.io/docs/about/contributing).

## üë• Community
Welcome with a huge coconut hug ü••‚ãÜÔΩ°Àöü§ó. We are super excited for community contributions of all kinds - whether it&#039;s code improvements, documentation updates, issue reports, feature requests, and discussions in our Discord.

Join our community here:

- üåü [Star us on GitHub](https://github.com/cocoindex-io/cocoindex)
- üëã [Join our Discord community](https://discord.com/invite/zpA9S2DR7s)
- ‚ñ∂Ô∏è [Subscribe to our YouTube channel](https://www.youtube.com/@cocoindex-io)
- üìú [Read our blog posts](https://cocoindex.io/blogs/)

## Support us:
We are constantly improving, and more features and examples are coming soon. If you love this project, please drop us a star ‚≠ê at GitHub repo [![GitHub](https://img.shields.io/github/stars/cocoindex-io/cocoindex?color=5B5BD6)](https://github.com/cocoindex-io/cocoindex) to stay tuned and help us grow.

## License
CocoIndex is Apache 2.0 licensed.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[openai/codex]]></title>
            <link>https://github.com/openai/codex</link>
            <guid>https://github.com/openai/codex</guid>
            <pubDate>Thu, 07 Aug 2025 00:05:57 GMT</pubDate>
            <description><![CDATA[Lightweight coding agent that runs in your terminal]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/openai/codex">openai/codex</a></h1>
            <p>Lightweight coding agent that runs in your terminal</p>
            <p>Language: Rust</p>
            <p>Stars: 31,946</p>
            <p>Forks: 3,735</p>
            <p>Stars today: 121 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;OpenAI Codex CLI&lt;/h1&gt;
&lt;p align=&quot;center&quot;&gt;Lightweight coding agent that runs in your terminal&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;code&gt;npm i -g @openai/codex&lt;/code&gt;&lt;br /&gt;or &lt;code&gt;brew install codex&lt;/code&gt;&lt;/p&gt;

This is the home of the **Codex CLI**, which is a coding agent from OpenAI that runs locally on your computer. If you are looking for the _cloud-based agent_ from OpenAI, **Codex [Web]**, see &lt;https://chatgpt.com/codex&gt;.

&lt;!-- ![Codex demo GIF using: codex &quot;explain this codebase to me&quot;](./.github/demo.gif) --&gt;

---

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;/summary&gt;

&lt;!-- Begin ToC --&gt;

- [Experimental technology disclaimer](#experimental-technology-disclaimer)
- [Quickstart](#quickstart)
  - [OpenAI API Users](#openai-api-users)
  - [OpenAI Plus/Pro Users](#openai-pluspro-users)
- [Using Open Source Models](#using-open-source-models)
- [Why Codex?](#why-codex)
- [Security model &amp; permissions](#security-model--permissions)
  - [Platform sandboxing details](#platform-sandboxing-details)
- [System requirements](#system-requirements)
- [CLI reference](#cli-reference)
- [Memory &amp; project docs](#memory--project-docs)
- [Non-interactive / CI mode](#non-interactive--ci-mode)
- [Model Context Protocol (MCP)](#model-context-protocol-mcp)
- [Tracing / verbose logging](#tracing--verbose-logging)
- [Recipes](#recipes)
- [Installation](#installation)
  - [DotSlash](#dotslash)
- [Configuration](#configuration)
- [FAQ](#faq)
- [Zero data retention (ZDR) usage](#zero-data-retention-zdr-usage)
- [Codex open source fund](#codex-open-source-fund)
- [Contributing](#contributing)
  - [Development workflow](#development-workflow)
  - [Writing high-impact code changes](#writing-high-impact-code-changes)
  - [Opening a pull request](#opening-a-pull-request)
  - [Review process](#review-process)
  - [Community values](#community-values)
  - [Getting help](#getting-help)
  - [Contributor license agreement (CLA)](#contributor-license-agreement-cla)
    - [Quick fixes](#quick-fixes)
  - [Releasing `codex`](#releasing-codex)
- [Security &amp; responsible AI](#security--responsible-ai)
- [License](#license)

&lt;!-- End ToC --&gt;

&lt;/details&gt;

---

## Experimental technology disclaimer

Codex CLI is an experimental project under active development. It is not yet stable, may contain bugs, incomplete features, or undergo breaking changes. We&#039;re building it in the open with the community and welcome:

- Bug reports
- Feature requests
- Pull requests
- Good vibes

Help us improve by filing issues or submitting PRs (see the section below for how to contribute)!

## Quickstart

Install globally with your preferred package manager:

```shell
npm install -g @openai/codex  # Alternatively: `brew install codex`
```

Or go to the [latest GitHub Release](https://github.com/openai/codex/releases/latest) and download the appropriate binary for your platform.

### OpenAI API Users

Next, set your OpenAI API key as an environment variable:

```shell
export OPENAI_API_KEY=&quot;your-api-key-here&quot;
```

&gt; [!NOTE]
&gt; This command sets the key only for your current terminal session. You can add the `export` line to your shell&#039;s configuration file (e.g., `~/.zshrc`), but we recommend setting it for the session.

### OpenAI Plus/Pro Users

If you have a paid OpenAI account, run the following to start the login process:

```
codex login
```

If you complete the process successfully, you should have a `~/.codex/auth.json` file that contains the credentials that Codex will use.

To verify whether you are currently logged in, run:

```
codex login status
```

If you encounter problems with the login flow, please comment on &lt;https://github.com/openai/codex/issues/1243&gt;.

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Use &lt;code&gt;--profile&lt;/code&gt; to use other models&lt;/strong&gt;&lt;/summary&gt;

Codex also allows you to use other providers that support the OpenAI Chat Completions (or Responses) API.

To do so, you must first define custom [providers](./config.md#model_providers) in `~/.codex/config.toml`. For example, the provider for a standard Ollama setup would be defined as follows:

```toml
[model_providers.ollama]
name = &quot;Ollama&quot;
base_url = &quot;http://localhost:11434/v1&quot;
```

The `base_url` will have `/chat/completions` appended to it to build the full URL for the request.

For providers that also require an `Authorization` header of the form `Bearer: SECRET`, an `env_key` can be specified, which indicates the environment variable to read to use as the value of `SECRET` when making a request:

```toml
[model_providers.openrouter]
name = &quot;OpenRouter&quot;
base_url = &quot;https://openrouter.ai/api/v1&quot;
env_key = &quot;OPENROUTER_API_KEY&quot;
```

Providers that speak the Responses API are also supported by adding `wire_api = &quot;responses&quot;` as part of the definition. Accessing OpenAI models via Azure is an example of such a provider, though it also requires specifying additional `query_params` that need to be appended to the request URL:

```toml
[model_providers.azure]
name = &quot;Azure&quot;
# Make sure you set the appropriate subdomain for this URL.
base_url = &quot;https://YOUR_PROJECT_NAME.openai.azure.com/openai&quot;
env_key = &quot;AZURE_OPENAI_API_KEY&quot;  # Or &quot;OPENAI_API_KEY&quot;, whichever you use.
# Newer versions appear to support the responses API, see https://github.com/openai/codex/pull/1321
query_params = { api-version = &quot;2025-04-01-preview&quot; }
wire_api = &quot;responses&quot;
```

Once you have defined a provider you wish to use, you can configure it as your default provider as follows:

```toml
model_provider = &quot;azure&quot;
```

&gt; [!TIP]
&gt; If you find yourself experimenting with a variety of models and providers, then you likely want to invest in defining a _profile_ for each configuration like so:

```toml
[profiles.o3]
model_provider = &quot;azure&quot;
model = &quot;o3&quot;

[profiles.mistral]
model_provider = &quot;ollama&quot;
model = &quot;mistral&quot;
```

This way, you can specify one command-line argument (.e.g., `--profile o3`, `--profile mistral`) to override multiple settings together.

&lt;/details&gt;
&lt;br /&gt;

Run interactively:

```shell
codex
```

Or, run with a prompt as input (and optionally in `Full Auto` mode):

```shell
codex &quot;explain this codebase to me&quot;
```

```shell
codex --full-auto &quot;create the fanciest todo-list app&quot;
```

That&#039;s it - Codex will scaffold a file, run it inside a sandbox, install any
missing dependencies, and show you the live result. Approve the changes and
they&#039;ll be committed to your working directory.

---

## Using Open Source Models

Codex can run fully locally against an OpenAI-compatible OSS host (like Ollama) using the `--oss` flag:

- Interactive UI:
  - codex --oss
- Non-interactive (programmatic) mode:
  - echo &quot;Refactor utils&quot; | codex exec --oss

Model selection when using `--oss`:

- If you omit `-m/--model`, Codex defaults to -m gpt-oss:20b and will verify it exists locally (downloading if needed).
- To pick a different size, pass one of:
  - -m &quot;gpt-oss:20b&quot;
  - -m &quot;gpt-oss:120b&quot;

Point Codex at your own OSS host:

- By default, `--oss` talks to http://localhost:11434/v1.
- To use a different host, set one of these environment variables before running Codex:
  - CODEX_OSS_BASE_URL, for example:
    - CODEX_OSS_BASE_URL=&quot;http://my-ollama.example.com:11434/v1&quot; codex --oss -m gpt-oss:20b
  - or CODEX_OSS_PORT (when the host is localhost):
    - CODEX_OSS_PORT=11434 codex --oss

Advanced: you can persist this in your config instead of environment variables by overriding the built-in `oss` provider in `~/.codex/config.toml`:

```toml
[model_providers.oss]
name = &quot;Open Source&quot;
base_url = &quot;http://my-ollama.example.com:11434/v1&quot;
```

---

## Why Codex?

Codex CLI is built for developers who already **live in the terminal** and want
ChatGPT-level reasoning **plus** the power to actually run code, manipulate
files, and iterate - all under version control. In short, it&#039;s _chat-driven
development_ that understands and executes your repo.

- **Zero setup** - bring your OpenAI API key and it just works!
- **Full auto-approval, while safe + secure** by running network-disabled and directory-sandboxed
- **Multimodal** - pass in screenshots or diagrams to implement features ‚ú®

And it&#039;s **fully open-source** so you can see and contribute to how it develops!

---

## Security model &amp; permissions

Codex lets you decide _how much autonomy_ you want to grant the agent. The following options can be configured independently:

- [`approval_policy`](./codex-rs/config.md#approval_policy) determines when you should be prompted to approve whether Codex can execute a command
- [`sandbox`](./codex-rs/config.md#sandbox) determines the _sandbox policy_ that Codex uses to execute untrusted commands

By default, Codex runs with `--ask-for-approval untrusted` and `--sandbox read-only`, which means that:

- The user is prompted to approve every command not on the set of &quot;trusted&quot; commands built into Codex (`cat`, `ls`, etc.)
- Approved commands are run outside of a sandbox because user approval implies &quot;trust,&quot; in this case.

Running Codex with the `--full-auto` convenience flag changes the configuration to `--ask-for-approval on-failure` and `--sandbox workspace-write`, which means that:

- Codex does not initially ask for user approval before running an individual command.
- Though when it runs a command, it is run under a sandbox in which:
  - It can read any file on the system.
  - It can only write files under the current directory (or the directory specified via `--cd`).
  - Network requests are completely disabled.
- Only if the command exits with a non-zero exit code will it ask the user for approval. If granted, it will re-attempt the command outside of the sandbox. (A common case is when Codex cannot `npm install` a dependency because that requires network access.)

Again, these two options can be configured independently. For example, if you want Codex to perform an &quot;exploration&quot; where you are happy for it to read anything it wants but you never want to be prompted, you could run Codex with `--ask-for-approval never` and `--sandbox read-only`.

### Platform sandboxing details

The mechanism Codex uses to implement the sandbox policy depends on your OS:

- **macOS 12+** uses **Apple Seatbelt** and runs commands using `sandbox-exec` with a profile (`-p`) that corresponds to the `--sandbox` that was specified.
- **Linux** uses a combination of Landlock/seccomp APIs to enforce the `sandbox` configuration.

Note that when running Linux in a containerized environment such as Docker, sandboxing may not work if the host/container configuration does not support the necessary Landlock/seccomp APIs. In such cases, we recommend configuring your Docker container so that it provides the sandbox guarantees you are looking for and then running `codex` with `--sandbox danger-full-access` (or, more simply, the `--dangerously-bypass-approvals-and-sandbox` flag) within your container.

---

## System requirements

| Requirement                 | Details                                                         |
| --------------------------- | --------------------------------------------------------------- |
| Operating systems           | macOS 12+, Ubuntu 20.04+/Debian 10+, or Windows 11 **via WSL2** |
| Git (optional, recommended) | 2.23+ for built-in PR helpers                                   |
| RAM                         | 4-GB minimum (8-GB recommended)                                 |

---

## CLI reference

| Command            | Purpose                            | Example                         |
| ------------------ | ---------------------------------- | ------------------------------- |
| `codex`            | Interactive TUI                    | `codex`                         |
| `codex &quot;...&quot;`      | Initial prompt for interactive TUI | `codex &quot;fix lint errors&quot;`       |
| `codex exec &quot;...&quot;` | Non-interactive &quot;automation mode&quot;  | `codex exec &quot;explain utils.ts&quot;` |

Key flags: `--model/-m`, `--ask-for-approval/-a`.

---

## Memory &amp; project docs

You can give Codex extra instructions and guidance using `AGENTS.md` files. Codex looks for `AGENTS.md` files in the following places, and merges them top-down:

1. `~/.codex/AGENTS.md` - personal global guidance
2. `AGENTS.md` at repo root - shared project notes
3. `AGENTS.md` in the current working directory - sub-folder/feature specifics

---

## Non-interactive / CI mode

Run Codex head-less in pipelines. Example GitHub Action step:

```yaml
- name: Update changelog via Codex
  run: |
    npm install -g @openai/codex
    export OPENAI_API_KEY=&quot;${{ secrets.OPENAI_KEY }}&quot;
    codex exec --full-auto &quot;update CHANGELOG for next release&quot;
```

## Model Context Protocol (MCP)

The Codex CLI can be configured to leverage MCP servers by defining an [`mcp_servers`](./codex-rs/config.md#mcp_servers) section in `~/.codex/config.toml`. It is intended to mirror how tools such as Claude and Cursor define `mcpServers` in their respective JSON config files, though the Codex format is slightly different since it uses TOML rather than JSON, e.g.:

```toml
# IMPORTANT: the top-level key is `mcp_servers` rather than `mcpServers`.
[mcp_servers.server-name]
command = &quot;npx&quot;
args = [&quot;-y&quot;, &quot;mcp-server&quot;]
env = { &quot;API_KEY&quot; = &quot;value&quot; }
```

&gt; [!TIP]
&gt; It is somewhat experimental, but the Codex CLI can also be run as an MCP _server_ via `codex mcp`. If you launch it with an MCP client such as `npx @modelcontextprotocol/inspector codex mcp` and send it a `tools/list` request, you will see that there is only one tool, `codex`, that accepts a grab-bag of inputs, including a catch-all `config` map for anything you might want to override. Feel free to play around with it and provide feedback via GitHub issues.

## Tracing / verbose logging

Because Codex is written in Rust, it honors the `RUST_LOG` environment variable to configure its logging behavior.

The TUI defaults to `RUST_LOG=codex_core=info,codex_tui=info` and log messages are written to `~/.codex/log/codex-tui.log`, so you can leave the following running in a separate terminal to monitor log messages as they are written:

```
tail -F ~/.codex/log/codex-tui.log
```

By comparison, the non-interactive mode (`codex exec`) defaults to `RUST_LOG=error`, but messages are printed inline, so there is no need to monitor a separate file.

See the Rust documentation on [`RUST_LOG`](https://docs.rs/env_logger/latest/env_logger/#enabling-logging) for more information on the configuration options.

---

## Recipes

Below are a few bite-size examples you can copy-paste. Replace the text in quotes with your own task. See the [prompting guide](https://github.com/openai/codex/blob/main/codex-cli/examples/prompting_guide.md) for more tips and usage patterns.

| ‚ú®  | What you type                                                                   | What happens                                                               |
| --- | ------------------------------------------------------------------------------- | -------------------------------------------------------------------------- |
| 1   | `codex &quot;Refactor the Dashboard component to React Hooks&quot;`                       | Codex rewrites the class component, runs `npm test`, and shows the diff.   |
| 2   | `codex &quot;Generate SQL migrations for adding a users table&quot;`                      | Infers your ORM, creates migration files, and runs them in a sandboxed DB. |
| 3   | `codex &quot;Write unit tests for utils/date.ts&quot;`                                    | Generates tests, executes them, and iterates until they pass.              |
| 4   | `codex &quot;Bulk-rename *.jpeg -&gt; *.jpg with git mv&quot;`                               | Safely renames files and updates imports/usages.                           |
| 5   | `codex &quot;Explain what this regex does: ^(?=.*[A-Z]).{8,}$&quot;`                      | Outputs a step-by-step human explanation.                                  |
| 6   | `codex &quot;Carefully review this repo, and propose 3 high impact well-scoped PRs&quot;` | Suggests impactful PRs in the current codebase.                            |
| 7   | `codex &quot;Look for vulnerabilities and create a security review report&quot;`          | Finds and explains security bugs.                                          |

---

## Installation

&lt;details open&gt;
&lt;summary&gt;&lt;strong&gt;Install Codex CLI using your preferred package manager.&lt;/strong&gt;&lt;/summary&gt;

From `brew` (recommended, downloads only the binary for your platform):

```bash
brew install codex
```

From `npm` (generally more readily available, but downloads binaries for all supported platforms):

```bash
npm i -g @openai/codex
```

Or go to the [latest GitHub Release](https://github.com/openai/codex/releases/latest) and download the appropriate binary for your platform.

Admittedly, each GitHub Release contains many executables, but in practice, you likely want one of these:

- macOS
  - Apple Silicon/arm64: `codex-aarch64-apple-darwin.tar.gz`
  - x86_64 (older Mac hardware): `codex-x86_64-apple-darwin.tar.gz`
- Linux
  - x86_64: `codex-x86_64-unknown-linux-musl.tar.gz`
  - arm64: `codex-aarch64-unknown-linux-musl.tar.gz`

Each archive contains a single entry with the platform baked into the name (e.g., `codex-x86_64-unknown-linux-musl`), so you likely want to rename it to `codex` after extracting it.

### DotSlash

The GitHub Release also contains a [DotSlash](https://dotslash-cli.com/) file for the Codex CLI named `codex`. Using a DotSlash file makes it possible to make a lightweight commit to source control to ensure all contributors use the same version of an executable, regardless of what platform they use for development.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Build from source&lt;/strong&gt;&lt;/summary&gt;

```bash
# Clone the repository and navigate to the root of the Cargo workspace.
git clone https://github.com/openai/codex.git
cd codex/codex-rs

# Install the Rust toolchain, if necessary.
curl --proto &#039;=https&#039; --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
source &quot;$HOME/.cargo/env&quot;
rustup component add rustfmt
rustup component add clippy

# Build Codex.
cargo build

# Launch the TUI with a sample prompt.
cargo run --bin codex -- &quot;explain this codebase to me&quot;

# After making changes, ensure the code is clean.
cargo fmt -- --config imports_granularity=Item
cargo clippy --tests

# Run the tests.
cargo test
```

&lt;/details&gt;

---

## Configuration

Codex supports a rich set of configuration options documented in [`codex-rs/config.md`](./codex-rs/config.md).

By default, Codex loads its configuration from `~/.codex/config.toml`.

Though `--config` can be used to set/override ad-hoc config values for individual invocations of `codex`.

---

## FAQ

&lt;details&gt;
&lt;summary&gt;OpenAI released a model called Codex in 2021 - is this related?&lt;/summary&gt;

In 2021, OpenAI released Codex, an AI system designed to generate code from natural language prompts. That original Codex model was deprecated as of March 2023 and is separate from the CLI tool.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Which models are supported?&lt;/summary&gt;

Any model available with [Responses API](https://platform.openai.com/docs/api-reference/responses). The default is `o4-mini`, but pass `--model gpt-4.1` or set `model: gpt-4.1` in your config file to override.

&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;Why does &lt;code&gt;o3&lt;/code&gt; or &lt;code&gt;o4-mini&lt;/code&gt; not work for me?&lt;/summary&gt;

It&#039;s possible that your [API account needs to be verified](https://help.openai.com/en/articles/10910291-api-organization-verification) in order to start streaming responses and seeing chain of thought summaries from the API. If you&#039;re still running into issues, please let us know!

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;How do I stop Codex from editing my files?&lt;/summary&gt;

Codex runs model-generated commands in a sandbox. If a proposed command or file change doesn&#039;t look right, you can simply type **n** to deny the command or give the model feedback.

&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;Does it work on Windows?&lt;/summary&gt;

Not directly. It requires [Windows Subsystem for Linux (WSL2)](https://learn.microsoft.com/en-us/windows/wsl/install) - Codex has been

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[risingwavelabs/risingwave]]></title>
            <link>https://github.com/risingwavelabs/risingwave</link>
            <guid>https://github.com/risingwavelabs/risingwave</guid>
            <pubDate>Thu, 07 Aug 2025 00:05:56 GMT</pubDate>
            <description><![CDATA[Real-time event streaming platform. real-time data ingestion, stream processing, online serving, and data management.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/risingwavelabs/risingwave">risingwavelabs/risingwave</a></h1>
            <p>Real-time event streaming platform. real-time data ingestion, stream processing, online serving, and data management.</p>
            <p>Language: Rust</p>
            <p>Stars: 8,206</p>
            <p>Forks: 664</p>
            <p>Stars today: 5 stars today</p>
            <h2>README</h2><pre>
&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source srcset=&quot;.github/RisingWave-logo-dark.svg&quot; width=&quot;500px&quot; media=&quot;(prefers-color-scheme: dark)&quot;&gt;
    &lt;img src=&quot;.github/RisingWave-logo-light.svg&quot; width=&quot;500px&quot;&gt;
  &lt;/picture&gt;
&lt;/p&gt;


&lt;div align=&quot;center&quot;&gt;

### üåä Ride the Wave of Streaming Data.

&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://docs.risingwave.com/&quot;&gt;Docs&lt;/a&gt; | &lt;a href=&quot;https://docs.risingwave.com/get-started/rw-benchmarks-stream-processing&quot;&gt;Benchmarks&lt;/a&gt; | &lt;a href=&quot;https://docs.risingwave.com/demos/overview&quot;&gt;Demos&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a
    href=&quot;https://github.com/risingwavelabs/risingwave/releases/latest&quot;
    target=&quot;_blank&quot;
  &gt;
    &lt;img alt=&quot;Release&quot; src=&quot;https://img.shields.io/github/v/release/risingwavelabs/risingwave.svg?sort=semver&quot; /&gt;
  &lt;/a&gt;
  &lt;a
    href=&quot;https://go.risingwave.com/slack&quot;
    target=&quot;_blank&quot;
  &gt;
    &lt;img alt=&quot;Slack&quot; src=&quot;https://badgen.net/badge/Slack/Join%20RisingWave/0abd59?icon=slack&quot; /&gt;
  &lt;/a&gt;
  &lt;a
    href=&quot;https://x.com/risingwavelabs&quot;
    target=&quot;_blank&quot;
  &gt;
    &lt;img alt=&quot;X&quot; src=&quot;https://img.shields.io/twitter/follow/risingwavelabs&quot; /&gt;
  &lt;/a&gt;
  &lt;a
    href=&quot;https://www.youtube.com/@risingwave-labs&quot;
    target=&quot;_blank&quot;
  &gt;
    &lt;img alt=&quot;YouTube&quot; src=&quot;https://img.shields.io/youtube/channel/views/UCsHwdyBRxBpmkA5RRd0YNEA&quot; /&gt;
  &lt;/a&gt;
&lt;/div&gt;

RisingWave is a real-time event streaming platform designed to offer the &lt;i&gt;&lt;b&gt;simplest&lt;/b&gt;&lt;/i&gt; and &lt;i&gt;&lt;b&gt;most cost-effective&lt;/b&gt;&lt;/i&gt; way to &lt;b&gt;process&lt;/b&gt;, &lt;b&gt;analyze&lt;/b&gt;, and &lt;b&gt;manage&lt;/b&gt; real-time event data ‚Äî with built-in support for the [Apache Iceberg‚Ñ¢](https://iceberg.apache.org/) open table format. It provides both a Postgres-compatible [SQL interface](https://docs.risingwave.com/sql/overview) and a DataFrame-style [Python interface](https://docs.risingwave.com/python-sdk/intro).

RisingWave can &lt;b&gt;ingest&lt;/b&gt; millions of events per second, continuously &lt;b&gt;join and analyze&lt;/b&gt; live streams with historical data, &lt;b&gt;serve&lt;/b&gt; ad-hoc queries at low latency, and &lt;b&gt;persist&lt;/b&gt; fresh, consistent results to Apache Iceberg‚Ñ¢ or any other downstream system.

![RisingWave](./docs/dev/src/images/architecture_20250609.jpg)

## Try it out in 60 seconds

Install RisingWave standalone mode:
```shell
curl -L https://risingwave.com/sh | sh
```

To learn about other installation options, such as using a Docker image, see [Quick Start](https://docs.risingwave.com/docs/current/get-started/).

## Stream, Store, and Query ‚Äî All in One

RisingWave delivers a full **end-to-end streaming data platform** ‚Äî combining real-time processing with built-in storage and open-format persistence.

It supports:

* **Ingestion:** Ingest millions of events per second from streaming and batch sources.
* **Stream processing:** Perform real-time incremental processing to join and analyze live data with historical tables.
* **Delivery:** Deliver fresh, consistent results to data lakes (e.g., Apache Iceberg‚Ñ¢) or any destination.

What sets RisingWave apart is its integrated storage engine:

* **Online serving:** Row-based storage optimized for point and range queries with single-digit millisecond latency.
* **Offline persistence:** Built-in Apache Iceberg‚Ñ¢ integration for low-cost, durable storage with open access for external query engines.

With RisingWave, real-time data isn‚Äôt just processed ‚Äî it‚Äôs stored, queried, and shared across your entire stack.

## Key design decisions

RisingWave is designed to be easier to use and more cost-efficient:

### PostgreSQL compatibility

* **Seamless integration:** Connects via the PostgreSQL wire protocol, working with psql, JDBC, and any Postgres tool.
* **Expressive SQL:** Supports structured, semi-structured, and unstructured data with a familiar SQL dialect.
* **No manual state tuning:** Eliminates complex state management configurations.

### S3 as primary storage

RisingWave stores tables, materialized views, and internal states of stream processing jobs in S3 (or equivalent object storage), providing:
- **High performance:** Optimized for complex queries, including joins and time windowing.
- **Fast recovery:** Restores from system failures within seconds.
- **[Dynamic scaling](https://docs.risingwave.com/deploy/k8s-cluster-scaling):** Instantly adjusts resources to handle workload spikes.

### Elastic disk cache

Beyond caching hot data in memory, RisingWave supports [**elastic disk cache**](https://docs.risingwave.com/get-started/disk-cache), a powerful performance optimization that uses local disks or EBS for efficient data caching. This minimizes access to S3, lowering processing latency and cutting S3 access costs.

### Apache Iceberg‚Ñ¢ native support
RisingWave [**natively integrates with Apache Iceberg‚Ñ¢**](https://docs.risingwave.com/iceberg/overview), enabling continuous ingestion of streaming data into Iceberg tables. It can also read directly from Iceberg, perform automatic compaction, and maintain table health over time. Since Iceberg is an open table format, results are accessible by other query engines ‚Äî making storage not only cost-efficient, but interoperable by design.

## In what use cases does RisingWave excel?
RisingWave is particularly effective for the following use cases:

* **Streaming analytics**: Achieve sub-second data freshness in live dashboards, ideal for high-stakes scenarios like stock trading, sports betting, and IoT monitoring.
* **Event-driven applications**: Develop sophisticated monitoring and alerting systems for critical applications such as fraud and anomaly detection.
* **Real-time data enrichment**: Continuously ingest data from diverse sources, conduct real-time data enrichment, and efficiently deliver the results to downstream systems.
* **Feature engineering**: Transform batch and streaming data into features in your machine learning models using a unified codebase, ensuring seamless integration and consistency.

## Production deployments

[**RisingWave Cloud**](https://cloud.risingwave.com) offers the easiest way to run RisingWave in production.

For **Docker deployment**, please refer to [Docker Compose](https://docs.risingwave.com/docs/current/risingwave-docker-compose/).

For **Kubernetes deployment**, please refer to [Kubernetes with Helm](https://docs.risingwave.com/docs/current/risingwave-k8s-helm/) or [Kubernetes with Operator](https://docs.risingwave.com/docs/current/risingwave-kubernetes/).

## Community

Looking for help, discussions, collaboration opportunities, or a casual afternoon chat with our fellow engineers and community members? Join our [Slack workspace](https://risingwave.com/slack)!

## Notes on telemetry


RisingWave uses [Scarf](https://scarf.sh/) to collect anonymized installation analytics. These analytics help support us understand and improve the distribution of our package. The privacy policy of Scarf is available at [https://about.scarf.sh/privacy-policy](https://about.scarf.sh/privacy-policy).

RisingWave also collects anonymous usage statistics to better understand how the community is using RisingWave. The sole intention of this exercise is to help improve the product. Users may opt out easily at any time. Please refer to the [user documentation](https://docs.risingwave.com/docs/current/telemetry/) for more details.

## License

RisingWave is distributed under the Apache License (Version 2.0). Please refer to [LICENSE](LICENSE) for more information.

## Contributing

Thanks for your interest in contributing to the project! Please refer to [RisingWave Developer Guide](https://risingwavelabs.github.io/risingwave/) for more information.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[Myriad-Dreamin/tinymist]]></title>
            <link>https://github.com/Myriad-Dreamin/tinymist</link>
            <guid>https://github.com/Myriad-Dreamin/tinymist</guid>
            <pubDate>Thu, 07 Aug 2025 00:05:55 GMT</pubDate>
            <description><![CDATA[Tinymist [Ààta…™ni m…™st] is an integrated language service for Typst [ta…™pst].]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Myriad-Dreamin/tinymist">Myriad-Dreamin/tinymist</a></h1>
            <p>Tinymist [Ààta…™ni m…™st] is an integrated language service for Typst [ta…™pst].</p>
            <p>Language: Rust</p>
            <p>Stars: 1,888</p>
            <p>Forks: 84</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre>&lt;!-- This file is generated by scripts/link-docs.mjs. Do not edit manually. --&gt;
# Tinymist

Tinymist \[Ààta…™ni m…™st\] is an integrated language service for [Typst](https://typst.app/) \[ta…™pst\]. You can also call it &lt;ruby&gt;ÂæÆ&lt;rt&gt;wƒìi&lt;/rt&gt;&lt;/ruby&gt;&lt;ruby&gt;Èú≠&lt;rt&gt;«éi&lt;/rt&gt;&lt;/ruby&gt; in Chinese.

It contains:

- an analyzing library for Typst, see [tinymist-query](/crates/tinymist-query/).
- a CLI for Typst, see [tinymist](/crates/tinymist/).
  - which provides a language server for Typst, see [Language Features](https://myriad-dreamin.github.io/tinymist/feature/language.html).
  - which provides a preview server for Typst, see [Preview Feature](https://myriad-dreamin.github.io/tinymist/feature/preview.html).
- a VSCode extension for Typst, see [Tinymist VSCode Extension](/editors/vscode/).

## Features

Language service (LSP) features:

- [Semantic highlighting](https://code.visualstudio.com/api/language-extensions/semantic-highlight-guide)
  - The &quot;semantic highlighting&quot; is supplementary to [&quot;syntax highlighting&quot;](https://code.visualstudio.com/api/language-extensions/syntax-highlight-guide).
- [Code actions](https://code.visualstudio.com/api/language-extensions/programmatic-language-features#provide-code-actions)
  - Also known as &quot;quick fixes&quot; or &quot;refactorings&quot;.
- [Formatting (Reformatting)](https://code.visualstudio.com/api/language-extensions/programmatic-language-features#format-source-code-in-an-editor)
  - Provide the user with support for formatting whole documents, using [typstfmt](https://github.com/astrale-sharp/typstfmt) or [typstyle](https://github.com/Enter-tainer/typstyle).
- [Document highlight](https://code.visualstudio.com/api/language-extensions/programmatic-language-features#highlight-all-occurrences-of-a-symbol-in-a-document)
  - Highlight all break points in a loop context.
  - (Todo) Highlight all exit points in a function context.
  - (Todo) Highlight all captures in a closure context.
  - (Todo) Highlight all occurrences of a symbol in a document.
- [Document links](https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/#textDocument_documentLink)
  - Renders path or link references in the document, such as `image(&quot;path.png&quot;)` or `bibliography(style: &quot;path.csl&quot;)`.
- [Document symbols](https://code.visualstudio.com/docs/getstarted/userinterface#_outline-view)
  - Also known as &quot;document outline&quot; or &quot;table of contents&quot; _in Typst_.
- [Folding ranges](https://burkeholland.gitbook.io/vs-code-can-do-that/exercise-3-navigation-and-refactoring/folding-sections)
  - You can collapse code/content blocks and headings.
- [Goto definitions](https://code.visualstudio.com/api/language-extensions/programmatic-language-features#show-definitions-of-a-symbol)
  - Right-click on a symbol and select &quot;Go to Definition&quot;.
  - Or ctrl+click on a symbol.
- [References](https://code.visualstudio.com/api/language-extensions/programmatic-language-features#find-all-references-to-a-symbol)
  - Right-click on a symbol and select &quot;Go to References&quot; or &quot;Find References&quot;.
  - Or ctrl+click on a symbol.
- [Hover tips](https://code.visualstudio.com/api/language-extensions/programmatic-language-features#show-hovers)
  - Also known as &quot;hovering tooltip&quot;.
  - Render docs according to [tidy](https://github.com/Mc-Zen/tidy) style.
- [Inlay hints](https://www.jetbrains.com/help/idea/inlay-hints.html)
  - Inlay hints are special markers that appear in the editor and provide you with additional information about your code, like the names of the parameters that a called method expects.
- [Color Provider](https://code.visualstudio.com/api/language-extensions/programmatic-language-features#show-color-decorators)
  - View all inlay colorful label for color literals in your document.
  - Change the color literal&#039;s value by a color picker or its code presentation.
- [Code Lens](https://code.visualstudio.com/blogs/2017/02/12/code-lens-roundup)
  - Should give contextual buttons along with code. For example, a button for exporting your document to various formats at the start of the document.
- [Rename symbols and embedded paths](https://code.visualstudio.com/api/language-extensions/programmatic-language-features#rename-symbols)
- [Help with function and method signatures](https://code.visualstudio.com/api/language-extensions/programmatic-language-features#help-with-function-and-method-signatures)
- [Workspace Symbols](https://code.visualstudio.com/api/language-extensions/programmatic-language-features#show-all-symbol-definitions-in-folder)
- [Code Action](https://learn.microsoft.com/en-us/dynamics365/business-central/dev-itpro/developer/devenv-code-actions)
  - Increasing/Decreasing heading levels.
  - Turn equation into &quot;inline&quot;, &quot;block&quot; or &quot;multiple-line block&quot; styles.
- [experimental/onEnter](https://github.com/rust-lang/rust-analyzer/blob/master/docs/dev/lsp-extensions.md#on-enter)
  - &lt;kbd&gt;Enter&lt;/kbd&gt; inside triple-slash comments automatically inserts `///`
  - &lt;kbd&gt;Enter&lt;/kbd&gt; in the middle or after a trailing space in `//` inserts `//`
  - &lt;kbd&gt;Enter&lt;/kbd&gt; inside `//!` doc comments automatically inserts `//!`
  - &lt;kbd&gt;Enter&lt;/kbd&gt; inside equation markups automatically inserts indents.

Extra features:

- Compiles to PDF on save (configurable to as-you-type, or other options). Check [Docs: Exporting Documents](https://myriad-dreamin.github.io/tinymist/feature/export.html).
- Also compiles to SVG, PNG, HTML, Markdown, Text, and other formats by commands, vscode tasks, or code lenses.
- Provides test, benchmark, coverage collecting on documents and modules. Check [Docs: Testing Features](https://myriad-dreamin.github.io/tinymist/feature/testing.html).
- Provides builtin linting. Check [Docs: Linting Features](https://myriad-dreamin.github.io/tinymist/feature/linting.html).
- Provides a status bar item to show the current document&#039;s compilation status and words count.
- [Editor tools](/tools/editor-tools/):
  - View a list of templates in template gallery. (`tinymist.showTemplateGallery`)
  - Click a button in template gallery to initialize a new project with a template. (`tinymist.initTemplate` and `tinymist.initTemplateInPlace`)
  - Trace execution in current document (`tinymist.profileCurrentFile`).

## Versioning and Release Cycle

Tinymist&#039;s versions follow the [Semantic Versioning](https://semver.org/) scheme, in format of `MAJOR.MINOR.PATCH`. Besides, tinymist follows special rules for the version number:

- If a version is suffixed with `-rcN` (&lt;picture&gt;&lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/images/introduction/frame_0.svg&quot; /&gt;&lt;img src=&quot;assets/images/introduction/frame_1.svg&quot; alt=&quot;typst-frame&quot; /&gt;&lt;/picture&gt;), e.g. `0.11.0-rc1` and `0.12.1-rc1`, it means this version is a release candidate. It is used to test publish script and E2E functionalities. These versions will not be published to the marketplace.
- If the `PATCH` number is odd, e.g. `0.11.1` and `0.12.3`, it means this version is a nightly release. The nightly release will use both [tinymist](https://github.com/Myriad-Dreamin/tinymist/tree/main) and [typst](https://github.com/typst/typst/tree/main) at **main branch**. They will be published as prerelease version to the marketplace. Note that in nightly releases, we change `#sys.version` to the next minor release to help develop documents with nightly features. For example, in tinymist nightly v0.12.1 or v0.12.3, the `#sys.version` is changed to `version(0, 13, 0)`.
- Otherwise, if the `PATCH` number is even, e.g. `0.11.0` and `0.12.2`, it means this version is a regular release. The regular release will always use the recent stable version of tinymist and typst.

The release cycle is as follows:

- If there is a typst version update, a new major or minor version will be released intermediately. This means tinymist will always align the minor version with typst.
- If there is at least a bug or feature added this week, a new patch version will be released.

## Installation

Follow the instructions to enable tinymist in your favorite editor.

- [VS Cod(e,ium)](https://myriad-dreamin.github.io/tinymist/frontend/vscode.html)
- [Neovim](https://myriad-dreamin.github.io/tinymist/frontend/neovim.html)
- [Emacs](https://myriad-dreamin.github.io/tinymist/frontend/emacs.html)
- [Sublime Text](https://myriad-dreamin.github.io/tinymist/frontend/sublime-text.html)
- [Helix](https://myriad-dreamin.github.io/tinymist/frontend/helix.html)
- [Zed](https://myriad-dreamin.github.io/tinymist/frontend/zed.html)

## Installing Regular/Nightly Prebuilds from GitHub

Note: if you are not knowing what is a regular/nightly release, please don&#039;t follow this section.

Besides published releases specific for each editors, you can also download the latest regular/nightly prebuilts from GitHub and install them manually.

- Regular prebuilts can be found in [GitHub Releases](https://github.com/Myriad-Dreamin/tinymist/releases).
- Nightly prebuilts can be found in [GitHub Actions](https://github.com/Myriad-Dreamin/tinymist/actions).
  - (Suggested) Use the [tinymist-nightly-installer](https://github.com/hongjr03/tinymist-nightly-installer) to install the nightly prebuilts automatically.
    - Unix (Bash):
      ```bash
      curl -sSL https://github.com/hongjr03/tinymist-nightly-installer/releases/latest/download/run.sh | bash
      ```
    - Windows (PowerShell):
      ```bash
      iwr https://github.com/hongjr03/tinymist-nightly-installer/releases/latest/download/run.ps1 -UseBasicParsing | iex
      ```
  - The prebuilts for other revisions can also be found manually. For example, if you are seeking a nightly release for the featured [PR: build: bump version to 0.11.17-rc1](https://github.com/Myriad-Dreamin/tinymist/pull/468), you could click and go to the [action page](https://github.com/Myriad-Dreamin/tinymist/actions/runs/10120639466) run for the related commits and download the artifacts.

To install extension file (the file with `.vsix` extension) manually, please &lt;kbd&gt;Ctrl+Shift+X&lt;/kbd&gt; in the editor window and drop the downloaded vsix file into the opened extensions view.

## Documentation

See [Online Documentation](https://myriad-dreamin.github.io/tinymist/).

## Packaging

Stable Channel:

&lt;a href=&quot;https://repology.org/project/tinymist/versions&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://repology.org/badge/vertical-allrepos/tinymist.svg&quot; alt=&quot;Packaging status&quot; style=&quot;max-width: 100%; height: auto;&quot; /&gt;&lt;/a&gt;

Nightly Channel:

&lt;a href=&quot;https://repology.org/project/tinymist-nightly/versions&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://repology.org/badge/vertical-allrepos/tinymist-nightly.svg&quot; alt=&quot;Packaging status&quot; style=&quot;max-width: 100%; height: auto;&quot; /&gt;&lt;/a&gt;

## Roadmap

### Short Terms

To encourage contributions, we create many [Pull Requests](https://github.com/Myriad-Dreamin/tinymist/pulls) in draft to navigate short-term plans. They give you a hint of what or where to start in this large repository.

### Long Terms

We are planning to implement the following features in typst v0.14.0 or spare time in weekend:

- Type checking: complete the type checker.
- Periscope renderer: It is disabled since vscode reject to render SVGs containing foreignObjects.
- Inlay hint: It is disabled _by default_ because of performance issues.
- Find references of dictionary fields and named function arguments.
- Improve symbol view&#039;s appearance.
- Improve package view.
  - Navigate to symbols by clicking on the symbol name in the view.
  - Automatically locate the symbol item in the view when viewing local documentation.
  - Remember the recently invoked package commands, e.g. &quot;Open Docs of @preview/cetz:0.3.1&quot;, &quot;Open directory of @preview/touying:0.5.3&quot;.
- Improve label view.
  - Group labels.
  - Search labels.
  - Keep (persist) group preferences.
- Improve Typst Preview.
  - Pin drop-down: Set the file to preview in the drop-down for clients that doesn&#039;t support passing arguments to the preview command.
  - Render in web worker (another thread) to reduce overhead on the electron&#039;s main thread.
- Spell checking: There is already a branch but no suitable (default) spell checking library is found.
  - [typos](https://github.com/crate-ci/typos) is great for typst. [harper](https://github.com/Automattic/harper) looks promise.

If you are interested by any above features, please feel free to send Issues to discuss or PRs to implement to [GitHub.](https://github.com/Myriad-Dreamin/tinymist)

## Contributing

Please read the [CONTRIBUTING.md](CONTRIBUTING.md) file for contribution guidelines.

## Sponsoring

Tinymist thrives on community love and remains proudly independent. While we don&#039;t accept direct project funding, we warmly welcome support for our maintainers&#039; personal efforts. Please go to [Maintainers Page](/MAINTAINERS.md) and [Contributors Page](https://github.com/Myriad-Dreamin/tinymist/graphs/contributors) and find their personal pages for more information. It is also welcomed to directly ask questions about sponsoring on the [GitHub Issues](https://github.com/Myriad-Dreamin/tinymist/issues/new).

## Acknowledgements

- Partially code is inherited from [typst-lsp](https://github.com/nvarner/typst-lsp)
- The [integrating](/editors/vscode#symbol-view) **offline** handwritten-stroke recognizer is powered by [Detypify](https://detypify.quarticcat.com/).
- The [integrating](/editors/vscode#preview-command) preview service is powered by [typst-preview](https://github.com/Enter-tainer/typst-preview).
- The [integrating](/editors/vscode#managing-local-packages) local package management functions are adopted from [vscode-typst-sync](https://github.com/OrangeX4/vscode-typst-sync).
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[valadaptive/ntsc-rs]]></title>
            <link>https://github.com/valadaptive/ntsc-rs</link>
            <guid>https://github.com/valadaptive/ntsc-rs</guid>
            <pubDate>Thu, 07 Aug 2025 00:05:54 GMT</pubDate>
            <description><![CDATA[Free, open-source VHS effect. Standalone application + plugin (After Effects, Premiere, and OpenFX).]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/valadaptive/ntsc-rs">valadaptive/ntsc-rs</a></h1>
            <p>Free, open-source VHS effect. Standalone application + plugin (After Effects, Premiere, and OpenFX).</p>
            <p>Language: Rust</p>
            <p>Stars: 838</p>
            <p>Forks: 15</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://ntsc.rs&quot;&gt;
        &lt;picture&gt;
            &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;./docs/img/logo-darkmode.svg&quot;&gt;
            &lt;img alt=&quot;ntsc-rs logo&quot; src=&quot;./docs/img/logo-lightmode.svg&quot;&gt;
        &lt;/picture&gt;
    &lt;/a&gt;
&lt;/p&gt;

---

**ntsc-rs** is a video effect which emulates NTSC and VHS video artifacts. It can be used as an After Effects, Premiere, or OpenFX plugin, or as a standalone application.

![Screenshot of the ntsc-rs standalone application](./docs/img/appdemo.png)

## Download and Install

The latest version of ntsc-rs can be downloaded from [the releases page](https://github.com/valadaptive/ntsc-rs/releases).

After downloading, [read the documentation for how to run it](https://ntsc.rs/docs/standalone-installation/). In particular, ntsc-rs will not work properly on Linux unless you install all of the GStreamer packages listed in the documentation.

## More information

ntsc-rs is a rough Rust port of [ntscqt](https://github.com/JargeZ/ntscqt), a PyQt-based GUI for [ntsc](https://github.com/zhuker/ntsc), itself a Python port of [composite-video-simulator](https://github.com/joncampbell123/composite-video-simulator). Reimplementing the image processing in multithreaded Rust allows it to run at (mostly) real-time speeds.

It&#039;s not an exact port--some processing passes have visibly different results, and some new ones have been added.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[rerun-io/rerun]]></title>
            <link>https://github.com/rerun-io/rerun</link>
            <guid>https://github.com/rerun-io/rerun</guid>
            <pubDate>Thu, 07 Aug 2025 00:05:53 GMT</pubDate>
            <description><![CDATA[Visualize streams of multimodal data. Free, fast, easy to use, and simple to integrate. Built in Rust.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/rerun-io/rerun">rerun-io/rerun</a></h1>
            <p>Visualize streams of multimodal data. Free, fast, easy to use, and simple to integrate. Built in Rust.</p>
            <p>Language: Rust</p>
            <p>Stars: 8,993</p>
            <p>Forks: 501</p>
            <p>Stars today: 12 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.rerun.io/&quot;&gt;
    &lt;img alt=&quot;banner&quot; src=&quot;https://user-images.githubusercontent.com/1148717/218142418-1d320929-6b7a-486e-8277-fbeef2432529.png&quot;&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;h1 align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pypi.org/project/rerun-sdk/&quot;&gt;                        &lt;img alt=&quot;PyPi&quot;           src=&quot;https://img.shields.io/pypi/v/rerun-sdk.svg&quot;&gt;                              &lt;/a&gt;
  &lt;a href=&quot;https://crates.io/crates/rerun&quot;&gt;                             &lt;img alt=&quot;crates.io&quot;      src=&quot;https://img.shields.io/crates/v/rerun.svg&quot;&gt;                                &lt;/a&gt;
  &lt;a href=&quot;https://github.com/rerun-io/rerun/blob/main/LICENSE-MIT&quot;&gt;    &lt;img alt=&quot;MIT&quot;            src=&quot;https://img.shields.io/badge/license-MIT-blue.svg&quot;&gt;                        &lt;/a&gt;
  &lt;a href=&quot;https://github.com/rerun-io/rerun/blob/main/LICENSE-APACHE&quot;&gt; &lt;img alt=&quot;Apache&quot;         src=&quot;https://img.shields.io/badge/license-Apache-blue.svg&quot;&gt;                     &lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/Gcm8BbTaAj&quot;&gt;                              &lt;img alt=&quot;Rerun Discord&quot;  src=&quot;https://img.shields.io/discord/1062300748202921994?label=Rerun%20Discord&quot;&gt; &lt;/a&gt;
&lt;/h1&gt;

# Time-aware multimodal data stack and visualizations
Rerun is building the multimodal data stack to model, ingest, store, query and view robotics-style data.
It&#039;s used in areas like robotics, spatial and embodied AI, generative media, industrial processing, simulation, security, and health.

Rerun is easy to use!
Use the Rerun SDK (available for C++, Python and Rust) to log data like images, tensors, point clouds, and text.
Logs are streamed to the Rerun Viewer for live visualization or to file for later use.
You can also query the logged data through [our dataframe API](https://rerun.io/docs/howto/dataframe-api).

[Get started](#getting-started) in minutes ‚Äì no account needed.

* [Run the Rerun Viewer in your browser](https://www.rerun.io/viewer)
* [Read about what Rerun is and who it is for](https://www.rerun.io/docs/getting-started/what-is-rerun)

### A short taste
```py
import rerun as rr  # pip install rerun-sdk

rr.init(&quot;rerun_example_app&quot;)

rr.spawn()  # Spawn a child process with a viewer and connect
# rr.save(&quot;recording.rrd&quot;)  # Stream all logs to disk
# rr.connect_grpc()  # Connect to a remote viewer

# Associate subsequent data with 42 on the ‚Äúframe‚Äù timeline
rr.set_time(&quot;frame&quot;, sequence=42)

# Log colored 3D points to the entity at `path/to/points`
rr.log(&quot;path/to/points&quot;, rr.Points3D(positions, colors=colors))
‚Ä¶
```

&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;img src=&quot;https://static.rerun.io/opf_screenshot/bee51040cba93c0bae62ef6c57fa703704012a41/full.png&quot; alt=&quot;&quot;&gt;
    &lt;source media=&quot;(max-width: 480px)&quot; srcset=&quot;https://static.rerun.io/opf_screenshot/bee51040cba93c0bae62ef6c57fa703704012a41/480w.png&quot;&gt;
    &lt;source media=&quot;(max-width: 768px)&quot; srcset=&quot;https://static.rerun.io/opf_screenshot/bee51040cba93c0bae62ef6c57fa703704012a41/768w.png&quot;&gt;
    &lt;source media=&quot;(max-width: 1024px)&quot; srcset=&quot;https://static.rerun.io/opf_screenshot/bee51040cba93c0bae62ef6c57fa703704012a41/1024w.png&quot;&gt;
    &lt;source media=&quot;(max-width: 1200px)&quot; srcset=&quot;https://static.rerun.io/opf_screenshot/bee51040cba93c0bae62ef6c57fa703704012a41/1200w.png&quot;&gt;
  &lt;/picture&gt;
&lt;/p&gt;

## Getting started
* [**C++**](https://www.rerun.io/docs/getting-started/quick-start/cpp)
* [**Python**](https://www.rerun.io/docs/getting-started/quick-start/python): `pip install rerun-sdk` or on [`conda`](https://github.com/conda-forge/rerun-sdk-feedstock)
* [**Rust**](https://www.rerun.io/docs/getting-started/quick-start/rust): `cargo add rerun`

### Installing the Rerun Viewer binary
To stream log data over the network or load our `.rrd` data files you also need the `rerun` binary.
It can be installed with `pip install rerun-sdk` or with `cargo install rerun-cli --locked --features nasm` (see note below).
Note that only the Python SDK comes bundled with the Viewer whereas C++ &amp; Rust always rely on a separate install.

**Note**: the `nasm` Cargo feature requires the [`nasm`](https://github.com/netwide-assembler/nasm) CLI to be installed and available in your path.
Alternatively, you may skip enabling this feature, but this may result in inferior video decoding performance.

You should now be able to run `rerun --help` in any terminal.


### Documentation
- üìö [High-level docs](http://rerun.io/docs)
- ‚èÉ [Loggable Types](https://www.rerun.io/docs/reference/types)
- ‚öôÔ∏è [Examples](http://rerun.io/examples)
- üìñ [Code snippets](./docs/snippets/INDEX.md)
- üåä [C++ API docs](https://ref.rerun.io/docs/cpp)
- üêç [Python API docs](https://ref.rerun.io/docs/python)
- ü¶Ä [Rust API docs](https://docs.rs/rerun/)
- ‚ÅâÔ∏è [Troubleshooting](https://www.rerun.io/docs/getting-started/troubleshooting)


## Status
We are in active development.
There are many features we want to add, and the API is still evolving.
_Expect breaking changes!_

Some shortcomings:
* [The viewer slows down when there are too many entities](https://github.com/rerun-io/rerun/issues/7115)
* [We don&#039;t support transparency yet](https://github.com/rerun-io/rerun/issues/1611)
* The data you want to visualize must fit in RAM
  - See &lt;https://www.rerun.io/docs/howto/limit-ram&gt; for how to bound memory use.
  - We plan on having a disk-based data store some time in the future.
* [Multi-million point clouds can be slow](https://github.com/rerun-io/rerun/issues/1136)


## What is Rerun for?

Rerun is built to help you understand and improve complex processes that include rich multimodal data, like 2D, 3D, text, time series, tensors, etc.
It is used in many industries, including robotics, simulation, computer vision,
or anything that involves a lot of sensors or other signals that evolve over time.

### Example use case
Say you&#039;re building a vacuum cleaning robot and it keeps running into walls. Why is it doing that? You need some tool to debug it, but a normal debugger isn&#039;t gonna be helpful. Similarly, just logging text won&#039;t be very helpful either. The robot may log &quot;Going through doorway&quot; but that won&#039;t explain why it thinks the wall is a door.

What you need is a visual and temporal debugger, that can log all the different representations of the world the robots holds in its little head, such as:

* RGB camera feed
* depth images
* lidar scan
* segmentation image (how the robot interprets what it sees)
* its 3D map of the apartment
* all the objects the robot has detected (or thinks it has detected), as 3D shapes in the 3D map
* its confidence in its prediction
* etc

You also want to see how all these streams of data evolve over time so you can go back and pinpoint exactly what went wrong, when and why.

Maybe it turns out that a glare from the sun hit one of the sensors in the wrong way, confusing the segmentation network leading to bad object detection. Or maybe it was a bug in the lidar scanning code. Or maybe the robot thought it was somewhere else in the apartment, because its odometry was broken. Or it could be one of a thousand other things. Rerun will help you find out!

But seeing the world from the point of the view of the robot is not just for debugging - it will also give you ideas on how to improve the algorithms, new test cases to set up, or datasets to collect. It will also let you explain the brains of the robot to your colleagues, boss, and customers. And so on. Seeing is believing, and an image is worth a thousand words, and multimodal temporal logging is worth a thousand images :)

While seeing and understanding your data is core to making progress in robotics, there is one more thing:
You can also use the data you collected for visualization to create new datasets for training and evaluating the models and algorithms that run on your robot.
Rerun provides query APIs to make it easy to extract clean datasets from your recording for exactly that purpose.

Of course, Rerun is useful for much more than just robots. Any time you have any form of sensors, or 2D or 3D state evolving over time, Rerun is a great tool.


## Business model
Rerun uses an open-core model. Everything in this repository will stay open source and free (both as in beer and as in freedom).

We are also building a commercial data platform.
Right now that is only available for a few select design partners.
[Click here if you&#039;re interested](https://rerun.io/pricing).

The Rerun open source project targets the needs of individual developers.
The commercial product targets the needs specific to teams that build and run computer vision and robotics products.

## How to cite Rerun

When using Rerun in your research, please cite it to acknowledge its contribution to your work. This can be done by
including a reference to Rerun in the software or methods section of your paper.

Suggested citation format:

```bibtex
@software{RerunSDK,
  title = {Rerun: A Visualization SDK for Multimodal Data},
  author = {{Rerun Development Team}},
  url = {https://www.rerun.io},
  version = {insert version number},
  date = {insert date of usage},
  year = {2024},
  publisher = {{Rerun Technologies AB}},
  address = {Online},
  note = {Available from https://www.rerun.io/ and https://github.com/rerun-io/rerun}
}
```

Please replace &quot;insert version number&quot; with the version of Rerun you used and &quot;insert date of usage&quot; with the date(s)
you used the tool in your research.
This citation format helps ensure that Rerun&#039;s development team receives appropriate credit for their work and
facilitates the tool&#039;s discovery by other researchers.

# Development
* [`ARCHITECTURE.md`](ARCHITECTURE.md)
* [`CODE_OF_CONDUCT.md`](CODE_OF_CONDUCT.md)
* [`CODE_STYLE.md`](CODE_STYLE.md)
* [`CONTRIBUTING.md`](CONTRIBUTING.md)
* [`BUILD.md`](BUILD.md)
* [`rerun_py/README.md`](rerun_py/README.md) - instructions for Python SDK
* [`rerun_cpp/README.md`](rerun_cpp/README.md) - instructions for C++ SDK


## Installing a pre-release Python SDK

1. Download the correct `.whl` from [GitHub Releases](https://github.com/rerun-io/rerun/releases)
2. Run `pip install rerun_sdk&lt;‚Ä¶&gt;.whl` (replace `&lt;‚Ä¶&gt;` with the actual filename)
3. Test it: `rerun --version`
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
    </channel>
</rss>