<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for rust - Rust Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for rust.</description>
        <lastBuildDate>Sat, 28 Feb 2026 00:06:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2026, GitHub</copyright>
        <item>
            <title><![CDATA[ruvnet/ruvector]]></title>
            <link>https://github.com/ruvnet/ruvector</link>
            <guid>https://github.com/ruvnet/ruvector</guid>
            <pubDate>Sat, 28 Feb 2026 00:06:05 GMT</pubDate>
            <description><![CDATA[RuVector is a High Performance, Real-Time, Self-Learning, Vector Graph Neural Network, and Database built in Rust.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ruvnet/ruvector">ruvnet/ruvector</a></h1>
            <p>RuVector is a High Performance, Real-Time, Self-Learning, Vector Graph Neural Network, and Database built in Rust.</p>
            <p>Language: Rust</p>
            <p>Stars: 1,904</p>
            <p>Forks: 197</p>
            <p>Stars today: 410 stars today</p>
            <h2>README</h2><pre># RuVector ‚Äî A Self-Learning, Agentic Operating System
[![CES 2026 Innovation Award](https://img.shields.io/badge/üèÖ_CES_2026-Innovation_Award-gold.svg)](https://cognitum.one)
[![GitHub Trending](https://img.shields.io/badge/üî•_GitHub-Trending-orange.svg)](https://github.com/ruvnet/ruvector)

[![Crates.io](https://img.shields.io/crates/v/ruvector-core.svg)](https://crates.io/crates/ruvector-core)
[![npm](https://img.shields.io/npm/v/ruvector.svg)](https://www.npmjs.com/package/ruvector)
[![Downloads](https://img.shields.io/npm/dt/ruvector.svg?label=Downloads)](https://www.npmjs.com/package/ruvector)
[![Monthly Downloads](https://img.shields.io/npm/dm/ruvector.svg?label=Monthly%20Downloads)](https://www.npmjs.com/package/ruvector)
[![ruv.io](https://img.shields.io/badge/ruv.io-website-purple.svg)](https://ruv.io)
[![MIT License](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)

### **The self-learning, self-optimizing vector database ‚Äî with graph intelligence, local AI, and PostgreSQL built in.**

&gt; Created by [rUv](https://ruv.io) and powering [Cognitum](https://cognitum.one), a üèÖ **CES 2026 Innovation Awards Honoree** ‚Äî the world&#039;s first Agentic Chip designed to be always running for AI agents. Tens of thousands of agents, near-zero power, learns from every signal. [Learn more ‚Üí](https://cognitum.one)


```bash
npx ruvector
```

####  Most vector databases store your data and search it ‚Äî the same way, every time. 

#### **RuVector** is fundamentally different. It watches how you use it and gets smarter: search results improve automatically, the system tunes itself to your workload, and it runs AI models right on your hardware ‚Äî no cloud APIs, no per-query bills, GPUs optional, CPUs preferred. It drops into PostgreSQL, runs in browsers, and ships as a single file. 

Open source. ‚ù§Ô∏è Free forever.

```
User Query ‚Üí [SONA Engine] ‚Üí Model Response ‚Üí User Feedback
                  ‚Üë                                 ‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Learning Signal ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         (&lt; 1ms adaptation)
```

&lt;details&gt;
&lt;summary&gt;üîç RuVector vs Typical Vector Databases (20 differences)&lt;/summary&gt;

| | RuVector | Typical Vector DB |
|---|---|---|
| **Self-Learning &amp; Optimization** | | |
| [Search quality](./crates/ruvector-gnn) | üß† GNN learns from every query ‚Äî results improve over time | Static ‚Äî same results every time |
| [Self-optimizing](./crates/sona) | ‚ö° SONA auto-tunes routing, ranking, and compression to your workload | Manual tuning required |
| [46 attention mechanisms](./crates/ruvector-attention) | üéØ Flash, linear, graph, hyperbolic, [mincut-gated](./crates/ruvector-attn-mincut) (cuts compute 50%) | Basic similarity only |
| [Transfer learning](./crates/ruvector-domain-expansion) | üîÑ Knowledge transfers across domains ‚Äî new tasks bootstrap from past learning | Start from scratch each time |
| **Graph &amp; Relationships** | | |
| [Graph queries](./crates/ruvector-graph) | üîó Full Cypher engine ‚Äî `MATCH (a)-[:KNOWS]-&gt;(b)` like Neo4j | Flat list of results |
| [Graph transformers](./crates/ruvector-graph-transformer) | üî¨ 8 verified modules: physics, bio, manifold, temporal, economic | No graph support |
| [Hyperedges](./crates/ruvector-graph) | üï∏Ô∏è Connect 3+ nodes at once ‚Äî model group relationships natively | Pairwise only |
| **AI &amp; Compute** | | |
| [Local LLMs](./crates/ruvllm) | ü§ñ Run models on your hardware ‚Äî Metal, CUDA, WebGPU, no API costs | Cloud API required (pay per call) |
| [Sublinear solvers](./crates/ruvector-solver) | üìê O(log n) PageRank, spectral methods, sparse linear systems | Not available |
| [Genomics](./examples/dna) | üß¨ Variant calling, protein translation, HNSW k-mer search in 12 ms | Not available |
| [Quantum coherence](./crates/ruqu) | ‚öõÔ∏è Error correction via dynamic min-cut optimization | Not available |
| **Database &amp; Platform** | | |
| [PostgreSQL](./crates/ruvector-postgres) | üêò 230+ SQL functions ‚Äî drop into your existing database, [pgvector replacement](./docs/postgres/) | Separate service to manage |
| [Deploy anywhere](./crates/rvf/README.md) | üåê One file ‚Äî servers, browsers, phones, IoT, bare metal, WASM (58 KB) | Cloud server required |
| [Cognitive containers](./crates/rvf/README.md) | üöÄ Single `.rvf` file boots as a service in 125 ms ‚Äî includes vectors, models, kernel | Configure a cluster |
| [Live updates](./crates/ruvector-core) | ‚ö° Update vectors and graph connections instantly, no downtime | Rebuild index or wait |
| **Operations** | | |
| [Tamper-proof audit](./crates/rvf/rvf-crypto) | üîê Cryptographic witness chain records every operation automatically | Manual logging |
| [Branch your data](./crates/rvf/rvf-cow) | üåø Git-like COW branching ‚Äî 1M vectors, 100 edits = ~2.5 MB branch | Copy everything |
| [Scale out](./crates/ruvector-replication) | üìà Raft consensus, multi-master replication, auto-sharding | Paid tiers, per-vector pricing |
| [Post-quantum crypto](./crates/rvf/rvf-crypto) | üõ°Ô∏è ML-DSA-65 and Ed25519 signatures on every segment | Not available |
| Cost | üí∞ Free forever ‚Äî open source (MIT) | Per-query or per-vector pricing |

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üìã See Full Capabilities (75 features across 10 categories)&lt;/summary&gt;

**Core Vector Database**
| # | Capability | What It Does |
|---|------------|--------------|
| 1 | [**Store vectors**](./crates/ruvector-core) | Embeddings from OpenAI, Cohere, local ONNX with HNSW indexing and SIMD acceleration |
| 2 | [**Query with Cypher**](./crates/ruvector-graph) | Graph queries like Neo4j ‚Äî `MATCH (a)-[:SIMILAR]-&gt;(b)` with hyperedges |
| 3 | [**The index learns**](./crates/ruvector-gnn) | GNN layers make search results improve over time ‚Äî every query teaches the system |
| 4 | [**Hyperbolic HNSW**](./crates/ruvector-hyperbolic-hnsw) | Hierarchy-aware search in Poincare ball space ‚Äî better for trees and taxonomies |
| 5 | [**Compress automatically**](./crates/ruvector-temporal-tensor) | 2-32x memory reduction with adaptive tiered compression and temporal tensor reuse |
| 6 | [**Metadata filtering**](./crates/ruvector-filter) | Filter search results by any field before scanning vectors ‚Äî fast hybrid queries |
| 7 | [**Collections**](./crates/ruvector-collections) | Multi-tenant, schema-managed collections ‚Äî isolate data per customer or project |
| 8 | [**Snapshots**](./crates/ruvector-snapshot) | Point-in-time backups ‚Äî restore your database to any previous state |

**Distributed Systems**
| # | Capability | What It Does |
|---|------------|--------------|
| 9 | [**Raft consensus**](./crates/ruvector-raft) | Leader election and log replication ‚Äî nodes agree on state even when some fail |
| 10 | [**Multi-master replication**](./crates/ruvector-replication) | Vector clocks, conflict resolution, geo-distributed sync across data centers |
| 11 | [**Cluster management**](./crates/ruvector-cluster) | Horizontal scaling with consistent hashing ‚Äî add nodes without rebalancing everything |
| 12 | [**Delta consensus**](./crates/ruvector-delta-consensus) | Track behavioral changes across distributed nodes with CRDTs and causal ordering |
| 13 | **Burst scaling** | 10-50x capacity scaling for traffic spikes ‚Äî absorb load then scale back down |
| 14 | **Auto-sharding** | Automatic data partitioning across nodes based on access patterns |

**AI &amp; Machine Learning**
| # | Capability | What It Does |
|---|------------|--------------|
| 15 | [**Run LLMs locally**](./crates/ruvllm) | Load GGUF models and run inference on your hardware ‚Äî Metal, CUDA, ANE, WebGPU |
| 16 | [**RuvLTRA models**](https://huggingface.co/ruv/ruvltra) | Pre-trained GGUF for routing and embeddings in under 10 ms |
| 17 | [**SONA learning**](./crates/sona) | Self-Optimizing Neural Architecture ‚Äî LoRA fine-tuning + EWC++ memory preservation |
| 18 | [**46 attention mechanisms**](./crates/ruvector-attention) | Flash, linear, graph, hyperbolic, mincut-gated (cuts compute 50%) |
| 19 | [**Semantic routing**](./crates/ruvector-router-core) | Route AI requests to the right model or handler using FastGRNN neural inference |
| 20 | [**Sparse inference**](./crates/ruvector-sparse-inference) | PowerInfer-style engine ‚Äî only activate the neurons you need, 2-10x faster on edge |
| 21 | [**Tiny Dancer**](./crates/ruvector-tiny-dancer-core) | Production-grade agent routing with FastGRNN ‚Äî lightweight alternative to full LLM |
| 22 | [**Domain expansion**](./crates/ruvector-domain-expansion) | Cross-domain transfer learning ‚Äî new tasks bootstrap from past learning automatically |
| 23 | [**Advanced math**](./crates/ruvector-math) | Optimal transport, Sinkhorn distances, KL divergence, spectral clustering |
| 24 | [**Coherence measurement**](./crates/ruvector-coherence) | Measure signal quality and compare attention mechanisms objectively |

**Graph Transformers** ([8 verified modules](./crates/ruvector-graph-transformer))
| # | Capability | What It Does |
|---|------------|--------------|
| 25 | [**Proof-gated mutation**](./crates/ruvector-verified) | Every write to graph state requires a formal proof ‚Äî bugs cannot corrupt data |
| 26 | **Sublinear attention** | O(n log n) via LSH bucketing, PPR sampling, and spectral sparsification |
| 27 | **Physics-informed layers** | Hamiltonian dynamics, gauge equivariant message passing ‚Äî energy conserved by construction |
| 28 | **Biological layers** | Spiking attention, Hebbian/STDP learning, dendritic branching |
| 29 | **Self-organizing layers** | Morphogenetic fields, reaction-diffusion growth ‚Äî graphs that restructure themselves |
| 30 | **Verified training** | Training certificates, delta-apply rollback ‚Äî bad gradient steps auto-reversed |
| 31 | **Manifold geometry** | Product manifolds S^n x H^m x R^k ‚Äî work in curved spaces, not just flat |
| 32 | **Temporal-causal layers** | Causal masking, Granger causality extraction, continuous-time ODE integration |
| 33 | **Economic layers** | Nash equilibrium attention, Shapley attribution ‚Äî fair value assignment in multi-agent graphs |

**Cognitive Containers** ([RVF format](./crates/rvf/README.md))
| # | Capability | What It Does |
|---|------------|--------------|
| 34 | **Self-boot as a microservice** | A single `.rvf` file contains vectors, models, and a Linux kernel ‚Äî boots in 125 ms |
| 35 | **eBPF acceleration** | Hot vectors served in kernel data path via XDP, socket filter, and TC programs |
| 36 | **5.5 KB WASM runtime** | Same `.rvf` file runs queries in a browser tab with zero backend |
| 37 | [**COW branching**](./crates/rvf) | Git-like copy-on-write ‚Äî 1M vectors, 100 edits = ~2.5 MB branch |
| 38 | [**Witness chains**](./crates/rvf/rvf-crypto) | Tamper-evident hash-linked audit trail records every operation automatically |
| 39 | [**Post-quantum signatures**](./crates/rvf/rvf-crypto) | ML-DSA-65 and SLH-DSA-128s alongside Ed25519 ‚Äî future-proof cryptography |
| 40 | **DNA-style lineage** | Track parent/child derivation chains with cryptographic hashes |
| 41 | **25 segment types** | VEC, INDEX, KERNEL, EBPF, WASM, COW_MAP, WITNESS, CRYPTO, and 17 more |

**PostgreSQL Extension** ([230+ SQL functions](./crates/ruvector-postgres))
| # | Capability | What It Does |
|---|------------|--------------|
| 42 | **Drop-in pgvector replacement** | Same SQL interface but with self-learning search ‚Äî no app changes needed |
| 43 | **Sublinear solvers in SQL** | PageRank, conjugate gradient, Laplacian solver ‚Äî O(log n) to O(sqrt(n)) |
| 44 | **Math distances in SQL** | Wasserstein, Sinkhorn, KL divergence, spectral clustering ‚Äî all from SQL |
| 45 | **Topological data analysis** | Persistent homology, Betti numbers, embedding drift detection |
| 46 | **SONA learning in SQL** | Micro-LoRA trajectory learning with EWC++ forgetting prevention |
| 47 | **Extended attention in SQL** | O(n) linear, MoE, hyperbolic, sliding window attention ‚Äî all callable from SQL |

**Specialized Processing**
| # | Capability | What It Does |
|---|------------|--------------|
| 48 | [**SciPix OCR**](./examples/scipix) | Extract LaTeX and MathML from scientific documents and PDFs |
| 49 | [**DAG workflows**](./crates/ruvector-dag) | Self-learning directed acyclic graph execution for multi-step pipelines |
| 50 | [**Cognitum Gate**](./crates/cognitum-gate-kernel) | Cognitive AI gateway with TileZero acceleration for fast routing |
| 51 | [**FPGA transformer**](./crates/ruvector-fpga-transformer) | Hardware-accelerated transformer inference on programmable chips |
| 52 | [**Quantum coherence**](./crates/ruQu) | Error correction via dynamic min-cut optimization for quantum circuits |
| 53 | [**Sublinear solvers**](./crates/ruvector-solver) | 8 algorithms (Neumann, CG, Forward Push, TRUE, BMSSP) ‚Äî O(log n) to O(sqrt(n)) |
| 54 | [**Mincut-gated transformer**](./crates/ruvector-mincut-gated-transformer) | Dynamic attention that prunes irrelevant connections using graph min-cut |
| 55 | [**Nervous system**](./crates/ruvector-nervous-system) | 5-layer bio-inspired adaptive system with spiking networks and BTSP learning |
| 56 | [**Prime Radiant**](./crates/prime-radiant) | Coherence engine using sheaf Laplacian math for AI safety and hallucination detection |

**Genomics &amp; Health** ([rvDNA](./examples/dna))
| # | Capability | What It Does |
|---|------------|--------------|
| 57 | **rvDNA genomic analysis** | Variant calling, protein translation, HNSW k-mer search in 12 ms |
| 58 | **`.rvdna` file format** | AI-native binary with pre-computed vectors, tensors, and embeddings |
| 59 | **Instant diagnostics** | Sickle cell, cancer mutations, drug dosing ‚Äî runs on any device |
| 60 | **Privacy-first WASM** | Browser-based genomics ‚Äî your DNA data never leaves the device |
| 61 | **Biomarker engine** | Composite polygenic risk scoring (20 SNPs, 6 gene-gene interactions, 2 us) |
| 62 | **Streaming biomarkers** | Real-time anomaly detection, CUSUM changepoints, trend analysis (&gt;100k readings/sec) |

**Platform &amp; Integration**
| # | Capability | What It Does |
|---|------------|--------------|
| 63 | **Run anywhere** | Rust, Node.js, browser (WASM), edge ([rvLite](./crates/rvlite)), HTTP server, bare metal |
| 64 | [**MCP server**](./crates/mcp-gate) | Model Context Protocol for AI assistants ‚Äî Claude, GPT, and other agents can use RuVector as a tool |
| 65 | **Cloud deployment** | One-click deploy to [Google Cloud Run](./examples/google-cloud), Kubernetes |
| 66 | [**iOS App Clip**](./examples/app-clip) | Scan a QR code to load an RVF cognitive seed on your phone ‚Äî under 15 MB |
| 67 | [**Prometheus metrics**](./crates/ruvector-metrics) | Built-in monitoring ‚Äî export latency, throughput, and memory stats to Grafana |
| 68 | **90+ Rust crates + npm packages** | Published on [crates.io](https://crates.io/crates/rvf-runtime) and [npm](https://www.npmjs.com/package/@ruvector/rvf) |

**Examples &amp; Applications**
| # | Capability | What It Does |
|---|------------|--------------|
| 69 | [**Neural Trader**](./examples/neural-trader) | Algorithmic trading with Kelly Criterion position sizing and LSTM-Transformer prediction |
| 70 | [**Spiking Neural Network**](./examples/meta-cognition-spiking-neural-network) | Hybrid AI combining spiking networks, SIMD vector ops, and 5 attention types |
| 71 | [**ReFrag Pipeline**](./examples/refrag-pipeline) | Compress-Sense-Expand architecture ‚Äî ~30x RAG latency reduction |
| 72 | [**Edge Network**](./examples/edge-net) | Distributed collective AI ‚Äî share idle compute across devices |
| 73 | [**Vibecast 7Sense**](./examples/vibecast-7sense) | Transform bird calls into navigable geometric space using vector search |
| 74 | [**Ultra-Low Latency Sim**](./examples/ultra-low-latency-sim) | Meta-simulation achieving quadrillion simulations per second on CPU with SIMD |
| 75 | [**Verified Applications**](./examples/verified-applications) | 10 exotic proof-carrying apps: weapons filters, legal forensics, medical diagnostics |

&lt;/details&gt;

### Built by rUv, powered by [Cognitum.one](https://cognitum.one)

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Cognitum Hardware ‚Äî The Agentic Appliance &amp; Chip&lt;/strong&gt;&lt;/summary&gt;

**Cognitum v0 ‚Äî The Agentic Appliance**: Run tens of thousands of always-on agents at no incremental cost beyond the box. Learns in proximity to any signal ‚Äî sensors, networks, machines ‚Äî at near-zero power (~5 uW/inference, &lt;15W total). Sub-millisecond response, 500x cheaper than cloud AI. No cloud bills, no per-agent fees. Like a nervous system, not a brain.

**Cognitum v1 ‚Äî The Agentic Chip**: Same architecture on a single 257-core custom chip. Runs on less than 2W ‚Äî a AA battery. Idle-to-8 GHz burst on demand, 2 TB/s interconnect, built-in encryption per core.

&lt;/details&gt;

### A Complete Agentic AI Operating System

RuVector isn&#039;t a database you add to your stack ‚Äî it&#039;s the entire stack. Self-learning, self-optimizing, and self-deploying. Everything an AI application needs to run, from bare metal hardware up to the application layer, in one package:


**Intelligence**

| | Layer | Replaces | What It Does |
|---|-------|----------|--------------|
| üîÑ | [**Self-Learning**](./crates/sona/README.md) | Manual retraining, MLOps | SONA adapts in &lt;1 ms ‚Äî LoRA fine-tuning + EWC++ memory on every request |
| ‚ö° | [**Self-Optimizing**](./crates/ruvector-gnn/README.md) | Manual tuning, config files | Auto-tunes routing, ranking, compression, and index parameters |
| üéØ | [**Embeddings**](./crates/ruvllm/README.md) | OpenAI API, Cohere, static models | Contrastive training, triplet loss, real-time fine-tuning ‚Äî embeddings improve as you use them |
| ‚úÖ | [**Verified Training**](./crates/ruvector-verified/README.md) | Manual validation | Formal proofs + statistical tests on every training step ‚Äî gradients only apply if invariants pass |

**Data &amp; Search**

| | Layer | Replaces | What It Does |
|---|-------|----------|--------------|
| üîç | [**Search**](./crates/ruvector-core/README.md) | Pinecone, Weaviate, Qdrant | Self-learning HNSW ‚Äî GNN improves results from every query |
| üóÑÔ∏è | [**Storage**](./crates/ruvector-core/README.md) | Separate database + cache | Vector store, graph DB, key-value cache ‚Äî unified engine |
| üêò | [**PostgreSQL**](./crates/ruvector-postgres/README.md) | pgvector, pg_embedding | Drop-in replacement ‚Äî 230+ SQL functions, same interface but search gets smarter over time |
| üîó | [**Graph**](./crates/ruvector-graph/README.md) | Neo4j, Amazon Neptune | Cypher, W3C SPARQL 1.1, hyperedges ‚Äî all built in |

**AI &amp; ML**

| | Layer | Replaces | What It Does |
|---|-------|----------|--------------|
| ü§ñ | [**AI Runtime**](./crates/ruvllm/README.md) | llama.cpp, vLLM, Ollama | ruvllm ‚Äî GGUF models, MicroLoRA (&lt;1 ms), speculative decoding, continuous batching, WASM |
| üß† | [**ML Framework**](./crates/ruvector-attention/README.md) | PyTorch, TensorFlow | 46 attention types, 8 graph transformers, spiking networks, sparse inference, sublinear solvers |
| üî¨ | [**Coherence**](./crates/ruvector-mincut/README.md) | Manual testing, guardrails | Min-cut finds the weakest links in any network ‚Äî detects AI drift, prunes wasted compute (50% reduction), keeps agents in sync |
| üß¨ | [**Domain Models**](./crates/ruvector-domain-expansion/README.md) | Custom ML pipelines | Genomics (DNA variant calling), physics simulation, economic modeling, biological networks |

**Infrastructure**

| | Layer | Replaces | What It Does |
|---|-------|----------|--------------|
| üîß | [**Hardware**](./crates/ruvector-fpga-transformer/README.md) | CUDA toolkit, driver configs | Sparse/spiking CPU (AVX-512, NEON) ‚Äî GPU for bursts (Metal, CUDA, ANE, WebGPU, FPGA) |
| üêß | [**Kernel**](./crates/rvf/README.md) | Linux + Docker + eBPF | `.rvf` file boots its own kernel in 125 ms ‚Äî eBPF accelerates hot paths |
| üåê | [**Coordination**](./crates/ruvector-raft/README.md) | etcd, ZooKeeper, Consul | Raft consensus, multi-master replication, CRDT delta sync, auto-sharding |
| üì¶ | [**Packaging**](./crates/rvf/README.md) | Docker, Kubernetes | One `.rvf` file = your entire service

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[chroma-core/chroma]]></title>
            <link>https://github.com/chroma-core/chroma</link>
            <guid>https://github.com/chroma-core/chroma</guid>
            <pubDate>Sat, 28 Feb 2026 00:06:04 GMT</pubDate>
            <description><![CDATA[Open-source search and retrieval database for AI applications.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/chroma-core/chroma">chroma-core/chroma</a></h1>
            <p>Open-source search and retrieval database for AI applications.</p>
            <p>Language: Rust</p>
            <p>Stars: 26,354</p>
            <p>Forks: 2,080</p>
            <p>Stars today: 21 stars today</p>
            <h2>README</h2><pre>![Chroma](./docs/assets/chroma-wordmark-color.png#gh-light-mode-only)
![Chroma](./docs/assets/chroma-wordmark-white.png#gh-dark-mode-only)

&lt;p align=&quot;center&quot;&gt;
    &lt;b&gt;Chroma - the open-source search engine for AI&lt;/b&gt;. &lt;br /&gt;
    The fastest way to build Python or JavaScript LLM apps that search over your data!
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://discord.gg/MMeYNTmh3x&quot; target=&quot;_blank&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/discord/1073293645303795742?cacheSeconds=3600&quot; alt=&quot;Discord&quot;&gt;
  &lt;/a&gt; |
  &lt;a href=&quot;https://github.com/chroma-core/chroma/blob/master/LICENSE&quot; target=&quot;_blank&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/badge/License-Apache_2.0-blue.svg&quot; alt=&quot;License&quot;&gt;
  &lt;/a&gt; |
  &lt;a href=&quot;https://docs.trychroma.com/&quot; target=&quot;_blank&quot;&gt;
      Docs
  &lt;/a&gt; |
  &lt;a href=&quot;https://www.trychroma.com/&quot; target=&quot;_blank&quot;&gt;
      Homepage
  &lt;/a&gt;
&lt;/p&gt;

```bash
pip install chromadb # python client
# for javascript, npm install chromadb!
# for client-server mode, chroma run --path /chroma_db_path
```

## Chroma Cloud

Our hosted service, Chroma Cloud, powers serverless vector, hybrid, and full-text search. It&#039;s extremely fast, cost-effective, scalable and painless. Create a DB and try it out in under 30 seconds with $5 of free credits.

[Get started with Chroma Cloud](https://trychroma.com/signup)

## API

The core API is only 4 functions (run our [üí° Google Colab](https://colab.research.google.com/drive/1QEzFyqnoFxq7LUGyP1vzR4iLt9PpCDXv?usp=sharing)):

```python
import chromadb
# setup Chroma in-memory, for easy prototyping. Can add persistence easily!
client = chromadb.Client()

# Create collection. get_collection, get_or_create_collection, delete_collection also available!
collection = client.create_collection(&quot;all-my-documents&quot;)

# Add docs to the collection. Can also update and delete. Row-based API coming soon!
collection.add(
    documents=[&quot;This is document1&quot;, &quot;This is document2&quot;], # we handle tokenization, embedding, and indexing automatically. You can skip that and add your own embeddings as well
    metadatas=[{&quot;source&quot;: &quot;notion&quot;}, {&quot;source&quot;: &quot;google-docs&quot;}], # filter on these!
    ids=[&quot;doc1&quot;, &quot;doc2&quot;], # unique for each doc
)

# Query/search 2 most similar results. You can also .get by id
results = collection.query(
    query_texts=[&quot;This is a query document&quot;],
    n_results=2,
    # where={&quot;metadata_field&quot;: &quot;is_equal_to_this&quot;}, # optional filter
    # where_document={&quot;$contains&quot;:&quot;search_string&quot;}  # optional filter
)
```

Learn about all features on our [Docs](https://docs.trychroma.com)

## Features
- __Simple__: Fully-typed, fully-tested, fully-documented == happiness
- __Integrations__: [`ü¶úÔ∏èüîó LangChain`](https://blog.langchain.dev/langchain-chroma/) (python and js), [`ü¶ô LlamaIndex`](https://twitter.com/atroyn/status/1628557389762007040) and more soon
- __Dev, Test, Prod__: the same API that runs in your python notebook, scales to your cluster
- __Feature-rich__: Queries, filtering, regex and more
- __Free &amp; Open Source__: Apache 2.0 Licensed 

## Use case: ChatGPT for ______

For example, the `&quot;Chat your data&quot;` use case:
1. Add documents to your database. You can pass in your own embeddings, embedding function, or let Chroma embed them for you.
2. Query relevant documents with natural language.
3. Compose documents into the context window of an LLM like `GPT4` for additional summarization or analysis.

## Embeddings?

What are embeddings?

- [Read the guide from OpenAI](https://platform.openai.com/docs/guides/embeddings)
- __Literal__: Embedding something turns it from image/text/audio into a list of numbers. üñºÔ∏è or üìÑ =&gt; `[1.2, 2.1, ....]`. This process makes documents &quot;understandable&quot; to a machine learning model.
- __By analogy__: An embedding represents the essence of a document. This enables documents and queries with the same essence to be &quot;near&quot; each other and therefore easy to find.
- __Technical__: An embedding is the latent-space position of a document at a layer of a deep neural network. For models trained specifically to embed data, this is the last layer.
- __A small example__: If you search your photos for &quot;famous bridge in San Francisco&quot;. By embedding this query and comparing it to the embeddings of your photos and their metadata - it should return photos of the Golden Gate Bridge.

Chroma allows you to store these vectors or embeddings and search by nearest neighbors rather than by substrings like a traditional database. By default, Chroma uses [Sentence Transformers](https://docs.trychroma.com/integrations/embedding-models/sentence-transformer#sentence-transformer) to embed for you but you can also use OpenAI embeddings, Cohere (multilingual) embeddings, or your own.

## Get involved

Chroma is a rapidly developing project. We welcome PR contributors and ideas for how to improve the project.
- [Join the conversation on Discord](https://discord.com/invite/chromadb) - `#contributing` channel
- [Review the üõ£Ô∏è Roadmap and contribute your ideas](https://docs.trychroma.com/docs/overview/oss#roadmap)
- [Grab an issue and open a PR](https://github.com/chroma-core/chroma/issues) - [`Good first issue tag`](https://github.com/chroma-core/chroma/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22)
- [Read our contributing guide](https://docs.trychroma.com/docs/overview/oss#contributing)

**Release Cadence**
We currently release new tagged versions of the `pypi` and `npm` packages on Mondays. Hotfixes go out at any time during the week.

## License

[Apache 2.0](./LICENSE)
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[farion1231/cc-switch]]></title>
            <link>https://github.com/farion1231/cc-switch</link>
            <guid>https://github.com/farion1231/cc-switch</guid>
            <pubDate>Sat, 28 Feb 2026 00:06:03 GMT</pubDate>
            <description><![CDATA[A cross-platform desktop All-in-One assistant tool for Claude Code, Codex, OpenCode & Gemini CLI.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/farion1231/cc-switch">farion1231/cc-switch</a></h1>
            <p>A cross-platform desktop All-in-One assistant tool for Claude Code, Codex, OpenCode & Gemini CLI.</p>
            <p>Language: Rust</p>
            <p>Stars: 21,261</p>
            <p>Forks: 1,312</p>
            <p>Stars today: 706 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# All-in-One Assistant for Claude Code, Codex &amp; Gemini CLI

[![Version](https://img.shields.io/badge/version-3.10.2-blue.svg)](https://github.com/farion1231/cc-switch/releases)
[![Platform](https://img.shields.io/badge/platform-Windows%20%7C%20macOS%20%7C%20Linux-lightgrey.svg)](https://github.com/farion1231/cc-switch/releases)
[![Built with Tauri](https://img.shields.io/badge/built%20with-Tauri%202-orange.svg)](https://tauri.app/)
[![Downloads](https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/downloads/farion1231/cc-switch/total)](https://github.com/farion1231/cc-switch/releases/latest)

&lt;a href=&quot;https://trendshift.io/repositories/15372&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/15372&quot; alt=&quot;farion1231%2Fcc-switch | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

English | [‰∏≠Êñá](README_ZH.md) | [Êó•Êú¨Ë™û](README_JA.md) | [Changelog](CHANGELOG.md)

&lt;/div&gt;

## ‚ù§Ô∏èSponsor

[![MiniMax](assets/partners/banners/minimax-en.jpeg)](https://platform.minimax.io/subscribe/coding-plan?code=ClLhgxr2je&amp;source=link)

MiniMax-M2.5 is a SOTA large language model designed for real-world productivity. Trained in a diverse range of complex real-world digital working environments, M2.5 builds upon the coding expertise of M2.1 to extend into general office work, reaching fluency in generating and operating Word, Excel, and Powerpoint files, context switching between diverse software environments, and working across different agent and human teams. Scoring 80.2% on SWE-Bench Verified, 51.3% on Multi-SWE-Bench, and 76.3% on BrowseComp, M2.5 is also more token efficient than previous generations, having been trained to optimize its actions and output through planning.

[Click](https://platform.minimax.io/subscribe/coding-plan?code=ClLhgxr2je&amp;source=link) to get an exclusive 12% off the MiniMax Coding Plan!

---

&lt;table&gt;
&lt;tr&gt;
&lt;td width=&quot;180&quot;&gt;&lt;a href=&quot;https://www.packyapi.com/register?aff=cc-switch&quot;&gt;&lt;img src=&quot;assets/partners/logos/packycode.png&quot; alt=&quot;PackyCode&quot; width=&quot;150&quot;&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Thanks to PackyCode for sponsoring this project! PackyCode is a reliable and efficient API relay service provider, offering relay services for Claude Code, Codex, Gemini, and more. PackyCode provides special discounts for our software users: register using &lt;a href=&quot;https://www.packyapi.com/register?aff=cc-switch&quot;&gt;this link&lt;/a&gt; and enter the &quot;cc-switch&quot; promo code during first recharge to get 10% off.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td width=&quot;180&quot;&gt;&lt;a href=&quot;https://aigocode.com/invite/CC-SWITCH&quot;&gt;&lt;img src=&quot;assets/partners/logos/aigocode.png&quot; alt=&quot;AIGoCode&quot; width=&quot;150&quot;&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Thanks to AIGoCode for sponsoring this project! AIGoCode is an all-in-one platform that integrates Claude Code, Codex, and the latest Gemini models, providing you with stable, efficient, and highly cost-effective AI coding services. The platform offers flexible subscription plans, zero risk of account suspension, direct access with no VPN required, and lightning-fast responses. AIGoCode has prepared a special benefit for CC Switch users: if you register via &lt;a href=&quot;https://aigocode.com/invite/CC-SWITCH&quot;&gt;this link&lt;/a&gt;, you&#039;ll receive an extra 10% bonus credit on your first top-up!&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td width=&quot;180&quot;&gt;&lt;a href=&quot;https://www.aicodemirror.com/register?invitecode=9915W3&quot;&gt;&lt;img src=&quot;assets/partners/logos/aicodemirror.jpg&quot; alt=&quot;AICodeMirror&quot; width=&quot;150&quot;&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Thanks to AICodeMirror for sponsoring this project! AICodeMirror provides official high-stability relay services for Claude Code / Codex / Gemini CLI, with enterprise-grade concurrency, fast invoicing, and 24/7 dedicated technical support.
Claude Code / Codex / Gemini official channels at 38% / 2% / 9% of original price, with extra discounts on top-ups! AICodeMirror offers special benefits for CC Switch users: register via &lt;a href=&quot;https://www.aicodemirror.com/register?invitecode=9915W3&quot;&gt;this link&lt;/a&gt; to enjoy 20% off your first top-up, and enterprise customers can get up to 25% off!&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td width=&quot;180&quot;&gt;&lt;a href=&quot;https://cubence.com/signup?code=CCSWITCH&amp;source=ccs&quot;&gt;&lt;img src=&quot;assets/partners/logos/cubence.png&quot; alt=&quot;Cubence&quot; width=&quot;150&quot;&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Thanks to Cubence for sponsoring this project! Cubence is a reliable and efficient API relay service provider, offering relay services for Claude Code, Codex, Gemini, and more with flexible billing options including pay-as-you-go and monthly plans. Cubence provides special discounts for CC Switch users: register using &lt;a href=&quot;https://cubence.com/signup?code=CCSWITCH&amp;source=ccs&quot;&gt;this link&lt;/a&gt; and enter the &quot;CCSWITCH&quot; promo code during recharge to get 10% off every top-up!&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td width=&quot;180&quot;&gt;&lt;a href=&quot;https://www.dmxapi.cn/register?aff=bUHu&quot;&gt;&lt;img src=&quot;assets/partners/logos/dmx-en.jpg&quot; alt=&quot;DMXAPI&quot; width=&quot;150&quot;&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Thanks to DMXAPI for sponsoring this project! DMXAPI provides global large model API services to 200+ enterprise users. One API key for all global models. Features include: instant invoicing, unlimited concurrency, starting from $0.15, 24/7 technical support. GPT/Claude/Gemini all at 32% off, domestic models 20-50% off, Claude Code exclusive models at 66% off! &lt;a href=&quot;https://www.dmxapi.cn/register?aff=bUHu&quot;&gt;Register here&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td width=&quot;180&quot;&gt;&lt;a href=&quot;https://www.right.codes/register?aff=CCSWITCH&quot;&gt;&lt;img src=&quot;assets/partners/logos/rightcode.jpg&quot; alt=&quot;RightCode&quot; width=&quot;150&quot;&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Thank you to Right Code for sponsoring this project! Right Code reliably provides routing services for models such as Claude Code, Codex, and Gemini. It features a highly cost-effective Codex monthly subscription plan and &lt;strong&gt;supports quota rollovers‚Äîunused quota from one day can be carried over and used the next day.&lt;/strong&gt; Invoices are available upon top-up. Enterprise and team users can receive dedicated one-on-one support. Right Code also offers an exclusive discount for CC Switch users: register via &lt;a href=&quot;https://www.right.codes/register?aff=CCSWITCH&quot;&gt;this link&lt;/a&gt;, and with every top-up you will receive pay-as-you-go credit equivalent to 25% of the amount paid.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td width=&quot;180&quot;&gt;&lt;a href=&quot;https://aicoding.sh/i/CCSWITCH&quot;&gt;&lt;img src=&quot;assets/partners/logos/aicoding.jpg&quot; alt=&quot;AICoding&quot; width=&quot;150&quot;&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Thanks to AICoding.sh for sponsoring this project! AICoding.sh ‚Äî Global AI Model API Relay Service at Unbeatable Prices! Claude Code at 19% of original price, GPT at just 1%! Trusted by hundreds of enterprises for cost-effective AI services. Supports Claude Code, GPT, Gemini and major domestic models, with enterprise-grade high concurrency, fast invoicing, and 24/7 dedicated technical support. CC Switch users who register via &lt;a href=&quot;https://aicoding.sh/i/CCSWITCH&quot;&gt;this link&lt;/a&gt; get 10% off their first top-up!&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td width=&quot;180&quot;&gt;&lt;a href=&quot;https://crazyrouter.com/register?aff=OZcm&amp;ref=cc-switch&quot;&gt;&lt;img src=&quot;assets/partners/logos/crazyrouter.jpg&quot; alt=&quot;AICoding&quot; width=&quot;150&quot;&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Thanks to Crazyrouter for sponsoring this project! Crazyrouter is a high-performance AI API aggregation platform ‚Äî one API key for 300+ models including Claude Code, Codex, Gemini CLI, and more. All models at 55% of official pricing with auto-failover, smart routing, and unlimited concurrency. Crazyrouter offers an exclusive deal for CC Switch users: register via &lt;a href=&quot;https://crazyrouter.com/register?aff=OZcm&amp;ref=cc-switch&quot;&gt;this link&lt;/a&gt;  to get &lt;strong&gt;$2 free credit&lt;/strong&gt; instantly, plus enter promo code `CCSWITCH` on your first top-up for an extra &lt;strong&gt;30% bonus credit&lt;/strong&gt;! &lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td width=&quot;180&quot;&gt;&lt;a href=&quot;https://www.sssaicode.com/register?ref=DCP0SM&quot;&gt;&lt;img src=&quot;assets/partners/logos/sssaicode.png&quot; alt=&quot;SSSAiCode&quot; width=&quot;150&quot;&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Thanks to SSSAiCode for sponsoring this project! SSSAiCode is a stable and reliable API relay service, dedicated to providing stable, reliable, and affordable Claude and Codex model services, &lt;strong&gt;offering high cost-effective official Claude service at just ¬•0.5/$ equivalent&lt;/strong&gt;, supporting monthly and pay-as-you-go billing plans with same-day fast invoicing. SSSAiCode offers a special deal for CC Switch users: register via &lt;a href=&quot;https://www.sssaicode.com/register?ref=DCP0SM&quot;&gt;this link&lt;/a&gt; to enjoy $10 extra credit on every top-up!&lt;/td&gt;
&lt;/tr&gt;

&lt;/table&gt;

## Screenshots

|                  Main Interface                   |                  Add Provider                  |
| :-----------------------------------------------: | :--------------------------------------------: |
| ![Main Interface](assets/screenshots/main-en.png) | ![Add Provider](assets/screenshots/add-en.png) |

## Features

### Current Version: v3.10.2 | [Full Changelog](CHANGELOG.md) | [Release Notes](docs/release-note-v3.9.0-en.md)

**v3.8.0 Major Update (2025-11-28)**

**Persistence Architecture Upgrade &amp; Brand New UI**

- **SQLite + JSON Dual-layer Architecture**
  - Migrated from JSON file storage to SQLite + JSON dual-layer structure
  - Syncable data (providers, MCP, Prompts, Skills) stored in SQLite
  - Device-level data (window state, local paths) stored in JSON
  - Lays the foundation for future cloud sync functionality
  - Schema version management for database migrations

- **Brand New User Interface**
  - Completely redesigned interface layout
  - Unified component styles and smoother animations
  - Optimized visual hierarchy
  - Tailwind CSS downgraded from v4 to v3.4 for better browser compatibility

- **Japanese Language Support**
  - Added Japanese interface support (now supports Chinese/English/Japanese)

- **Auto Launch on Startup**
  - One-click enable/disable in settings
  - Platform-native APIs (Registry/LaunchAgent/XDG autostart)

- **Skills Recursive Scanning**
  - Support for multi-level directory structures
  - Allow same-named skills from different repositories

- **Critical Bug Fixes**
  - Fixed custom endpoints lost when updating providers
  - Fixed Gemini configuration write issues
  - Fixed Linux WebKitGTK rendering issues

**v3.7.0 Highlights**

**Six Core Features, 18,000+ Lines of New Code**

- **Gemini CLI Integration**
  - Third supported AI CLI (Claude Code / Codex / Gemini)
  - Dual-file configuration support (`.env` + `settings.json`)
  - Complete MCP server management
  - Presets: Google Official (OAuth) / PackyCode / Custom

- **Claude Skills Management System**
  - Auto-scan skills from GitHub repositories (3 pre-configured curated repos)
  - One-click install/uninstall to `~/.claude/skills/`
  - Custom repository support + subdirectory scanning
  - Complete lifecycle management (discover/install/update)

- **Prompts Management System**
  - Multi-preset system prompt management (unlimited presets, quick switching)
  - Cross-app support (Claude: `CLAUDE.md` / Codex: `AGENTS.md` / Gemini: `GEMINI.md`)
  - Markdown editor (CodeMirror 6 + real-time preview)
  - Smart backfill protection, preserves manual modifications

- **MCP v3.7.0 Unified Architecture**
  - Single panel manages MCP servers across three applications
  - New SSE (Server-Sent Events) transport type
  - Smart JSON parser + Codex TOML format auto-correction
  - Unified import/export + bidirectional sync

- **Deep Link Protocol**
  - `ccswitch://` protocol registration (all platforms)
  - One-click import provider configs via shared links
  - Security validation + lifecycle integration

- **Environment Variable Conflict Detection**
  - Auto-detect cross-app configuration conflicts (Claude/Codex/Gemini/MCP)
  - Visual conflict indicators + resolution suggestions
  - Override warnings + backup before changes

**Core Capabilities**

- **Provider Management**: One-click switching between Claude Code, Codex, and Gemini API configurations
- **AWS Bedrock Support**: Built-in AWS Bedrock provider presets with AKSK and API Key authentication, cross-region inference support (global/us/eu/apac), covering Claude Code and OpenCode
- **Speed Testing**: Measure API endpoint latency with visual quality indicators
- **Import/Export**: Backup and restore configs with auto-rotation (keep 10 most recent)
- **i18n Support**: Complete Chinese/English localization (UI, errors, tray)
- **Claude Plugin Sync**: One-click apply/restore Claude plugin configurations

**v3.6 Highlights**

- Provider duplication &amp; drag-and-drop sorting
- Multi-endpoint management &amp; custom config directory (cloud sync ready)
- Granular model configuration (4-tier: Haiku/Sonnet/Opus/Custom)
- WSL environment support with auto-sync on directory change
- 100% hooks test coverage &amp; complete architecture refactoring

**System Features**

- System tray with quick switching
- Single instance daemon
- Built-in auto-updater
- Atomic writes with rollback protection

## Download &amp; Installation

### System Requirements

- **Windows**: Windows 10 and above
- **macOS**: macOS 10.15 (Catalina) and above
- **Linux**: Ubuntu 22.04+ / Debian 11+ / Fedora 34+ and other mainstream distributions

### Windows Users

Download the latest `CC-Switch-v{version}-Windows.msi` installer or `CC-Switch-v{version}-Windows-Portable.zip` portable version from the [Releases](../../releases) page.

### macOS Users

**Method 1: Install via Homebrew (Recommended)**

```bash
brew tap farion1231/ccswitch
brew install --cask cc-switch
```

Update:

```bash
brew upgrade --cask cc-switch
```

**Method 2: Manual Download**

Download `CC-Switch-v{version}-macOS.zip` from the [Releases](../../releases) page and extract to use.

&gt; **Note**: Since the author doesn&#039;t have an Apple Developer account, you may see an &quot;unidentified developer&quot; warning on first launch. Please close it first, then go to &quot;System Settings&quot; ‚Üí &quot;Privacy &amp; Security&quot; ‚Üí click &quot;Open Anyway&quot;, and you&#039;ll be able to open it normally afterwards.

### Arch Linux Users

**Install via paru (Recommended)**

```bash
paru -S cc-switch-bin
```

### Linux Users

Download the latest Linux build from the [Releases](../../releases) page:

- `CC-Switch-v{version}-Linux.deb` (Debian/Ubuntu)
- `CC-Switch-v{version}-Linux.rpm` (Fedora/RHEL/openSUSE)
- `CC-Switch-v{version}-Linux.AppImage` (Universal)
- `CC-Switch-v{version}-Linux.flatpak` (Flatpak)

Flatpak install &amp; run:

```bash
flatpak install --user ./CC-Switch-v{version}-Linux.flatpak
flatpak run com.ccswitch.desktop
```

## Quick Start

### Basic Usage

1. **Add Provider**: Click &quot;Add Provider&quot; ‚Üí Choose preset or create custom configuration
2. **Switch Provider**:
   - Main UI: Select provider ‚Üí Click &quot;Enable&quot;
   - System Tray: Click provider name directly (instant effect)
3. **Takes Effect**: Restart your terminal or Claude Code / Codex / Gemini clients to apply changes
4. **Back to Official**: Select the &quot;Official Login&quot; preset (Claude/Codex) or &quot;Google Official&quot; preset (Gemini), restart the corresponding client, then follow its login/OAuth flow

### MCP Management

- **Location**: Click &quot;MCP&quot; button in top-right corner
- **Add Server**:
  - Use built-in templates (mcp-fetch, mcp-filesystem, etc.)
  - Support stdio / http / sse transport types
  - Configure independent MCP servers for different apps
- **Enable/Disable**: Toggle switches to control which servers sync to live config
- **Sync**: Enabled servers auto-sync to each app&#039;s live files
- **Import/Export**: Import existing MCP servers from Claude/Codex/Gemini config files

### Skills Management (v3.7.0 New)

- **Location**: Click &quot;Skills&quot; button in top-right corner
- **Discover Skills**:
  - Auto-scan pre-configured GitHub repositories (Anthropic official, ComposioHQ, community, etc.)
  - Add custom repositories (supports subdirectory scanning)
- **Install Skills**: Click &quot;Install&quot; to one-click install to `~/.claude/skills/`
- **Uninstall Skills**: Click &quot;Uninstall&quot; to safely remove and clean up state
- **Manage Repositories**: Add/remove custom GitHub repositories

### Prompts Management (v3.7.0 New)

- **Location**: Click &quot;Prompts&quot; button in top-right corner
- **Create Presets**:
  - Create unlimited system prompt presets
  - Use Markdown editor to write prompts (syntax highlighting + real-time preview)
- **Switch Presets**: Select preset ‚Üí Click &quot;Activate&quot; to apply immediately
- **Sync Mechanism**:
  - Claude: `~/.claude/CLAUDE.md`
  - Codex: `~/.codex/AGENTS.md`
  - Gemini: `~/.gemini/GEMINI.md`
- **Protection Mechanism**: Auto-save current prompt content before switching, preserves manual modifications

### Configuration Files

**Claude Code**

- Live config: `~/.claude/settings.json` (or `claude.json`)
- API key field: `env.ANTHROPIC_AUTH_TOKEN` or `env.ANTHROPIC_API_KEY`
- MCP servers: `~/.claude.json` ‚Üí `mcpServers`

**Codex**

- Live config: `~/.codex/auth.json` (required) + `config.toml` (optional)
- API key field: `OPENAI_API_KEY` in `auth.json`
- MCP servers: `~/.codex/config.toml` ‚Üí `[mcp_servers]` tables

**Gemini**

- Live config: `~/.gemini/.env` (API key) + `~/.gemini/settings.json` (auth mode)
- API key field: `GEMINI_API_KEY` or `GOOGLE_GEMINI_API_KEY` in `.env`
- Environment variables: Support `GOOGLE_GEMINI_BASE_URL`, `GEMINI_MODEL`, etc.
- MCP servers: `~/.gemini/settings.json` ‚Üí `mcpServers`
- Tray quick switch: Each provider switch rewrites `~/.gemini/.env`, no need to restart Gemini CLI

**CC Switch Storage (v3.8.0 New Architecture)**

- Database (SSOT): `~/.cc-switch/cc-switch.db` (SQLite, stores providers, MCP, Prompts, Skills)
- Local settings: `~/.cc-switch/settings.json` (device-level settings)
- Backups: `~/.cc-switch/backups/` (auto-rotate, keep 10)

### Cloud Sync Setup

1. Go to Settings ‚Üí &quot;Custom Configuration Directory&quot;
2. Choose your cloud sync folder (Dropbox, OneDrive, iCloud, etc.)
3. Restart app to apply
4. Repeat on other devices to enable cross-device sync

&gt; **Note**: First launch auto-imports existing Claude/Codex configs as default provider.

## Architecture Overview

### Design Principles

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Frontend (React + TS)                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ Components  ‚îÇ  ‚îÇ    Hooks     ‚îÇ  ‚îÇ  TanStack Query  ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ   (UI)      ‚îÇ‚îÄ‚îÄ‚îÇ (Bus. Logic) ‚îÇ‚îÄ‚îÄ‚îÇ   (Cache/Sync)   ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ Tauri IPC
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  Backend (Tauri + Rust)                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  Commands   ‚îÇ  ‚îÇ   Services   ‚îÇ  ‚îÇ  Models/Config   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ (API Layer) ‚îÇ‚îÄ‚îÄ‚îÇ (Bus. Layer) ‚îÇ‚îÄ‚îÄ‚îÇ     (Data)       ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Core Design Patterns**

- **SSOT** (Single Source of Truth): All data stored in `~/.cc-switch/cc-switch.db` (SQLite)
- **Dual-layer Storage**: SQLite for syncable data, JSON for device-level settings
- **Dual-way Sync**: Write to live files on switch, backfill from live when editing active provider
- **Atomic Writes**: Temp file + rename pattern prevents config corruption
- **Concurrency Safe**: Mutex-protected database connection avoids race conditions
- **Layered Architecture**: Clear separation (Commands ‚Üí Services ‚Üí DAO ‚Üí Database)

**Key Components**

- **ProviderService**: Provider CRUD, switching, backfill, sorting
- **McpService**: MCP server management, import/export, live file sync
- **ConfigService**: Config import/export, backup rotation
- **SpeedtestService**: API endpoint latency measurement

**v3.6 Refactoring**

- Backend: 5-phase refactoring (error handling ‚Üí command split ‚Üí tests ‚Üí services ‚Üí concurrency)
- Frontend: 4-stage refactoring (test infra ‚Üí hooks ‚Üí components ‚Üí cleanup)
- Testing: 100% hooks coverage + integration tests (vitest + MSW)

## Development

### Environment Requirements

- Node.js 18+
- pnpm 8+
- Rust 1.85+
- Tauri CLI 2.8+

### D

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[bytecodealliance/wasmtime]]></title>
            <link>https://github.com/bytecodealliance/wasmtime</link>
            <guid>https://github.com/bytecodealliance/wasmtime</guid>
            <pubDate>Sat, 28 Feb 2026 00:06:02 GMT</pubDate>
            <description><![CDATA[A lightweight WebAssembly runtime that is fast, secure, and standards-compliant]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/bytecodealliance/wasmtime">bytecodealliance/wasmtime</a></h1>
            <p>A lightweight WebAssembly runtime that is fast, secure, and standards-compliant</p>
            <p>Language: Rust</p>
            <p>Stars: 17,653</p>
            <p>Forks: 1,630</p>
            <p>Stars today: 8 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;h1&gt;&lt;code&gt;wasmtime&lt;/code&gt;&lt;/h1&gt;

  &lt;p&gt;
    &lt;strong&gt;A standalone runtime for
    &lt;a href=&quot;https://webassembly.org/&quot;&gt;WebAssembly&lt;/a&gt;&lt;/strong&gt;
  &lt;/p&gt;

  &lt;strong&gt;A &lt;a href=&quot;https://bytecodealliance.org/&quot;&gt;Bytecode Alliance&lt;/a&gt; project&lt;/strong&gt;

  &lt;p&gt;
    &lt;a href=&quot;https://github.com/bytecodealliance/wasmtime/actions?query=workflow%3ACI&quot;&gt;&lt;img src=&quot;https://github.com/bytecodealliance/wasmtime/workflows/CI/badge.svg&quot; alt=&quot;build status&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;https://bytecodealliance.zulipchat.com/#narrow/stream/217126-wasmtime&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/zulip-join_chat-brightgreen.svg&quot; alt=&quot;zulip chat&quot; /&gt;&lt;/a&gt;
    &lt;img src=&quot;https://img.shields.io/badge/rustc-stable+-green.svg&quot; alt=&quot;supported rustc stable&quot; /&gt;
    &lt;a href=&quot;https://docs.rs/wasmtime&quot;&gt;&lt;img src=&quot;https://docs.rs/wasmtime/badge.svg&quot; alt=&quot;Documentation Status&quot; /&gt;&lt;/a&gt;
  &lt;/p&gt;

  &lt;h3&gt;
    &lt;a href=&quot;https://bytecodealliance.github.io/wasmtime/&quot;&gt;Guide&lt;/a&gt;
    &lt;span&gt; | &lt;/span&gt;
    &lt;a href=&quot;https://bytecodealliance.github.io/wasmtime/contributing.html&quot;&gt;Contributing&lt;/a&gt;
    &lt;span&gt; | &lt;/span&gt;
    &lt;a href=&quot;https://wasmtime.dev/&quot;&gt;Website&lt;/a&gt;
    &lt;span&gt; | &lt;/span&gt;
    &lt;a href=&quot;https://bytecodealliance.zulipchat.com/#narrow/stream/217126-wasmtime&quot;&gt;Chat&lt;/a&gt;
  &lt;/h3&gt;
&lt;/div&gt;

## Installation

The Wasmtime CLI can be installed on Linux and macOS (locally) with a small install
script:

```console
curl https://wasmtime.dev/install.sh -sSf | bash
```
This script installs into `$WASMTIME_HOME` (defaults to `$HOME/.wasmtime`), and executable is placed in `$WASMTIME_HOME/bin`.

After running the install script above, follow the on-screen instructions.

Windows or otherwise interested users can download installers and
binaries directly from the [GitHub
Releases](https://github.com/bytecodealliance/wasmtime/releases) page.

For additional installation options, refer to the [online book CLI installation page](https://docs.wasmtime.dev/cli-install.html).

Documentation on Wasmtime&#039;s currently supported versions can be found [in the
online book
documentation](https://docs.wasmtime.dev/stability-release.html#current-versions).

## Example

If you&#039;ve got the [Rust compiler
installed](https://www.rust-lang.org/tools/install) then you can take some Rust
source code:

```rust
fn main() {
    println!(&quot;Hello, world!&quot;);
}
```

and compile it into a WebAssembly component with:

```console
rustup target add wasm32-wasip2
rustc hello.rs --target wasm32-wasip2
```

Once compiled, you can run your component:

```console
wasmtime hello.wasm
```

You should see the following output:

```text
Hello, world!
```

(Note: make sure you installed Rust using the [`rustup`][rustup] method in the official
instructions above, and do not have a copy of the Rust toolchain installed on
your system in some other way as well (e.g. the system package manager). Otherwise, the `rustup target add...`
command may not install the target for the correct copy of Rust.)

[rustup]: https://rustup.rs

## Features

* **Fast**. Wasmtime is built on the optimizing [Cranelift] code generator to
  quickly generate high-quality machine code either at runtime or
  ahead-of-time. Wasmtime is optimized for efficient instantiation, low-overhead
  calls between the embedder and wasm, and scalability of concurrent instances.

* **[Secure]**. Wasmtime&#039;s development is strongly focused on correctness and
  security. Building on top of Rust&#039;s runtime safety guarantees, each Wasmtime
  feature goes through careful review and consideration via an [RFC
  process]. Once features are designed and implemented, they undergo 24/7
  fuzzing donated by [Google&#039;s OSS Fuzz]. As features stabilize they become part
  of a [release][release policy], and when things go wrong we have a
  well-defined [security policy] in place to quickly mitigate and patch any
  issues. We follow best practices for defense-in-depth and integrate
  protections and mitigations for issues like Spectre. Finally, we&#039;re working to
  push the state-of-the-art by collaborating with academic researchers to
  formally verify critical parts of Wasmtime and Cranelift.

* **[Configurable]**. Wasmtime uses sensible defaults, but can also be
  configured to provide more fine-grained control over things like CPU and
  memory consumption. Whether you want to run Wasmtime in a tiny environment or
  on massive servers with many concurrent instances, we&#039;ve got you covered.

* **[WASI]**. Wasmtime supports a rich set of APIs for interacting with the host
  environment through the [WASI standard](https://wasi.dev).

* **[Standards Compliant]**. Wasmtime passes the [official WebAssembly test
  suite](https://github.com/WebAssembly/testsuite), implements the [official C
  API of wasm](https://github.com/WebAssembly/wasm-c-api), and implements
  [future proposals to WebAssembly](https://github.com/WebAssembly/proposals) as
  well. Wasmtime developers are intimately engaged with the WebAssembly
  standards process all along the way too.

[Wasmtime]: https://github.com/bytecodealliance/wasmtime
[Cranelift]: https://cranelift.dev/
[Google&#039;s OSS Fuzz]: https://google.github.io/oss-fuzz/
[security policy]: https://bytecodealliance.org/security
[RFC process]: https://github.com/bytecodealliance/rfcs
[release policy]: https://docs.wasmtime.dev/stability-release.html
[Secure]: https://docs.wasmtime.dev/security.html
[Configurable]: https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html
[WASI]: https://docs.rs/wasmtime-wasi/latest/wasmtime_wasi/
[Standards Compliant]: https://docs.wasmtime.dev/stability-tiers.html

## Language Support

You can use Wasmtime from a variety of different languages through embeddings of
the implementation.

Languages supported by the Bytecode Alliance:

* **[Rust]** - the [`wasmtime` crate]
* **[C]** - the [`wasm.h`, `wasi.h`, and `wasmtime.h` headers][c-headers], [CMake](crates/c-api/CMakeLists.txt)
* **C++** - the [`wasmtime.hh` header][c-headers]
* **[Python]** - the [`wasmtime` PyPI package]
* **[.NET]** - the [`Wasmtime` NuGet package]
* **[Go]** - the [`wasmtime-go` repository]
* **[Ruby]** - the [`wasmtime` gem]

Languages supported by the community:

* **[Elixir]** - the [`wasmex` hex package]
* **Perl** - the [`Wasm` Perl package&#039;s `Wasm::Wasmtime`]

[Rust]: https://bytecodealliance.github.io/wasmtime/lang-rust.html
[C]: https://bytecodealliance.github.io/wasmtime/lang-c.html
[`wasmtime` crate]: https://crates.io/crates/wasmtime
[c-headers]: https://bytecodealliance.github.io/wasmtime/c-api/
[Python]: https://bytecodealliance.github.io/wasmtime/lang-python.html
[`wasmtime` PyPI package]: https://pypi.org/project/wasmtime/
[.NET]: https://bytecodealliance.github.io/wasmtime/lang-dotnet.html
[`Wasmtime` NuGet package]: https://www.nuget.org/packages/Wasmtime
[Go]: https://bytecodealliance.github.io/wasmtime/lang-go.html
[`wasmtime-go` repository]: https://pkg.go.dev/github.com/bytecodealliance/wasmtime-go
[Ruby]: https://bytecodealliance.github.io/wasmtime/lang-ruby.html
[`wasmtime` gem]: https://rubygems.org/gems/wasmtime
[Elixir]: https://docs.wasmtime.dev/lang-elixir.html
[`wasmex` hex package]: https://hex.pm/packages/wasmex
[`Wasm` Perl package&#039;s `Wasm::Wasmtime`]: https://metacpan.org/pod/Wasm::Wasmtime

## Documentation

[üìö Read the Wasmtime guide here! üìö][guide]

The [wasmtime guide][guide] is the best starting point to learn about what
Wasmtime can do for you or help answer your questions about Wasmtime. If you&#039;re
curious in contributing to Wasmtime, [it can also help you do
that][contributing]!

[contributing]: https://bytecodealliance.github.io/wasmtime/contributing.html
[guide]: https://bytecodealliance.github.io/wasmtime

---

It&#039;s Wasmtime.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[clockworklabs/SpacetimeDB]]></title>
            <link>https://github.com/clockworklabs/SpacetimeDB</link>
            <guid>https://github.com/clockworklabs/SpacetimeDB</guid>
            <pubDate>Sat, 28 Feb 2026 00:06:01 GMT</pubDate>
            <description><![CDATA[Development at the speed of light]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/clockworklabs/SpacetimeDB">clockworklabs/SpacetimeDB</a></h1>
            <p>Development at the speed of light</p>
            <p>Language: Rust</p>
            <p>Stars: 21,387</p>
            <p>Forks: 775</p>
            <p>Stars today: 570 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://spacetimedb.com#gh-dark-mode-only&quot; target=&quot;_blank&quot;&gt;
	&lt;img width=&quot;320&quot; src=&quot;./images/dark/logo.svg&quot; alt=&quot;SpacetimeDB Logo&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://spacetimedb.com#gh-light-mode-only&quot; target=&quot;_blank&quot;&gt;
	&lt;img width=&quot;320&quot; src=&quot;./images/light/logo.svg&quot; alt=&quot;SpacetimeDB Logo&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://spacetimedb.com#gh-dark-mode-only&quot; target=&quot;_blank&quot;&gt;
        &lt;img width=&quot;250&quot; src=&quot;./images/dark/logo-text.svg&quot; alt=&quot;SpacetimeDB&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://spacetimedb.com#gh-light-mode-only&quot; target=&quot;_blank&quot;&gt;
        &lt;img width=&quot;250&quot; src=&quot;./images/light/logo-text.svg&quot; alt=&quot;SpacetimeDB&quot;&gt;
    &lt;/a&gt;
    &lt;h3 align=&quot;center&quot;&gt;
        Development at the speed of light.
    &lt;/h3&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://github.com/clockworklabs/spacetimedb&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/clockworklabs/spacetimedb?color=%23ff00a0&amp;include_prereleases&amp;label=version&amp;sort=semver&amp;style=flat-square&quot;&gt;&lt;/a&gt;
    &amp;nbsp;
    &lt;a href=&quot;https://github.com/clockworklabs/spacetimedb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/built_with-Rust-dca282.svg?style=flat-square&quot;&gt;&lt;/a&gt;
    &amp;nbsp;
	&lt;a href=&quot;https://github.com/clockworklabs/spacetimedb/actions&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/clockworklabs/spacetimedb/ci.yml?style=flat-square&amp;branch=master&quot;&gt;&lt;/a&gt;
    &amp;nbsp;
    &lt;a href=&quot;https://status.spacetimedb.com&quot;&gt;&lt;img src=&quot;https://img.shields.io/uptimerobot/ratio/7/m784409192-e472ca350bb615372ededed7?label=cloud%20uptime&amp;style=flat-square&quot;&gt;&lt;/a&gt;
    &amp;nbsp;
    &lt;a href=&quot;https://hub.docker.com/r/clockworklabs/spacetimedb&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/clockworklabs/spacetimedb?style=flat-square&quot;&gt;&lt;/a&gt;
    &amp;nbsp;
    &lt;a href=&quot;https://github.com/clockworklabs/spacetimedb/blob/master/LICENSE.txt&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/license-BSL_1.1-00bfff.svg?style=flat-square&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://crates.io/crates/spacetimedb&quot;&gt;&lt;img src=&quot;https://img.shields.io/crates/d/spacetimedb?color=e45928&amp;label=Rust%20Crate&amp;style=flat-square&quot;&gt;&lt;/a&gt;
    &amp;nbsp;
    &lt;a href=&quot;https://www.nuget.org/packages/SpacetimeDB.Runtime&quot;&gt;&lt;img src=&quot;https://img.shields.io/nuget/dt/spacetimedb.runtime?color=0b6cff&amp;label=NuGet%20Package&amp;style=flat-square&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://discord.gg/spacetimedb&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/1037340874172014652?label=discord&amp;style=flat-square&amp;color=5a66f6&quot;&gt;&lt;/a&gt;
    &amp;nbsp;
    &lt;a href=&quot;https://twitter.com/spacetime_db&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/twitter-Follow_us-1d9bf0.svg?style=flat-square&quot;&gt;&lt;/a&gt;
    &amp;nbsp;
    &lt;a href=&quot;https://clockworklabs.io/join&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/careers-Join_us-86f7b7.svg?style=flat-square&quot;&gt;&lt;/a&gt;
    &amp;nbsp;
    &lt;a href=&quot;https://www.linkedin.com/company/clockworklabs/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/linkedin-Connect_with_us-0a66c2.svg?style=flat-square&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://discord.gg/spacetimedb&quot;&gt;&lt;img height=&quot;25&quot; src=&quot;./images/social/discord.svg&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt;
    &amp;nbsp;
    &lt;a href=&quot;https://twitter.com/spacetime_db&quot;&gt;&lt;img height=&quot;25&quot; src=&quot;./images/social/twitter.svg&quot; alt=&quot;Twitter&quot;&gt;&lt;/a&gt;
    &amp;nbsp;
    &lt;a href=&quot;https://github.com/clockworklabs/spacetimedb&quot;&gt;&lt;img height=&quot;25&quot; src=&quot;./images/social/github.svg&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt;
    &amp;nbsp;
    &lt;a href=&quot;https://twitch.tv/SpacetimeDB&quot;&gt;&lt;img height=&quot;25&quot; src=&quot;./images/social/twitch.svg&quot; alt=&quot;Twitch&quot;&gt;&lt;/a&gt;
    &amp;nbsp;
    &lt;a href=&quot;https://youtube.com/@SpacetimeDB&quot;&gt;&lt;img height=&quot;25&quot; src=&quot;./images/social/youtube.svg&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;
    &amp;nbsp;
    &lt;a href=&quot;https://www.linkedin.com/company/clockwork-labs/&quot;&gt;&lt;img height=&quot;25&quot; src=&quot;./images/social/linkedin.svg&quot; alt=&quot;LinkedIn&quot;&gt;&lt;/a&gt;
    &amp;nbsp;
    &lt;a href=&quot;https://stackoverflow.com/questions/tagged/spacetimedb&quot;&gt;&lt;img height=&quot;25&quot; src=&quot;./images/social/stackoverflow.svg&quot; alt=&quot;StackOverflow&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;br&gt;

## What is [SpacetimeDB](https://spacetimedb.com)?

You can think of SpacetimeDB as both a database and server combined into one.

It is a relational database system that lets you upload your application logic directly into the database by way of fancy stored procedures called &quot;modules.&quot;

Instead of deploying a web or game server that sits in between your clients and your database, your clients connect directly to the database and execute your application logic inside the database itself. You can write all of your permission and authorization logic right inside your module just as you would in a normal server.

This means that you can write your entire application in a single language, Rust, and deploy it as a single binary. No more microservices, no more containers, no more Kubernetes, no more Docker, no more VMs, no more DevOps, no more infrastructure, no more ops, no more servers.

&lt;figure&gt;
    &lt;img src=&quot;./images/basic-architecture-diagram.png&quot; alt=&quot;SpacetimeDB Architecture&quot; style=&quot;width:100%&quot;&gt;
    &lt;figcaption align=&quot;center&quot;&gt;
        &lt;p align=&quot;center&quot;&gt;&lt;b&gt;SpacetimeDB application architecture&lt;/b&gt;&lt;br /&gt;&lt;sup&gt;&lt;sub&gt;(elements in white are provided by SpacetimeDB)&lt;/sub&gt;&lt;/sup&gt;&lt;/p&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

It&#039;s actually similar to the idea of smart contracts, except that SpacetimeDB is a database, has nothing to do with blockchain, and is orders of magnitude faster than any smart contract system.

So fast, in fact, that the entire backend of our MMORPG [BitCraft Online](https://bitcraftonline.com) is just a SpacetimeDB module. We don&#039;t have any other servers or services running, which means that everything in the game, all of the chat messages, items, resources, terrain, and even the locations of the players are stored and processed by the database before being synchronized out to all of the clients in real-time.

SpacetimeDB is optimized for maximum speed and minimum latency rather than batch processing or OLAP workloads. It is designed to be used for real-time applications like games, chat, and collaboration tools.

This speed and latency is achieved by holding all of application state in memory, while persisting the data in a write-ahead-log (WAL) which is used to recover application state.

## Installation

You can run SpacetimeDB as a standalone database server via the `spacetime` CLI tool.
Install instructions for supported platforms are outlined below.
The same install instructions can be found on our website at https://spacetimedb.com/install.

#### Install on macOS

Installing on macOS is as simple as running our install script. After that you can use the spacetime command to manage versions.

```bash
curl -sSf https://install.spacetimedb.com | sh
```

#### Install on Linux

Installing on Linux is as simple as running our install script. After that you can use the spacetime command to manage versions.

```bash
curl -sSf https://install.spacetimedb.com | sh
```

#### Install on Windows

Installing on Windows is as simple as pasting the snippet below into PowerShell. If you would like to use WSL instead, please follow the Linux install instructions.

```ps1
iwr https://windows.spacetimedb.com -useb | iex
```

#### Installing from Source

A quick note on installing from source: we recommend that you don&#039;t install from source unless there is a feature that is available in `master` that hasn&#039;t been released yet, otherwise follow the official installation instructions.

##### MacOS + Linux

Installing on macOS + Linux is pretty straightforward. First we are going to build all of the binaries that we need:

```bash
# Install rustup, you can skip this step if you have cargo and the wasm32-unknown-unknown target already installed.
curl https://sh.rustup.rs -sSf | sh
# Clone SpacetimeDB
git clone https://github.com/clockworklabs/SpacetimeDB
# Build and install the CLI
cd SpacetimeDB
cargo build --locked --release -p spacetimedb-standalone -p spacetimedb-update -p spacetimedb-cli

# Create directories
mkdir -p ~/.local/bin
export STDB_VERSION=&quot;$(./target/release/spacetimedb-cli --version | sed -n &#039;s/.*spacetimedb tool version \([0-9.]*\);.*/\1/p&#039;)&quot;
mkdir -p ~/.local/share/spacetime/bin/$STDB_VERSION

# Install the update binary
cp target/release/spacetimedb-update ~/.local/bin/spacetime
cp target/release/spacetimedb-cli ~/.local/share/spacetime/bin/$STDB_VERSION
cp target/release/spacetimedb-standalone ~/.local/share/spacetime/bin/$STDB_VERSION
```

At this stage you&#039;ll need to add ~/.local/bin to your path if you haven&#039;t already.

```
# Please add the following line to your shell configuration and open a new shell session:
export PATH=&quot;$HOME/.local/bin:$PATH&quot;

```

Then finally set your SpacetimeDB version:
```

# Then, in a new shell, set the current version:
spacetime version use $STDB_VERSION

# If STDB_VERSION is not set anymore then you can use the following command to list your versions:
spacetime version list
```

You can verify that the correct version has been installed via `spacetime --version`.

##### Windows

Building on windows is a bit more complicated. You&#039;ll need a slightly different version of perl compared to what comes pre-bundled in most Windows terminals. We recommend [Strawberry Perl](https://strawberryperl.com/). You may also need access to an `openssl` binary which actually comes pre-installed with [Git for Windows](https://git-scm.com/downloads/win). Also, you&#039;ll need to install [rustup](https://rustup.rs/) for Windows.

In a Git for Windows shell you should have something that looks like this:
```
$ which perl
/c/Strawberry/perl/bin/perl
$ which openssl
/mingw64/bin/openssl
$ which cargo 
/c/Users/&lt;user&gt;/.cargo/bin/cargo
```

If that looks correct then you&#039;re ready to proceed!

```powershell
# Clone SpacetimeDB
git clone https://github.com/clockworklabs/SpacetimeDB

# Build and install the CLI
cd SpacetimeDB
cargo build --locked --release -p spacetimedb-standalone -p spacetimedb-update -p spacetimedb-cli

# Create directories
$stdbDir = &quot;$HOME\AppData\Local\SpacetimeDB&quot;
$stdbVersion = &amp; &quot;.\target\release\spacetimedb-cli&quot; --version | Select-String -Pattern &#039;spacetimedb tool version ([0-9.]+);&#039; | ForEach-Object { $_.Matches.Groups[1].Value }
New-Item -ItemType Directory -Path &quot;$stdbDir\bin\$stdbVersion&quot; -Force | Out-Null

# Install the update binary
Copy-Item &quot;target\release\spacetimedb-update.exe&quot; &quot;$stdbDir\spacetime.exe&quot;
Copy-Item &quot;target\release\spacetimedb-cli.exe&quot; &quot;$stdbDir\bin\$stdbVersion\&quot;
Copy-Item &quot;target\release\spacetimedb-standalone.exe&quot; &quot;$stdbDir\bin\$stdbVersion\&quot;

```

Now add the directory we just created to your path. We recommend adding it to the system path because then it will be available to all of your applications (including Unity3D). After you do this, restart your shell!

```
%USERPROFILE%\AppData\Local\SpacetimeDB
```

Then finally, open a new shell and use the installed SpacetimeDB version:
```
spacetime version use $stdbVersion

# If stdbVersion is no longer set, list versions using the following command:
spacetime version list
```

You can verify that the correct version has been installed via `spacetime --version`.

If you&#039;re using Git for Windows you can follow these instructions instead:

```bash
# Clone SpacetimeDB
git clone https://github.com/clockworklabs/SpacetimeDB
# Build and install the CLI
cd SpacetimeDB
# Build the CLI binaries - this takes a while on windows so go grab a coffee :)
cargo build --locked --release -p spacetimedb-standalone -p spacetimedb-update -p spacetimedb-cli

# Create directories
export STDB_VERSION=&quot;$(./target/release/spacetimedb-cli --version | sed -n &#039;s/.*spacetimedb tool version \([0-9.]*\);.*/\1/p&#039;)&quot;
mkdir -p ~/AppData/Local/SpacetimeDB/bin/$STDB_VERSION

# Install the update binary
cp target/release/spacetimedb-update ~/AppData/Local/SpacetimeDB/spacetime
cp target/release/spacetimedb-cli ~/AppData/Local/SpacetimeDB/bin/$STDB_VERSION
cp target/release/spacetimedb-standalone ~/AppData/Local/SpacetimeDB/bin/$STDB_VERSION

# Now add the directory we just created to your path. We recommend adding it to the system path because then it will be available to all of your applications (including Unity3D). After you do this, restart your shell!
# %USERPROFILE%\AppData\Local\SpacetimeDB

# Set the current version
spacetime version use $STDB_VERSION
```

You can verify that the correct version has been installed via `spacetime --version`.

#### Running with Docker

If you prefer to run Spacetime in a container, you can use the following command to start a new instance.

```bash
docker run --rm --pull always -p 3000:3000 clockworklabs/spacetime start
```

## Documentation

For more information about SpacetimeDB, getting started guides, game development guides, and reference material please see our [documentation](https://spacetimedb.com/docs).

## Getting Started

We&#039;ve prepared several getting started guides in each of our supported languages to help you get up and running with SpacetimeDB as quickly as possible. You can find them on our [docs page](https://spacetimedb.com/docs).

In summary there are only 4 steps to getting started with SpacetimeDB.

1. Install the `spacetime` CLI tool.
2. Start a SpacetimeDB standalone node with `spacetime start`.
3. Write and upload a module in one of our supported module languages.
4. Connect to the database with one of our client libraries.

You can see a summary of the supported languages below with a link to the getting started guide for each.

## Language Support

You can write SpacetimeDB modules in several popular languages, with more to come in the future!

#### Serverside Libraries

- [Rust](https://spacetimedb.com/docs/modules/rust/quickstart)
- [C#](https://spacetimedb.com/docs/modules/c-sharp/quickstart)

#### Client Libraries

- [Rust](https://spacetimedb.com/docs/sdks/rust/quickstart)
- [C#](https://spacetimedb.com/docs/sdks/c-sharp/quickstart)
- [Typescript](https://spacetimedb.com/docs/sdks/typescript/quickstart)

## License

SpacetimeDB is licensed under the BSL 1.1 license. This is not an open source or free software license, however, it converts to the AGPL v3.0 license with a linking exception after a few years.

Note that the AGPL v3.0 does not typically include a linking exception. We have added a custom linking exception to the AGPL license for SpacetimeDB. Our motivation for choosing a free software license is to ensure that contributions made to SpacetimeDB are propagated back to the community. We are expressly not interested in forcing users of SpacetimeDB to open source their own code if they link with SpacetimeDB, so we needed to include a linking exception.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[lbjlaq/Antigravity-Manager]]></title>
            <link>https://github.com/lbjlaq/Antigravity-Manager</link>
            <guid>https://github.com/lbjlaq/Antigravity-Manager</guid>
            <pubDate>Sat, 28 Feb 2026 00:06:00 GMT</pubDate>
            <description><![CDATA[Professional Antigravity Account Manager & Switcher. One-click seamless account switching for Antigravity Tools. Built with Tauri v2 + React (Rust).‰∏ì‰∏öÁöÑ Antigravity Ë¥¶Âè∑ÁÆ°ÁêÜ‰∏éÂàáÊç¢Â∑•ÂÖ∑„ÄÇ‰∏∫ Antigravity Êèê‰æõ‰∏ÄÈîÆÊó†ÁºùË¥¶Âè∑ÂàáÊç¢ÂäüËÉΩ„ÄÇ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lbjlaq/Antigravity-Manager">lbjlaq/Antigravity-Manager</a></h1>
            <p>Professional Antigravity Account Manager & Switcher. One-click seamless account switching for Antigravity Tools. Built with Tauri v2 + React (Rust).‰∏ì‰∏öÁöÑ Antigravity Ë¥¶Âè∑ÁÆ°ÁêÜ‰∏éÂàáÊç¢Â∑•ÂÖ∑„ÄÇ‰∏∫ Antigravity Êèê‰æõ‰∏ÄÈîÆÊó†ÁºùË¥¶Âè∑ÂàáÊç¢ÂäüËÉΩ„ÄÇ</p>
            <p>Language: Rust</p>
            <p>Stars: 24,631</p>
            <p>Forks: 2,746</p>
            <p>Stars today: 182 stars today</p>
            <h2>README</h2><pre># Antigravity Tools üöÄ
&gt; ‰∏ì‰∏öÁ∫ß AI Ë¥¶Âè∑ÁÆ°ÁêÜ‰∏éÂçèËÆÆ‰ª£ÁêÜÁ≥ªÁªü (v4.1.26)
&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;public/icon.png&quot; alt=&quot;Antigravity Logo&quot; width=&quot;120&quot; height=&quot;120&quot; style=&quot;border-radius: 24px; box-shadow: 0 10px 30px rgba(0,0,0,0.15);&quot;&gt;

  &lt;h3&gt;ÊÇ®ÁöÑ‰∏™‰∫∫È´òÊÄßËÉΩ AI Ë∞ÉÂ∫¶ÁΩëÂÖ≥&lt;/h3&gt;
  &lt;p&gt;‰∏ç‰ªÖ‰ªÖÊòØË¥¶Âè∑ÁÆ°ÁêÜÔºåÊõ¥ÊòØÊâìÁ†¥ API Ë∞ÉÁî®Â£ÅÂûíÁöÑÁªàÊûÅËß£ÂÜ≥ÊñπÊ°à„ÄÇ&lt;/p&gt;
  
  &lt;p&gt;
    &lt;a href=&quot;https://github.com/lbjlaq/Antigravity-Manager&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/badge/Version-4.1.26-blue?style=flat-square&quot; alt=&quot;Version&quot;&gt;
    &lt;/a&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Tauri-v2-orange?style=flat-square&quot; alt=&quot;Tauri&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Backend-Rust-red?style=flat-square&quot; alt=&quot;Rust&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Frontend-React-61DAFB?style=flat-square&quot; alt=&quot;React&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/License-CC--BY--NC--SA--4.0-lightgrey?style=flat-square&quot; alt=&quot;License&quot;&gt;
  &lt;/p&gt;

  &lt;p&gt;
    &lt;a href=&quot;#-Ê†∏ÂøÉÂäüËÉΩ&quot;&gt;Ê†∏ÂøÉÂäüËÉΩ&lt;/a&gt; ‚Ä¢ 
    &lt;a href=&quot;#-ÁïåÈù¢ÂØºËßà&quot;&gt;ÁïåÈù¢ÂØºËßà&lt;/a&gt; ‚Ä¢ 
    &lt;a href=&quot;#-ÊäÄÊúØÊû∂ÊûÑ&quot;&gt;ÊäÄÊúØÊû∂ÊûÑ&lt;/a&gt; ‚Ä¢ 
    &lt;a href=&quot;#-ÂÆâË£ÖÊåáÂçó&quot;&gt;ÂÆâË£ÖÊåáÂçó&lt;/a&gt; ‚Ä¢ 
    &lt;a href=&quot;#-Âø´ÈÄüÊé•ÂÖ•&quot;&gt;Âø´ÈÄüÊé•ÂÖ•&lt;/a&gt;
  &lt;/p&gt;

  &lt;p&gt;
    &lt;strong&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/strong&gt; | 
    &lt;a href=&quot;./README_EN.md&quot;&gt;English&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

---

**Antigravity Tools** ÊòØ‰∏Ä‰∏™‰∏ì‰∏∫ÂºÄÂèëËÄÖÂíå AI Áà±Â•ΩËÄÖËÆæËÆ°ÁöÑÂÖ®ÂäüËÉΩÊ°åÈù¢Â∫îÁî®„ÄÇÂÆÉÂ∞ÜÂ§öË¥¶Âè∑ÁÆ°ÁêÜ„ÄÅÂçèËÆÆËΩ¨Êç¢ÂíåÊô∫ËÉΩËØ∑Ê±ÇË∞ÉÂ∫¶ÂÆåÁæéÁªìÂêàÔºå‰∏∫ÊÇ®Êèê‰æõ‰∏Ä‰∏™Á®≥ÂÆö„ÄÅÊûÅÈÄü‰∏îÊàêÊú¨‰ΩéÂªâÁöÑ **Êú¨Âú∞ AI ‰∏≠ËΩ¨Á´ô**„ÄÇ

ÈÄöËøáÊú¨Â∫îÁî®ÔºåÊÇ®ÂèØ‰ª•Â∞ÜÂ∏∏ËßÅÁöÑ Web Á´Ø Session (Google/Anthropic) ËΩ¨Âåñ‰∏∫Ê†áÂáÜÂåñÁöÑ API Êé•Âè£ÔºåÊ∂àÈô§‰∏çÂêåÂéÇÂïÜÈó¥ÁöÑÂçèËÆÆÈ∏øÊ≤ü„ÄÇ

## üíñ ËµûÂä©ÂïÜ (Sponsors)

| ËµûÂä©ÂïÜ (Sponsor) | ÁÆÄ‰ªã (Description) |
| :---: | :--- |
| &lt;img src=&quot;docs/images/packycode_logo.png&quot; width=&quot;200&quot; alt=&quot;PackyCode Logo&quot;&gt; | ÊÑüË∞¢ **PackyCode** ÂØπÊú¨È°πÁõÆÁöÑËµûÂä©ÔºÅPackyCode ÊòØ‰∏ÄÂÆ∂ÂèØÈù†È´òÊïàÁöÑ API ‰∏≠ËΩ¨ÊúçÂä°ÂïÜÔºåÊèê‰æõ Claude Code„ÄÅCodex„ÄÅGemini Á≠âÂ§öÁßçÊúçÂä°ÁöÑ‰∏≠ËΩ¨„ÄÇPackyCode ‰∏∫Êú¨È°πÁõÆÁöÑÁî®Êà∑Êèê‰æõ‰∫ÜÁâπÂà´‰ºòÊÉ†Ôºö‰ΩøÁî®[Ê≠§ÈìæÊé•](https://www.packyapi.com/register?aff=Ctrler)Ê≥®ÂÜåÔºåÂπ∂Âú®ÂÖÖÂÄºÊó∂ËæìÂÖ• **‚ÄúCtrler‚Äù** ‰ºòÊÉ†Á†ÅÂç≥ÂèØ‰∫´Âèó **‰πùÊäò‰ºòÊÉ†**„ÄÇ |
| &lt;img src=&quot;docs/images/AICodeMirror.jpg&quot; width=&quot;200&quot; alt=&quot;AICodeMirror Logo&quot;&gt; | ÊÑüË∞¢ AICodeMirror ËµûÂä©‰∫ÜÊú¨È°πÁõÆÔºÅAICodeMirror Êèê‰æõ Claude Code / Codex / Gemini CLI ÂÆòÊñπÈ´òÁ®≥ÂÆö‰∏≠ËΩ¨ÊúçÂä°ÔºåÊîØÊåÅ‰ºÅ‰∏öÁ∫ßÈ´òÂπ∂Âèë„ÄÅÊûÅÈÄüÂºÄÁ•®„ÄÅ7√ó24 ‰∏ìÂ±ûÊäÄÊúØÊîØÊåÅ„ÄÇ Claude Code / Codex / Gemini ÂÆòÊñπÊ∏†ÈÅì‰ΩéËá≥ 3.8 / 0.2 / 0.9 ÊäòÔºåÂÖÖÂÄºÊõ¥ÊúâÊäò‰∏äÊäòÔºÅAICodeMirror ‰∏∫ Antigravity-Manager ÁöÑÁî®Êà∑Êèê‰æõ‰∫ÜÁâπÂà´Á¶èÂà©ÔºåÈÄöËøá[Ê≠§ÈìæÊé•](https://www.aicodemirror.com/register?invitecode=MV5XUM)Ê≥®ÂÜåÁöÑÁî®Êà∑ÔºåÂèØ‰∫´ÂèóÈ¶ñÂÖÖ8ÊäòÔºå‰ºÅ‰∏öÂÆ¢Êà∑ÊúÄÈ´òÂèØ‰∫´ 7.5 ÊäòÔºÅ |

### ‚òï ÊîØÊåÅÈ°πÁõÆ (Support)

Â¶ÇÊûúÊÇ®ËßâÂæóÊú¨È°πÁõÆÂØπÊÇ®ÊúâÊâÄÂ∏ÆÂä©ÔºåÊ¨¢ËøéÊâìËµè‰ΩúËÄÖÔºÅ

&lt;a href=&quot;https://www.buymeacoffee.com/Ctrler&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://cdn.buymeacoffee.com/buttons/v2/default-green.png&quot; alt=&quot;ËØ∑ÊàëÂñùÊùØÂíñÂï°&quot; style=&quot;height: 60px !important; width: 217px !important;&quot;&gt;&lt;/a&gt;

| ÊîØ‰ªòÂÆù (Alipay) | ÂæÆ‰ø°ÊîØ‰ªò (WeChat) | Buy Me a Coffee |
| :---: | :---: | :---: |
| ![Alipay](./docs/images/donate_alipay.png) | ![WeChat](./docs/images/donate_wechat.png) | ![Coffee](./docs/images/donate_coffee.png) |

## üåü Ê∑±Â∫¶ÂäüËÉΩËß£Êûê (Detailed Features)

### 1. üéõÔ∏è Êô∫ËÉΩË¥¶Âè∑‰ª™Ë°®Áõò (Smart Dashboard)
*   **ÂÖ®Â±ÄÂÆûÊó∂ÁõëÊéß**: ‰∏ÄÁúºÊ¥ûÂØüÊâÄÊúâË¥¶Âè∑ÁöÑÂÅ•Â∫∑Áä∂ÂÜµÔºåÂåÖÊã¨ Gemini Pro„ÄÅGemini Flash„ÄÅClaude ‰ª•Âèä Gemini ÁªòÂõæÁöÑ **Âπ≥ÂùáÂâ©‰ΩôÈÖçÈ¢ù**„ÄÇ
*   **ÊúÄ‰Ω≥Ë¥¶Âè∑Êé®Ëçê (Smart Recommendation)**: Á≥ªÁªü‰ºöÊ†πÊçÆÂΩìÂâçÊâÄÊúâË¥¶Âè∑ÁöÑÈÖçÈ¢ùÂÜó‰ΩôÂ∫¶ÔºåÂÆûÊó∂ÁÆóÊ≥ïÁ≠õÈÄâÂπ∂Êé®Ëçê‚ÄúÊúÄ‰Ω≥Ë¥¶Âè∑‚ÄùÔºåÊîØÊåÅ **‰∏ÄÈîÆÂàáÊç¢**„ÄÇ
*   **Ê¥ªË∑ÉË¥¶Âè∑Âø´ÁÖß**: Áõ¥ËßÇÊòæÁ§∫ÂΩìÂâçÊ¥ªË∑ÉË¥¶Âè∑ÁöÑÂÖ∑‰ΩìÈÖçÈ¢ùÁôæÂàÜÊØîÂèäÊúÄÂêéÂêåÊ≠•Êó∂Èó¥„ÄÇ

### 2. üîê Âº∫Â§ßÁöÑË¥¶Âè∑ÁÆ°ÂÆ∂ (Account Management)
*   **OAuth 2.0 ÊéàÊùÉÔºàËá™Âä®/ÊâãÂä®Ôºâ**: Ê∑ªÂä†Ë¥¶Âè∑Êó∂‰ºöÊèêÂâçÁîüÊàêÂèØÂ§çÂà∂ÁöÑÊéàÊùÉÈìæÊé•ÔºåÊîØÊåÅÂú®‰ªªÊÑèÊµèËßàÂô®ÂÆåÊàêÊéàÊùÉÔºõÂõûË∞ÉÊàêÂäüÂêéÂ∫îÁî®‰ºöËá™Âä®ÂÆåÊàêÂπ∂‰øùÂ≠òÔºàÂøÖË¶ÅÊó∂ÂèØÁÇπÂáª‚ÄúÊàëÂ∑≤ÊéàÊùÉÔºåÁªßÁª≠‚ÄùÊâãÂä®Êî∂Â∞æÔºâ„ÄÇ
*   **Â§öÁª¥Â∫¶ÂØºÂÖ•**: ÊîØÊåÅÂçïÊù° Token ÂΩïÂÖ•„ÄÅJSON ÊâπÈáèÂØºÂÖ•ÔºàÂ¶ÇÊù•Ëá™ÂÖ∂‰ªñÂ∑•ÂÖ∑ÁöÑÂ§á‰ªΩÔºâÔºå‰ª•Âèä‰ªé V1 ÊóßÁâàÊú¨Êï∞ÊçÆÂ∫ìËá™Âä®ÁÉ≠ËøÅÁßª„ÄÇ
*   **ÁΩëÂÖ≥Á∫ßËßÜÂõæ**: ÊîØÊåÅ‚ÄúÂàóË°®‚Äù‰∏é‚ÄúÁΩëÊ†º‚ÄùÂèåËßÜÂõæÂàáÊç¢„ÄÇÊèê‰æõ 403 Â∞ÅÁ¶ÅÊ£ÄÊµãÔºåËá™Âä®Ê†áÊ≥®Âπ∂Ë∑≥ËøáÊùÉÈôêÂºÇÂ∏∏ÁöÑË¥¶Âè∑„ÄÇ

### 3. üîå ÂçèËÆÆËΩ¨Êç¢‰∏é‰∏≠Áªß (API Proxy)
*   **ÂÖ®ÂçèËÆÆÈÄÇÈÖç (Multi-Sink)**:
    *   **OpenAI Ê†ºÂºè**: Êèê‰æõ `/v1/chat/completions` Á´ØÁÇπÔºåÂÖºÂÆπ 99% ÁöÑÁé∞Êúâ AI Â∫îÁî®„ÄÇ
    *   **Anthropic Ê†ºÂºè**: Êèê‰æõÂéüÁîü `/v1/messages` Êé•Âè£ÔºåÊîØÊåÅ **Claude Code CLI** ÁöÑÂÖ®ÂäüËÉΩÔºàÂ¶ÇÊÄùÊÄùÁª¥Èìæ„ÄÅÁ≥ªÁªüÊèêÁ§∫ËØçÔºâ„ÄÇ
    *   **Gemini Ê†ºÂºè**: ÊîØÊåÅ Google ÂÆòÊñπ SDK Áõ¥Êé•Ë∞ÉÁî®„ÄÇ
*   **Êô∫ËÉΩÁä∂ÊÄÅËá™ÊÑà**: ÂΩìËØ∑Ê±ÇÈÅáÂà∞ `429 (Too Many Requests)` Êàñ `401 (Expire)` Êó∂ÔºåÂêéÁ´Ø‰ºöÊØ´ÁßíÁ∫ßËß¶Âèë **Ëá™Âä®ÈáçËØï‰∏éÈùôÈªòËΩÆÊç¢**ÔºåÁ°Æ‰øù‰∏öÂä°‰∏ç‰∏≠Êñ≠„ÄÇ

### 4. üîÄ Ê®°ÂûãË∑ØÁî±‰∏≠ÂøÉ (Model Router)
*   **Á≥ªÂàóÂåñÊò†Â∞Ñ**: ÊÇ®ÂèØ‰ª•Â∞ÜÂ§çÊùÇÁöÑÂéüÂßãÊ®°Âûã ID ÂΩíÁ±ªÂà∞‚ÄúËßÑÊ†ºÂÆ∂Êóè‚ÄùÔºàÂ¶ÇÂ∞ÜÊâÄÊúâ GPT-4 ËØ∑Ê±ÇÁªü‰∏ÄË∑ØÁî±Âà∞ `gemini-3-pro-high`Ôºâ„ÄÇ
*   **‰∏ìÂÆ∂Á∫ßÈáçÂÆöÂêë**: ÊîØÊåÅËá™ÂÆö‰πâÊ≠£ÂàôË°®ËææÂºèÁ∫ßÊ®°ÂûãÊò†Â∞ÑÔºåÁ≤æÂáÜÊéßÂà∂ÊØè‰∏Ä‰∏™ËØ∑Ê±ÇÁöÑËêΩÂú∞Ê®°Âûã„ÄÇ
*   **Êô∫ËÉΩÂàÜÁ∫ßË∑ØÁî± (Tiered Routing)**: [Êñ∞] Á≥ªÁªüÊ†πÊçÆË¥¶Âè∑Á±ªÂûãÔºàUltra/Pro/FreeÔºâÂíåÈÖçÈ¢ùÈáçÁΩÆÈ¢ëÁéáËá™Âä®‰ºòÂÖàÁ∫ßÊéíÂ∫èÔºå‰ºòÂÖàÊ∂àËÄóÈ´òÈÄüÈáçÁΩÆË¥¶Âè∑ÔºåÁ°Æ‰øùÈ´òÈ¢ëË∞ÉÁî®‰∏ãÁöÑÊúçÂä°Á®≥ÂÆöÊÄß„ÄÇ
*   **ÂêéÂè∞‰ªªÂä°ÈùôÈªòÈôçÁ∫ß**: [Êñ∞] Ëá™Âä®ËØÜÂà´ Claude CLI Á≠âÂ∑•ÂÖ∑ÁîüÊàêÁöÑÂêéÂè∞ËØ∑Ê±ÇÔºàÂ¶ÇÊ†áÈ¢òÁîüÊàêÔºâÔºåÊô∫ËÉΩÈáçÂÆöÂêëËá≥ Flash Ê®°ÂûãÔºå‰øùÊä§È´òÁ∫ßÊ®°ÂûãÈÖçÈ¢ù‰∏çË¢´Êµ™Ë¥π„ÄÇ

### 5. üé® Â§öÊ®°ÊÄÅ‰∏é Imagen 3 ÊîØÊåÅ
*   **È´òÁ∫ßÁîªË¥®ÊéßÂà∂**: ÊîØÊåÅÈÄöËøá OpenAI `size` (Â¶Ç `1024x1024`, `16:9`) ÂèÇÊï∞Ëá™Âä®Êò†Â∞ÑÂà∞ Imagen 3 ÁöÑÁõ∏Â∫îËßÑÊ†º„ÄÇ
*   **Ë∂ÖÂº∫ Body ÊîØÊåÅ**: ÂêéÁ´ØÊîØÊåÅÈ´òËææ **100MB** (ÂèØÈÖçÁΩÆ) ÁöÑ PayloadÔºåÂ§ÑÁêÜ 4K È´òÊ∏ÖÂõæËØÜÂà´Áª∞Áª∞Êúâ‰Ωô„ÄÇ

## üì∏ ÁïåÈù¢ÂØºËßà (GUI Overview)

| | |
| :---: | :---: |
| ![‰ª™Ë°®Áõò - ÂÖ®Â±ÄÈÖçÈ¢ùÁõëÊéß‰∏é‰∏ÄÈîÆÂàáÊç¢](docs/images/dashboard-light.png) &lt;br&gt; ‰ª™Ë°®Áõò | ![Ë¥¶Âè∑ÂàóË°® - È´òÂØÜÂ∫¶ÈÖçÈ¢ùÂ±ïÁ§∫‰∏é 403 Êô∫ËÉΩÊ†áÊ≥®](docs/images/accounts-light.png) &lt;br&gt; Ë¥¶Âè∑ÂàóË°® |
| ![ÂÖ≥‰∫éÈ°µÈù¢ - ÂÖ≥‰∫é Antigravity Tools](docs/images/about-dark.png) &lt;br&gt; ÂÖ≥‰∫éÈ°µÈù¢ | ![API Âèç‰ª£ - ÊúçÂä°ÊéßÂà∂](docs/images/v3/proxy-settings.png) &lt;br&gt; API Âèç‰ª£ |
| ![Á≥ªÁªüËÆæÁΩÆ - ÈÄöÁî®ÈÖçÁΩÆ](docs/images/settings-dark.png) &lt;br&gt; Á≥ªÁªüËÆæÁΩÆ | |

### üí° ‰ΩøÁî®Ê°à‰æã (Usage Examples)

| | |
| :---: | :---: |
| ![Claude Code ËÅîÁΩëÊêúÁ¥¢ - ÁªìÊûÑÂåñÊù•Ê∫ê‰∏éÂºïÊñáÊòæÁ§∫](docs/images/usage/claude-code-search.png) &lt;br&gt; Claude Code ËÅîÁΩëÊêúÁ¥¢ | ![Cherry Studio Ê∑±Â∫¶ÈõÜÊàê - ÂéüÁîüÂõûÊòæÊêúÁ¥¢ÂºïÊñá‰∏éÊù•Ê∫êÈìæÊé•](docs/images/usage/cherry-studio-citations.png) &lt;br&gt; Cherry Studio Ê∑±Â∫¶ÈõÜÊàê |
| ![Imagen 3 È´òÁ∫ßÁªòÂõæ - ÂÆåÁæéËøòÂéü Prompt ÊÑèÂ¢É‰∏éÁªÜËäÇ](docs/images/usage/image-gen-nebula.png) &lt;br&gt; Imagen 3 È´òÁ∫ßÁªòÂõæ | ![Kilo Code Êé•ÂÖ• - Â§öË¥¶Âè∑ÊûÅÈÄüËΩÆÊç¢‰∏éÊ®°ÂûãÁ©øÈÄè](docs/images/usage/kilo-code-integration.png) &lt;br&gt; Kilo Code Êé•ÂÖ• |

## üèóÔ∏è ÊäÄÊúØÊû∂ÊûÑ (Architecture)

```mermaid
graph TD
    Client([Â§ñÈÉ®Â∫îÁî®: Claude Code/NextChat]) --&gt;|OpenAI/Anthropic| Gateway[Antigravity Axum Server]
    Gateway --&gt; Middleware[‰∏≠Èó¥‰ª∂: Èâ¥ÊùÉ/ÈôêÊµÅ/Êó•Âøó]
    Middleware --&gt; Router[Model Router: ID Êò†Â∞Ñ]
    Router --&gt; Dispatcher[Ë¥¶Âè∑ÂàÜÂèëÂô®: ËΩÆËØ¢/ÊùÉÈáç]
    Dispatcher --&gt; Mapper[ÂçèËÆÆËΩ¨Êç¢Âô®: Request Mapper]
    Mapper --&gt; Upstream[‰∏äÊ∏∏ËØ∑Ê±Ç: Google/Anthropic API]
    Upstream --&gt; ResponseMapper[ÂìçÂ∫îËΩ¨Êç¢Âô®: Response Mapper]
    ResponseMapper --&gt; Client
```

##  ÂÆâË£ÖÊåáÂçó (Installation)

### ÈÄâÈ°π A: ÁªàÁ´ØÂÆâË£Ö (Êé®Ëçê)

#### Ë∑®Âπ≥Âè∞‰∏ÄÈîÆÂÆâË£ÖËÑöÊú¨

Ëá™Âä®Ê£ÄÊµãÊìç‰ΩúÁ≥ªÁªü„ÄÅÊû∂ÊûÑÂíåÂåÖÁÆ°ÁêÜÂô®Ôºå‰∏ÄÊù°ÂëΩ‰ª§ÂÆåÊàê‰∏ãËΩΩ‰∏éÂÆâË£Ö„ÄÇ

**Linux / macOS:**
```bash
curl -fsSL https://raw.githubusercontent.com/lbjlaq/Antigravity-Manager/v4.1.26/install.sh | bash
```

**Windows (PowerShell):**
```powershell
irm https://raw.githubusercontent.com/lbjlaq/Antigravity-Manager/main/install.ps1 | iex
```

&gt; **ÊîØÊåÅÁöÑÊ†ºÂºè**: Linux (`.deb` / `.rpm` / `.AppImage`) | macOS (`.dmg`) | Windows (NSIS `.exe`)
&gt;
&gt; **È´òÁ∫ßÁî®Ê≥ï**: ÂÆâË£ÖÊåáÂÆöÁâàÊú¨ `curl -fsSL ... | bash -s -- --version 4.1.26`ÔºåÈ¢ÑËßàÊ®°Âºè `curl -fsSL ... | bash -s -- --dry-run`

#### macOS - Homebrew
Â¶ÇÊûúÊÇ®Â∑≤ÂÆâË£Ö [Homebrew](https://brew.sh/)Ôºå‰πüÂèØ‰ª•ÈÄöËøá‰ª•‰∏ãÂëΩ‰ª§ÂÆâË£ÖÔºö

```bash
# 1. ËÆ¢ÈòÖÊú¨‰ªìÂ∫ìÁöÑ Tap
brew tap lbjlaq/antigravity-manager https://github.com/lbjlaq/Antigravity-Manager

# 2. ÂÆâË£ÖÂ∫îÁî®
brew install --cask antigravity-tools
```
&gt; **ÊèêÁ§∫**: Â¶ÇÊûúÈÅáÂà∞ÊùÉÈôêÈóÆÈ¢òÔºåÂª∫ËÆÆÊ∑ªÂä† `--no-quarantine` ÂèÇÊï∞„ÄÇ

#### Arch Linux
ÊÇ®ÂèØ‰ª•ÈÄâÊã©ÈÄöËøá‰∏ÄÈîÆÂÆâË£ÖËÑöÊú¨Êàñ Homebrew ËøõË°åÂÆâË£ÖÔºö

**ÊñπÂºè 1Ôºö‰∏ÄÈîÆÂÆâË£ÖËÑöÊú¨ (Êé®Ëçê)**
```bash
curl -sSL https://raw.githubusercontent.com/lbjlaq/Antigravity-Manager/main/deploy/arch/install.sh | bash
```

**ÊñπÂºè 2ÔºöÈÄöËøá Homebrew** (Â¶ÇÊûúÊÇ®Â∑≤ÂÆâË£Ö [Linuxbrew](https://sh.brew.sh/))
```bash
brew tap lbjlaq/antigravity-manager https://github.com/lbjlaq/Antigravity-Manager
brew install --cask antigravity-tools
```

#### ÂÖ∂‰ªñ Linux ÂèëË°åÁâà
ÂÆâË£ÖÂêé‰ºöËá™Âä®Â∞Ü AppImage Ê∑ªÂä†Âà∞‰∫åËøõÂà∂Ë∑ØÂæÑÂπ∂ÈÖçÁΩÆÂèØÊâßË°åÊùÉÈôê„ÄÇ

### ÈÄâÈ°π B: ÊâãÂä®‰∏ãËΩΩ
ÂâçÂæÄ [GitHub Releases](https://github.com/lbjlaq/Antigravity-Manager/releases) ‰∏ãËΩΩÂØπÂ∫îÁ≥ªÁªüÁöÑÂåÖÔºö
*   **macOS**: `.dmg` (ÊîØÊåÅ Apple Silicon &amp; Intel)
*   **Windows**: `.msi` Êàñ ‰æøÊê∫Áâà `.zip`
*   **Linux**: `.deb` Êàñ `AppImage`

### ÈÄâÈ°π C: Docker ÈÉ®ÁΩ≤ (Êé®ËçêÁî®‰∫é NAS/ÊúçÂä°Âô®)
Â¶ÇÊûúÊÇ®Â∏åÊúõÂú®ÂÆπÂô®ÂåñÁéØÂ¢É‰∏≠ËøêË°åÔºåÊàë‰ª¨Êèê‰æõ‰∫ÜÂéüÁîüÁöÑ Docker ÈïúÂÉè„ÄÇËØ•ÈïúÂÉèÂÜÖÁΩÆ‰∫ÜÂØπ v4.0.2 ÂéüÁîü Headless Êû∂ÊûÑÁöÑÊîØÊåÅÔºåÂèØËá™Âä®ÊâòÁÆ°ÂâçÁ´ØÈùôÊÄÅËµÑÊ∫êÔºåÂπ∂ÈÄöËøáÊµèËßàÂô®Áõ¥Êé•ËøõË°åÁÆ°ÁêÜ„ÄÇ

```bash
# ÊñπÂºè 1: Áõ¥Êé•ËøêË°å (Êé®Ëçê)
# - API_KEY: ÂøÖÂ°´„ÄÇÁî®‰∫éÊâÄÊúâÂçèËÆÆÁöÑ AI ËØ∑Ê±ÇÈâ¥ÂÆö„ÄÇ
# - WEB_PASSWORD: ÂèØÈÄâ„ÄÇÁî®‰∫éÁÆ°ÁêÜÂêéÂè∞ÁôªÂΩï„ÄÇËã•‰∏çËÆæÁΩÆÂàôÈªòËÆ§‰ΩøÁî® API_KEY„ÄÇ
docker run -d --name antigravity-manager \
  -p 8045:8045 \
  -e API_KEY=sk-your-api-key \
  -e WEB_PASSWORD=your-login-password \
  -e ABV_MAX_BODY_SIZE=104857600 \
  -v ~/.antigravity_tools:/root/.antigravity_tools \
  lbjlaq/antigravity-manager:latest

# ÂøòËÆ∞ÂØÜÈí•ÔºüÊâßË°å docker logs antigravity-manager Êàñ grep -E &#039;&quot;api_key&quot;|&quot;admin_password&quot;&#039; ~/.antigravity_tools/gui_config.json

#### üîê Èâ¥ÊùÉÈÄªËæëËØ¥Êòé
*   **Âú∫ÊôØ AÔºö‰ªÖËÆæÁΩÆ‰∫Ü `API_KEY`**
    - **Web ÁôªÂΩï**Ôºö‰ΩøÁî® `API_KEY` ËøõÂÖ•ÂêéÂè∞„ÄÇ
    - **API Ë∞ÉÁî®**Ôºö‰ΩøÁî® `API_KEY` ËøõË°å AI ËØ∑Ê±ÇÈâ¥ÊùÉ„ÄÇ
*   **Âú∫ÊôØ BÔºöÂêåÊó∂ËÆæÁΩÆ‰∫Ü `API_KEY` Âíå `WEB_PASSWORD` (Êé®Ëçê)**
    - **Web ÁôªÂΩï**Ôºö**ÂøÖÈ°ª**‰ΩøÁî® `WEB_PASSWORD`Ôºå‰ΩøÁî® API Key Â∞ÜË¢´ÊãíÁªùÔºàÊõ¥ÂÆâÂÖ®Ôºâ„ÄÇ
    - **API Ë∞ÉÁî®**ÔºöÁªü‰∏Ä‰ΩøÁî® `API_KEY`„ÄÇËøôÊ†∑ÊÇ®ÂèØ‰ª•Â∞Ü API Key ÂàÜÂèëÁªôÊàêÂëòÔºåËÄå‰øùÁïôÂØÜÁ†Å‰ªÖ‰æõÁÆ°ÁêÜÂëò‰ΩøÁî®„ÄÇ

#### üÜô ÊóßÁâàÊú¨ÂçáÁ∫ßÊåáÂºï
Â¶ÇÊûúÊÇ®ÊòØ‰ªé v4.0.1 ÂèäÊõ¥Êó©ÁâàÊú¨ÂçáÁ∫ßÔºåÁ≥ªÁªüÈªòËÆ§Êú™ËÆæÁΩÆ `WEB_PASSWORD`„ÄÇÊÇ®ÂèØ‰ª•ÈÄöËøá‰ª•‰∏ã‰ªª‰∏ÄÊñπÂºèËÆæÁΩÆÔºö
1.  **Web UI ÁïåÈù¢ (Êé®Ëçê)**Ôºö‰ΩøÁî®ÂéüÊúâ `API_KEY` ÁôªÂΩïÂêéÔºåÂú® **API Âèç‰ª£ËÆæÁΩÆ** È°µÈù¢ÊâãÂä®ËÆæÁΩÆÂπ∂‰øùÂ≠ò„ÄÇÊñ∞ÂØÜÁ†ÅÂ∞ÜÊåÅ‰πÖÂåñÂ≠òÂÇ®Âú® `gui_config.json` ‰∏≠„ÄÇ
2.  **ÁéØÂ¢ÉÂèòÈáè (Docker)**ÔºöÂú®ÂêØÂä®ÂÆπÂô®Êó∂Â¢ûÂä† `-e WEB_PASSWORD=ÊÇ®ÁöÑÊñ∞ÂØÜÁ†Å`„ÄÇ**Ê≥®ÊÑèÔºöÁéØÂ¢ÉÂèòÈáèÂÖ∑ÊúâÊúÄÈ´ò‰ºòÂÖàÁ∫ßÔºåÂ∞ÜË¶ÜÁõñ UI ‰∏≠ÁöÑ‰ªª‰Ωï‰øÆÊîπ„ÄÇ**
3.  **ÈÖçÁΩÆÊñá‰ª∂ (ÊåÅ‰πÖÂåñ)**ÔºöÁõ¥Êé•‰øÆÊîπ `~/.antigravity_tools/gui_config.json`ÔºåÂú® `proxy` ÂØπË±°‰∏≠‰øÆÊîπÊàñÊ∑ªÂä† `&quot;admin_password&quot;: &quot;ÊÇ®ÁöÑÊñ∞ÂØÜÁ†Å&quot;` Â≠óÊÆµ„ÄÇ
    - *Ê≥®Ôºö`WEB_PASSWORD` ÊòØÁéØÂ¢ÉÂèòÈáèÂêçÔºå`admin_password` ÊòØÈÖçÁΩÆÊñá‰ª∂‰∏≠ÁöÑ JSON ÈîÆÂêç„ÄÇ*

&gt; [!TIP]
&gt; **ÂØÜÁ†Å‰ºòÂÖàÁ∫ßÈÄªËæë (Priority)**:
&gt; - **Á¨¨‰∏Ä‰ºòÂÖàÁ∫ß (ÁéØÂ¢ÉÂèòÈáè)**: `ABV_WEB_PASSWORD` Êàñ `WEB_PASSWORD`„ÄÇÂè™Ë¶ÅËÆæÁΩÆ‰∫ÜÁéØÂ¢ÉÂèòÈáèÔºåÁ≥ªÁªüÂ∞ÜÂßãÁªà‰ΩøÁî®ÂÆÉ„ÄÇ
&gt; - **Á¨¨‰∫å‰ºòÂÖàÁ∫ß (ÈÖçÁΩÆÊñá‰ª∂)**: `gui_config.json` ‰∏≠ÁöÑ `admin_password` Â≠óÊÆµ„ÄÇUI ÁöÑ‚Äú‰øùÂ≠ò‚ÄùÊìç‰Ωú‰ºöÊõ¥Êñ∞Ê≠§ÂÄº„ÄÇ
&gt; - **‰øùÂ∫ïÂõûÈÄÄ (ÂêëÂêéÂÖºÂÆπ)**: Ëã•‰∏äËø∞ÂùáÊú™ËÆæÁΩÆÔºåÂàôÂõûÈÄÄ‰ΩøÁî® `API_KEY` ‰Ωú‰∏∫ÁôªÂΩïÂØÜÁ†Å„ÄÇ

# ÊñπÂºè 2: ‰ΩøÁî® Docker Compose
# 1. ËøõÂÖ•È°πÁõÆÁöÑ docker ÁõÆÂΩï
cd docker
# 2. ÂêØÂä®ÊúçÂä°
docker compose up -d
```
&gt; **ËÆøÈóÆÂú∞ÂùÄ**: `http://localhost:8045` (ÁÆ°ÁêÜÂêéÂè∞) | `http://localhost:8045/v1` (API Base)
&gt; **Á≥ªÁªüË¶ÅÊ±Ç**:
&gt; - **ÂÜÖÂ≠ò**: Âª∫ËÆÆ **1GB** (ÊúÄÂ∞è 256MB)„ÄÇ
&gt; - **ÊåÅ‰πÖÂåñ**: ÈúÄÊåÇËΩΩ `/root/.antigravity_tools` ‰ª•‰øùÂ≠òÊï∞ÊçÆ„ÄÇ
&gt; - **Êû∂ÊûÑ**: ÊîØÊåÅ x86_64 Âíå ARM64„ÄÇ
&gt; **ËØ¶ÊÉÖËßÅ**: [Docker ÈÉ®ÁΩ≤ÊåáÂçó (docker)](./docker/README.md)

---

Copyright ¬© 2024-2026 [lbjlaq](https://github.com/lbjlaq)

### üõ†Ô∏è Â∏∏ËßÅÈóÆÈ¢òÊéíÊü• (Troubleshooting)

#### macOS ÊèêÁ§∫‚ÄúÂ∫îÁî®Â∑≤ÊçüÂùèÔºåÊó†Ê≥ïÊâìÂºÄ‚ÄùÔºü
Áî±‰∫é macOS ÁöÑÂÆâÂÖ®Êú∫Âà∂ÔºåÈùû App Store ‰∏ãËΩΩÁöÑÂ∫îÁî®ÂèØËÉΩ‰ºöËß¶ÂèëÊ≠§ÊèêÁ§∫„ÄÇÊÇ®ÂèØ‰ª•ÊåâÁÖß‰ª•‰∏ãÊ≠•È™§Âø´ÈÄü‰øÆÂ§çÔºö

1.  **ÂëΩ‰ª§Ë°å‰øÆÂ§ç** (Êé®Ëçê):
    ÊâìÂºÄÁªàÁ´ØÔºåÊâßË°å‰ª•‰∏ãÂëΩ‰ª§Ôºö
    ```bash
    sudo xattr -rd com.apple.quarantine &quot;/Applications/Antigravity Tools.app&quot;
    ```
2.  **Homebrew ÂÆâË£ÖÊäÄÂ∑ß**:
    Â¶ÇÊûúÊÇ®‰ΩøÁî® brew ÂÆâË£ÖÔºåÂèØ‰ª•Ê∑ªÂä† `--no-quarantine` ÂèÇÊï∞Êù•ËßÑÈÅøÊ≠§ÈóÆÈ¢òÔºö
    ```bash
    brew install --cask --no-quarantine antigravity-tools
    ```

## üîå Âø´ÈÄüÊé•ÂÖ•Á§∫‰æã

### üîê OAuth ÊéàÊùÉÊµÅÁ®ãÔºàÊ∑ªÂä†Ë¥¶Âè∑Ôºâ
1. ÊâìÂºÄ‚ÄúAccounts / Ë¥¶Âè∑‚Äù ‚Üí ‚ÄúÊ∑ªÂä†Ë¥¶Âè∑‚Äù ‚Üí ‚ÄúOAuth‚Äù„ÄÇ
2. ÂºπÁ™ó‰ºöÂú®ÁÇπÂáªÊåâÈíÆÂâçÈ¢ÑÁîüÊàêÊéàÊùÉÈìæÊé•ÔºõÁÇπÂáªÈìæÊé•Âç≥ÂèØÂ§çÂà∂Âà∞Á≥ªÁªüÂâ™Ë¥¥ÊùøÔºåÁÑ∂ÂêéÁî®‰Ω†Â∏åÊúõÁöÑÊµèËßàÂô®ÊâìÂºÄÂπ∂ÂÆåÊàêÊéàÊùÉ„ÄÇ
3. ÊéàÊùÉÂÆåÊàêÂêéÊµèËßàÂô®‰ºöÊâìÂºÄÊú¨Âú∞ÂõûË∞ÉÈ°µÂπ∂ÊòæÁ§∫‚Äú‚úÖ ÊéàÊùÉÊàêÂäü!‚Äù„ÄÇ
4. Â∫îÁî®‰ºöËá™Âä®ÁªßÁª≠ÂÆåÊàêÊéàÊùÉÂπ∂‰øùÂ≠òË¥¶Âè∑ÔºõÂ¶ÇÊú™Ëá™Âä®ÂÆåÊàêÔºåÂèØÁÇπÂáª‚ÄúÊàëÂ∑≤ÊéàÊùÉÔºåÁªßÁª≠‚ÄùÊâãÂä®ÂÆåÊàê„ÄÇ

&gt; ÊèêÁ§∫ÔºöÊéàÊùÉÈìæÊé•ÂåÖÂê´‰∏ÄÊ¨°ÊÄßÂõûË∞ÉÁ´ØÂè£ÔºåËØ∑ÂßãÁªà‰ΩøÁî®ÂºπÁ™óÈáåÁîüÊàêÁöÑÊúÄÊñ∞ÈìæÊé•ÔºõÂ¶ÇÊûúÊéàÊùÉÊó∂Â∫îÁî®Êú™ËøêË°åÊàñÂºπÁ™óÂ∑≤ÂÖ≥Èó≠ÔºåÊµèËßàÂô®ÂèØËÉΩ‰ºöÊèêÁ§∫ `localhost refused connection`„ÄÇ

### Â¶Ç‰ΩïÊé•ÂÖ• Claude Code CLI?
1.  ÂêØÂä® AntigravityÔºåÂπ∂Âú®‚ÄúAPI Âèç‰ª£‚ÄùÈ°µÈù¢ÂºÄÂêØÊúçÂä°„ÄÇ
2.  Âú®ÁªàÁ´ØÊâßË°åÔºö
```bash
export ANTHROPIC_API_KEY=&quot;sk-antigravity&quot;
export ANTHROPIC_BASE_URL=&quot;http://127.0.0.1:8045&quot;
claude
```

### Â¶Ç‰ΩïÊé•ÂÖ• OpenCode?
1.  ËøõÂÖ• **API Âèç‰ª£**È°µÈù¢ ‚Üí **Â§ñÈÉ® Providers** ‚Üí ÁÇπÂáª **OpenCode Sync** Âç°Áâá„ÄÇ
2.  ÁÇπÂáª **Sync** ÊåâÈíÆÔºåÂ∞ÜËá™Âä®ÁîüÊàê `~/.config/opencode/opencode.json` ÈÖçÁΩÆÊñá‰ª∂Ôºö
    - ÂàõÂª∫Áã¨Á´ã provider `antigravity-manager`Ôºà‰∏çË¶ÜÁõñ google/anthropic ÂéüÁîüÈÖçÁΩÆÔºâ
    - ÂèØÈÄâÔºöÂãæÈÄâ **Sync accounts** ÂØºÂá∫ `antigravity-accounts.json`Ôºàplugin-compatible v3 Ê†ºÂºèÔºâÔºå‰æõ OpenCode Êèí‰ª∂Áõ¥Êé•ÂØºÂÖ•
3.  ÁÇπÂáª **Clear Config** ÂèØ‰∏ÄÈîÆÊ∏ÖÈô§ Manager ÈÖçÁΩÆÂπ∂Ê∏ÖÁêÜ legacy ÊÆãÁïôÔºõÁÇπÂáª **Restore** ÂèØ‰ªéÂ§á‰ªΩÊÅ¢Â§ç„ÄÇ
4.  Windows Áî®Êà∑Ë∑ØÂæÑ‰∏∫ `C:\Users\&lt;Áî®Êà∑Âêç&gt;\.config\opencode\`Ôºà‰∏é `~/.config/opencode` ËßÑÂàô‰∏ÄËá¥Ôºâ„ÄÇ

**Âø´ÈÄüÈ™åËØÅÂëΩ‰ª§Ôºö**
```bash
# ÊµãËØï antigravity-manager providerÔºàÊîØÊåÅ --variantÔºâ
opencode run &quot;test&quot; --model antigravity-manager/claude-sonnet-4-5-thinking --variant high

# Ëã•Â∑≤ÂÆâË£Ö opencode-antigravity-auth Êèí‰ª∂ÔºåÈ™åËØÅ google provider ‰ªçÂèØÁã¨Á´ãÂ∑•‰Ωú
opencode run &quot;test&quot; --model google/antigravity-claude-sonnet-4-5-thinking --variant max
```

### Â¶Ç‰ΩïÊé•ÂÖ• Kilo Code?
1.  **ÂçèËÆÆÈÄâÊã©**: Âª∫ËÆÆ‰ºòÂÖà‰ΩøÁî® **Gemini ÂçèËÆÆ**„ÄÇ
2.  **Base URL**: Â°´ÂÜô `http://127.0.0.1:8045`„ÄÇ
3.  **Ê≥®ÊÑè**: 
    - **OpenAI ÂçèËÆÆÈôêÂà∂**: Kilo Code Âú®‰ΩøÁî® OpenAI Ê®°ÂºèÊó∂ÔºåÂÖ∂ËØ∑Ê±ÇË∑ØÂæÑ‰ºöÂè†Âä†‰∫ßÁîü `/v1/chat/completions/responses` ËøôÁßçÈùûÊ†áÂáÜË∑ØÂæÑÔºåÂØºËá¥ Antigravity ËøîÂõû 404„ÄÇÂõ†Ê≠§ËØ∑Âä°ÂøÖÂ°´ÂÖ• Base URL ÂêéÈÄâÊã© Gemini Ê®°Âºè„ÄÇ
    - **Ê®°ÂûãÊò†Â∞Ñ**: Kilo Code ‰∏≠ÁöÑÊ®°ÂûãÂêçÁß∞ÂèØËÉΩ‰∏é Antigravity ÈªòËÆ§ËÆæÁΩÆ‰∏ç‰∏ÄËá¥ÔºåÂ¶ÇÈÅáÂà∞Êó†Ê≥ïËøûÊé•ÔºåËØ∑Âú®‚ÄúÊ®°ÂûãÊò†Â∞Ñ‚ÄùÈ°µÈù¢ËÆæÁΩÆËá™ÂÆö‰πâÊò†Â∞ÑÔºåÂπ∂Êü•Áúã**Êó•ÂøóÊñá‰ª∂**ËøõË°åË∞ÉËØï„ÄÇ

### Â¶Ç‰ΩïÂú® Python ‰∏≠‰ΩøÁî®?
```python
import openai

client = openai.OpenAI(
    api_key=&quot;sk-antigravity&quot;,
    base_url=&quot;http://127.0.0.1:8045/v1&quot;
)

response = client.chat.completions.create(
    model=&quot;gemini-3-flash&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;‰Ω†Â•ΩÔºåËØ∑Ëá™Êàë‰ªãÁªç&quot;}]
)
print(response.choices[0].message.content)
```

### Â¶Ç‰Ωï‰ΩøÁî®ÂõæÁâáÁîüÊàê (Imagen 3)?

#### ÊñπÂºè‰∏ÄÔºöOpenAI Images API (Êé®Ëçê)
```python
import openai

client = openai.OpenAI(
    api_key=&quot;sk-antigravity&quot;,
    base_url=&quot;http://127.0.0.1:8045/v1&quot;
)

# ÁîüÊàêÂõæÁâá
response = client.images.generate(
    model=&quot;gemini-3-pro-image&quot;,
    prompt=&quot;‰∏ÄÂ∫ßÊú™Êù•‰∏ª‰πâÈ£éÊ†ºÁöÑÂüéÂ∏ÇÔºåËµõÂçöÊúãÂÖãÔºåÈúìËôπÁÅØ&quot;,
    size=&quot;1920x1080&quot;,      # ÊîØÊåÅ‰ªªÊÑè WIDTHxHEIGHT Ê†ºÂºèÔºåËá™Âä®ËÆ°ÁÆóÂÆΩÈ´òÊØî
    quality=&quot;hd&quot;,          # &quot;standard&quot; | &quot;hd&quot; | &quot;medium&quot;
    n=1,
    response_format=&quot;b64_json&quot;
)

# ‰øùÂ≠òÂõæÁâá
import base64
image_data = base64.b64decode(response.data[0].b64_json)
with open(&quot;output.png&quot;, &quot;wb&quot;) as f:
    f.write(image_data)
```

**ÊîØÊåÅÁöÑÂèÇÊï∞**Ôºö
- **`size`**: ‰ªªÊÑè `WIDTHxHEIGHT` Ê†ºÂºèÔºàÂ¶Ç `1280x720`, `1024x1024`, `1920x1080`ÔºâÔºåËá™Âä®ËÆ°ÁÆóÂπ∂Êò†Â∞ÑÂà∞Ê†áÂáÜÂÆΩÈ´òÊØîÔºà21:9, 16:9, 9:16, 4:3, 3:4, 1:1Ôºâ
- **`quality`**: 
  - `&quot;hd&quot;` ‚Üí 4K ÂàÜËæ®ÁéáÔºàÈ´òË¥®ÈáèÔºâ
  - `&quot;medium&quot;` ‚Üí 2K ÂàÜËæ®ÁéáÔºà‰∏≠Á≠âË¥®ÈáèÔºâ
  - `&quot;standard&quot;` ‚Üí ÈªòËÆ§ÂàÜËæ®ÁéáÔºàÊ†áÂáÜË¥®ÈáèÔºâ
- **`n`**: ÁîüÊàêÂõæÁâáÊï∞ÈáèÔºà1-10Ôºâ
- **`response_format`**: `&quot;b64_json&quot;` Êàñ `&quot;url&quot;`ÔºàData URIÔºâ

#### ÊñπÂºè‰∫åÔºöChat API + ÂèÇÊï∞ËÆæÁΩÆ (‚ú® Êñ∞Â¢û)

**ÊâÄÊúâÂçèËÆÆ**ÔºàOpenAI„ÄÅClaudeÔºâÁöÑ Chat API Áé∞Âú®ÈÉΩÊîØÊåÅÁõ¥Êé•‰º†ÈÄí `size` Âíå `quality` ÂèÇÊï∞Ôºö

```python
# OpenAI Chat API
response = client.chat.completions.create(
    model=&quot;gemini-3-pro-image&quot;,
    size=&quot;1920x1080&quot;,      # ‚úÖ ÊîØÊåÅ‰ªªÊÑè WIDTHxHEIGHT Ê†ºÂºè
    quality=&quot;hd&quot;,          # ‚úÖ &quot;standard&quot; | &quot;hd&quot; | &quot;medium&quot;
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;‰∏ÄÂ∫ßÊú™Êù•‰∏ª‰πâÈ£éÊ†ºÁöÑÂüéÂ∏Ç&quot;}]
)
```

```bash
# Claude Messages API
curl -X POST http://127.0.0.1:8045/v1/messages \
  -H &quot;Content-Type: application/json&quot; \
  -H &quot;x-api-key: sk-antigravity&quot; \
  -d &#039;{
    &quot;model&quot;: &quot;gemini-3-pro-image&quot;,
    &quot;size&quot;: &quot;1280x720&quot;,
    &quot;quality&quot;: &quot;hd&quot;,
    &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;‰∏ÄÂè™ÂèØÁà±ÁöÑÁå´Âí™&quot;}]
  }&#039;
```

```

**ÂèÇÊï∞‰ºòÂÖàÁ∫ß**: `imageSize` ÂèÇÊï∞ &gt; `quality` ÂèÇÊï∞ &gt; Ê®°ÂûãÂêéÁºÄ

**‚ú® Êñ∞Â¢û `imageSize` ÂèÇÊï∞ÊîØÊåÅ**:

Èô§‰∫Ü `quality` ÂèÇÊï∞Â§ñ,Áé∞Âú®ËøòÊîØÊåÅÁõ¥Êé•‰ΩøÁî® Gemini ÂéüÁîüÁöÑ `imageSize` ÂèÇÊï∞:

```python
# ‰ΩøÁî® imageSize ÂèÇÊï∞(ÊúÄÈ´ò‰ºòÂÖàÁ∫ß)
response = client.chat.completions.create(
    model=&quot;gemini-3-pro-image&quot;,
    size=&quot;16:9&quot;,           # ÂÆΩÈ´òÊØî
    imageSize=&quot;4K&quot;,        # ‚ú® Áõ¥Êé•ÊåáÂÆöÂàÜËæ®Áéá: &quot;1K&quot; | &quot;2K&quot; | &quot;4K&quot;
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;‰∏ÄÂ∫ßÊú™Êù•‰∏ª‰πâÈ£éÊ†ºÁöÑÂüéÂ∏Ç&quot;}]
)
```

```bash
# Claude Messages API ‰πüÊîØÊåÅ imageSize
curl -X POST http://127.0.0.1:8045/v1/messages \
  -H &quot;Content-Type: application/json&quot; \
  -H &quot;x-api-key: sk-antigravity&quot; \
  -d &#039;{
    &quot;model&quot;: &quot;gemini-3-pro-image&quot;,
    &quot;size&quot;: &quot;1280x720&quot;,
    &quot;imageSize&quot;: &quot;4K&quot;,
    &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;‰∏ÄÂè™ÂèØÁà±ÁöÑÁå´Âí™&quot;}]
  }&#039;
```

**ÂèÇÊï∞ËØ¥Êòé**:
- **`imageSize`**: Áõ¥Êé•ÊåáÂÆöÂàÜËæ®Áéá (`&quot;1K&quot;` / `&quot;2K&quot;` / `&quot;4K&quot;`)
- **`quality`**: ÈÄöËøáË¥®ÈáèÁ≠âÁ∫ßÊé®Êñ≠ÂàÜËæ®Áéá (`&quot;standard&quot;` ‚Üí 1K, `&quot;medium&quot;` ‚Üí 2K, `&quot;hd&quot;` ‚Üí 4K)
- **‰ºòÂÖàÁ∫ß**: Â¶ÇÊûúÂêåÊó∂ÊåáÂÆö `imageSize` Âíå `quality`,Á≥ªÁªü‰ºö‰ºòÂÖà‰ΩøÁî® `imageSize`


#### ÊñπÂºè‰∏âÔºöChat Êé•Âè£ + Ê®°ÂûãÂêéÁºÄ
```python
response = client.chat.completions.create(
    model=&quot;gemini-3-pro-image-16-9-4k&quot;,  # Ê†ºÂºèÔºögemini-3-pro-image-[ÊØî‰æã]-[Ë¥®Èáè]
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;‰∏ÄÂ∫ßÊú™Êù•‰∏ª‰πâÈ£éÊ†ºÁöÑÂüéÂ∏Ç&quot;}]
)
```

**Ê®°ÂûãÂêéÁºÄËØ¥Êòé**Ôºö
- **ÂÆΩÈ´òÊØî**: `-16-9`, `-9-16`, `-4-3`, `-3-4`, `-21-9`, `-1-1`
- **Ë¥®Èáè**: `-4k` (4K), `-2k` (2K), ‰∏çÂä†ÂêéÁºÄÔºàÊ†áÂáÜÔºâ
- **Á§∫‰æã**: `gemini-3-pro-image-16-9-4k` ‚Üí 16:9 ÊØî‰æã + 4K ÂàÜËæ®Áéá

#### ÊñπÂºèÂõõÔºöCherry Studio Á≠âÂÆ¢Êà∑Á´ØËÆæÁΩÆ
Âú®ÊîØÊåÅ OpenAI ÂçèËÆÆÁöÑÂÆ¢Êà∑Á´ØÔºàÂ¶Ç Cherry StudioÔºâ‰∏≠ÔºåÂèØ‰ª•ÈÄöËøá**Ê®°ÂûãËÆæÁΩÆ**È°µÈù¢ÈÖçÁΩÆÂõæÁâáÁîüÊàêÂèÇÊï∞Ôºö

1. **ËøõÂÖ•Ê®°ÂûãËÆæÁΩÆ**ÔºöÈÄâÊã© `gemini-3-pro-image` Ê®°Âûã
2. **ÈÖçÁΩÆÂèÇÊï∞**Ôºö
   - **Size (Â∞∫ÂØ∏)**: ËæìÂÖ•‰ªªÊÑè `WIDTHxHEIGHT` Ê†ºÂºèÔºàÂ¶Ç `1920x1080`, `1024x1024`Ôºâ
   - **Quality (Ë¥®Èáè)**: ÈÄâÊã© `standard` / `hd` / `medium`
   - **Number (Êï∞Èáè)**: ËÆæÁΩÆÁîüÊàêÂõæÁâáÊï∞ÈáèÔºà1-10Ôºâ
3. **ÂèëÈÄÅËØ∑Ê±Ç**ÔºöÁõ¥Êé•Âú®ÂØπËØùÊ°Ü‰∏≠ËæìÂÖ•ÂõæÁâáÊèèËø∞Âç≥ÂèØ

**ÂèÇÊï∞Êò†Â∞ÑËßÑÂàô**Ôºö
- `size: &quot;1920x1080&quot;` ‚Üí Ëá™Âä®ËÆ°ÁÆó‰∏∫ `16:9` ÂÆΩÈ´òÊØî
- `quality: &quot;hd&quot;` ‚Üí Êò†Â∞Ñ‰∏∫ `4K` ÂàÜËæ®Áéá
- `quality: &quot;medium&quot;` ‚Üí Êò†Â∞Ñ‰∏∫ `2K` ÂàÜËæ®Áéá


## üìù ÂºÄÂèëËÄÖ‰∏éÁ§æÂå∫

*   **ÁâàÊú¨ÊºîËøõ (Changelog)**:
    *   **v4.1.26 (2026-02-27)**:
        -   **[ÂäüËÉΩÂ¢ûÂº∫] ‰ºòÂåñÈÖçÈ¢ùÂà∑Êñ∞ÈÄªËæëÔºåÊîØÊåÅÂêåÊ≠•Á¶ÅÁî®Ë¥¶Âè∑**:
            -   **ÈÄªËæëÊîæÂÆΩ**: ‚ÄúÂà∑Êñ∞ÊâÄÊúâ‚ÄùÂíå‚ÄúÊâπÈáèÂà∑Êñ∞‚ÄùÁé∞Âú®‰∏çÂÜçË∑≥ËøáÊ†áËÆ∞‰∏∫ `disabled` Êàñ `proxy_disabled` ÁöÑË¥¶Âè∑„ÄÇ
            -   **Ëá™Âä®ÊÅ¢Â§ç**: ÂÖÅËÆ∏ÈÄöËøáÂà∑Êñ∞Êìç‰ΩúÂ∞ùËØïÈáçÊñ∞ÊøÄÊ¥ªÂõ† Token ËøáÊúüÊàñ‰∏¥Êó∂ÈîôËØØËÄåË¢´Á¶ÅÁî®ÁöÑË¥¶Âè∑ÔºåÊèêÂçá‰∫ÜÂ§öË¥¶Âè∑ÁÆ°ÁêÜÁöÑÁÅµÊ¥ªÊÄß„ÄÇ
        -   **[Ê†∏ÂøÉ‰øÆÂ§ç] ‰øÆÂ§ç Windows Á≥ªÁªü‰∏ãÂêéÂè∞‰ªªÂä°ÂØºËá¥ cmd ÈªëÊ°ÜÈó™ÁÉÅÁöÑÈóÆÈ¢ò**:
            -   **ÈùôÈªòÊâßË°å**: ÈÄöËøá‰∏∫ `std::process::Command` Â∞ÅË£ÖÊ≥®ÂÖ• `CREATE_NO_WINDOW` Ê†áÂøóÔºåËß£ÂÜ≥‰∫ÜÂú® Windows Á´ØÂ∫îÁî®Â∫ïÂ±ÇÁªÑ‰ª∂ÔºàÂ¶ÇÁâàÊú¨Êé¢Êµã„ÄÅÈáçÂêØÊõ¥Êñ∞Á≠âÔºâË∞ÉÁî®Á≥ªÁªüÂëΩ‰ª§Êó∂ÂºïÂèëÁöÑÂëΩ‰ª§Ë°åÁ™óÂè£‰∏ÄÈó™ËÄåËøáÁöÑËßÜËßâÂπ≤Êâ∞ÔºåÁ°Æ‰øùÂÖ®ËøáÁ®ãÊó†ËæπÊ°ÜÈùôÈªòÊâßË°å„ÄÇ
    *   **v4.1.25 (2026-02-27)**:
        -   **[Ê†∏ÂøÉÂäüËÉΩ] Âä®ÊÄÅÁîªÂõæÊ®°Âûã‰∏éÊñ∞Êû∂ÊûÑÊîØÊåÅ**:
            -   **Âä®ÊÄÅËß£Êûê**: ÁßªÈô§‰∫ÜÈíàÂØπ `gemini-3-pro-image` ÁöÑÁ°¨ÁºñÁ†ÅÈôêÂà∂„ÄÇÈÄöËøáÊñ∞Â¢ûÁöÑ `clean_image_model_name` Êô∫ËÉΩÊ∏ÖÊ¥óÂêéÁºÄÔºàÂ¶Ç `-4k`, `-16x9`ÔºâÔºåÂÖ®Èù¢ÂÖºÂÆπÂ¶Ç `gemini-3.1-flash-image` Á≠â‰ªªÊÑèÊú™Êù•Êñ∞Â¢ûÁöÑÁîªÂõæÊ®°Âûã„ÄÇ
            -   **ÈÖçÈ¢ùËá™ÈÄÇÂ∫î**: ‰ºòÂåñ‰∫Ü `normalize_to_standard_id`Ôºå‰ΩøÁî® `image` ÂÖ≥ÈîÆËØçÂÆΩÊ≥õÂåπÈÖçÔºåÁ°Æ‰øùÊñ∞Ê®°Âûã‰πüËÉΩÊ≠£Á°ÆËß¶ÂèëÈÖçÈ¢ù‰øùÊä§Êú∫Âà∂„ÄÇ
        -   **[Ê†∏ÂøÉÂäüËÉΩ] ËÅäÂ§©Êé•Âè£ (Chat Completions) ÁîªÂõæÊã¶Êà™ÊîØÊåÅ**:
            -   **Ë∑®ÁïåËûçÂêà**: OpenAI Âíå Claude ÂçèËÆÆÁöÑÂØπËØùÊµÅÁé∞Âú®ËÉΩÊô∫ËÉΩÊé¢ÊµãÁîªÂÉèÁîüÊàêÊÑèÂõæ„ÄÇÂΩì‰ΩøÁî®Â∏¶Êúâ `image` ÁöÑÊ®°ÂûãÂêçÊó∂ÔºåÁ≥ªÁªü‰ºöÂ∞ÜÂ∏∏ËßÑÊñáÊú¨ÁîüÊàêËØ∑Ê±ÇÈùôÈªòËΩ¨ÁßªÁªôÈ´òÁ∫ßÁîªÂõæÂºïÊìé„ÄÇ
            -   **ÊµÅÂºèÂõûÊòæ**: ÁîüÊàêÂÆåÊàêÂêéÔºåÈÄöËøá Markdown Ê†ºÂºèÔºà`![Generated Image](url)`Ôºâ‰ª• SSE ÊµÅÂºèËøîÂõûÂõæÁâáÈìæÊé•ÔºåÂÆåÁæéÈÄÇÈÖçÊâÄÊúâÊîØÊåÅ Markdown ÁöÑËÅäÂ§©ÂÆ¢Êà∑Á´Ø„ÄÇ
        -   **[Ê†∏ÂøÉ‰øÆÂ§ç] ÂΩªÂ∫ï‰øÆÂ§çÁîªÂõæÈáçÂÆöÂêë 404 ‰∏éÂèÇÊï∞Á©øÈÄèÂ§±Êïà**:
            -   **404 ÁßªÈô§**: ÁßªÈô§‰∫ÜÂ∫ïÂ±ÇË∞ÉÁî®‰∏≠ÊÆãÁïôÁöÑÊóßÊ®°ÂûãÁ°¨ÁºñÁ†ÅÔºåÊ†πÈô§Âõ†Ê®°Âûã‰ø°ÊÅØ‰∏ç‰∏ÄËá¥ÂØºËá¥ÁöÑ 404 Not Found Â¥©Ê∫ÉÂèäË¥¶Âè∑ÂèóÊçü„ÄÇ
            -   **Á≤æÂáÜÂèÇÊï∞ÁªßÊâø**: ‰øÆÂ§ç‰∫ÜÊú™‰º†ÂèÇÊï∞Êó∂Á≥ªÁªüÂº∫Âà∂Â°ûÂÖ•ÈªòËÆ§ `1024x1024` ÁöÑË°å‰∏∫„ÄÇÁé∞Âú®ÔºåÂ¶ÇÊûúÊ®°ÂûãÂêçÂ∏¶ÊúâÂêéÁºÄÔºàÂ¶Ç `gemini-3-pro-image-16x9-4k`ÔºâÔºåÂêéÂè∞‰ºö‰∏•Ê†º‰ºòÂÖàËß£ÊûêÂêéÁºÄÂàÜËæ®ÁéáËøõË°åÁ©øÈÄèÁªòÂõæ„ÄÇ
    *   **v4.1.24 (2026-02-26)**:
        -   **[ÂäüËÉΩË∞ÉÊï¥] Á¶ÅÁî®Ëá™Âä®È¢ÑÁÉ≠Ë∞ÉÂ∫¶Á®ãÂ∫èÔºå‰øùÁïôÊâãÂä®È¢ÑÁÉ≠**:
            -   **ÂèòÊõ¥ËØ¥Êòé**: ‰∏∫‰∫ÜÂáèÂ∞ë‰∏çÂøÖË¶ÅÁöÑÂêéÂè∞ËµÑÊ∫êÂç†Áî®ÔºåÊú¨ÁâàÊú¨Â∑≤Ê≥®ÈáäÊéâËá™Âä®È¢ÑÁÉ≠ÔºàSmart WarmupÔºâÁöÑÂêéÂè∞Ë∞ÉÂ∫¶ÈÄªËæë„ÄÇ
            -   **ËÆæÁΩÆÈöêËóè**: ËÆæÁΩÆÈ°µÈù¢‰∏≠ÁöÑ‚ÄúÊô∫ËÉΩÈ¢ÑÁÉ≠‚ÄùÈÖçÁΩÆÈ°πÂ∑≤ÈöêËóè„ÄÇ
            -   **ÊâãÂä®‰øùÁïô**: Ë¥¶Âè∑ÁÆ°ÁêÜÈ°µÈù¢ÁöÑÊâãÂä®È¢ÑÁÉ≠ÂäüËÉΩ‰øùÊåÅ‰∏çÂèòÔºå‰ªçÂèØÊ≠£Â∏∏‰ΩøÁî®„ÄÇ
            -   **ÊÅ¢Â§çÊåáÂºï**: Â¶ÇÊûúÊÇ®ÈúÄË¶ÅËá™Âä®È¢ÑÁÉ≠ÂäüËÉΩÔºåÂèØ‰ª•Ëá™Ë°åÊãâÂèñÊú¨È°πÁõÆÊ∫ê‰ª£Á†ÅÔºåÂú® `src-tauri/src/lib.rs` ‰∏≠ÂèñÊ∂à `start_scheduler` ÁöÑÊ≥®ÈáäÂπ∂Ëß£Èô§ `Settings.tsx` ‰∏≠Áõ∏ÂÖ≥ UI ÁöÑÊ≥®ÈáäÂêéÈáçÊñ∞ÁºñËØë‰ΩøÁî®„ÄÇ
        -   **[Ê†∏ÂøÉ‰øÆÂ§ç] Êô∫ËÉΩÁâàÊú¨ÊåáÁ∫πÈÄâÊã©‰∏éÂêØÂä® Panic ‰øÆÂ§ç (Issue #2123)**:
            -   **ÈóÆÈ¢òÊ†πÊ∫ê**: 1) `constants.rs` ‰∏≠ÁöÑ `KNOWN_STABLE_VERSION` Á°¨ÁºñÁ†Å‰∫Ü‰ΩéÁâàÊú¨Âè∑ÔºåÂΩìÊú¨Âú∞ IDE Ê£ÄÊµãÂ§±Ë¥•Êó∂ÂõûÈÄÄËØ•ÁâàÊú¨‰Ωú‰∏∫ËØ∑Ê±ÇÂ§¥ÔºåÂØºËá¥ Google ÊãíÁªù Gemini 3.1 Pro Ê®°Âûã„ÄÇ2) Êñ∞Â¢ûÁöÑËøúÁ´ØÁâàÊú¨ÁΩëÁªúË∞ÉÁî®Áõ¥Êé•Âú® `LazyLock` ÂàùÂßãÂåñÔºàTokio ÂºÇÊ≠•‰∏ä‰∏ãÊñáÔºâ‰∏≠ÊâßË°åÔºåÂØºËá¥ `Cannot block the current thread` ‰∏•ÈáçÂ¥©Ê∫É„ÄÇ
            -   **‰øÆÂ§çÊñπÊ°à**: 1) ÂºïÂÖ•&quot;Êô∫ËÉΩÊúÄÂ§ßÁâàÊú¨&quot;Á≠ñÁï• `max(Êú¨Âú∞ÁâàÊú¨, ËøúÁ´ØÁâàÊú¨, 4.1.26)`ÔºåÂßãÁªàÂèñÊúÄÈ´òÂÄº„ÄÇ2) Â∞ÜÁΩëÁªúÊé¢ÊµãÈÄªËæëÁßªËá≥Áã¨Á´ã OS Á∫øÁ®ãÂπ∂ÈÖçÂêà `mpsc` ÈÄöÈÅìÔºåÂÆâÂÖ®ÈÅøÂºÄÂºÇÊ≠•ËøêË°åÊó∂ÈôêÂà∂„ÄÇ‰øùËØÅÊó†ËÆ∫Êú¨Âú∞ÁâàÊú¨Êñ∞ÊóßÔºåÊåáÁ∫πÂùá‰∏ç‰Ωé‰∫é‰∏äÊ∏∏Ë¶ÅÊ±ÇÔºå‰∏îÂ∫îÁî®ËÉΩÁ®≥ÂÆöÂêØÂä®„ÄÇ
        -   **[Ê†∏ÂøÉ‰øÆÂ§ç] Âä®ÊÄÅÊ®°Âûã maxOutputTokens ÈôêÈ¢ùÁ≥ªÁªü (Êõø‰ª£ PR #2119 Á°¨ÁºñÁ†ÅÊñπÊ°à)**:
            -   **ÈóÆÈ¢òÊ†πÊ∫ê**: ÈÉ®ÂàÜÂÆ¢Êà∑Á´ØÂèëÈÄÅÁöÑ `maxOutputTokens` Ë∂ÖËøáÊ®°ÂûãÁâ©ÁêÜ‰∏äÈôêÔºàÂ¶Ç Flash ÈôêÂà∂ 64kÔºâÔºåÂØºËá¥‰∏äÊ∏∏ËøîÂõû 400 ÈîôËØØ„ÄÇ
            -   **‰∏âÂ±ÇÈôêÈ¢ùÊû∂ÊûÑ**:
                -   **Á¨¨‰∏ÄÂ±ÇÔºàÂä®ÊÄÅ‰ºòÂÖàÔºâ**: ÂÆûÊó∂ËØªÂèñË¥¶Âè∑ `quota.models` Êï∞ÊçÆ„ÄÇ
                -   **Á¨¨‰∫åÂ±ÇÔºàÈùôÊÄÅÈªòËÆ§Ë°®Ôºâ**: `model_limits.rs` ÂÜÖÁΩÆÂ∑≤Áü•ÈôêÈ¢ùÔºàÂ¶Ç Flash 65536Ôºâ„ÄÇ
                -   **Á¨¨‰∏âÂ±ÇÔºàÂÖ®Â±ÄÂÖúÂ∫ïÔºâ**: ÈªòËÆ§ 131072„ÄÇ
            -   **ÂÆûÁé∞ÁªÜËäÇ**: Âú® `wrap_request()` ‰∏≠Ê≥®ÂÖ•Ë£ÅÂâ™ÈÄªËæëÔºåÁ°Æ‰øùËØ∑Ê±ÇÂèÇÊï∞ÂêàÊ≥ï„ÄÇ
    *   **v4.1.23 (2026-02-25)**:
        -   **[ÂÆâÂÖ®Â¢ûÂº∫] ‰ºòÂåñ‰∏éÂéüÁîüÂØπÈΩêÂ∫îÁî®Â±Ç‰∏éÂ∫ïÂ±ÇÁâπÂæÅÊåáÁ∫πÔºåÊèêÂçáËØ∑Ê±ÇÁ®≥ÂÆöÊÄß‰∏éÈò≤Êã¶Êà™ËÉΩÂäõ„ÄÇ**
        -   **[Ê†∏ÂøÉ‰øÆÂ§ç] Â∞Ü v1beta thinkingLevel ËΩ¨Êç¢‰∏∫ v1internal thinkingBudget (PR #2095)**:
            -   **ÈóÆÈ¢òÊ†πÊ∫ê**: OpenClaw„ÄÅCline Á≠âÂÆ¢Êà∑Á´ØÂèëÈÄÅ v1beta Ê†ºÂºèÁöÑ `thinkingLevel` Â≠óÁ¨¶‰∏≤Ôºà`&quot;NONE&quot;` / `&quot;LOW&quot;` / `&quot;MEDIUM&quot;` / `&quot;HIGH&quot;`ÔºâÂà∞ `generationConfig.thinkingConfig`„ÄÇÂΩì AGM ÈÄöËøá Google v1internal API ‰ª£ÁêÜËØ∑Ê±ÇÊó∂ÔºåGoogle ‰ºöÂõ†‰∏∫ v1internal ‰ªÖÊé•ÂèóÊï∞Â≠óÂûã `thinkingBudget` ËÄåÊãíÁªùËØ∑Ê±ÇÔºåËøîÂõû `400 INVALID_ARGUMENT`„ÄÇ
            -   **‰øÆÂ§çÊñπÊ°à**: Âú® `wrap_request()` ÁöÑÁé∞Êúâ budget Â§ÑÁêÜÈÄªËæë‰πãÂâçÔºåÊñ∞Â¢û‰∏Ä‰∏™Êó©ÊúüËΩ¨Êç¢Ê≠•È™§ÔºöÊ£ÄÊµã `thinkingLevel` Â≠óÁ¨¶‰∏≤ÔºåÂ∞ÜÂÖ∂Êò†Â∞Ñ‰∏∫ÂØπÂ∫îÁöÑÊï∞Â≠ó `thinkingBudget`Ôºà`NONE`‚Üí0, `LOW`‚Üí4096, `MEDIUM`‚Üí8192, `HIGH`‚Üí24576ÔºâÔºåÁÑ∂ÂêéÂà†Èô§ `thinkingLevel` Â≠óÊÆµÂπ∂ÂÜôÂÖ• `thinkingBudget`ÔºåÁ°Æ‰øù‰∏ãÊ∏∏ÊâÄÊúâ budget Â§ÑÁêÜÈÄªËæëÔºàÈ¢ÑÁÆóÂ∞ÅÈ°∂„ÄÅ`maxOutputTokens` Ë∞ÉÊï¥„ÄÅËá™ÈÄÇÂ∫îÊ£ÄÊµãÔºâÈÉΩËÉΩÁúãÂà∞Ê≠£Á°ÆÁöÑÊï∞ÂÄºÈ¢ÑÁÆó„ÄÇ
            -   **ÊµãËØï**: Â∑≤È™åËØÅ OpenClaw ÂèëÈÄÅ `thinkingLevel: &quot;LOW&quot;` Âà∞ `gemini-3.1-pro-high`ÔºàGemini ÂéüÁîüÂçèËÆÆÔºâÔºåËØ∑Ê±ÇÁé∞ËøîÂõû `200 OK`Ôºå‰∏çÂÜçÊä• 400 ÈîôËØØ„ÄÇ
        -   **[Ê†∏ÂøÉ‰øÆÂ§ç] Ë¥¶Âè∑Êï∞ÊçÆÊçüÂùè‰∏éÂêéÂè∞‰ªªÂä°Êó†ÈôêÂæ™ÁéØ‰øÆÂ§ç (PR #2094)**:
            -   **ÈóÆÈ¢òÊ†πÊ∫ê**: ÂΩìÁî®Êà∑Âú®ËÆæÁΩÆ‰∏≠ËæìÂÖ•ËøáÂ§ßÁöÑÂà∑Êñ∞Èó¥ÈöîÂÄºÔºàÂ¶Ç 999999999ÔºâÊó∂Ôºå`interval * 60 * 1000` Ë∂ÖËøá JS ÂºïÊìé 32 ‰ΩçÊúâÁ¨¶Âè∑Êï¥Êï∞‰∏äÈôê `2,147,483,647ms`ÔºåÊµèËßàÂô®‰ºöÂ∞Ü `setInterval` Âª∂ËøüÈùôÈªòÊà™Êñ≠‰∏∫ 1msÔºåÂØºËá¥ÂâçÁ´ØÊØèÁßíËß¶ÂèëÊï∞ÂçÉÊ¨° `refreshAllQuotas`/`syncAccountFromDb` ËØ∑Ê±ÇÔºåËøõËÄåÂºïÂèëÂ§öÁ∫øÁ®ãÂπ∂ÂèëÂÜôÂêå‰∏Ä `[uuid].json` Êñá‰ª∂ÔºåÈÄ†ÊàêÂ≠óËäÇÊµÅ‰∫§Èîô„ÄÅJSON Â∞æÈÉ®ÊÆãÁïôÔºåË¥¶Âè∑Êï∞ÊçÆÊ∞∏‰πÖÊçüÂùè„ÄÇ
            -   **ÂéüÂ≠êÊñá‰ª∂ÂÜôÂÖ• (`account.rs`)**: `save_account` Êîπ‰∏∫ÂÖàÂÜôÂÖ• UUID ÂêéÁºÄÁöÑ‰∏¥Êó∂Êñá‰ª∂ÔºåÂÜçÈÄöËøá `fs::rename`ÔºàPOSIXÔºâ/ `MoveFileExW`ÔºàWindowsÔºâÂéüÂ≠êÊõøÊç¢ÁõÆÊ†áÊñá‰ª∂Ôºå‰∏éÂ∑≤ÊúâÁöÑ `save_account_index` ‰øùÊåÅ‰∏ÄËá¥Ôºå‰ªéÊ†πÊú¨‰∏äÊ∂àÈô§Âπ∂ÂèëÂÜôÂØºËá¥ÁöÑ JSON ÊçüÂùè„ÄÇ
            -   **setInterval Ê∫¢Âá∫‰øùÊä§ (`BackgroundTaskRunner.tsx`)**: ÂØπ `refresh_interval` Âíå `sync_interval` ‰∏§‰∏™ÂÆöÊó∂Âô®ÁöÑÂª∂ËøüÂèÇÊï∞Âä†‰∏ä `Math.min(..., 2147483647)` ‰∏äÁïåÈôêÂà∂ÔºåÈò≤Ê≠¢Ë∂ÖËøá INT32_MAX ÂêéË¢´ÊµèËßàÂô®Êà™Êñ≠‰∏∫ 1ms Êó†ÈôêÂæ™ÁéØ„ÄÇ
            -   **ËæìÂÖ•È™åËØÅ (`Settings.tsx`)**: Â∞Ü `refresh_interval` Âíå `sync_interval` ËæìÂÖ•Ê°ÜÁöÑ `max` Â±ûÊÄß‰ªé `60` Êõ¥Êñ∞‰∏∫ `35791`Ôºà35791 min √ó 60000 &lt; INT32_MAXÔºâÔºåÂπ∂Âú® `onChange` ‰∏≠Ê∑ªÂä† `NaN` fallbackÔºàÈªòËÆ§‰∏∫ 1ÔºâÂèäËåÉÂõ¥Â§πÁ¥ß `[1, 35791]`Ôºå‰ªéÊ∫êÂ§¥ÈòªÊñ≠ÈùûÊ≥ïÂÄºËæìÂÖ•„ÄÇ
        -   **[Ê†∏ÂøÉ‰ºòÂåñ] OAuth Êç¢Á•®‰∏ìÂ±ûÔºöÂâîÈô§ JA3 ÊåáÁ∫π‰∏éÂä®ÊÄÅ User-Agent ‰º™Ë£Ö**:
            -   **Á∫ØÂáÄËØ∑Ê±Ç**: ‰ªÖÈíàÂØπ `exchange_code`ÔºàÈ¶ñÊ¨°ÊéàÊùÉÔºâÂíå `refresh_access_token`ÔºàÈùôÈªòÁª≠ÊúüÔºâÁöÑÊç¢Á•®ËØ∑Ê±ÇÔºåÁßªÈô§‰∫ÜÂ∫ïÂ±ÇÁΩëÁªúÂ∫ìÁöÑ Chrome JA3 ÊåáÁ∫π‰º™Ë£ÖÔºåÊÅ¢Â§çÊ†áÂáÜÁ∫ØÂáÄÁöÑ TLSÁâπÂæÅ„ÄÇ
            -   **Âä®ÊÄÅ UA**: Êç¢Á•®Êó∂Ëá™Âä®ÊèêÂèñÁºñËØëÊó∂ÁâàÊú¨Âè∑ (`CURRENT_VERSION`) ÊûÑÂª∫‰∏ìÂ±ûÁöÑ `User-Agent`ÔºàÂ¶Ç `vscode/1.X.X (Antigravity/4.1.26)`ÔºâÔºå‰ª•ÂåπÈÖçÁ∫ØÂáÄ TLS ÈìæË∑Ø„ÄÇ
        -   **[ÂäüËÉΩÂ¢ûÂº∫] API Âèç‰ª£È°µÈù¢‰∏éËÆæÁΩÆÈ°µÊ®°ÂûãÂàóË°®ÂÖ®Èù¢Êé•ÂÖ•Âä®ÊÄÅÊ®°ÂûãÊï∞ÊçÆ**:
            -   **ÈóÆÈ¢òÊ†πÊ∫ê**: &quot;API Âèç‰ª£ ‚Üí ÊîØÊåÅÊ®°Âûã‰∏éÈõÜÊàê&quot;ÂàóË°®‰∏é&quot;Ê®°ÂûãË∑ØÁî±‰∏≠ÂøÉ&quot;ÁöÑÁõÆÊ†áÊ®°ÂûãÈÄâÊã©‰∏ãÊãâÊ°ÜÔºå‰ª•Âèä&quot;ËÆæÁΩÆ ‚Üí Âõ∫ÂÆöÈÖçÈ¢ùÊ®°Âûã&quot;ÂàóË°®ÔºåÊ≠§ÂâçÂùá‰ªÖ‰ªéÈùôÊÄÅ `MODEL_CONFIG` ËØªÂèñÁ°¨ÁºñÁ†ÅÊ®°Âûã‰ø°ÊÅØÔºåÂØºËá¥Ë¥¶Âè∑ÂÆûÈôÖ‰∏ãÂèëÁöÑÂä®ÊÄÅÊñ∞Ê®°ÂûãÔºàÂ¶Ç `GPT-OSS 120B`„ÄÅ`Gemini 3.1 Pro (High)` Á≠âÔºâÊó†Ê≥ïÂá∫Áé∞Âú®Ëøô‰∫õÂàóË°®‰∏≠„ÄÇ
            -   **‰øÆÂ§çÊñπÊ°à**:
                -   ÈáçÊûÑ `useProxyModels` HookÔºö‰ª•Ë¥¶Âè∑ `quota.models` Âä®ÊÄÅÊï∞ÊçÆ‰∏∫Á¨¨‰∏Ä‰ºòÂÖàÊï∞ÊçÆÊ∫êÔºåËÅöÂêàÊâÄÊúâË¥¶Âè∑ÈáåÊâÄÊúâÊ®°ÂûãÁöÑ `display_name`Ôºà‰∏∫‰∏ªÂ±ïÁ§∫ÂêçÁß∞ÔºâÂíå `name`Ôºà‰∏∫Ê®°Âûã IDÔºâÔºõ`MODEL_CONFIG` ‰ªÖ‰Ωú‰∏∫ÂõæÊ†á/ÂàÜÁªÑÁöÑÊ†∑ÂºèË°•ÂÖÖÔºå‰ª•ÂèäÊó†Ë¥¶Âè∑Êï∞ÊçÆÊó∂ÁöÑÈùôÊÄÅÂÖúÂ∫ï„ÄÇ
                -   Êñ∞Â¢ûËá™Âä®ÊáíÂä†ËΩΩÈÄªËæëÔºö`ApiProxy` È°µÈù¢Êú¨Ë∫´‰∏çË∞ÉÁî® `fetchAccounts`ÔºåÁé∞Âú® Hook ÂÜÖÈÉ®Ê£ÄÊµãÂà∞ store ‰∏∫Á©∫Êó∂Ëá™Âä®Ëß¶ÂèëÔºå‰øùËØÅÂä®ÊÄÅÊ®°ÂûãÂú®‰ªªÊÑèÂØºËà™Ë∑ØÂæÑ‰∏ãÂùáÂèØÊ≠£Â∏∏Â±ïÁ§∫„ÄÇ
                -   ÈáçÊûÑ `PinnedQuotaModels` ÁªÑ‰ª∂ÔºöÈááÁî®ÂêåÁ≠âÁ≠ñÁï•Ôºå‰ªé `useAccountStore` ÊãâÂèñÂÖ®Ë¥¶Âè∑Âä®ÊÄÅÊ®°ÂûãÔºåÂπ∂‰øÆÂ§ç‰∫ÜÂ∑≤Âõ∫ÂÆöÁöÑ &quot;thinking&quot; 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[openai/codex]]></title>
            <link>https://github.com/openai/codex</link>
            <guid>https://github.com/openai/codex</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:59 GMT</pubDate>
            <description><![CDATA[Lightweight coding agent that runs in your terminal]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/openai/codex">openai/codex</a></h1>
            <p>Lightweight coding agent that runs in your terminal</p>
            <p>Language: Rust</p>
            <p>Stars: 62,275</p>
            <p>Forks: 8,274</p>
            <p>Stars today: 228 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;code&gt;npm i -g @openai/codex&lt;/code&gt;&lt;br /&gt;or &lt;code&gt;brew install --cask codex&lt;/code&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;Codex CLI&lt;/strong&gt; is a coding agent from OpenAI that runs locally on your computer.
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/openai/codex/blob/main/.github/codex-cli-splash.png&quot; alt=&quot;Codex CLI splash&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;
&lt;/br&gt;
If you want Codex in your code editor (VS Code, Cursor, Windsurf), &lt;a href=&quot;https://developers.openai.com/codex/ide&quot;&gt;install in your IDE.&lt;/a&gt;
&lt;/br&gt;If you want the desktop app experience, run &lt;code&gt;codex app&lt;/code&gt; or visit &lt;a href=&quot;https://chatgpt.com/codex?app-landing-page=true&quot;&gt;the Codex App page&lt;/a&gt;.
&lt;/br&gt;If you are looking for the &lt;em&gt;cloud-based agent&lt;/em&gt; from OpenAI, &lt;strong&gt;Codex Web&lt;/strong&gt;, go to &lt;a href=&quot;https://chatgpt.com/codex&quot;&gt;chatgpt.com/codex&lt;/a&gt;.&lt;/p&gt;

---

## Quickstart

### Installing and running Codex CLI

Install globally with your preferred package manager:

```shell
# Install using npm
npm install -g @openai/codex
```

```shell
# Install using Homebrew
brew install --cask codex
```

Then simply run `codex` to get started.

&lt;details&gt;
&lt;summary&gt;You can also go to the &lt;a href=&quot;https://github.com/openai/codex/releases/latest&quot;&gt;latest GitHub Release&lt;/a&gt; and download the appropriate binary for your platform.&lt;/summary&gt;

Each GitHub Release contains many executables, but in practice, you likely want one of these:

- macOS
  - Apple Silicon/arm64: `codex-aarch64-apple-darwin.tar.gz`
  - x86_64 (older Mac hardware): `codex-x86_64-apple-darwin.tar.gz`
- Linux
  - x86_64: `codex-x86_64-unknown-linux-musl.tar.gz`
  - arm64: `codex-aarch64-unknown-linux-musl.tar.gz`

Each archive contains a single entry with the platform baked into the name (e.g., `codex-x86_64-unknown-linux-musl`), so you likely want to rename it to `codex` after extracting it.

&lt;/details&gt;

### Using Codex with your ChatGPT plan

Run `codex` and select **Sign in with ChatGPT**. We recommend signing into your ChatGPT account to use Codex as part of your Plus, Pro, Team, Edu, or Enterprise plan. [Learn more about what&#039;s included in your ChatGPT plan](https://help.openai.com/en/articles/11369540-codex-in-chatgpt).

You can also use Codex with an API key, but this requires [additional setup](https://developers.openai.com/codex/auth#sign-in-with-an-api-key).

## Docs

- [**Codex Documentation**](https://developers.openai.com/codex)
- [**Contributing**](./docs/contributing.md)
- [**Installing &amp; building**](./docs/install.md)
- [**Open source fund**](./docs/open-source-fund.md)

This repository is licensed under the [Apache-2.0 License](LICENSE).
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[BloopAI/vibe-kanban]]></title>
            <link>https://github.com/BloopAI/vibe-kanban</link>
            <guid>https://github.com/BloopAI/vibe-kanban</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:58 GMT</pubDate>
            <description><![CDATA[Get 10X more out of Claude Code, Codex or any coding agent]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/BloopAI/vibe-kanban">BloopAI/vibe-kanban</a></h1>
            <p>Get 10X more out of Claude Code, Codex or any coding agent</p>
            <p>Language: Rust</p>
            <p>Stars: 22,043</p>
            <p>Forks: 2,114</p>
            <p>Stars today: 75 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://vibekanban.com&quot;&gt;
    &lt;picture&gt;
      &lt;source srcset=&quot;packages/public/vibe-kanban-logo-dark.svg&quot; media=&quot;(prefers-color-scheme: dark)&quot;&gt;
      &lt;source srcset=&quot;packages/public/vibe-kanban-logo.svg&quot; media=&quot;(prefers-color-scheme: light)&quot;&gt;
      &lt;img src=&quot;packages/public/vibe-kanban-logo.svg&quot; alt=&quot;Vibe Kanban Logo&quot;&gt;
    &lt;/picture&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;Get 10X more out of Claude Code, Gemini CLI, Codex, Amp and other coding agents...&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.npmjs.com/package/vibe-kanban&quot;&gt;&lt;img alt=&quot;npm&quot; src=&quot;https://img.shields.io/npm/v/vibe-kanban?style=flat-square&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/BloopAI/vibe-kanban/blob/main/.github/workflows/publish.yml&quot;&gt;&lt;img alt=&quot;Build status&quot; src=&quot;https://img.shields.io/github/actions/workflow/status/BloopAI/vibe-kanban/.github%2Fworkflows%2Fpublish.yml&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://deepwiki.com/BloopAI/vibe-kanban&quot;&gt;&lt;img src=&quot;https://deepwiki.com/badge.svg&quot; alt=&quot;Ask DeepWiki&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://jobs.polymer.co/vibe-kanban?source=github&quot;&gt;&lt;strong&gt;We&#039;re hiring!&lt;/strong&gt;&lt;/a&gt;
&lt;/h1&gt;

![](packages/public/vibe-kanban-screenshot-overview.png)

## Overview

AI coding agents are increasingly writing the world&#039;s code and human engineers now spend the majority of their time planning, reviewing, and orchestrating tasks. Vibe Kanban streamlines this process, enabling you to:

- Easily switch between different coding agents
- Orchestrate the execution of multiple coding agents in parallel or in sequence
- Quickly review work and start dev servers
- Track the status of tasks that your coding agents are working on
- Centralise configuration of coding agent MCP configs
- Open projects remotely via SSH when running Vibe Kanban on a remote server

You can watch a video overview [here](https://youtu.be/TFT3KnZOOAk).

## Installation

Make sure you have authenticated with your favourite coding agent. A full list of supported coding agents can be found in the [docs](https://vibekanban.com/docs). Then in your terminal run:

```bash
npx vibe-kanban
```

## Documentation

Please head to the [website](https://vibekanban.com/docs) for the latest documentation and user guides.

## Self-Hosting

Want to host your own Vibe Kanban Cloud instance? See our [self-hosting guide](https://vibekanban.com/docs/self-hosting).
  
## Support

We use [GitHub Discussions](https://github.com/BloopAI/vibe-kanban/discussions) for feature requests. Please open a discussion to create a feature request. For bugs please open an issue on this repo.

## Contributing

We would prefer that ideas and changes are first raised with the core team via [GitHub Discussions](https://github.com/BloopAI/vibe-kanban/discussions) or [Discord](https://discord.gg/AC4nwVtJM3), where we can discuss implementation details and alignment with the existing roadmap. Please do not open PRs without first discussing your proposal with the team.

## Development

### Prerequisites

- [Rust](https://rustup.rs/) (latest stable)
- [Node.js](https://nodejs.org/) (&gt;=20)
- [pnpm](https://pnpm.io/) (&gt;=8)

Additional development tools:
```bash
cargo install cargo-watch
cargo install sqlx-cli
```

Install dependencies:
```bash
pnpm i
```

### Running the dev server

```bash
pnpm run dev
```

This will start the backend and web app. A blank DB will be copied from the `dev_assets_seed` folder.

### Building the web app

To build just the web app:

```bash
cd packages/local-web
pnpm run build
```

### Build from source (macOS)

1. Run `./local-build.sh`
2. Test with `cd npx-cli &amp;&amp; node bin/cli.js`

### Environment Variables

The following environment variables can be configured at build time or runtime:

| Variable | Type | Default | Description |
|----------|------|---------|-------------|
| `POSTHOG_API_KEY` | Build-time | Empty | PostHog analytics API key (disables analytics if empty) |
| `POSTHOG_API_ENDPOINT` | Build-time | Empty | PostHog analytics endpoint (disables analytics if empty) |
| `PORT` | Runtime | Auto-assign | **Production**: Server port. **Dev**: Frontend port (backend uses PORT+1) |
| `BACKEND_PORT` | Runtime | `0` (auto-assign) | Backend server port (dev mode only, overrides PORT+1) |
| `FRONTEND_PORT` | Runtime | `3000` | Frontend dev server port (dev mode only, overrides PORT) |
| `HOST` | Runtime | `127.0.0.1` | Backend server host |
| `MCP_HOST` | Runtime | Value of `HOST` | MCP server connection host (use `127.0.0.1` when `HOST=0.0.0.0` on Windows) |
| `MCP_PORT` | Runtime | Value of `BACKEND_PORT` | MCP server connection port |
| `DISABLE_WORKTREE_CLEANUP` | Runtime | Not set | Disable all git worktree cleanup including orphan and expired workspace cleanup (for debugging) |
| `VK_ALLOWED_ORIGINS` | Runtime | Not set | Comma-separated list of origins that are allowed to make backend API requests (e.g., `https://my-vibekanban-frontend.com`) |
| `VK_SHARED_API_BASE` | Runtime | Not set | Base URL for the remote/cloud API used by the local desktop app |
| `VK_SHARED_RELAY_API_BASE` | Runtime | Not set | Base URL for the relay API used by tunnel-mode connections |
| `VK_TUNNEL` | Runtime | Not set | Enable relay tunnel mode when set (requires relay API base URL) |

**Build-time variables** must be set when running `pnpm run build`. **Runtime variables** are read when the application starts.

#### Self-Hosting with a Reverse Proxy or Custom Domain

When running Vibe Kanban behind a reverse proxy (e.g., nginx, Caddy, Traefik) or on a custom domain, you must set the `VK_ALLOWED_ORIGINS` environment variable. Without this, the browser&#039;s Origin header won&#039;t match the backend&#039;s expected host, and API requests will be rejected with a 403 Forbidden error.

Set it to the full origin URL(s) where your frontend is accessible:

```bash
# Single origin
VK_ALLOWED_ORIGINS=https://vk.example.com

# Multiple origins (comma-separated)
VK_ALLOWED_ORIGINS=https://vk.example.com,https://vk-staging.example.com
```

### Remote Deployment

When running Vibe Kanban on a remote server (e.g., via systemctl, Docker, or cloud hosting), you can configure your editor to open projects via SSH:

1. **Access via tunnel**: Use Cloudflare Tunnel, ngrok, or similar to expose the web UI
2. **Configure remote SSH** in Settings ‚Üí Editor Integration:
   - Set **Remote SSH Host** to your server hostname or IP
   - Set **Remote SSH User** to your SSH username (optional)
3. **Prerequisites**:
   - SSH access from your local machine to the remote server
   - SSH keys configured (passwordless authentication)
   - VSCode Remote-SSH extension

When configured, the &quot;Open in VSCode&quot; buttons will generate URLs like `vscode://vscode-remote/ssh-remote+user@host/path` that open your local editor and connect to the remote server.

See the [documentation](https://vibekanban.com/docs/configuration-customisation/global-settings#remote-ssh-configuration) for detailed setup instructions.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[katanemo/plano]]></title>
            <link>https://github.com/katanemo/plano</link>
            <guid>https://github.com/katanemo/plano</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:57 GMT</pubDate>
            <description><![CDATA[Delivery infrastructure for agentic apps - Plano is an AI-native proxy and data plane that offloads plumbing work, so you stay focused on your agent's core logic (via any AI framework).]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/katanemo/plano">katanemo/plano</a></h1>
            <p>Delivery infrastructure for agentic apps - Plano is an AI-native proxy and data plane that offloads plumbing work, so you stay focused on your agent's core logic (via any AI framework).</p>
            <p>Language: Rust</p>
            <p>Stars: 5,767</p>
            <p>Forks: 342</p>
            <p>Stars today: 35 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/source/_static/img/PlanoTagline.svg&quot; alt=&quot;Plano Logo&quot; width=&quot;75%&quot; height=auto&gt;
&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt;

 _The AI-native proxy server and data plane for agentic apps._&lt;br&gt;&lt;br&gt;
 Plano pulls out the rote plumbing work and decouples you from brittle framework abstractions, centralizing what shouldn‚Äôt be bespoke in every codebase - like agent routing and orchestration, rich agentic signals and traces for continuous improvement, guardrail filters for safety and moderation, and smart LLM routing APIs for model agility. Use any language or AI framework, and deliver agents faster to production.


[Quickstart Guide](https://docs.planoai.dev/get_started/quickstart.html) ‚Ä¢
[Build Agentic Apps with Plano](#Build-Agentic-Apps-with-Plano) ‚Ä¢
[Documentation](https://docs.planoai.dev) ‚Ä¢
[Contact](#Contact)

[![CI](https://github.com/katanemo/plano/actions/workflows/ci.yml/badge.svg)](https://github.com/katanemo/plano/actions/workflows/ci.yml)
[![Docker Image](https://github.com/katanemo/plano/actions/workflows/docker-push-main.yml/badge.svg)](https://github.com/katanemo/plano/actions/workflows/docker-push-main.yml)
[![Build and Deploy Documentation](https://github.com/katanemo/plano/actions/workflows/static.yml/badge.svg)](https://github.com/katanemo/plano/actions/workflows/static.yml)

Star ‚≠êÔ∏è the repo if you found Plano useful ‚Äî new releases and updates land here first.
&lt;/div&gt;

# Overview
Building agentic demos is easy. Shipping agentic applications safely, reliably, and repeatably to production is hard. After the thrill of a quick hack, you end up building the ‚Äúhidden middleware‚Äù to reach production: routing logic to reach the right agent, guardrail hooks for safety and moderation, evaluation and observability glue for continuous learning, and model/provider quirks scattered across frameworks and application code.

Plano solves this by moving core delivery concerns into a unified, out-of-process dataplane.

- **üö¶ Orchestration:** Low-latency orchestration between agents; add new agents without modifying app code.
- **üîó Model Agility:** Route [by model name, alias (semantic names) or automatically via preferences](#use-plano-as-a-llm-router).
- **üïµ Agentic Signals&amp;trade;:** Zero-code capture of [Signals](https://docs.planoai.dev/concepts/signals.html) plus OTEL traces/metrics across every agent.
- **üõ°Ô∏è Moderation &amp; Memory Hooks:** Build jailbreak protection, add moderation policies and memory consistently via [Filter Chains](https://docs.planoai.dev/concepts/filter_chain.html).

Plano pulls rote plumbing out of your framework so you can stay focused on what matters most: the core product logic of your agentic applications. Plano is backed by [industry-leading LLM research](https://planoai.dev/research) and built on [Envoy](https://envoyproxy.io) by its core contributors, who built critical infrastructure at scale for modern worklaods.

**High-Level Network Sequence Diagram**:
![high-level network plano arcitecture for Plano](docs/source/_static/img/plano_network_diagram_high_level.png)

**Jump to our [docs](https://docs.planoai.dev)** to learn how you can use Plano to improve the speed, safety and obervability of your agentic applications.

&gt; [!IMPORTANT]
&gt; Plano and the Arch family of LLMs (like Plano-Orchestrator-4B, Arch-Router, etc) are hosted free of charge in the US-central region to give you a great first-run developer experience of Plano. To scale and run in production, you can either run these LLMs locally or contact us on [Discord](https://discord.gg/pGZf2gcwEc) for API keys.

---

## Build Agentic Apps with Plano

Plano handles **orchestration, model management, and observability** as modular building blocks - letting you configure only what you need (edge proxying for agentic orchestration and guardrails, or LLM routing from your services, or both together) to fit cleanly into existing architectures. Below is a simple multi-agent travel agent built with Plano that showcases all three core capabilities

&gt; üìÅ **Full working code:** See [`demos/agent_orchestration/travel_agents/`](demos/agent_orchestration/travel_agents/) for complete weather and flight agents you can run locally.



### 1. Define Your Agents in YAML

```yaml
# config.yaml
version: v0.3.0

# What you declare: Agent URLs and natural language descriptions
# What you don&#039;t write: Intent classifiers, routing logic, model fallbacks, provider adapters, or tracing instrumentation

agents:
  - id: weather_agent
    url: http://localhost:10510
  - id: flight_agent
    url: http://localhost:10520

model_providers:
  - model: openai/gpt-4o
    access_key: $OPENAI_API_KEY
    default: true
  - model: anthropic/claude-3-5-sonnet
    access_key: $ANTHROPIC_API_KEY

listeners:
  - type: agent
    name: travel_assistant
    port: 8001
    router: plano_orchestrator_v1  # Powered by our 4B-parameter routing model. You can change this to different models
    agents:
      - id: weather_agent
        description: |
          Gets real-time weather and forecasts for any city worldwide.
          Handles: &quot;What&#039;s the weather in Paris?&quot;, &quot;Will it rain in Tokyo?&quot;

      - id: flight_agent
        description: |
          Searches flights between airports with live status and schedules.
          Handles: &quot;Flights from NYC to LA&quot;, &quot;Show me flights to Seattle&quot;

tracing:
  random_sampling: 100  # Auto-capture traces for evaluation
```

### 2. Write Simple Agent Code

Your agents are just HTTP servers that implement the OpenAI-compatible chat completions endpoint. Use any language or framework:

```python
# weather_agent.py
from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse
from openai import AsyncOpenAI

app = FastAPI()

# Point to Plano&#039;s LLM gateway - it handles model routing for you
llm = AsyncOpenAI(base_url=&quot;http://localhost:12001/v1&quot;, api_key=&quot;EMPTY&quot;)

@app.post(&quot;/v1/chat/completions&quot;)
async def chat(request: Request):
    body = await request.json()
    messages = body.get(&quot;messages&quot;, [])
    days = 7

    # Your agent logic: fetch data, call APIs, run tools
    # See demos/agent_orchestration/travel_agents/ for the full implementation
    weather_data = await get_weather_data(request, messages, days)

    # Stream the response back through Plano
    async def generate():
        stream = await llm.chat.completions.create(
            model=&quot;openai/gpt-4o&quot;,
            messages=[{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: f&quot;Weather: {weather_data}&quot;}, *messages],
            stream=True
        )
        async for chunk in stream:
            yield f&quot;data: {chunk.model_dump_json()}\n\n&quot;

    return StreamingResponse(generate(), media_type=&quot;text/event-stream&quot;)
```

### 3. Start Plano &amp; Query Your Agents

**Prerequisites:** Follow the [prerequisites guide](https://docs.planoai.dev/get_started/quickstart.html#prerequisites) to install Plano and set up your environment.

```bash
# Start Plano
planoai up config.yaml
...

# Query - Plano intelligently routes to both agents in a single conversation
curl http://localhost:8001/v1/chat/completions \
  -H &quot;Content-Type: application/json&quot; \
  -d &#039;{
    &quot;model&quot;: &quot;gpt-4o&quot;,
    &quot;messages&quot;: [
      {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;I want to travel from NYC to Paris next week. What is the weather like there, and can you find me some flights?&quot;}
    ]
  }&#039;
# ‚Üí Plano routes to weather_agent for Paris weather ‚úì
# ‚Üí Then routes to flight_agent for NYC ‚Üí Paris flights ‚úì
# ‚Üí Returns a complete travel plan with both weather info and flight options
```

### 4. Get Observability and Model Agility for Free

Every request is traced end-to-end with OpenTelemetry - no instrumentation code needed.

![Atomatic Tracing](docs/source/_static/img/demo_tracing.png)

### What You Didn&#039;t Have to Build

| Infrastructure Concern | Without Plano | With Plano |
|---------|---------------|------------|
| **Agent Orchestration** | Write intent classifier + routing logic | Declare agent descriptions in YAML |
| **Model Management** | Handle each provider&#039;s API quirks | Unified LLM APIs with state management |
| **Rich Tracing** | Instrument every service with OTEL | Automatic end-to-end traces and logs |
| **Learning Signals** | Build pipeline to capture/export spans | Zero-code agentic signals |
| **Adding Agents** | Update routing code, test, redeploy | Add to config, restart |

**Why it&#039;s efficient:** Plano uses purpose-built, lightweight LLMs (like our 4B-parameter orchestrator) instead of heavyweight frameworks or GPT-4 for routing - giving you production-grade routing at a fraction of the cost and latency.

---

## Contact
To get in touch with us, please join our [discord server](https://discord.gg/pGZf2gcwEc). We actively monitor that and offer support there.

## Getting Started

Ready to try Plano? Check out our comprehensive documentation:

- **[Quickstart Guide](https://docs.planoai.dev/get_started/quickstart.html)** - Get up and running in minutes
- **[LLM Routing](https://docs.planoai.dev/guides/llm_router.html)** - Route by model name, alias, or intelligent preferences
- **[Agent Orchestration](https://docs.planoai.dev/guides/orchestration.html)** - Build multi-agent workflows
- **[Filter Chains](https://docs.planoai.dev/concepts/filter_chain.html)** - Add guardrails, moderation, and memory hooks
- **[Prompt Targets](https://docs.planoai.dev/concepts/prompt_target.html)** - Turn prompts into deterministic API calls
- **[Observability](https://docs.planoai.dev/guides/observability/observability.html)** - Traces, metrics, and logs

## Contribution
We would love feedback on our [Roadmap](https://github.com/orgs/katanemo/projects/1) and we welcome contributions to **Plano**! Whether you&#039;re fixing bugs, adding new features, improving documentation, or creating tutorials, your help is much appreciated. Please visit our [Contribution Guide](CONTRIBUTING.md) for more details

Star ‚≠êÔ∏è the repo if you found Plano useful ‚Äî new releases and updates land here first.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[foundry-rs/foundry]]></title>
            <link>https://github.com/foundry-rs/foundry</link>
            <guid>https://github.com/foundry-rs/foundry</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:56 GMT</pubDate>
            <description><![CDATA[Foundry is a blazing fast, portable and modular toolkit for Ethereum application development written in Rust.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/foundry-rs/foundry">foundry-rs/foundry</a></h1>
            <p>Foundry is a blazing fast, portable and modular toolkit for Ethereum application development written in Rust.</p>
            <p>Language: Rust</p>
            <p>Stars: 10,152</p>
            <p>Forks: 2,416</p>
            <p>Stars today: 5 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;.github/assets/banner.png&quot; alt=&quot;Foundry banner&quot; /&gt;

&amp;nbsp;

[![Github Actions][gha-badge]][gha-url] [![Telegram Chat][tg-badge]][tg-url] [![Telegram Support][tg-support-badge]][tg-support-url]

[gha-badge]: https://img.shields.io/github/actions/workflow/status/foundry-rs/foundry/test.yml?branch=master&amp;style=flat-square
[gha-url]: https://github.com/foundry-rs/foundry/actions
[tg-badge]: https://img.shields.io/endpoint?color=neon&amp;logo=telegram&amp;label=chat&amp;style=flat-square&amp;url=https%3A%2F%2Ftg.sumanjay.workers.dev%2Ffoundry_rs
[tg-url]: https://t.me/foundry_rs
[tg-support-badge]: https://img.shields.io/endpoint?color=neon&amp;logo=telegram&amp;label=support&amp;style=flat-square&amp;url=https%3A%2F%2Ftg.sumanjay.workers.dev%2Ffoundry_support
[tg-support-url]: https://t.me/foundry_support

**[Install](https://getfoundry.sh/getting-started/installation)**
| [Docs][foundry-docs]
| [Benchmarks](https://www.getfoundry.sh/benchmarks)
| [Developer Guidelines](./docs/dev/README.md)
| [Contributing](./CONTRIBUTING.md)
| [Crate Docs](https://foundry-rs.github.io/foundry)

&lt;/div&gt;

---

Blazing fast, portable and modular toolkit for Ethereum application development, written in Rust.

- [**Forge**](https://getfoundry.sh/forge) ‚Äî Build, test, fuzz, debug and deploy Solidity contracts.
- [**Cast**](https://getfoundry.sh/cast) ‚Äî Swiss Army knife for interacting with EVM smart contracts, sending transactions and getting chain data.
- [**Anvil**](https://getfoundry.sh/anvil) ‚Äî Fast local Ethereum development node.
- [**Chisel**](https://getfoundry.sh/chisel) ‚Äî Fast, utilitarian and verbose Solidity REPL.

![Demo](.github/assets/demo.gif)

## Installation

```sh
curl -L https://foundry.paradigm.xyz | bash
foundryup
```

See the [installation guide](https://getfoundry.sh/getting-started/installation) for more details.

## Getting Started

Initialize a new project, build and test:

```sh
forge init counter &amp;&amp; cd counter
forge build
forge test
```

Interact with a live network:

```sh
cast block-number --rpc-url https://eth.merkle.io
cast balance vitalik.eth --ether --rpc-url https://eth.merkle.io
```

Fork mainnet locally:

```sh
anvil --fork-url https://eth.merkle.io
```

Read the [Foundry Docs][foundry-docs] to learn more.

## Contributing

Contributions are welcome and highly appreciated. To get started, check out the [contributing guidelines](./CONTRIBUTING.md).

Join our [Telegram][tg-url] to chat about the development of Foundry.

## Support

Having trouble? Check the [Foundry Docs][foundry-docs], join the [support Telegram][tg-support-url], or [open an issue](https://github.com/foundry-rs/foundry/issues/new).

#### License

&lt;sup&gt;
Licensed under either of &lt;a href=&quot;LICENSE-APACHE&quot;&gt;Apache License, Version
2.0&lt;/a&gt; or &lt;a href=&quot;LICENSE-MIT&quot;&gt;MIT license&lt;/a&gt; at your option.
&lt;/sup&gt;

&lt;br&gt;

&lt;sub&gt;
Unless you explicitly state otherwise, any contribution intentionally submitted
for inclusion in these crates by you, as defined in the Apache-2.0 license,
shall be dual licensed as above, without any additional terms or conditions.
&lt;/sub&gt;

[foundry-docs]: https://getfoundry.sh
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[dani-garcia/vaultwarden]]></title>
            <link>https://github.com/dani-garcia/vaultwarden</link>
            <guid>https://github.com/dani-garcia/vaultwarden</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:55 GMT</pubDate>
            <description><![CDATA[Unofficial Bitwarden compatible server written in Rust, formerly known as bitwarden_rs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/dani-garcia/vaultwarden">dani-garcia/vaultwarden</a></h1>
            <p>Unofficial Bitwarden compatible server written in Rust, formerly known as bitwarden_rs</p>
            <p>Language: Rust</p>
            <p>Stars: 56,016</p>
            <p>Forks: 2,583</p>
            <p>Stars today: 85 stars today</p>
            <h2>README</h2><pre>![Vaultwarden Logo](./resources/vaultwarden-logo-auto.svg)

An alternative server implementation of the Bitwarden Client API, written in Rust and compatible with [official Bitwarden clients](https://bitwarden.com/download/) [[disclaimer](#disclaimer)], perfect for self-hosted deployment where running the official resource-heavy service might not be ideal.

---

[![GitHub Release](https://img.shields.io/github/release/dani-garcia/vaultwarden.svg?style=for-the-badge&amp;logo=vaultwarden&amp;color=005AA4)](https://github.com/dani-garcia/vaultwarden/releases/latest)
[![ghcr.io Pulls](https://img.shields.io/badge/dynamic/json?style=for-the-badge&amp;logo=github&amp;logoColor=fff&amp;color=005AA4&amp;url=https%3A%2F%2Fipitio.github.io%2Fbackage%2Fdani-garcia%2Fvaultwarden%2Fvaultwarden.json&amp;query=%24.downloads&amp;label=ghcr.io%20pulls&amp;cacheSeconds=14400)](https://github.com/dani-garcia/vaultwarden/pkgs/container/vaultwarden)
[![Docker Pulls](https://img.shields.io/docker/pulls/vaultwarden/server.svg?style=for-the-badge&amp;logo=docker&amp;logoColor=fff&amp;color=005AA4&amp;label=docker.io%20pulls)](https://hub.docker.com/r/vaultwarden/server)
[![Quay.io](https://img.shields.io/badge/quay.io-download-005AA4?style=for-the-badge&amp;logo=redhat&amp;cacheSeconds=14400)](https://quay.io/repository/vaultwarden/server) &lt;br&gt;
[![Contributors](https://img.shields.io/github/contributors-anon/dani-garcia/vaultwarden.svg?style=flat-square&amp;logo=vaultwarden&amp;color=005AA4)](https://github.com/dani-garcia/vaultwarden/graphs/contributors)
[![Forks](https://img.shields.io/github/forks/dani-garcia/vaultwarden.svg?style=flat-square&amp;logo=github&amp;logoColor=fff&amp;color=005AA4)](https://github.com/dani-garcia/vaultwarden/network/members)
[![Stars](https://img.shields.io/github/stars/dani-garcia/vaultwarden.svg?style=flat-square&amp;logo=github&amp;logoColor=fff&amp;color=005AA4)](https://github.com/dani-garcia/vaultwarden/stargazers)
[![Issues Open](https://img.shields.io/github/issues/dani-garcia/vaultwarden.svg?style=flat-square&amp;logo=github&amp;logoColor=fff&amp;color=005AA4&amp;cacheSeconds=300)](https://github.com/dani-garcia/vaultwarden/issues)
[![Issues Closed](https://img.shields.io/github/issues-closed/dani-garcia/vaultwarden.svg?style=flat-square&amp;logo=github&amp;logoColor=fff&amp;color=005AA4&amp;cacheSeconds=300)](https://github.com/dani-garcia/vaultwarden/issues?q=is%3Aissue+is%3Aclosed)
[![AGPL-3.0 Licensed](https://img.shields.io/github/license/dani-garcia/vaultwarden.svg?style=flat-square&amp;logo=vaultwarden&amp;color=944000&amp;cacheSeconds=14400)](https://github.com/dani-garcia/vaultwarden/blob/main/LICENSE.txt) &lt;br&gt;
[![Dependency Status](https://img.shields.io/badge/dynamic/xml?url=https%3A%2F%2Fdeps.rs%2Frepo%2Fgithub%2Fdani-garcia%2Fvaultwarden%2Fstatus.svg&amp;query=%2F*%5Blocal-name()%3D&#039;svg&#039;%5D%2F*%5Blocal-name()%3D&#039;g&#039;%5D%5B2%5D%2F*%5Blocal-name()%3D&#039;text&#039;%5D%5B4%5D&amp;style=flat-square&amp;logo=rust&amp;label=dependencies&amp;color=005AA4)](https://deps.rs/repo/github/dani-garcia/vaultwarden)
[![GHA Release](https://img.shields.io/github/actions/workflow/status/dani-garcia/vaultwarden/release.yml?style=flat-square&amp;logo=github&amp;logoColor=fff&amp;label=Release%20Workflow)](https://github.com/dani-garcia/vaultwarden/actions/workflows/release.yml)
[![GHA Build](https://img.shields.io/github/actions/workflow/status/dani-garcia/vaultwarden/build.yml?style=flat-square&amp;logo=github&amp;logoColor=fff&amp;label=Build%20Workflow)](https://github.com/dani-garcia/vaultwarden/actions/workflows/build.yml) &lt;br&gt;
[![Matrix Chat](https://img.shields.io/matrix/vaultwarden:matrix.org.svg?style=flat-square&amp;logo=matrix&amp;logoColor=fff&amp;color=953B00&amp;cacheSeconds=14400)](https://matrix.to/#/#vaultwarden:matrix.org)
[![GitHub Discussions](https://img.shields.io/github/discussions/dani-garcia/vaultwarden?style=flat-square&amp;logo=github&amp;logoColor=fff&amp;color=953B00&amp;cacheSeconds=300)](https://github.com/dani-garcia/vaultwarden/discussions)
[![Discourse Discussions](https://img.shields.io/discourse/topics?server=https%3A%2F%2Fvaultwarden.discourse.group%2F&amp;style=flat-square&amp;logo=discourse&amp;color=953B00)](https://vaultwarden.discourse.group/)

&gt; [!IMPORTANT]
&gt; **When using this server, please report any bugs or suggestions directly to us (see [Get in touch](#get-in-touch)), regardless of whatever clients you are using (mobile, desktop, browser...). DO NOT use the official Bitwarden support channels.**

&lt;br&gt;

## Features

A nearly complete implementation of the Bitwarden Client API is provided, including:

 * [Personal Vault](https://bitwarden.com/help/managing-items/)
 * [Send](https://bitwarden.com/help/about-send/)
 * [Attachments](https://bitwarden.com/help/attachments/)
 * [Website icons](https://bitwarden.com/help/website-icons/)
 * [Personal API Key](https://bitwarden.com/help/personal-api-key/)
 * [Organizations](https://bitwarden.com/help/getting-started-organizations/)
   - [Collections](https://bitwarden.com/help/about-collections/),
     [Password Sharing](https://bitwarden.com/help/sharing/),
     [Member Roles](https://bitwarden.com/help/user-types-access-control/),
     [Groups](https://bitwarden.com/help/about-groups/),
     [Event Logs](https://bitwarden.com/help/event-logs/),
     [Admin Password Reset](https://bitwarden.com/help/admin-reset/),
     [Directory Connector](https://bitwarden.com/help/directory-sync/),
     [Policies](https://bitwarden.com/help/policies/)
 * [Multi/Two Factor Authentication](https://bitwarden.com/help/bitwarden-field-guide-two-step-login/)
   - [Authenticator](https://bitwarden.com/help/setup-two-step-login-authenticator/),
     [Email](https://bitwarden.com/help/setup-two-step-login-email/),
     [FIDO2 WebAuthn](https://bitwarden.com/help/setup-two-step-login-fido/),
     [YubiKey](https://bitwarden.com/help/setup-two-step-login-yubikey/),
     [Duo](https://bitwarden.com/help/setup-two-step-login-duo/)
 * [Emergency Access](https://bitwarden.com/help/emergency-access/)
 * [Vaultwarden Admin Backend](https://github.com/dani-garcia/vaultwarden/wiki/Enabling-admin-page)
 * [Modified Web Vault client](https://github.com/dani-garcia/bw_web_builds) (Bundled within our containers)

&lt;br&gt;

## Usage

&gt; [!IMPORTANT]
&gt; The web-vault requires the use a secure context for the [Web Crypto API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Crypto_API).
&gt; That means it will only work via `http://localhost:8000` (using the port from the example below) or if you [enable HTTPS](https://github.com/dani-garcia/vaultwarden/wiki/Enabling-HTTPS).

The recommended way to install and use Vaultwarden is via our container images which are published to [ghcr.io](https://github.com/dani-garcia/vaultwarden/pkgs/container/vaultwarden), [docker.io](https://hub.docker.com/r/vaultwarden/server) and [quay.io](https://quay.io/repository/vaultwarden/server).
See [which container image to use](https://github.com/dani-garcia/vaultwarden/wiki/Which-container-image-to-use) for an explanation of the provided tags.

There are also [community driven packages](https://github.com/dani-garcia/vaultwarden/wiki/Third-party-packages) which can be used, but those might be lagging behind the latest version or might deviate in the way Vaultwarden is configured, as described in our [Wiki](https://github.com/dani-garcia/vaultwarden/wiki).

Alternatively, you can also [build Vaultwarden](https://github.com/dani-garcia/vaultwarden/wiki/Building-binary) yourself.

While Vaultwarden is based upon the [Rocket web framework](https://rocket.rs) which has built-in support for TLS our recommendation would be that you setup a reverse proxy (see [proxy examples](https://github.com/dani-garcia/vaultwarden/wiki/Proxy-examples)).

&gt; [!TIP]
&gt;**For more detailed examples on how to install, use and configure Vaultwarden you can check our [Wiki](https://github.com/dani-garcia/vaultwarden/wiki).**

### Docker/Podman CLI

Pull the container image and mount a volume from the host for persistent storage.&lt;br&gt;
You can replace `docker` with `podman` if you prefer to use podman.

```shell
docker pull vaultwarden/server:latest
docker run --detach --name vaultwarden \
  --env DOMAIN=&quot;https://vw.domain.tld&quot; \
  --volume /vw-data/:/data/ \
  --restart unless-stopped \
  --publish 127.0.0.1:8000:80 \
  vaultwarden/server:latest
```

This will preserve any persistent data under `/vw-data/`, you can adapt the path to whatever suits you.

### Docker Compose

To use Docker compose you need to create a `compose.yaml` which will hold the configuration to run the Vaultwarden container.

```yaml
services:
  vaultwarden:
    image: vaultwarden/server:latest
    container_name: vaultwarden
    restart: unless-stopped
    environment:
      DOMAIN: &quot;https://vw.domain.tld&quot;
    volumes:
      - ./vw-data/:/data/
    ports:
      - 127.0.0.1:8000:80
```

&lt;br&gt;

## Get in touch

Have a question, suggestion or need help? Join our community on [Matrix](https://matrix.to/#/#vaultwarden:matrix.org), [GitHub Discussions](https://github.com/dani-garcia/vaultwarden/discussions) or [Discourse Forums](https://vaultwarden.discourse.group/).

Encountered a bug or crash? Please search our issue tracker and discussions to see if it&#039;s already been reported. If not, please [start a new discussion](https://github.com/dani-garcia/vaultwarden/discussions) or [create a new issue](https://github.com/dani-garcia/vaultwarden/issues/). Ensure you&#039;re using the latest version of Vaultwarden and there aren&#039;t any similar issues open or closed!

&lt;br&gt;

## Contributors

Thanks for your contribution to the project!

[![Contributors Count](https://img.shields.io/github/contributors-anon/dani-garcia/vaultwarden?style=for-the-badge&amp;logo=vaultwarden&amp;color=005AA4)](https://github.com/dani-garcia/vaultwarden/graphs/contributors)&lt;br&gt;
[![Contributors Avatars](https://contributors-img.web.app/image?repo=dani-garcia/vaultwarden)](https://github.com/dani-garcia/vaultwarden/graphs/contributors)

&lt;br&gt;

## Disclaimer

**This project is not associated with [Bitwarden](https://bitwarden.com/) or Bitwarden, Inc.**

However, one of the active maintainers for Vaultwarden is employed by Bitwarden and is allowed to contribute to the project on their own time. These contributions are independent of Bitwarden and are reviewed by other maintainers.

The maintainers work together to set the direction for the project, focusing on serving the self-hosting community, including individuals, families, and small organizations, while ensuring the project&#039;s sustainability.

**Please note:** We cannot be held liable for any data loss that may occur while using Vaultwarden. This includes passwords, attachments, and other information handled by the application. We highly recommend performing regular backups of your files and database. However, should you experience data loss, we encourage you to contact us immediately.

&lt;br&gt;

## Bitwarden_RS

This project was known as Bitwarden_RS and has been renamed to separate itself from the official Bitwarden server in the hopes of avoiding confusion and trademark/branding issues.&lt;br&gt;
Please see [#1642 - v1.21.0 release and project rename to Vaultwarden](https://github.com/dani-garcia/vaultwarden/discussions/1642) for more explanation.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[qdrant/qdrant]]></title>
            <link>https://github.com/qdrant/qdrant</link>
            <guid>https://github.com/qdrant/qdrant</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:54 GMT</pubDate>
            <description><![CDATA[Qdrant - High-performance, massive-scale Vector Database and Vector Search Engine for the next generation of AI. Also available in the cloud https://cloud.qdrant.io/]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/qdrant/qdrant">qdrant/qdrant</a></h1>
            <p>Qdrant - High-performance, massive-scale Vector Database and Vector Search Engine for the next generation of AI. Also available in the cloud https://cloud.qdrant.io/</p>
            <p>Language: Rust</p>
            <p>Stars: 29,169</p>
            <p>Forks: 2,058</p>
            <p>Stars today: 38 stars today</p>
            <h2>README</h2><pre>## How to interview using this template

Open ONLY the `shared/` folder within VSCode and in the Extensions Marketplace find `CodeTogether Live`. Install it.

Inside the extension, you&#039;ll have to click on &#039;Host New Session&#039; and then &#039;Start&#039;.

You now have a link in your clipboard, which you should share with the candidate.

Don&#039;t forget to run `npm run vite.dev` inside your editor terminal, since this is not currently possible from the browser editor (link) with the free version of the extension.

The locally running server will display localhost:5173 as well as a the project within the network, which the user can access remotely to see the SPA in action, something like: `10.5.52.144:5173`.

The candidate should be sharing their screen while they read and later approach the challenge!

## React TemplateÔºà‚ö°Ô∏èÔºâ

‚ö°Ô∏è A minimal React Vite starter template.

### Feature

- ‚ö°Ô∏è Fast - Build tools based on vite.
- üëª Small - Based on the smallest runnable build.
- üíÑ Prettier - Integrated Prettier to help you format the code.
- ‚úÖ Safety - Https is enabled by default.
- üòé Reliable - Integrated eslint and commitlint.
- ü§ñ Intelligent - Integrated renovate to help you maintain the dependent version.

### Preview

[![qekup8.png](https://s1.ax1x.com/2022/03/20/qekup8.png)](https://imgtu.com/i/qekup8)

### Getting Started

```bash
npx degit lzm0x219/template-vite-react myapp

cd myapp

git init
```

#### Prerequisites

- `npm` and/or `pnpm` should be installed.
- `git` should be installed (recommended v2.4.11 or higher)

#### Available scripts

##### `npm run vite.dev`

Runs the app in development mode.
Open https://localhost:5173 to view it in the browser.

The page will automatically reload if you make changes to the code.
You will see the build errors and lint warnings in the console.

##### `npm run vite.build`

Builds the app for production to the `dist` folder.
It correctly bundles React in production mode and optimizes the build for the best performance.

The build is minified and the filenames include the hashes.

Your app is ready to be deployed.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[vectordotdev/vector]]></title>
            <link>https://github.com/vectordotdev/vector</link>
            <guid>https://github.com/vectordotdev/vector</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:53 GMT</pubDate>
            <description><![CDATA[A high-performance observability data pipeline.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/vectordotdev/vector">vectordotdev/vector</a></h1>
            <p>A high-performance observability data pipeline.</p>
            <p>Language: Rust</p>
            <p>Stars: 21,396</p>
            <p>Forks: 2,022</p>
            <p>Stars today: 8 stars today</p>
            <h2>README</h2><pre>[![Nightly](https://github.com/vectordotdev/vector/actions/workflows/nightly.yml/badge.svg)](https://github.com/vectordotdev/vector/actions/workflows/nightly.yml)
[![Integration/E2E Test Suite](https://github.com/vectordotdev/vector/actions/workflows/integration.yml/badge.svg)](https://github.com/vectordotdev/vector/actions/workflows/integration.yml/badge.svg?event=merge_group)
[![Component Features](https://github.com/vectordotdev/vector/actions/workflows/component_features.yml/badge.svg)](https://github.com/vectordotdev/vector/actions/workflows/component_features.yml)

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;website/static/img/diagram.svg&quot; alt=&quot;Vector&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;
    &lt;a href=&quot;https://vector.dev/docs/setup/quickstart/&quot;&gt;Quickstart&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://vector.dev/docs/&quot;&gt;Docs&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://vector.dev/guides/&quot;&gt;Guides&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://vector.dev/components/&quot;&gt;Integrations&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://chat.vector.dev&quot;&gt;Chat&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://vector.dev/releases/latest/download/&quot;&gt;Download&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://rust-doc.vector.dev/&quot;&gt;Rust Crate Docs&lt;/a&gt;
  &lt;/strong&gt;
&lt;/p&gt;

## What is Vector?

Vector is a high-performance, end-to-end (agent &amp; aggregator) observability data
pipeline that puts you in control of your observability data.
[Collect][docs.sources], [transform][docs.transforms], and [route][docs.sinks]
all your logs and metrics to any vendors you want today and any other
vendors you may want tomorrow. Vector enables dramatic cost reduction, novel
data enrichment, and data security where you need it, not where it is most
convenient for your vendors. Additionally, it is open source and up to 10x
faster than every alternative in the space.

To get started, follow our [**quickstart guide**][docs.quickstart] or [**install
Vector**][docs.installation].

Vector is maintained by Datadog&#039;s [Community Open Source Engineering team](https://opensource.datadoghq.com/about/#the-community-open-source-engineering-team).

### Principles

* **Reliable** - Built in [Rust][urls.rust], Vector&#039;s primary design goal is reliability.
* **End-to-end** - Deploys as an [agent][docs.roles#agent] or [aggregator][docs.roles#aggregator]. Vector is a complete platform.
* **Unified** - [Logs][docs.data-model.log], [metrics][docs.data-model.metric] (beta), and traces (coming soon). One tool for all of your data.

### Use cases

* Reduce total observability costs.
* Transition vendors without disrupting workflows.
* Enhance data quality and improve insights.
* Consolidate agents and eliminate agent fatigue.
* Improve overall observability performance and reliability.

### Community

* Vector is relied on by startups and enterprises like **Atlassian**, **T-Mobile**,
  **Comcast**, **Zendesk**, **Discord**, **Fastly**, **CVS**, **Trivago**,
  **Tuple**, **Douban**, **Visa**, **Mambu**, **Blockfi**, **Claranet**,
  **Instacart**, **Forcepoint**, and [many more][urls.production_users].
* Vector is **downloaded over 100,000 times per day**.
* Vector&#039;s largest user **processes over 500TB daily**.
* Vector has **over 500 contributors** and growing.

## Documentation

All user documentation is available at **[vector.dev/docs](https://vector.dev/docs)**.

Other Resources:

* [**Vector Calendar**][urls.vector_calendar]
* **Policies**:
  * [**Code of Conduct**][urls.vector_code_of_conduct]
  * [**Contributing**][urls.vector_contributing_policy]
  * [**Privacy**][urls.vector_privacy_policy]
  * [**Releases**][urls.vector_releases_policy]
  * [**Versioning**][urls.vector_versioning_policy]
  * [**Security**][urls.vector_security_policy]

## Comparisons

### Performance

The following performance tests demonstrate baseline performance between
common protocols with the exception of the Regex Parsing test.

| Test                                                                                                                   | Vector          | Filebeat | FluentBit       | FluentD   | Logstash  | SplunkUF        | SplunkHF |
| ---------------------------------------------------------------------------------------------------------------------- | --------------- | -------- | --------------- | --------- | --------- | --------------- | -------- |
| [TCP to Blackhole](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/tcp_to_blackhole_performance) | _**86mib/s**_   | n/a      | 64.4mib/s       | 27.7mib/s | 40.6mib/s | n/a             | n/a      |
| [File to TCP](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_to_tcp_performance)           | _**76.7mib/s**_ | 7.8mib/s | 35mib/s         | 26.1mib/s | 3.1mib/s  | 40.1mib/s       | 39mib/s  |
| [Regex Parsing](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/regex_parsing_performance)       | 13.2mib/s       | n/a      | _**20.5mib/s**_ | 2.6mib/s  | 4.6mib/s  | n/a             | 7.8mib/s |
| [TCP to HTTP](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/tcp_to_http_performance)           | _**26.7mib/s**_ | n/a      | 19.6mib/s       | &lt;1mib/s   | 2.7mib/s  | n/a             | n/a      |
| [TCP to TCP](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/tcp_to_tcp_performance)             | 69.9mib/s       | 5mib/s   | 67.1mib/s       | 3.9mib/s  | 10mib/s   | _**70.4mib/s**_ | 7.6mib/s |

To learn more about our performance tests, please see the [Vector test harness][urls.vector_test_harness].

### Correctness

The following correctness tests are not exhaustive, but they demonstrate
fundamental differences in quality and attention to detail:

| Test                                                                                                                                 | Vector | Filebeat | FluentBit | FluentD | Logstash | Splunk UF | Splunk HF |
| ------------------------------------------------------------------------------------------------------------------------------------ | ------ | -------- | --------- | ------- | -------- | --------- | --------- |
| [Disk Buffer Persistence](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/disk_buffer_persistence_correctness) | **‚úì**  | ‚úì        |           |         | ‚ö†        | ‚úì         | ‚úì         |
| [File Rotate (create)](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_rotate_create_correctness)         | **‚úì**  | ‚úì        | ‚úì         | ‚úì       | ‚úì        | ‚úì         | ‚úì         |
| [File Rotate (copytruncate)](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_rotate_truncate_correctness) | **‚úì**  |          |           |         |          | ‚úì         | ‚úì         |
| [File Truncation](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_truncate_correctness)                   | **‚úì**  | ‚úì        | ‚úì         | ‚úì       | ‚úì        | ‚úì         | ‚úì         |
| [Process (SIGHUP)](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/sighup_correctness)                         | **‚úì**  |          |           |         | ‚ö†        | ‚úì         | ‚úì         |
| [JSON (wrapped)](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/wrapped_json_correctness)                     | **‚úì**  | ‚úì        | ‚úì         | ‚úì       | ‚úì        | ‚úì         | ‚úì         |

To learn more about our correctness tests, please see the [Vector test harness][urls.vector_test_harness].

### Features

Vector is an end-to-end, unified, open data platform.

|                     | **Vector** | Beats | Fluentbit | Fluentd | Logstash | Splunk UF | Splunk HF | Telegraf |
| ------------------- | ---------- | ----- | --------- | ------- | -------- | --------- | --------- | -------- |
| **End-to-end**      | **‚úì**      |       |           |         |          |           |           | ‚úì        |
| Agent               | **‚úì**      | ‚úì     | ‚úì         |         |          | ‚úì         |           | ‚úì        |
| Aggregator          | **‚úì**      |       |           | ‚úì       | ‚úì        |           | ‚úì         | ‚úì        |
| **Unified**         | **‚úì**      |       |           |         |          |           |           | ‚úì        |
| Logs                | **‚úì**      | ‚úì     | ‚úì         | ‚úì       | ‚úì        | ‚úì         | ‚úì         | ‚úì        |
| Metrics             | **‚úì**      | ‚ö†     | ‚ö†         | ‚ö†       | ‚ö†        | ‚ö†         | ‚ö†         | ‚úì        |
| Traces              | üöß         |       |           |         |          |           |           |          |
| **Open**            | **‚úì**      |       | ‚úì         | ‚úì       |          |           |           | ‚úì        |
| Open-source         | **‚úì**      | ‚úì     | ‚úì         | ‚úì       | ‚úì        |           |           | ‚úì        |
| Vendor-neutral      | **‚úì**      |       | ‚úì         | ‚úì       |          |           |           | ‚úì        |
| **Reliability**     | **‚úì**      |       |           |         |          |           |           |          |
| Memory-safe         | **‚úì**      |       |           |         |          |           |           | ‚úì        |
| Delivery guarantees | **‚úì**      |       |           |         |          | ‚úì         | ‚úì         |          |
| Multi-core          | **‚úì**      | ‚úì     | ‚úì         | ‚úì       | ‚úì        | ‚úì         | ‚úì         | ‚úì        |


‚ö† = Not interoperable, metrics are represented as structured logs

---

&lt;p align=&quot;center&quot;&gt;
  Developed with ‚ù§Ô∏è by &lt;strong&gt;&lt;a href=&quot;https://datadoghq.com&quot;&gt;Datadog&lt;/a&gt;&lt;/strong&gt; - &lt;a href=&quot;https://github.com/vectordotdev/vector/security/policy&quot;&gt;Security Policy&lt;/a&gt; - &lt;a href=&quot;https://github.com/vectordotdev/vector/blob/master/PRIVACY.md&quot;&gt;Privacy Policy&lt;/a&gt;
&lt;/p&gt;

[docs.about.concepts]: https://vector.dev/docs/introduction/concepts/
[docs.about.introduction]: https://vector.dev/docs/introduction/
[docs.administration.monitoring]: https://vector.dev/docs/administration/monitoring/
[docs.administration.management]: https://vector.dev/docs/administration/management/
[docs.administration.upgrading]: https://vector.dev/docs/administration/upgrading/
[docs.administration.validating]: https://vector.dev/docs/administration/validating/
[docs.architecture.concurrency-model]: https://vector.dev/docs/architecture/concurrency-model/
[docs.architecture.data-model]: https://vector.dev/docs/architecture/data-model/
[docs.architecture.pipeline-model]: https://vector.dev/docs/architecture/pipeline-model/
[docs.architecture.runtime-model]: https://vector.dev/docs/architecture/runtime-model/
[docs.configuration.sinks]: https://vector.dev/docs/reference/configuration/sinks/
[docs.configuration.sources]: https://vector.dev/docs/reference/configuration/sources/
[docs.configuration.tests]: https://vector.dev/docs/reference/configuration/tests/
[docs.configuration.transforms]: https://vector.dev/docs/reference/configuration/transforms/
[docs.configuration.enrichment_tables]: https://vector.dev/docs/reference/configuration/global-options/#enrichment_tables
[docs.data-model.log]: https://vector.dev/docs/architecture/data-model/log/
[docs.data-model.metric]: https://vector.dev/docs/architecture/data-model/metric/
[docs.deployment.roles]: https://vector.dev/docs/setup/deployment/roles/
[docs.deployment.topologies]: https://vector.dev/docs/setup/deployment/topologies/
[docs.deployment]: https://vector.dev/docs/setup/deployment/
[docs.installation.manual]: https://vector.dev/docs/setup/installation/manual/
[docs.installation.operating_systems]: https://vector.dev/docs/setup/installation/operating-systems/
[docs.installation.package_managers]: https://vector.dev/docs/setup/installation/package-managers/
[docs.installation.platforms]: https://vector.dev/docs/setup/installation/platforms/
[docs.installation]: https://vector.dev/docs/setup/installation/
[docs.architecture.adaptive-request-concurrency]: https://vector.dev/docs/architecture/arc/
[docs.platforms.kubernetes]: https://vector.dev/docs/setup/installation/platforms/kubernetes/
[docs.quickstart]: https://vector.dev/docs/setup/quickstart/
[docs.reference.api]: https://vector.dev/docs/reference/api/
[docs.reference.cli]: https://vector.dev/docs/reference/cli/
[docs.reference.vrl]: https://vector.dev/docs/reference/vrl/
[docs.roles#agent]: https://vector.dev/docs/setup/deployment/roles/#agent
[docs.roles#aggregator]: https://vector.dev/docs/setup/deployment/roles/#aggregator
[docs.setup.installation]: https://vector.dev/docs/setup/installation/
[docs.setup.quickstart]: https://vector.dev/docs/setup/quickstart/
[docs.sinks.aws_cloudwatch_logs]: https://vector.dev/docs/reference/configuration/sinks/aws_cloudwatch_logs/
[docs.sinks.aws_s3]: https://vector.dev/docs/reference/configuration/sinks/aws_s3/
[docs.sinks.clickhouse]: https://vector.dev/docs/reference/configuration/sinks/clickhouse/
[docs.sinks.elasticsearch]: https://vector.dev/docs/reference/configuration/sinks/elasticsearch/
[docs.sinks.gcp_cloud_storage]: https://vector.dev/docs/reference/configuration/sinks/gcp_cloud_storage/
[docs.sinks]: https://vector.dev/docs/reference/configuration/sinks/
[docs.sources.docker_logs]: https://vector.dev/docs/reference/configuration/sources/docker_logs/
[docs.sources.file]: https://vector.dev/docs/reference/configuration/sources/file/
[docs.sources.http]: https://vector.dev/docs/reference/configuration/sources/http/
[docs.sources.journald]: https://vector.dev/docs/reference/configuration/sources/journald/
[docs.sources.kafka]: https://vector.dev/docs/reference/configuration/sources/kafka/
[docs.sources.socket]: https://vector.dev/docs/reference/configuration/sources/socket/
[docs.sources]: https://vector.dev/docs/reference/configuration/sources/
[docs.transforms.dedupe]: https://vector.dev/docs/reference/configuration/transforms/dedupe/
[docs.transforms.filter]: https://vector.dev/docs/reference/configuration/transforms/filter/
[docs.transforms.log_to_metric]: https://vector.dev/docs/reference/configuration/transforms/log_to_metric/
[docs.transforms.lua]: https://vector.dev/docs/reference/configuration/transforms/lua/
[docs.transforms.remap]: https://vector.dev/docs/reference/configuration/transforms/remap/
[docs.transforms]: https://vector.dev/docs/reference/configuration/transforms/
[docs.introduction.architecture]: https://vector.dev/docs/architecture/
[docs.introduction.guarantees]: https://vector.dev/docs/introduction/guarantees/
[docs.introduction.architecture]: https://vector.dev/docs/architecture/
[urls.production_users]: https://github.com/vectordotdev/vector/issues/790
[urls.rust]: https://www.rust-lang.org/
[urls.vector_calendar]: https://calendar.vector.dev
[urls.vector_chat]: https://chat.vector.dev
[urls.vector_code_of_conduct]: https://github.com/vectordotdev/vector/blob/master/CODE_OF_CONDUCT.md
[urls.vector_contributing_policy]: https://github.com/vectordotdev/vector/blob/master/CONTRIBUTING.md
[urls.vector_community]: https://vector.dev/community/
[urls.vector_privacy_policy]: https://github.com/vectordotdev/vector/blob/master/PRIVACY.md
[urls.vector_release_policy]: https://github.com/vectordotdev/vector/blob/master/RELEASING.md
[urls.vector_releases]: https://vector.dev/releases/
[urls.vector_releases_policy]: https://github.com/vectordotdev/vector/blob/master/RELEASES.md
[urls.vector_security_policy]: https://github.com/vectordotdev/vector/security/policy
[urls.vector_test_harness]: https://github.com/vectordotdev/vector-test-harness/
[urls.vector_twitter]: https://twitter.com/vectordotdev
[urls.vector_versioning_policy]: https://github.com/vectordotdev/vector/blob/master/VERSIONING.md
[urls.vote_feature]: https://github.com/vectordotdev/vector/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc+label%3A%22type%3A+feature%22
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[lance-format/lance]]></title>
            <link>https://github.com/lance-format/lance</link>
            <guid>https://github.com/lance-format/lance</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:52 GMT</pubDate>
            <description><![CDATA[Open Lakehouse Format for Multimodal AI. Convert from Parquet in 2 lines of code for 100x faster random access, vector index, and data versioning. Compatible with Pandas, DuckDB, Polars, Pyarrow, and PyTorch with more integrations coming..]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lance-format/lance">lance-format/lance</a></h1>
            <p>Open Lakehouse Format for Multimodal AI. Convert from Parquet in 2 lines of code for 100x faster random access, vector index, and data versioning. Compatible with Pandas, DuckDB, Polars, Pyarrow, and PyTorch with more integrations coming..</p>
            <p>Language: Rust</p>
            <p>Stars: 6,111</p>
            <p>Forks: 564</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;p align=&quot;center&quot;&gt;

&lt;img width=&quot;257&quot; alt=&quot;Lance Logo&quot; src=&quot;https://user-images.githubusercontent.com/917119/199353423-d3e202f7-0269-411d-8ff2-e747e419e492.png&quot;&gt;

**The Open Lakehouse Format for Multimodal AI**&lt;br/&gt;
**High-performance vector search, full-text search, random access, and feature engineering capabilities for the lakehouse.**&lt;br/&gt;
**Compatible with Pandas, DuckDB, Polars, PyArrow, Ray, Spark, and more integrations on the way.**

&lt;a href=&quot;https://lance.org&quot;&gt;Documentation&lt;/a&gt; ‚Ä¢
&lt;a href=&quot;https://lance.org/community&quot;&gt;Community&lt;/a&gt; ‚Ä¢
&lt;a href=&quot;https://discord.gg/lance&quot;&gt;Discord&lt;/a&gt;

[CI]: https://github.com/lance-format/lance/actions/workflows/rust.yml
[CI Badge]: https://github.com/lance-format/lance/actions/workflows/rust.yml/badge.svg
[Docs]: https://lance.org
[Docs Badge]: https://img.shields.io/badge/docs-passing-brightgreen
[crates.io]: https://crates.io/crates/lance
[crates.io badge]: https://img.shields.io/crates/v/lance.svg
[Python versions]: https://pypi.org/project/pylance/
[Python versions badge]: https://img.shields.io/pypi/pyversions/pylance

[![CI Badge]][CI]
[![Docs Badge]][Docs]
[![crates.io badge]][crates.io]
[![Python versions badge]][Python versions]

&lt;/p&gt;
&lt;/div&gt;

&lt;hr /&gt;

Lance is an open lakehouse format for multimodal AI. It contains a file format, table format, and catalog spec that allows you to build a complete lakehouse on top of object storage to power your AI workflows. Lance is perfect for:

1. Building search engines and feature stores with hybrid search capabilities.
2. Large-scale ML training requiring high performance IO and random access.
3. Storing, querying, and managing multimodal data including images, videos, audio, text, and embeddings.

The key features of Lance include:

* **Expressive hybrid search:** Combine vector similarity search, full-text search (BM25), and SQL analytics on the same dataset with accelerated secondary indices.

* **Lightning-fast random access:** 100x faster than Parquet or Iceberg for random access without sacrificing scan performance.

* **Native multimodal data support:** Store images, videos, audio, text, and embeddings in a single unified format with efficient blob encoding and lazy loading.

* **Data evolution:** Efficiently add columns with backfilled values without full table rewrites, perfect for ML feature engineering.

* **Zero-copy versioning:** Automatic versioning with ACID transactions, time travel, tags, and branches‚Äîno extra infrastructure needed.

* **Rich ecosystem integrations:** Apache Arrow, Pandas, Polars, DuckDB, Apache Spark, Ray, Trino, Apache Flink, and open catalogs (Apache Polaris, Unity Catalog, Apache Gravitino).

For more details, see the full [Lance format specification](https://lance.org/format).

&gt; [!TIP]
&gt; Lance is in active development and we welcome contributions. Please see our [contributing guide](https://lance.org/community/contributing/) for more information.

## Quick Start

**Installation**

```shell
pip install pylance
```

To install a preview release:

```shell
pip install --pre --extra-index-url https://pypi.fury.io/lance-format/pylance
```

&gt; [!NOTE]
&gt; For versions prior to 1.0.0-beta.4, you can find them at https://pypi.fury.io/lancedb/pylance

&gt; [!TIP]
&gt; Preview releases are released more often than full releases and contain the
&gt; latest features and bug fixes. They receive the same level of testing as full releases.
&gt; We guarantee they will remain published and available for download for at
&gt; least 6 months. When you want to pin to a specific version, prefer a stable release.

**Converting to Lance**

```python
import lance

import pandas as pd
import pyarrow as pa
import pyarrow.dataset

df = pd.DataFrame({&quot;a&quot;: [5], &quot;b&quot;: [10]})
uri = &quot;/tmp/test.parquet&quot;
tbl = pa.Table.from_pandas(df)
pa.dataset.write_dataset(tbl, uri, format=&#039;parquet&#039;)

parquet = pa.dataset.dataset(uri, format=&#039;parquet&#039;)
lance.write_dataset(parquet, &quot;/tmp/test.lance&quot;)
```

**Reading Lance data**
```python
dataset = lance.dataset(&quot;/tmp/test.lance&quot;)
assert isinstance(dataset, pa.dataset.Dataset)
```

**Pandas**
```python
df = dataset.to_table().to_pandas()
df
```

**DuckDB**
```python
import duckdb

# If this segfaults, make sure you have duckdb v0.7+ installed
duckdb.query(&quot;SELECT * FROM dataset LIMIT 10&quot;).to_df()
```

**Vector search**

Download the sift1m subset

```shell
wget ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz
tar -xzf sift.tar.gz
```

Convert it to Lance

```python
import lance
from lance.vector import vec_to_table
import numpy as np
import struct

nvecs = 1000000
ndims = 128
with open(&quot;sift/sift_base.fvecs&quot;, mode=&quot;rb&quot;) as fobj:
    buf = fobj.read()
    data = np.array(struct.unpack(&quot;&lt;128000000f&quot;, buf[4 : 4 + 4 * nvecs * ndims])).reshape((nvecs, ndims))
    dd = dict(zip(range(nvecs), data))

table = vec_to_table(dd)
uri = &quot;vec_data.lance&quot;
sift1m = lance.write_dataset(table, uri, max_rows_per_group=8192, max_rows_per_file=1024*1024)
```

Build the index

```python
sift1m.create_index(&quot;vector&quot;,
                    index_type=&quot;IVF_PQ&quot;,
                    num_partitions=256,  # IVF
                    num_sub_vectors=16)  # PQ
```

Search the dataset

```python
# Get top 10 similar vectors
import duckdb

dataset = lance.dataset(uri)

# Sample 100 query vectors. If this segfaults, make sure you have duckdb v0.7+ installed
sample = duckdb.query(&quot;SELECT vector FROM dataset USING SAMPLE 100&quot;).to_df()
query_vectors = np.array([np.array(x) for x in sample.vector])

# Get nearest neighbors for all of them
rs = [dataset.to_table(nearest={&quot;column&quot;: &quot;vector&quot;, &quot;k&quot;: 10, &quot;q&quot;: q})
      for q in query_vectors]
```

## Directory structure

| Directory          | Description              |
|--------------------|--------------------------|
| [rust](./rust)     | Core Rust implementation |
| [python](./python) | Python bindings (PyO3)   |
| [java](./java)     | Java bindings (JNI)      |
| [docs](./docs)     | Documentation source     |

## Benchmarks

### Vector search

We used the SIFT dataset to benchmark our results with 1M vectors of 128D

1. For 100 randomly sampled query vectors, we get &lt;1ms average response time (on a 2023 m2 MacBook Air)

![avg_latency.png](docs/src/images/avg_latency.png)

2. ANNs are always a trade-off between recall and performance

![avg_latency.png](docs/src/images/recall_vs_latency.png)

### Vs. parquet

We create a Lance dataset using the Oxford Pet dataset to do some preliminary performance testing of Lance as compared to Parquet and raw image/XMLs. For analytics queries, Lance is 50-100x better than reading the raw metadata. For batched random access, Lance is 100x better than both parquet and raw files.

![](docs/src/images/lance_perf.png)

## Why Lance for AI/ML workflows?

The machine learning development cycle involves multiple stages:

```mermaid
graph LR
    A[Collection] --&gt; B[Exploration];
    B --&gt; C[Analytics];
    C --&gt; D[Feature Engineer];
    D --&gt; E[Training];
    E --&gt; F[Evaluation];
    F --&gt; C;
    E --&gt; G[Deployment];
    G --&gt; H[Monitoring];
    H --&gt; A;
```

Traditional lakehouse formats were designed for SQL analytics and struggle with AI/ML workloads that require:
- **Vector search** for similarity and semantic retrieval
- **Fast random access** for sampling and interactive exploration
- **Multimodal data** storage (images, videos, audio alongside embeddings)
- **Data evolution** for feature engineering without full table rewrites
- **Hybrid search** combining vectors, full-text, and SQL predicates

While existing formats (Parquet, Iceberg, Delta Lake) excel at SQL analytics, they require additional specialized systems for AI capabilities. Lance brings these AI-first features directly into the lakehouse format.

A comparison of different formats across ML development stages:

|                     | Lance | Parquet &amp; ORC | JSON &amp; XML | TFRecord | Database | Warehouse |
|---------------------|-------|---------------|------------|----------|----------|-----------|
| Analytics           | Fast  | Fast          | Slow       | Slow     | Decent   | Fast      |
| Feature Engineering | Fast  | Fast          | Decent     | Slow     | Decent   | Good      |
| Training            | Fast  | Decent        | Slow       | Fast     | N/A      | N/A       |
| Exploration         | Fast  | Slow          | Fast       | Slow     | Fast     | Decent    |
| Infra Support       | Rich  | Rich          | Decent     | Limited  | Rich     | Rich      |

</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[oxc-project/oxc]]></title>
            <link>https://github.com/oxc-project/oxc</link>
            <guid>https://github.com/oxc-project/oxc</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:51 GMT</pubDate>
            <description><![CDATA[‚öì A collection of high-performance JavaScript tools.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/oxc-project/oxc">oxc-project/oxc</a></h1>
            <p>‚öì A collection of high-performance JavaScript tools.</p>
            <p>Language: Rust</p>
            <p>Stars: 19,501</p>
            <p>Forks: 856</p>
            <p>Stars today: 68 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;br&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://oxc.rs&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://oxc.rs/oxc-light.svg&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://oxc.rs/oxc-dark.svg&quot;&gt;
      &lt;img alt=&quot;Oxc logo&quot; src=&quot;https://oxc.rs/oxc-dark.svg&quot; height=&quot;60&quot;&gt;
    &lt;/picture&gt;
  &lt;/a&gt;
  &lt;br&gt;
  &lt;br&gt;
  &lt;br&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;

[![MIT licensed][license-badge]][license-url]
[![Build Status][ci-badge]][ci-url]
[![Code Coverage][code-coverage-badge]][code-coverage-url]
[![CodSpeed Badge](https://img.shields.io/endpoint?url=https://codspeed.io/badge.json)](https://codspeed.io/oxc-project/oxc)
[![Sponsors][sponsors-badge]][sponsors-url]

[![Discord chat][discord-badge]][discord-url]
[![Playground][playground-badge]][playground-url]
[![Website][website-badge]][website-url]

&lt;/div&gt;

## ‚öì Oxc

_/o ä …õks siÀê/_

The Oxidation Compiler is a collection of high-performance tools for JavaScript and TypeScript written in Rust.

Oxc is part of [VoidZero](https://voidzero.dev/)&#039;s vision for a unified, high-performance toolchain for JavaScript. It powers [Rolldown](https://rolldown.rs) ([Vite]&#039;s future bundler) and enables the next generation of ultra-fast development tools that work seamlessly together.

For more information, check out our website at [oxc.rs](https://oxc.rs).

&lt;sub&gt;\* Oxidation is the chemical process that creates rust&lt;/sub&gt;

## üèóÔ∏è Design Principles

- **Performance**: Through rigorous performance engineering.
- **Correctness**: Through conformance testing to standards and similar projects.
- **Developer Experience**: Clear APIs, comprehensive documentation, and sensible configuration.
- **Modular composability**: Use individual components independently or compose them into complete toolchains.

Read more about our [architecture](https://oxc.rs/docs/learn/architecture/parser.html) and [performance philosophy](https://oxc.rs/docs/learn/performance).

## üì¶ Tools &amp; Packages

| Tool        | npm                                                     | crates.io                                                   |
| ----------- | ------------------------------------------------------- | ----------------------------------------------------------- |
| Linter      | [oxlint](https://npmx.dev/package/oxlint)               | -                                                           |
| Formatter   | [oxfmt](https://npmx.dev/package/oxfmt)                 | -                                                           |
| Parser      | [oxc-parser](https://npmx.dev/package/oxc-parser)       | [oxc_parser](https://crates.io/crates/oxc_parser)           |
| Transformer | [oxc-transform](https://npmx.dev/package/oxc-transform) | [oxc_transformer](https://crates.io/crates/oxc_transformer) |
| Minifier    | [oxc-minify](https://npmx.dev/package/oxc-minify)       | [oxc_minifier](https://crates.io/crates/oxc_minifier)       |
| Resolver    | [oxc-resolver](https://npmx.dev/package/oxc-resolver)   | [oxc_resolver](https://crates.io/crates/oxc_resolver)       |

See [documentation](https://oxc.rs/) for detailed usage guides for each tool.

## ‚ö°Ô∏è Quick Start

### Linter

The production-ready linter catches mistakes for you with sensible defaults and optional configuration:

```bash
npx oxlint@latest
```

To give you an idea of its capabilities, here is an example from the [vscode] repository, which finishes linting 4800+ files in 0.7 seconds:

&lt;p float=&quot;left&quot; align=&quot;left&quot;&gt;
  &lt;img src=&quot;https://cdn.jsdelivr.net/gh/oxc-project/oxc-assets/linter-screenshot.png&quot; width=&quot;60%&quot;&gt;
&lt;/p&gt;

‚Üí [oxlint documentation](https://oxc.rs/docs/guide/usage/linter/cli.html)

### Formatter

Fast, opinionated code formatter compatible with [Prettier]:

```bash
npx oxfmt@latest
```

‚Üí [Formatter documentation](https://oxc.rs/docs/guide/usage/formatter)

### Parser (Node.js)

The fastest JavaScript/TypeScript parser written in Rust:

```bash
npm install oxc-parser
```

```js
import { parseSync } from &quot;oxc-parser&quot;;
const result = parseSync(&quot;const x = 1;&quot;);
```

‚Üí [Parser documentation](https://oxc.rs/docs/guide/usage/parser)

### Transformer (Node.js)

TypeScript, React, and modern JavaScript transformation:

```bash
npm install oxc-transform
```

```js
import { transform } from &quot;oxc-transform&quot;;
const result = transform(&quot;source.tsx&quot;, code, { typescript: true });
```

‚Üí [Transformer documentation](https://oxc.rs/docs/guide/usage/transformer)

### Minifier (Node.js)

High-performance JavaScript minifier:

```bash
npm install oxc-minify
```

```js
import { minify } from &quot;oxc-minify&quot;;
const result = minify(code, { mangle: true });
```

‚Üí [Minifier documentation](https://oxc.rs/docs/guide/usage/minifier)

### Rust

Individual crates are published for building your own JavaScript tools:

```toml
[dependencies]
oxc = &quot;0.x&quot;
```

‚Üí [Rust documentation](https://docs.rs/oxc)

## VoidZero Inc.

Oxc is a project of [VoidZero](https://voidzero.dev/), see our announcement [Announcing VoidZero - Next Generation Toolchain for JavaScript](https://voidzero.dev/blog).

If you have requirements for JavaScript tools at scale, please [get in touch](https://forms.gle/WQgjyzYJpwurpxWKA)!

## üôã Who&#039;s using Oxc?

[Rolldown] and [Nuxt] use Oxc for parsing. [Rolldown] also uses Oxc for transformation and minification. [Nova], [swc-node], and [knip] use [oxc_resolver][docs-resolver-url] for module resolution. [Preact], [Shopify], [ByteDance], and [Shopee] use oxlint for linting.

[See more projects using Oxc ‚Üí](https://oxc.rs/docs/guide/projects.html)

## ‚úçÔ∏è Contribute

Check out some of the [good first issues](https://github.com/oxc-project/oxc/contribute) or ask us on [Discord][discord-url].

See [CONTRIBUTING.md](./CONTRIBUTING.md) for guidance, or read the complete [contributing guide on our website ‚Üí](https://oxc.rs/docs/contribute/introduction.html)

If you are unable to contribute by code, you can still participate by:

- Add a [GitHub Star](https://github.com/oxc-project/oxc/stargazers) to the project
- Join us on [Discord][discord-url]
- [Follow me on X](https://x.com/boshen_c) and post about this project

## ü§ù Credits

This project was incubated with the assistance of these exceptional mentors and their projects:

- [Biome][biome] - [@ematipico](https://github.com/ematipico)
- [Ruff][ruff] - [@charliermarsh](https://github.com/charliermarsh), [@MichaReiser](https://github.com/MichaReiser)
- [quick-lint-js](https://github.com/quick-lint/quick-lint-js) - [@strager](https://github.com/strager)
- [elm-review](https://package.elm-lang.org/packages/jfmengels/elm-review/latest) - [@jfmengels](https://github.com/jfmengels)

Special thanks go to:

- [@domonji](https://github.com/domonji) for bootstrapping this project together and also completing the TypeScript parser
- [@tongtong-lu](https://github.com/tongtong-lu) and [@guan-wy](https://github.com/guan-wy) for designing the [project logo](https://github.com/oxc-project/oxc-assets)

## ‚ù§ Who&#039;s [Sponsoring Oxc](https://github.com/sponsors/Boshen)?

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/sponsors/Boshen&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/Boshen/sponsors/main/sponsors.svg&quot; alt=&quot;My sponsors&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

## üìñ License

Oxc is free and open-source software licensed under the [MIT License](./LICENSE).

Oxc ports or copies code from other open source projects, their licenses are listed in [**Third-party library licenses**](./THIRD-PARTY-LICENSE).

[discord-badge]: https://img.shields.io/discord/1079625926024900739?logo=discord&amp;label=Discord
[discord-url]: https://discord.gg/9uXCAwqQZW
[license-badge]: https://img.shields.io/badge/license-MIT-blue.svg
[license-url]: https://github.com/oxc-project/oxc/blob/main/LICENSE
[ci-badge]: https://github.com/oxc-project/oxc/actions/workflows/ci.yml/badge.svg?event=push&amp;branch=main
[ci-url]: https://github.com/oxc-project/oxc/actions/workflows/ci.yml?query=event%3Apush+branch%3Amain
[code-coverage-badge]: https://codecov.io/gh/oxc-project/oxc/graph/badge.svg?token=FVHEH0BQLJ
[code-coverage-url]: https://codecov.io/gh/oxc-project/oxc
[sponsors-badge]: https://img.shields.io/github/sponsors/Boshen
[sponsors-url]: https://github.com/sponsors/Boshen
[playground-badge]: https://img.shields.io/badge/Playground-blue?color=9BE4E0
[playground-url]: https://playground.oxc.rs/
[website-badge]: https://img.shields.io/badge/Website-blue
[website-url]: https://oxc.rs
[docs-resolver-url]: https://docs.rs/oxc_resolver
[biome]: https://biomejs.dev/
[ruff]: https://beta.ruff.rs
[vscode]: https://github.com/microsoft/vscode
[rolldown]: https://rolldown.rs
[vite]: https://vitejs.dev/
[nuxt]: https://nuxt.com/
[nova]: https://trynova.dev/
[swc-node]: https://github.com/swc-project/swc-node
[knip]: https://github.com/webpro/knip
[preact]: https://preactjs.com/
[shopify]: https://shopify.com/
[bytedance]: https://www.bytedance.com/
[shopee]: https://shopee.com/
[prettier]: https://prettier.io/
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[astral-sh/ruff]]></title>
            <link>https://github.com/astral-sh/ruff</link>
            <guid>https://github.com/astral-sh/ruff</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:50 GMT</pubDate>
            <description><![CDATA[An extremely fast Python linter and code formatter, written in Rust.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/astral-sh/ruff">astral-sh/ruff</a></h1>
            <p>An extremely fast Python linter and code formatter, written in Rust.</p>
            <p>Language: Rust</p>
            <p>Stars: 46,000</p>
            <p>Forks: 1,800</p>
            <p>Stars today: 30 stars today</p>
            <h2>README</h2><pre>&lt;!-- Begin section: Overview --&gt;

# Ruff

[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)
[![image](https://img.shields.io/pypi/v/ruff.svg)](https://pypi.python.org/pypi/ruff)
[![image](https://img.shields.io/pypi/l/ruff.svg)](https://github.com/astral-sh/ruff/blob/main/LICENSE)
[![image](https://img.shields.io/pypi/pyversions/ruff.svg)](https://pypi.python.org/pypi/ruff)
[![Actions status](https://github.com/astral-sh/ruff/workflows/CI/badge.svg)](https://github.com/astral-sh/ruff/actions)
[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;logoColor=white)](https://discord.com/invite/astral-sh)

[**Docs**](https://docs.astral.sh/ruff/) | [**Playground**](https://play.ruff.rs/)

An extremely fast Python linter and code formatter, written in Rust.

&lt;p align=&quot;center&quot;&gt;
  &lt;picture align=&quot;center&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://user-images.githubusercontent.com/1309177/232603514-c95e9b0f-6b31-43de-9a80-9e844173fd6a.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://user-images.githubusercontent.com/1309177/232603516-4fb4892d-585c-4b20-b810-3db9161831e4.svg&quot;&gt;
    &lt;img alt=&quot;Shows a bar chart with benchmark results.&quot; src=&quot;https://user-images.githubusercontent.com/1309177/232603516-4fb4892d-585c-4b20-b810-3db9161831e4.svg&quot;&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;i&gt;Linting the CPython codebase from scratch.&lt;/i&gt;
&lt;/p&gt;

- ‚ö°Ô∏è 10-100x faster than existing linters (like Flake8) and formatters (like Black)
- üêç Installable via `pip`
- üõ†Ô∏è `pyproject.toml` support
- ü§ù Python 3.14 compatibility
- ‚öñÔ∏è Drop-in parity with [Flake8](https://docs.astral.sh/ruff/faq/#how-does-ruffs-linter-compare-to-flake8), isort, and [Black](https://docs.astral.sh/ruff/faq/#how-does-ruffs-formatter-compare-to-black)
- üì¶ Built-in caching, to avoid re-analyzing unchanged files
- üîß Fix support, for automatic error correction (e.g., automatically remove unused imports)
- üìè Over [800 built-in rules](https://docs.astral.sh/ruff/rules/), with native re-implementations
    of popular Flake8 plugins, like flake8-bugbear
- ‚å®Ô∏è First-party [editor integrations](https://docs.astral.sh/ruff/editors) for [VS Code](https://github.com/astral-sh/ruff-vscode) and [more](https://docs.astral.sh/ruff/editors/setup)
- üåé Monorepo-friendly, with [hierarchical and cascading configuration](https://docs.astral.sh/ruff/configuration/#config-file-discovery)

Ruff aims to be orders of magnitude faster than alternative tools while integrating more
functionality behind a single, common interface.

Ruff can be used to replace [Flake8](https://pypi.org/project/flake8/) (plus dozens of plugins),
[Black](https://github.com/psf/black), [isort](https://pypi.org/project/isort/),
[pydocstyle](https://pypi.org/project/pydocstyle/), [pyupgrade](https://pypi.org/project/pyupgrade/),
[autoflake](https://pypi.org/project/autoflake/), and more, all while executing tens or hundreds of
times faster than any individual tool.

Ruff is extremely actively developed and used in major open-source projects like:

- [Apache Airflow](https://github.com/apache/airflow)
- [Apache Superset](https://github.com/apache/superset)
- [FastAPI](https://github.com/tiangolo/fastapi)
- [Hugging Face](https://github.com/huggingface/transformers)
- [Pandas](https://github.com/pandas-dev/pandas)
- [SciPy](https://github.com/scipy/scipy)

...and [many more](#whos-using-ruff).

Ruff is backed by [Astral](https://astral.sh), the creators of
[uv](https://github.com/astral-sh/uv) and [ty](https://github.com/astral-sh/ty).

Read the [launch
post](https://astral.sh/blog/announcing-astral-the-company-behind-ruff), or the
original [project
announcement](https://notes.crmarsh.com/python-tooling-could-be-much-much-faster).

## Testimonials

[**Sebasti√°n Ram√≠rez**](https://twitter.com/tiangolo/status/1591912354882764802), creator
of [FastAPI](https://github.com/tiangolo/fastapi):

&gt; Ruff is so fast that sometimes I add an intentional bug in the code just to confirm it&#039;s actually
&gt; running and checking the code.

[**Nick Schrock**](https://twitter.com/schrockn/status/1612615862904827904), founder of [Elementl](https://www.elementl.com/),
co-creator of [GraphQL](https://graphql.org/):

&gt; Why is Ruff a gamechanger? Primarily because it is nearly 1000x faster. Literally. Not a typo. On
&gt; our largest module (dagster itself, 250k LOC) pylint takes about 2.5 minutes, parallelized across 4
&gt; cores on my M1. Running ruff against our _entire_ codebase takes .4 seconds.

[**Bryan Van de Ven**](https://github.com/bokeh/bokeh/pull/12605), co-creator
of [Bokeh](https://github.com/bokeh/bokeh/), original author
of [Conda](https://docs.conda.io/en/latest/):

&gt; Ruff is ~150-200x faster than flake8 on my machine, scanning the whole repo takes ~0.2s instead of
&gt; ~20s. This is an enormous quality of life improvement for local dev. It&#039;s fast enough that I added
&gt; it as an actual commit hook, which is terrific.

[**Timothy Crosley**](https://twitter.com/timothycrosley/status/1606420868514877440),
creator of [isort](https://github.com/PyCQA/isort):

&gt; Just switched my first project to Ruff. Only one downside so far: it&#039;s so fast I couldn&#039;t believe
&gt; it was working till I intentionally introduced some errors.

[**Tim Abbott**](https://github.com/zulip/zulip/pull/23431#issuecomment-1302557034), lead developer of [Zulip](https://github.com/zulip/zulip) (also [here](https://github.com/astral-sh/ruff/issues/465#issuecomment-1317400028)):

&gt; This is just ridiculously fast... `ruff` is amazing.

&lt;!-- End section: Overview --&gt;

## Table of Contents

For more, see the [documentation](https://docs.astral.sh/ruff/).

1. [Getting Started](#getting-started)
1. [Configuration](#configuration)
1. [Rules](#rules)
1. [Contributing](#contributing)
1. [Support](#support)
1. [Acknowledgements](#acknowledgements)
1. [Who&#039;s Using Ruff?](#whos-using-ruff)
1. [License](#license)

## Getting Started&lt;a id=&quot;getting-started&quot;&gt;&lt;/a&gt;

For more, see the [documentation](https://docs.astral.sh/ruff/).

### Installation

Ruff is available as [`ruff`](https://pypi.org/project/ruff/) on PyPI.

Invoke Ruff directly with [`uvx`](https://docs.astral.sh/uv/):

```shell
uvx ruff check   # Lint all files in the current directory.
uvx ruff format  # Format all files in the current directory.
```

Or install Ruff with `uv` (recommended), `pip`, or `pipx`:

```shell
# With uv.
uv tool install ruff@latest  # Install Ruff globally.
uv add --dev ruff            # Or add Ruff to your project.

# With pip.
pip install ruff

# With pipx.
pipx install ruff
```

Starting with version `0.5.0`, Ruff can be installed with our standalone installers:

```shell
# On macOS and Linux.
curl -LsSf https://astral.sh/ruff/install.sh | sh

# On Windows.
powershell -c &quot;irm https://astral.sh/ruff/install.ps1 | iex&quot;

# For a specific version.
curl -LsSf https://astral.sh/ruff/0.15.4/install.sh | sh
powershell -c &quot;irm https://astral.sh/ruff/0.15.4/install.ps1 | iex&quot;
```

You can also install Ruff via [Homebrew](https://formulae.brew.sh/formula/ruff), [Conda](https://anaconda.org/conda-forge/ruff),
and with [a variety of other package managers](https://docs.astral.sh/ruff/installation/).

### Usage

To run Ruff as a linter, try any of the following:

```shell
ruff check                          # Lint all files in the current directory (and any subdirectories).
ruff check path/to/code/            # Lint all files in `/path/to/code` (and any subdirectories).
ruff check path/to/code/*.py        # Lint all `.py` files in `/path/to/code`.
ruff check path/to/code/to/file.py  # Lint `file.py`.
ruff check @arguments.txt           # Lint using an input file, treating its contents as newline-delimited command-line arguments.
```

Or, to run Ruff as a formatter:

```shell
ruff format                          # Format all files in the current directory (and any subdirectories).
ruff format path/to/code/            # Format all files in `/path/to/code` (and any subdirectories).
ruff format path/to/code/*.py        # Format all `.py` files in `/path/to/code`.
ruff format path/to/code/to/file.py  # Format `file.py`.
ruff format @arguments.txt           # Format using an input file, treating its contents as newline-delimited command-line arguments.
```

Ruff can also be used as a [pre-commit](https://pre-commit.com/) hook via [`ruff-pre-commit`](https://github.com/astral-sh/ruff-pre-commit):

```yaml
- repo: https://github.com/astral-sh/ruff-pre-commit
  # Ruff version.
  rev: v0.15.4
  hooks:
    # Run the linter.
    - id: ruff-check
      args: [ --fix ]
    # Run the formatter.
    - id: ruff-format
```

Ruff can also be used as a [VS Code extension](https://github.com/astral-sh/ruff-vscode) or with [various other editors](https://docs.astral.sh/ruff/editors/setup).

Ruff can also be used as a [GitHub Action](https://github.com/features/actions) via
[`ruff-action`](https://github.com/astral-sh/ruff-action):

```yaml
name: Ruff
on: [ push, pull_request ]
jobs:
  ruff:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: astral-sh/ruff-action@v3
```

### Configuration&lt;a id=&quot;configuration&quot;&gt;&lt;/a&gt;

Ruff can be configured through a `pyproject.toml`, `ruff.toml`, or `.ruff.toml` file (see:
[_Configuration_](https://docs.astral.sh/ruff/configuration/), or [_Settings_](https://docs.astral.sh/ruff/settings/)
for a complete list of all configuration options).

If left unspecified, Ruff&#039;s default configuration is equivalent to the following `ruff.toml` file:

```toml
# Exclude a variety of commonly ignored directories.
exclude = [
    &quot;.bzr&quot;,
    &quot;.direnv&quot;,
    &quot;.eggs&quot;,
    &quot;.git&quot;,
    &quot;.git-rewrite&quot;,
    &quot;.hg&quot;,
    &quot;.ipynb_checkpoints&quot;,
    &quot;.mypy_cache&quot;,
    &quot;.nox&quot;,
    &quot;.pants.d&quot;,
    &quot;.pyenv&quot;,
    &quot;.pytest_cache&quot;,
    &quot;.pytype&quot;,
    &quot;.ruff_cache&quot;,
    &quot;.svn&quot;,
    &quot;.tox&quot;,
    &quot;.venv&quot;,
    &quot;.vscode&quot;,
    &quot;__pypackages__&quot;,
    &quot;_build&quot;,
    &quot;buck-out&quot;,
    &quot;build&quot;,
    &quot;dist&quot;,
    &quot;node_modules&quot;,
    &quot;site-packages&quot;,
    &quot;venv&quot;,
]

# Same as Black.
line-length = 88
indent-width = 4

# Assume Python 3.10
target-version = &quot;py310&quot;

[lint]
# Enable Pyflakes (`F`) and a subset of the pycodestyle (`E`) codes by default.
select = [&quot;E4&quot;, &quot;E7&quot;, &quot;E9&quot;, &quot;F&quot;]
ignore = []

# Allow fix for all enabled rules (when `--fix`) is provided.
fixable = [&quot;ALL&quot;]
unfixable = []

# Allow unused variables when underscore-prefixed.
dummy-variable-rgx = &quot;^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$&quot;

[format]
# Like Black, use double quotes for strings.
quote-style = &quot;double&quot;

# Like Black, indent with spaces, rather than tabs.
indent-style = &quot;space&quot;

# Like Black, respect magic trailing commas.
skip-magic-trailing-comma = false

# Like Black, automatically detect the appropriate line ending.
line-ending = &quot;auto&quot;
```

Note that, in a `pyproject.toml`, each section header should be prefixed with `tool.ruff`. For
example, `[lint]` should be replaced with `[tool.ruff.lint]`.

Some configuration options can be provided via dedicated command-line arguments, such as those
related to rule enablement and disablement, file discovery, and logging level:

```shell
ruff check --select F401 --select F403 --quiet
```

The remaining configuration options can be provided through a catch-all `--config` argument:

```shell
ruff check --config &quot;lint.per-file-ignores = {&#039;some_file.py&#039; = [&#039;F841&#039;]}&quot;
```

To opt in to the latest lint rules, formatter style changes, interface updates, and more, enable
[preview mode](https://docs.astral.sh/ruff/preview/) by setting `preview = true` in your configuration
file or passing `--preview` on the command line. Preview mode enables a collection of unstable
features that may change prior to stabilization.

See `ruff help` for more on Ruff&#039;s top-level commands, or `ruff help check` and `ruff help format`
for more on the linting and formatting commands, respectively.

## Rules&lt;a id=&quot;rules&quot;&gt;&lt;/a&gt;

&lt;!-- Begin section: Rules --&gt;

**Ruff supports over 900 lint rules**, many of which are inspired by popular tools like Flake8,
isort, pyupgrade, and others. Regardless of the rule&#039;s origin, Ruff re-implements every rule in
Rust as a first-party feature.

By default, Ruff enables Flake8&#039;s `F` rules, along with a subset of the `E` rules, omitting any
stylistic rules that overlap with the use of a formatter, like `ruff format` or
[Black](https://github.com/psf/black).

If you&#039;re just getting started with Ruff, **the default rule set is a great place to start**: it
catches a wide variety of common errors (like unused imports) with zero configuration.

In [preview](https://docs.astral.sh/ruff/preview/), Ruff enables an expanded set of default rules
that includes rules from the `B`, `UP`, and `RUF` categories, as well as many more. If you give the
new defaults a try, feel free to leave feedback in the [GitHub
discussion](https://github.com/astral-sh/ruff/discussions/23203), where you can also find the new
rule set listed in full.

&lt;!-- End section: Rules --&gt;

Beyond the defaults, Ruff re-implements some of the most popular Flake8 plugins and related code
quality tools, including:

- [autoflake](https://pypi.org/project/autoflake/)
- [eradicate](https://pypi.org/project/eradicate/)
- [flake8-2020](https://pypi.org/project/flake8-2020/)
- [flake8-annotations](https://pypi.org/project/flake8-annotations/)
- [flake8-async](https://pypi.org/project/flake8-async)
- [flake8-bandit](https://pypi.org/project/flake8-bandit/) ([#1646](https://github.com/astral-sh/ruff/issues/1646))
- [flake8-blind-except](https://pypi.org/project/flake8-blind-except/)
- [flake8-boolean-trap](https://pypi.org/project/flake8-boolean-trap/)
- [flake8-bugbear](https://pypi.org/project/flake8-bugbear/)
- [flake8-builtins](https://pypi.org/project/flake8-builtins/)
- [flake8-commas](https://pypi.org/project/flake8-commas/)
- [flake8-comprehensions](https://pypi.org/project/flake8-comprehensions/)
- [flake8-copyright](https://pypi.org/project/flake8-copyright/)
- [flake8-datetimez](https://pypi.org/project/flake8-datetimez/)
- [flake8-debugger](https://pypi.org/project/flake8-debugger/)
- [flake8-django](https://pypi.org/project/flake8-django/)
- [flake8-docstrings](https://pypi.org/project/flake8-docstrings/)
- [flake8-eradicate](https://pypi.org/project/flake8-eradicate/)
- [flake8-errmsg](https://pypi.org/project/flake8-errmsg/)
- [flake8-executable](https://pypi.org/project/flake8-executable/)
- [flake8-future-annotations](https://pypi.org/project/flake8-future-annotations/)
- [flake8-gettext](https://pypi.org/project/flake8-gettext/)
- [flake8-implicit-str-concat](https://pypi.org/project/flake8-implicit-str-concat/)
- [flake8-import-conventions](https://github.com/joaopalmeiro/flake8-import-conventions)
- [flake8-logging](https://pypi.org/project/flake8-logging/)
- [flake8-logging-format](https://pypi.org/project/flake8-logging-format/)
- [flake8-no-pep420](https://pypi.org/project/flake8-no-pep420)
- [flake8-pie](https://pypi.org/project/flake8-pie/)
- [flake8-print](https://pypi.org/project/flake8-print/)
- [flake8-pyi](https://pypi.org/project/flake8-pyi/)
- [flake8-pytest-style](https://pypi.org/project/flake8-pytest-style/)
- [flake8-quotes](https://pypi.org/project/flake8-quotes/)
- [flake8-raise](https://pypi.org/project/flake8-raise/)
- [flake8-return](https://pypi.org/project/flake8-return/)
- [flake8-self](https://pypi.org/project/flake8-self/)
- [flake8-simplify](https://pypi.org/project/flake8-simplify/)
- [flake8-slots](https://pypi.org/project/flake8-slots/)
- [flake8-super](https://pypi.org/project/flake8-super/)
- [flake8-tidy-imports](https://pypi.org/project/flake8-tidy-imports/)
- [flake8-todos](https://pypi.org/project/flake8-todos/)
- [flake8-type-checking](https://pypi.org/project/flake8-type-checking/)
- [flake8-use-pathlib](https://pypi.org/project/flake8-use-pathlib/)
- [flynt](https://pypi.org/project/flynt/) ([#2102](https://github.com/astral-sh/ruff/issues/2102))
- [isort](https://pypi.org/project/isort/)
- [mccabe](https://pypi.org/project/mccabe/)
- [pandas-vet](https://pypi.org/project/pandas-vet/)
- [pep8-naming](https://pypi.org/project/pep8-naming/)
- [pydocstyle](https://pypi.org/project/pydocstyle/)
- [pygrep-hooks](https://github.com/pre-commit/pygrep-hooks)
- [pylint-airflow](https://pypi.org/project/pylint-airflow/)
- [pyupgrade](https://pypi.org/project/pyupgrade/)
- [tryceratops](https://pypi.org/project/tryceratops/)
- [yesqa](https://pypi.org/project/yesqa/)

For a complete enumeration of the supported rules, see [_Rules_](https://docs.astral.sh/ruff/rules/).

## Contributing&lt;a id=&quot;contributing&quot;&gt;&lt;/a&gt;

Contributions are welcome and highly appreciated. To get started, check out the
[**contributing guidelines**](https://docs.astral.sh/ruff/contributing/).

You can also join us on [**Discord**](https://discord.com/invite/astral-sh).

## Support&lt;a id=&quot;support&quot;&gt;&lt;/a&gt;

Having trouble? Check out the existing issues on [**GitHub**](https://github.com/astral-sh/ruff/issues),
or feel free to [**open a new one**](https://github.com/astral-sh/ruff/issues/new).

You can also ask for help on [**Discord**](https://discord.com/invite/astral-sh).

## Acknowledgements&lt;a id=&quot;acknowledgements&quot;&gt;&lt;/a&gt;

Ruff&#039;s linter draws on both the APIs and implementation details of many other
tools in the Python ecosystem, especially [Flake8](https://github.com/PyCQA/flake8), [Pyflakes](https://github.com/PyCQA/pyflakes),
[pycodestyle](https://github.com/PyCQA/pycodestyle), [pydocstyle](https://github.com/PyCQA/pydocstyle),
[pyupgrade](https://github.com/asottile/pyupgrade), and [isort](https://github.com/PyCQA/isort).

In some cases, Ruff includes a &quot;direct&quot; Rust port of the corresponding tool.
We&#039;re grateful to the maintainers of these tools for their work, and for all
the value they&#039;ve provided to the Python community.

Ruff&#039;s formatter is built on a fork of Rome&#039;s [`rome_formatter`](https://github.com/rome/tools/tree/main/crates/rome_formatter),
and again draws on both API and implementation details from [Rome](https://github.com/rome/tools),
[Prettier](https://github.com/prettier/prettier), and [Black](https://github.com/psf/black).

Ruff&#039;s import resolver is based on the import resolution algorithm from [Pyright](https://github.com/microsoft/pyright).

Ruff is also influenced by a number of tools outside the Python ecosystem, like
[Clippy](https://github.com/rust-lang/rust-clippy) and [ESLint](https://github.com/eslint/eslint).

Ruff is the beneficiary of a large number of [contributors](https://github.com/astral-sh/ruff/graphs/contributors).

Ruff is released under the MIT license.

## Who&#039;s Using Ruff?&lt;a id=&quot;whos-using-ruff&quot;&gt;&lt;/a&gt;

Ruff is used by a number of major open-source projects and companies, including:

- [Albumentations](https://github.com/albumentations-team/AlbumentationsX)
- Amazon ([AWS SAM](https://github.com/aws/serverless-application-model))
- [Anki](https://apps.ankiweb.net/)
- Anthropic ([Python SDK](https://github.com/anthropics/anthropic-sdk-python))
- [Apache Airflow](https://github.com/apache/airflow)
- AstraZeneca ([Magnus](https://github.com/AstraZeneca/magnus-core))
- [Babel](https://github.com/python-babel/babel)
- Benchling ([Refac](https://github.com/benchling/refac))
- [Bokeh](https://github.com/bokeh/bokeh)
- Capital One ([datacompy](https://github.com/capitalone/datacompy))
- CrowdCent ([NumerBlox](https://github.com/crowdcent/numerblox)) &lt;!-- typos: ignore --&gt;
- [Cryptography (PyCA)](https://github.com/pyca/cryptography)
- CERN ([Indico](https://getindico.io/))
- [DVC](https://github.com/iterative/dvc)
- [Dagger](https://github.com/dagger/dagger)
- [Dagster](https://github.com/dagster-io/dagster)
- Databricks ([MLflow](https://github.com/mlflow/mlflow))
- [Dify](https://github.com/langgenius/dify)
- [FastAPI](https://github.com/tiangolo/fastapi)
- [Godot](https://github.com/godotengine/godot)
- [Gradio](https://github.com/gradio-app/gradio)
- [Great Exp

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[servo/servo]]></title>
            <link>https://github.com/servo/servo</link>
            <guid>https://github.com/servo/servo</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:49 GMT</pubDate>
            <description><![CDATA[Servo aims to empower developers with a lightweight, high-performance alternative for embedding web technologies in applications.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/servo/servo">servo/servo</a></h1>
            <p>Servo aims to empower developers with a lightweight, high-performance alternative for embedding web technologies in applications.</p>
            <p>Language: Rust</p>
            <p>Stars: 35,589</p>
            <p>Forks: 3,490</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre># The Servo Parallel Browser Engine Project

Servo is a prototype web browser engine written in the
[Rust](https://github.com/rust-lang/rust) language. It is currently developed on
64-bit macOS, 64-bit Linux, 64-bit Windows, 64-bit OpenHarmony, and Android.

Servo welcomes contribution from everyone. Check out:

- The [Servo Book](https://book.servo.org) for documentation
- [servo.org](https://servo.org/) for news and guides

Coordination of Servo development happens:
- Here in the Github Issues
- On the [Servo Zulip](https://servo.zulipchat.com/)
- In video calls advertised in the [Servo Project](https://github.com/servo/project/issues) repo.

## Getting started

For more detailed build instructions, see the Servo Book under [Getting the Code] and [Building Servo].

[Getting the Code]: https://book.servo.org/building/getting-the-code.html
[Building Servo]: https://book.servo.org/building/building.html

### macOS

- Download and install [Xcode](https://developer.apple.com/xcode/) and [`brew`](https://brew.sh/).
- Install `uv`: `curl -LsSf https://astral.sh/uv/install.sh | sh` 
- Install `rustup`: `curl --proto &#039;=https&#039; --tlsv1.2 -sSf https://sh.rustup.rs | sh`
- Restart your shell to make sure `cargo` is available
- Install the other dependencies: `./mach bootstrap`
- Build servoshell: `./mach build`

### Linux

- Install `curl`:
  - Arch: `sudo pacman -S --needed curl`
  - Debian, Ubuntu: `sudo apt install curl`
  - Fedora: `sudo dnf install curl`
  - Gentoo: `sudo emerge net-misc/curl`
- Install `uv`: `curl -LsSf https://astral.sh/uv/install.sh | sh` 
- Install `rustup`: `curl --proto &#039;=https&#039; --tlsv1.2 -sSf https://sh.rustup.rs | sh`
- Restart your shell to make sure `cargo` is available
- Install the other dependencies: `./mach bootstrap`
- Build servoshell: `./mach build`

### Windows

- Download [`uv`](https://docs.astral.sh/uv/getting-started/installation/#standalone-installer), [`choco`](https://chocolatey.org/install#individual), and [`rustup`](https://win.rustup.rs/)
  - Be sure to select *Quick install via the Visual Studio Community installer*
- In the Visual Studio Installer, ensure the following components are installed:
  - **Windows 10/11 SDK (anything &gt;= 10.0.19041.0)** (`Microsoft.VisualStudio.Component.Windows{10, 11}SDK.{&gt;=19041}`)
  - **MSVC v143 - VS 2022 C++ x64/x86 build tools (Latest)** (`Microsoft.VisualStudio.Component.VC.Tools.x86.x64`)
  - **C++ ATL for latest v143 build tools (x86 &amp; x64)** (`Microsoft.VisualStudio.Component.VC.ATL`)
- Restart your shell to make sure `cargo` is available
- Install the other dependencies: `.\mach bootstrap`
- Build servoshell: `.\mach build`

### Android

- Ensure that the following environment variables are set:
  - `ANDROID_SDK_ROOT`
  - `ANDROID_NDK_ROOT`: `$ANDROID_SDK_ROOT/ndk/28.2.13676358/`
 `ANDROID_SDK_ROOT` can be any directory (such as `~/android-sdk`).
  All of the Android build dependencies will be installed there.
- Install the latest version of the [Android command-line
  tools](https://developer.android.com/studio#command-tools) to
  `$ANDROID_SDK_ROOT/cmdline-tools/latest`.
- Run the following command to install the necessary components:
  ```shell
  sudo $ANDROID_SDK_ROOT/cmdline-tools/latest/bin/sdkmanager --install \
   &quot;build-tools;34.0.0&quot; \
   &quot;emulator&quot; \
   &quot;ndk;28.2.13676358&quot; \
   &quot;platform-tools&quot; \
   &quot;platforms;android-33&quot; \
   &quot;system-images;android-33;google_apis;x86_64&quot;
  ```
- Follow the instructions above for the platform you are building on

### OpenHarmony

- Follow the instructions above for the platform you are building on to prepare the environment.
- Depending on the target distribution (e.g. `HarmonyOS NEXT` vs pure `OpenHarmony`) the build configuration will differ slightly.
- Ensure that the following environment variables are set
  - `DEVECO_SDK_HOME` (Required when targeting `HarmonyOS NEXT`)
  - `OHOS_BASE_SDK_HOME` (Required when targeting `OpenHarmony`)
  - `OHOS_SDK_NATIVE` (e.g. `${DEVECO_SDK_HOME}/default/openharmony/native` or `${OHOS_BASE_SDK_HOME}/${API_VERSION}/native`)
  - `SERVO_OHOS_SIGNING_CONFIG`: Path to json file containing a valid signing configuration for the demo app.
- Review the detailed instructions at [Building for OpenHarmony].
- The target distribution can be modified by passing `--flavor=&lt;default|harmonyos&gt;` to `mach &lt;build|package|install&gt;`.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[TabbyML/tabby]]></title>
            <link>https://github.com/TabbyML/tabby</link>
            <guid>https://github.com/TabbyML/tabby</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:48 GMT</pubDate>
            <description><![CDATA[Self-hosted AI coding assistant]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/TabbyML/tabby">TabbyML/tabby</a></h1>
            <p>Self-hosted AI coding assistant</p>
            <p>Language: Rust</p>
            <p>Stars: 32,950</p>
            <p>Forks: 1,688</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  
# üêæ Tabby

[üìö Docs](https://tabby.tabbyml.com/docs/welcome/) ‚Ä¢ [üí¨ Slack](https://links.tabbyml.com/join-slack) ‚Ä¢ [üó∫Ô∏è Roadmap](https://tabby.tabbyml.com/docs/roadmap/)

[![latest release](https://shields.io/github/v/release/TabbyML/tabby)](https://github.com/TabbyML/tabby/releases/latest)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](https://makeapullrequest.com)
[![Docker pulls](https://img.shields.io/docker/pulls/tabbyml/tabby)](https://hub.docker.com/r/tabbyml/tabby)
[![codecov](https://codecov.io/gh/TabbyML/tabby/graph/badge.svg?token=WYVVH8MKK3)](https://codecov.io/gh/TabbyML/tabby)

[English](/README.md) |
[ÁÆÄ‰Ωì‰∏≠Êñá](/README-zh.md) |
[Êó•Êú¨Ë™û](/README-ja.md)

&lt;/div&gt;

Tabby is a self-hosted AI coding assistant, offering an open-source and on-premises alternative to GitHub Copilot. It boasts several key features:
* Self-contained, with no need for a DBMS or cloud service.
* OpenAPI interface, easy to integrate with existing infrastructure (e.g Cloud IDE).
* Supports consumer-grade GPUs.

&lt;p align=&quot;center&quot;&gt;
  &lt;a target=&quot;_blank&quot; href=&quot;https://tabby.tabbyml.com&quot;&gt;&lt;img alt=&quot;Open Live Demo&quot; src=&quot;https://img.shields.io/badge/OPEN_LIVE_DEMO-blue?logo=xcode&amp;style=for-the-badge&amp;logoColor=green&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img alt=&quot;Demo&quot; src=&quot;https://user-images.githubusercontent.com/388154/230440226-9bc01d05-9f57-478b-b04d-81184eba14ca.gif&quot;&gt;
&lt;/p&gt;

## üî• What&#039;s New
* **12/12/2025** Get your GitHub issues implemented by connecting them to [Pochi](https://github.com/TabbyML/pochi) tasks and create PRs directly from the sidebar with a breakdown of CI/Lint/Test results [vscode@0.20.0](https://github.com/TabbyML/pochi/releases/tag/vscode%400.20.0).
* **07/02/2025** [v0.30](https://github.com/TabbyML/tabby/releases/tag/v0.30.0) supports indexing GitLab Merge Request as Context! 
* **05/25/2025** üí°Interested in joining [Agent](https://links.tabbyml.com/pochi-github-readme) private preview? DM in [X](https://x.com/getpochi) for early waitlist approval!üé´
* **05/20/2025** Enhance Tabby with your own documentationüìÉ through REST APIs in [v0.29](https://github.com/TabbyML/tabby/releases/tag/v0.29.0)! üéâ
* **05/01/2025** [v0.28](https://github.com/TabbyML/tabby/releases/tag/v0.28.0) transforming Answer Engine messages into persistent, shareable Pages
* **03/31/2025** [v0.27](https://github.com/TabbyML/tabby/releases/tag/v0.27.0) released with a richer `@` menu in the chat side panel.


&lt;details&gt;
  &lt;summary&gt;Archived&lt;/summary&gt;

* **02/05/2025** LDAP Authentication and better notification for background jobs coming in Tabby [v0.24.0](https://github.com/TabbyML/tabby/releases/tag/v0.24.0)!‚ú®
* **02/04/2025** [VSCode 1.20.0](https://marketplace.visualstudio.com/items/TabbyML.vscode-tabby/changelog) upgrade! @-mention files to add them as chat context, and edit inline with a new right-click option are available!
* **01/10/2025** Tabby [v0.23.0](https://github.com/TabbyML/tabby/releases/tag/v0.23.0) featuring enhanced code browser experience and chat side panel improvements!
* **12/24/2024** Introduce **Notification Box** in Tabby [v0.22.0](https://github.com/TabbyML/tabby/releases/tag/v0.22.0)!
* **12/06/2024** Llamafile deployment integration and enhanced Answer Engine user experience are coming in Tabby [v0.21.0](https://github.com/TabbyML/tabby/releases/tag/v0.21.0)!üöÄ
* **11/10/2024** Switching between different backend chat models is supported in Answer Engine with Tabby [v0.20.0](https://github.com/TabbyML/tabby/releases/tag/v0.20.0)!
* **10/30/2024** Tabby [v0.19.0](https://github.com/TabbyML/tabby/releases/tag/v0.19.0) featuring recent shared threads on the main page to improve their discoverability. 
* **07/09/2024** üéâAnnounce [Codestral integration in Tabby](https://tabby.tabbyml.com/blog/2024/07/09/tabby-codestral/)!
* **07/05/2024** Tabby [v0.13.0](https://github.com/TabbyML/tabby/releases/tag/v0.13.0) introduces ***Answer Engine***, a central knowledge engine for internal engineering teams. It seamlessly integrates with dev team&#039;s internal data, delivering reliable and precise answers to empower developers.
* **06/13/2024** [VSCode 1.7](https://marketplace.visualstudio.com/items/TabbyML.vscode-tabby/changelog) marks a significant milestone with a versatile Chat experience throughout your coding experience. Come and they the latest **chat in side-panel** and **editing via chat command**!
* **06/10/2024** Latest üìÉblogpost drop on [an enhanced code context understanding](https://tabby.tabbyml.com/blog/2024/06/11/rank-fusion-in-tabby-code-completion/) in Tabby!
* **06/06/2024** Tabby [v0.12.0](https://github.com/TabbyML/tabby/releases/tag/v0.12.0) release brings üîó**seamless integrations** (Gitlab SSO, Self-hosted GitHub/GitLab, etc.), to ‚öôÔ∏è**flexible configurations** (HTTP API integration) and üåê**expanded capabilities** (repo-context in Code Browser)! 
* **05/22/2024** Tabby [VSCode 1.6](https://marketplace.visualstudio.com/items?itemName=TabbyML.vscode-tabby) comes with **multiple choices** in inline completion, and the **auto-generated commit messages**üê±üíª!
* **05/11/2024** [v0.11.0](https://github.com/TabbyML/tabby/releases/tag/v0.11.0) brings significant enterprise upgrades, including üìä**storage usage** stats, üîó**GitHub &amp; GitLab** integration, üìã**Activities** page, and the long-awaited ü§ñ**Ask Tabby** feature!
* **04/22/2024** [v0.10.0](https://github.com/TabbyML/tabby/releases/tag/v0.10.0) released, featuring the latest **Reports** tab with team-wise analytics for Tabby usage.
* **04/19/2024** üì£ Tabby now incorporates [locally relevant snippets](https://github.com/TabbyML/tabby/pull/1844)(declarations from local LSP, and recently modified code) for code completion!
* **04/17/2024** CodeGemma and CodeQwen model series have now been added to the [official registry](https://tabby.tabbyml.com/docs/models/)!
* **03/20/2024** [v0.9](https://github.com/TabbyML/tabby/releases/tag/v0.9.1) released, highlighting a full feature admin UI.
* **12/23/2023** Seamlessly [deploy Tabby on any cloud](https://tabby.tabbyml.com/docs/installation/skypilot/) with [SkyServe](https://skypilot.readthedocs.io/en/latest/serving/sky-serve.html) üõ´ from SkyPilot.
* **12/15/2023** [v0.7.0](https://github.com/TabbyML/tabby/releases/tag/v0.7.0) released with team management and secured access!
* **10/15/2023** RAG-based code completion is enabled by detail in [v0.3.0](https://github.com/TabbyML/tabby/releases/tag/v0.3.0)üéâ! Check out the [blogpost](https://tabby.tabbyml.com/blog/2023/10/16/repository-context-for-code-completion/) explaining how Tabby utilizes repo-level context to get even smarter!
* **11/27/2023** [v0.6.0](https://github.com/TabbyML/tabby/releases/tag/v0.6.0) released!
* **11/09/2023** [v0.5.5](https://github.com/TabbyML/tabby/releases/tag/v0.5.5) released! With a redesign of UI + performance improvement.
* **10/24/2023** ‚õ≥Ô∏è Major updates for Tabby IDE plugins across [VSCode/Vim/IntelliJ](https://tabby.tabbyml.com/docs/extensions)!
* **10/04/2023** Check out the [model directory](https://tabby.tabbyml.com/docs/models/) for the latest models supported by Tabby.
* **09/18/2023** Apple&#039;s M1/M2 Metal inference support has landed in [v0.1.1](https://github.com/TabbyML/tabby/releases/tag/v0.1.1)!
* **08/31/2023** Tabby&#039;s first stable release [v0.0.1](https://github.com/TabbyML/tabby/releases/tag/v0.0.1) ü•≥.
* **08/28/2023** Experimental support for the [CodeLlama 7B](https://github.com/TabbyML/tabby/issues/370).
* **08/24/2023** Tabby is now on [JetBrains Marketplace](https://plugins.jetbrains.com/plugin/22379-tabby)!

&lt;/details&gt;

## üëã Getting Started

You can find our documentation [here](https://tabby.tabbyml.com/docs/getting-started).
- üìö [Installation](https://tabby.tabbyml.com/docs/installation/)
- üíª [IDE/Editor Extensions](https://tabby.tabbyml.com/docs/extensions/)
- ‚öôÔ∏è [Configuration](https://tabby.tabbyml.com/docs/configuration)

### Run Tabby in 1 Minute
The easiest way to start a Tabby server is by using the following Docker command:

```bash
docker run -it \
  --gpus all -p 8080:8080 -v $HOME/.tabby:/data \
  tabbyml/tabby \
  serve --model StarCoder-1B --device cuda --chat-model Qwen2-1.5B-Instruct
```
For additional options (e.g inference type, parallelism), please refer to the [documentation page](https://tabbyml.github.io/tabby).

## ü§ù Contributing

Full guide at [CONTRIBUTING.md](https://github.com/TabbyML/tabby/blob/main/CONTRIBUTING.md);

### Get the Code

```bash
git clone --recurse-submodules https://github.com/TabbyML/tabby
cd tabby
```

If you have already cloned the repository, you could run the `git submodule update --recursive --init` command to fetch all submodules.

### Build

1. Set up the Rust environment by following this [tutorial](https://www.rust-lang.org/learn/get-started).

2. Install the required dependencies:
```bash
# For MacOS
brew install protobuf

# For Ubuntu / Debian
apt install protobuf-compiler libopenblas-dev
```

3. Install useful tools:
```bash
# For Ubuntu
apt install make sqlite3 graphviz
```

4. Now, you can build Tabby by running the command `cargo build`.

### Start Hacking!
... and don&#039;t forget to submit a [Pull Request](https://github.com/TabbyML/tabby/compare)

## üåç Community
- üé§ [Twitter / X](https://twitter.com/Tabby_ML) - engage with TabbyML for all things possible 
- üìö [LinkedIn](https://www.linkedin.com/company/tabbyml/) - follow for the latest from the community 
- üíå [Newsletter](https://newsletter.tabbyml.com/archive) - subscribe to unlock Tabby insights and secrets

### üîÜ Activity

![Git Repository Activity](https://repobeats.axiom.co/api/embed/e4ef0fbd12e586ef9ea7d72d1fb4f5c5b88d78d5.svg &quot;Repobeats analytics image&quot;)

### üåü Star History

[![Star History Chart](https://api.star-history.com/svg?repos=tabbyml/tabby&amp;type=Date)](https://star-history.com/#tabbyml/tabby&amp;Date)
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[risingwavelabs/risingwave]]></title>
            <link>https://github.com/risingwavelabs/risingwave</link>
            <guid>https://github.com/risingwavelabs/risingwave</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:47 GMT</pubDate>
            <description><![CDATA[Event streaming platform for agents, apps, and analytics. Continuously ingest, transform, and serve event data in real time, at scale.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/risingwavelabs/risingwave">risingwavelabs/risingwave</a></h1>
            <p>Event streaming platform for agents, apps, and analytics. Continuously ingest, transform, and serve event data in real time, at scale.</p>
            <p>Language: Rust</p>
            <p>Stars: 8,820</p>
            <p>Forks: 734</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre>
&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source srcset=&quot;.github/RisingWave-logo-dark.svg&quot; width=&quot;500px&quot; media=&quot;(prefers-color-scheme: dark)&quot;&gt;
    &lt;img src=&quot;.github/RisingWave-logo-light.svg&quot; width=&quot;500px&quot;&gt;
  &lt;/picture&gt;
&lt;/p&gt;


&lt;div align=&quot;center&quot;&gt;

### üåä Ride the wave of event streaming

&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://docs.risingwave.com/&quot;&gt;Docs&lt;/a&gt; | &lt;a href=&quot;https://docs.risingwave.com/get-started/rw-benchmarks-stream-processing&quot;&gt;Benchmarks&lt;/a&gt; | &lt;a href=&quot;https://docs.risingwave.com/demos/overview&quot;&gt;Demos&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a
    href=&quot;https://github.com/risingwavelabs/risingwave/releases/latest&quot;
    target=&quot;_blank&quot;
  &gt;
    &lt;img alt=&quot;Release&quot; src=&quot;https://img.shields.io/github/v/release/risingwavelabs/risingwave.svg?sort=semver&quot; /&gt;
  &lt;/a&gt;
  &lt;a
    href=&quot;https://go.risingwave.com/slack&quot;
    target=&quot;_blank&quot;
  &gt;
    &lt;img alt=&quot;Slack&quot; src=&quot;https://badgen.net/badge/Slack/Join%20RisingWave/0abd59?icon=slack&quot; /&gt;
  &lt;/a&gt;
  &lt;a
    href=&quot;https://x.com/risingwavelabs&quot;
    target=&quot;_blank&quot;
  &gt;
    &lt;img alt=&quot;X&quot; src=&quot;https://img.shields.io/twitter/follow/risingwavelabs&quot; /&gt;
  &lt;/a&gt;
  &lt;a
    href=&quot;https://www.youtube.com/@risingwave-labs&quot;
    target=&quot;_blank&quot;
  &gt;
    &lt;img alt=&quot;YouTube&quot; src=&quot;https://img.shields.io/youtube/channel/views/UCsHwdyBRxBpmkA5RRd0YNEA&quot; /&gt;
  &lt;/a&gt;
&lt;/div&gt;

RisingWave is an enterprise-grade event streaming platform designed to offer the &lt;i&gt;&lt;b&gt;simplest&lt;/b&gt;&lt;/i&gt; and &lt;i&gt;&lt;b&gt;most cost-effective&lt;/b&gt;&lt;/i&gt; way to &lt;b&gt;ingest&lt;/b&gt;, &lt;b&gt;process&lt;/b&gt;, and &lt;b&gt;manage&lt;/b&gt; real-time event data ‚Äî with built-in support for the [Apache Iceberg‚Ñ¢](https://iceberg.apache.org/) open table format. It provides both a Postgres-compatible [SQL interface](https://docs.risingwave.com/sql/overview) and a DataFrame-style [Python interface](https://docs.risingwave.com/python-sdk/intro).

RisingWave can &lt;b&gt;ingest&lt;/b&gt; millions of events per second, continuously &lt;b&gt;join and analyze&lt;/b&gt; live streams with historical data, &lt;b&gt;serve&lt;/b&gt; ad-hoc queries at low latency, and &lt;b&gt;manage&lt;/b&gt; data reliably in Apache Iceberg‚Ñ¢ tables.

![RisingWave](./docs/dev/src/images/architecture_20250609.jpg)

## Try it out in 60 seconds

Install RisingWave standalone mode:
```shell
curl -L https://risingwave.com/sh | sh
```

To learn about other installation options, such as using a Docker image, see the [quick start guide](https://docs.risingwave.com/get-started/quickstart).

## Unified platform for streaming data

RisingWave delivers a unified streaming data platform that combines **ultra-low-latency stream processing** and **Iceberg-native data management**.

### Low-latency streaming ingestion and processing
RisingWave integrates real-time streaming ingestion, stream processing and low-latency serving in a single system. It continuously ingests data from streaming and batch sources, performs incremental computations across streams and tables with end-to-end freshness under 100 ms. Materialized views can be served directly within RisingWave with 10‚Äì20 ms p99 query latency, or delivered to downstream systems.

### Iceberg lakehouse ingestion, transformation, and management
RisingWave treats Apache Iceberg‚Ñ¢ as a first-class citizen. It directly hosts and manages the Iceberg REST catalog, allowing users to create and operate Iceberg tables through a PostgreSQL-compatible interface. RisingWave supports two write modes: Merge-on-Read (MoR) and Copy-on-Write (CoW), to suit different ingestion and query patterns. It also provides built-in table maintenance capabilities, including compaction, small-file optimization, vacuum, and snapshot cleanup, ensuring efficient and consistent data management without external tools or pipelines.

_Plug: [Nimtable](https://github.com/nimtable/nimtable) is an observability tool developed by RisingWave for easily exploring and managing Iceberg tables._



## Key design decisions

RisingWave is designed to be easier to use and more cost-efficient:

### PostgreSQL compatibility

* **Seamless integration:** Connects via the PostgreSQL wire protocol, working with psql, JDBC, and any Postgres tool.
* **Expressive SQL:** Supports structured, semi-structured, and unstructured data with a familiar SQL dialect.
* **No manual state tuning:** Eliminates complex state management configurations.

### S3 as primary storage

RisingWave stores tables, materialized views, and internal states of stream processing jobs in S3 (or equivalent object storage), providing:
- **High performance:** Optimized for complex queries, including joins and time windowing.
- **Fast recovery:** Restores from system failures within seconds.
- **[Dynamic scaling](https://docs.risingwave.com/deploy/k8s-cluster-scaling):** Instantly adjusts resources to handle workload spikes.

Beyond caching hot data in memory, RisingWave supports [**elastic disk cache**](https://docs.risingwave.com/get-started/disk-cache), a powerful performance optimization that uses local disks or EBS for efficient data caching. This minimizes access to S3, lowering processing latency and cutting S3 access costs.

### Apache Iceberg‚Ñ¢ native support
RisingWave [**natively integrates with Apache Iceberg‚Ñ¢**](https://docs.risingwave.com/iceberg/overview), enabling continuous ingestion of streaming data into Iceberg tables. It can also read directly from Iceberg, perform automatic compaction, and maintain table health over time. Since Iceberg is an open table format, results are accessible by other query engines ‚Äî making storage not only cost-efficient, but interoperable by design.

## In what use cases does RisingWave excel?
RisingWave is particularly effective for the following use cases:

* **Live dashboards**: Achieve sub-second data freshness in live dashboards, ideal for high-stakes scenarios like stock trading, sports betting, and IoT monitoring.
* **Monitoring and alerting**: Develop sophisticated monitoring and alerting systems for critical applications such as fraud and anomaly detection.
* **Real-time data enrichment**: Continuously ingest data from diverse sources, conduct real-time data enrichment, and efficiently deliver the results to downstream systems.
* **Feature engineering**: Transform batch and streaming data into features in your machine learning models using a unified codebase, ensuring seamless integration and consistency.
* **Iceberg-based lakehouses**: Power real-time lakehouse architectures where streaming data is continuously written to Apache Iceberg‚Ñ¢ tables for unified analytics, governance, and long-term retention in open formats.

## Production deployments

[**RisingWave Cloud**](https://cloud.risingwave.com) offers the easiest way to run RisingWave in production.

For **Docker deployment**, please refer to [Docker Compose](https://docs.risingwave.com/deploy/risingwave-docker-compose/).

For **Kubernetes deployment**, please refer to [Kubernetes with Helm](https://docs.risingwave.com/deploy/risingwave-k8s-helm/) or [Kubernetes with Operator](https://docs.risingwave.com/deploy/risingwave-kubernetes/).

## Community

Looking for help, discussions, collaboration opportunities, or a casual afternoon chat with our fellow engineers and community members? Join our [Slack workspace](https://risingwave.com/slack)!

## Notes on telemetry


RisingWave uses [Scarf](https://scarf.sh/) to collect anonymized installation analytics. These analytics help support us understand and improve the distribution of our package. The privacy policy of Scarf is available at [https://about.scarf.sh/privacy-policy](https://about.scarf.sh/privacy-policy).

RisingWave also collects anonymous usage statistics to better understand how the community is using RisingWave. The sole intention of this exercise is to help improve the product. Users may opt out easily at any time. Please refer to the [user documentation](https://docs.risingwave.com/operate/telemetry/) for more details.

## License

RisingWave is distributed under the Apache License (Version 2.0). Please refer to [LICENSE](LICENSE) for more information.

## Contributing

Thanks for your interest in contributing to the project! Please refer to [RisingWave Developer Guide](https://risingwavelabs.github.io/risingwave/) for more information.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[pola-rs/polars]]></title>
            <link>https://github.com/pola-rs/polars</link>
            <guid>https://github.com/pola-rs/polars</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:46 GMT</pubDate>
            <description><![CDATA[Extremely fast Query Engine for DataFrames, written in Rust]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pola-rs/polars">pola-rs/polars</a></h1>
            <p>Extremely fast Query Engine for DataFrames, written in Rust</p>
            <p>Language: Rust</p>
            <p>Stars: 37,565</p>
            <p>Forks: 2,650</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pola.rs&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/pola-rs/polars-static/master/banner/polars_github_banner.svg&quot; alt=&quot;Polars logo&quot;&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://crates.io/crates/polars&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/crates/v/polars.svg&quot; alt=&quot;crates.io Latest Release&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://pypi.org/project/polars/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/polars.svg&quot; alt=&quot;PyPi Latest Release&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.npmjs.com/package/nodejs-polars&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/npm/v/nodejs-polars.svg&quot; alt=&quot;NPM Latest Release&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://community.r-multiverse.org/polars&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fcommunity.r-multiverse.org%2Fapi%2Fpackages%2Fpolars&amp;query=%24.Version&amp;label=r-multiverse&quot; alt=&quot;R-multiverse Latest Release&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://doi.org/10.5281/zenodo.7697217&quot;&gt;
    &lt;img src=&quot;https://zenodo.org/badge/DOI/10.5281/zenodo.7697217.svg&quot; alt=&quot;DOI Latest Release&quot;/&gt;
  &lt;/a&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;b&gt;Documentation&lt;/b&gt;:
  &lt;a href=&quot;https://docs.pola.rs/api/python/stable/reference/index.html&quot;&gt;Python&lt;/a&gt;
  -
  &lt;a href=&quot;https://docs.rs/polars/latest/polars/&quot;&gt;Rust&lt;/a&gt;
  -
  &lt;a href=&quot;https://pola-rs.github.io/nodejs-polars/index.html&quot;&gt;Node.js&lt;/a&gt;
  -
  &lt;a href=&quot;https://pola-rs.github.io/r-polars/index.html&quot;&gt;R&lt;/a&gt;
  |
  &lt;b&gt;StackOverflow&lt;/b&gt;:
  &lt;a href=&quot;https://stackoverflow.com/questions/tagged/python-polars&quot;&gt;Python&lt;/a&gt;
  -
  &lt;a href=&quot;https://stackoverflow.com/questions/tagged/rust-polars&quot;&gt;Rust&lt;/a&gt;
  -
  &lt;a href=&quot;https://stackoverflow.com/questions/tagged/nodejs-polars&quot;&gt;Node.js&lt;/a&gt;
  -
  &lt;a href=&quot;https://stackoverflow.com/questions/tagged/r-polars&quot;&gt;R&lt;/a&gt;
  |
  &lt;a href=&quot;https://docs.pola.rs/&quot;&gt;User guide&lt;/a&gt;
  |
  &lt;a href=&quot;https://discord.gg/4UfP5cfBE7&quot;&gt;Discord&lt;/a&gt;
&lt;/p&gt;

## Polars: Extremely fast Query Engine for DataFrames, written in Rust

Polars is an analytical query engine written for DataFrames. It is designed to be fast, easy to use
and expressive. Key features are:

- Lazy | Eager execution
- Streaming (larger-than-RAM datasets)
- Query optimization
- Multi-threaded
- Written in Rust
- SIMD
- Powerful expression API
- Front end in Python | Rust | NodeJS | R | SQL
- [Apache Arrow Columnar Format](https://arrow.apache.org/docs/format/Columnar.html)

To learn more, read the [user guide](https://docs.pola.rs/).

## Performance üöÄüöÄ

### Blazingly fast

Polars is very fast. In fact, it is one of the best performing solutions available. See the
[PDS-H benchmarks](https://www.pola.rs/benchmarks.html) results.

### Lightweight

Polars is also very lightweight. It comes with zero required dependencies, and this shows in the
import times:

- polars: 70ms
- numpy: 104ms
- pandas: 520ms

### Handles larger-than-RAM data

If you have data that does not fit into memory, Polars&#039; query engine is able to process your query
(or parts of your query) in a streaming fashion. This drastically reduces memory requirements, so
you might be able to process your 250GB dataset on your laptop. Collect with
`collect(engine=&#039;streaming&#039;)` to run the query streaming.

## Setup

### Python

Install the latest Polars version with:

```sh
pip install polars
```

See the [User Guide](https://docs.pola.rs/user-guide/installation/#feature-flags) for more details
on optional dependencies

To see the current Polars version and a full list of its optional dependencies, run:

```python
pl.show_versions()
```

## Contributing

Want to contribute? Read our [contributing guide](https://docs.pola.rs/development/contributing/).

## Managed/Distributed Polars

Do you want a managed solution or scale out to distributed clusters? Consider our
[offering](https://cloud.pola.rs/) and help the project!

## Python: compile Polars from source

If you want a bleeding edge release or maximal performance you should compile Polars from source.

This can be done by going through the following steps in sequence:

1. Install the latest [Rust compiler](https://www.rust-lang.org/tools/install)
2. Install [maturin](https://maturin.rs/): `pip install maturin`
3. `cd py-polars` and choose one of the following:
   - `make build`, slow binary with debug assertions and symbols, fast compile times
   - `make build-release`, fast binary without debug assertions, minimal debug symbols, long compile
     times
   - `make build-nodebug-release`, same as build-release but without any debug symbols, slightly
     faster to compile
   - `make build-debug-release`, same as build-release but with full debug symbols, slightly slower
     to compile
   - `make build-dist-release`, fastest binary, extreme compile times

By default the binary is compiled with optimizations turned on for a modern CPU. Specify `LTS_CPU=1`
with the command if your CPU is older and does not support e.g. AVX2.

Note that the Rust crate implementing the Python bindings is called `py-polars` to distinguish from
the wrapped Rust crate `polars` itself. However, both the Python package and the Python module are
named `polars`, so you can `pip install polars` and `import polars`.

## Using custom Rust functions in Python

Extending Polars with UDFs compiled in Rust is easy. We expose PyO3 extensions for `DataFrame` and
`Series` data structures. See more in https://github.com/pola-rs/polars/tree/main/pyo3-polars.

## Going big...

Do you expect more than 2^32 (~4.2 billion) rows? Compile Polars with the `bigidx` feature flag or,
for Python users, install `pip install polars[rt64]`.

Don&#039;t use this unless you hit the row boundary as the default build of Polars is faster and consumes
less memory.

## Legacy

Do you want Polars to run on an old CPU (e.g. dating from before 2011), or on an `x86-64` build of
Python on Apple Silicon under Rosetta? Install `pip install polars[rtcompat]`. This version of
Polars is compiled without [AVX](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions) target
features.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[Eventual-Inc/Daft]]></title>
            <link>https://github.com/Eventual-Inc/Daft</link>
            <guid>https://github.com/Eventual-Inc/Daft</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:45 GMT</pubDate>
            <description><![CDATA[High-performance data engine for AI and multimodal workloads. Process images, audio, video, and structured data at any scale]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Eventual-Inc/Daft">Eventual-Inc/Daft</a></h1>
            <p>High-performance data engine for AI and multimodal workloads. Process images, audio, video, and structured data at any scale</p>
            <p>Language: Rust</p>
            <p>Stars: 5,261</p>
            <p>Forks: 404</p>
            <p>Stars today: 2 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[cloudflare/quiche]]></title>
            <link>https://github.com/cloudflare/quiche</link>
            <guid>https://github.com/cloudflare/quiche</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:44 GMT</pubDate>
            <description><![CDATA[ü•ß Savoury implementation of the QUIC transport protocol and HTTP/3]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/cloudflare/quiche">cloudflare/quiche</a></h1>
            <p>ü•ß Savoury implementation of the QUIC transport protocol and HTTP/3</p>
            <p>Language: Rust</p>
            <p>Stars: 11,278</p>
            <p>Forks: 942</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre>![quiche](quiche.svg)

[![crates.io](https://img.shields.io/crates/v/quiche.svg)](https://crates.io/crates/quiche)
[![docs.rs](https://docs.rs/quiche/badge.svg)](https://docs.rs/quiche)
[![license](https://img.shields.io/github/license/cloudflare/quiche.svg)](https://opensource.org/licenses/BSD-2-Clause)
![build](https://img.shields.io/github/actions/workflow/status/cloudflare/quiche/stable.yml?branch=master)

[quiche] is an implementation of the QUIC transport protocol and HTTP/3 as
specified by the [IETF]. It provides a low level API for processing QUIC packets
and handling connection state. The application is responsible for providing I/O
(e.g. sockets handling) as well as an event loop with support for timers.

For more information on how quiche came about and some insights into its design
you can read a [post] on Cloudflare&#039;s blog that goes into some more detail.

[quiche]: https://docs.quic.tech/quiche/
[ietf]: https://quicwg.org/
[post]: https://blog.cloudflare.com/enjoy-a-slice-of-quic-and-rust/

Who uses quiche?
----------------

### Cloudflare

quiche powers Cloudflare edge network&#039;s [HTTP/3 support][cloudflare-http3]. The
[cloudflare-quic.com](https://cloudflare-quic.com) website can be used for
testing and experimentation.

### Android

Android&#039;s DNS resolver uses quiche to [implement DNS over HTTP/3][android-http3].

### curl

quiche can be [integrated into curl][curl-http3] to provide support for HTTP/3.

[cloudflare-http3]: https://blog.cloudflare.com/http3-the-past-present-and-future/
[android-http3]: https://security.googleblog.com/2022/07/dns-over-http3-in-android.html
[curl-http3]: https://github.com/curl/curl/blob/master/docs/HTTP3.md#quiche-version

Getting Started
---------------

### Command-line apps

Before diving into the quiche API, here are a few examples on how to use the
quiche tools provided as part of the [quiche-apps](apps/) crate. These are not
suitable for production environments; see [disclaimers and
notes](#disclaimers-and-notes).

After cloning the project according to the command mentioned in the [building](#building) section, the client can be run as follows:

```bash
 $ cargo run --bin quiche-client -- https://cloudflare-quic.com/
```

while the server can be run as follows:

```bash
 $ cargo run --bin quiche-server -- --cert apps/src/bin/cert.crt --key apps/src/bin/cert.key
```

(note that the certificate provided is self-signed and should not be used in
production)

Use the `--help` command-line flag to get a more detailed description of each
tool&#039;s options.

### Configuring connections

The first step in establishing a QUIC connection using quiche is creating a
[`Config`] object:

```rust
let mut config = quiche::Config::new(quiche::PROTOCOL_VERSION)?;
config.set_application_protos(&amp;[b&quot;example-proto&quot;]);

// Additional configuration specific to application and use case...
```

The [`Config`] object controls important aspects of the QUIC connection such
as QUIC version, ALPN IDs, flow control, congestion control, idle timeout
and other properties or features.

QUIC is a general-purpose transport protocol and there are several
configuration properties where there is no reasonable default value. For
example, the permitted number of concurrent streams of any particular type
is dependent on the application running over QUIC, and other use-case
specific concerns.

quiche defaults several properties to zero, applications most likely need
to set these to something else to satisfy their needs using the following:

- [`set_initial_max_streams_bidi()`]
- [`set_initial_max_streams_uni()`]
- [`set_initial_max_data()`]
- [`set_initial_max_stream_data_bidi_local()`]
- [`set_initial_max_stream_data_bidi_remote()`]
- [`set_initial_max_stream_data_uni()`]

[`Config`] also holds TLS configuration. This can be changed by mutators on
the an existing object, or by constructing a TLS context manually and
creating a configuration using [`with_boring_ssl_ctx_builder()`].

A configuration object can be shared among multiple connections.

### Connection setup

On the client-side the [`connect()`] utility function can be used to create
a new connection, while [`accept()`] is for servers:

```rust
// Client connection.
let conn = quiche::connect(Some(&amp;server_name), &amp;scid, local, peer, &amp;mut config)?;

// Server connection.
let conn = quiche::accept(&amp;scid, None, local, peer, &amp;mut config)?;
```

### Handling incoming packets

Using the connection&#039;s [`recv()`] method the application can process
incoming packets that belong to that connection from the network:

```rust
let to = socket.local_addr().unwrap();

loop {
    let (read, from) = socket.recv_from(&amp;mut buf).unwrap();

    let recv_info = quiche::RecvInfo { from, to };

    let read = match conn.recv(&amp;mut buf[..read], recv_info) {
        Ok(v) =&gt; v,

        Err(e) =&gt; {
            // An error occurred, handle it.
            break;
        },
    };
}
```

### Generating outgoing packets

Outgoing packet are generated using the connection&#039;s [`send()`] method
instead:

```rust
loop {
    let (write, send_info) = match conn.send(&amp;mut out) {
        Ok(v) =&gt; v,

        Err(quiche::Error::Done) =&gt; {
            // Done writing.
            break;
        },

        Err(e) =&gt; {
            // An error occurred, handle it.
            break;
        },
    };

    socket.send_to(&amp;out[..write], &amp;send_info.to).unwrap();
}
```

When packets are sent, the application is responsible for maintaining a
timer to react to time-based connection events. The timer expiration can be
obtained using the connection&#039;s [`timeout()`] method.

```rust
let timeout = conn.timeout();
```

The application is responsible for providing a timer implementation, which
can be specific to the operating system or networking framework used. When
a timer expires, the connection&#039;s [`on_timeout()`] method should be called,
after which additional packets might need to be sent on the network:

```rust
// Timeout expired, handle it.
conn.on_timeout();

// Send more packets as needed after timeout.
loop {
    let (write, send_info) = match conn.send(&amp;mut out) {
        Ok(v) =&gt; v,

        Err(quiche::Error::Done) =&gt; {
            // Done writing.
            break;
        },

        Err(e) =&gt; {
            // An error occurred, handle it.
            break;
        },
    };

    socket.send_to(&amp;out[..write], &amp;send_info.to).unwrap();
}
```

#### Pacing

It is recommended that applications [pace] sending of outgoing packets to
avoid creating packet bursts that could cause short-term congestion and
losses in the network.

quiche exposes pacing hints for outgoing packets through the [`at`] field
of the [`SendInfo`] structure that is returned by the [`send()`] method.
This field represents the time when a specific packet should be sent into
the network.

Applications can use these hints by artificially delaying the sending of
packets through platform-specific mechanisms (such as the [`SO_TXTIME`]
socket option on Linux), or custom methods (for example by using user-space
timers).

[pace]: https://datatracker.ietf.org/doc/html/rfc9002#section-7.7
[`SO_TXTIME`]: https://man7.org/linux/man-pages/man8/tc-etf.8.html

### Sending and receiving stream data

After some back and forth, the connection will complete its handshake and
will be ready for sending or receiving application data.

Data can be sent on a stream by using the [`stream_send()`] method:

```rust
if conn.is_established() {
    // Handshake completed, send some data on stream 0.
    conn.stream_send(0, b&quot;hello&quot;, true)?;
}
```

The application can check whether there are any readable streams by using
the connection&#039;s [`readable()`] method, which returns an iterator over all
the streams that have outstanding data to read.

The [`stream_recv()`] method can then be used to retrieve the application
data from the readable stream:

```rust
if conn.is_established() {
    // Iterate over readable streams.
    for stream_id in conn.readable() {
        // Stream is readable, read until there&#039;s no more data.
        while let Ok((read, fin)) = conn.stream_recv(stream_id, &amp;mut buf) {
            println!(&quot;Got {} bytes on stream {}&quot;, read, stream_id);
        }
    }
}
```

### HTTP/3

The quiche [HTTP/3 module] provides a high level API for sending and
receiving HTTP requests and responses on top of the QUIC transport protocol.

[`Config`]: https://docs.quic.tech/quiche/struct.Config.html
[`set_initial_max_streams_bidi()`]: https://docs.rs/quiche/latest/quiche/struct.Config.html#method.set_initial_max_streams_bidi
[`set_initial_max_streams_uni()`]: https://docs.rs/quiche/latest/quiche/struct.Config.html#method.set_initial_max_streams_uni
[`set_initial_max_data()`]: https://docs.rs/quiche/latest/quiche/struct.Config.html#method.set_initial_max_data
[`set_initial_max_stream_data_bidi_local()`]: https://docs.rs/quiche/latest/quiche/struct.Config.html#method.set_initial_max_stream_data_bidi_local
[`set_initial_max_stream_data_bidi_remote()`]: https://docs.rs/quiche/latest/quiche/struct.Config.html#method.set_initial_max_stream_data_bidi_remote
[`set_initial_max_stream_data_uni()`]: https://docs.rs/quiche/latest/quiche/struct.Config.html#method.set_initial_max_stream_data_uni
[`with_boring_ssl_ctx_builder()`]: https://docs.quic.tech/quiche/struct.Config.html#method.with_boring_ssl_ctx_builder
[`connect()`]: https://docs.quic.tech/quiche/fn.connect.html
[`accept()`]: https://docs.quic.tech/quiche/fn.accept.html
[`recv()`]: https://docs.quic.tech/quiche/struct.Connection.html#method.recv
[`send()`]: https://docs.quic.tech/quiche/struct.Connection.html#method.send
[`timeout()`]: https://docs.quic.tech/quiche/struct.Connection.html#method.timeout
[`on_timeout()`]: https://docs.quic.tech/quiche/struct.Connection.html#method.on_timeout
[`stream_send()`]: https://docs.quic.tech/quiche/struct.Connection.html#method.stream_send
[`readable()`]: https://docs.quic.tech/quiche/struct.Connection.html#method.readable
[`stream_recv()`]: https://docs.quic.tech/quiche/struct.Connection.html#method.stream_recv
[HTTP/3 module]: https://docs.quic.tech/quiche/h3/index.html

Have a look at the [quiche/examples/] directory for more complete examples on
how to use the quiche API, including examples on how to use quiche in C/C++
applications (see below for more information).

[examples/]: quiche/examples/

Calling quiche from C/C++
-------------------------

quiche exposes a [thin C API] on top of the Rust API that can be used to more
easily integrate quiche into C/C++ applications (as well as in other languages
that allow calling C APIs via some form of FFI). The C API follows the same
design of the Rust one, modulo the constraints imposed by the C language itself.

When running ``cargo build``, a static library called ``libquiche.a`` will be
built automatically alongside the Rust one. This is fully stand-alone and can
be linked directly into C/C++ applications.

Note that in order to enable the FFI API, the ``ffi`` feature must be enabled (it
is disabled by default), by passing ``--features ffi`` to ``cargo``.

[thin C API]: https://github.com/cloudflare/quiche/blob/master/quiche/include/quiche.h

Building
--------

quiche requires Rust 1.85 or later to build. The latest stable Rust release can
be installed using [rustup](https://rustup.rs/).

Once the Rust build environment is setup, the quiche source code can be fetched
using git:

```bash
 $ git clone --recursive https://github.com/cloudflare/quiche
```

and then built using cargo:

```bash
 $ cargo build --examples
```

cargo can also be used to run the testsuite:

```bash
 $ cargo test
```

Note that [BoringSSL], which is used to implement QUIC&#039;s cryptographic handshake
based on TLS, needs to be built and linked to quiche. This is done automatically
when building quiche using cargo, but requires the `cmake` command to be
available during the build process. On Windows you also need
[NASM](https://www.nasm.us/). The [official BoringSSL
documentation](https://github.com/google/boringssl/blob/master/BUILDING.md) has
more details.

In alternative you can use your own custom build of BoringSSL by configuring
the BoringSSL directory with the ``QUICHE_BSSL_PATH`` environment variable:

```bash
 $ QUICHE_BSSL_PATH=&quot;/path/to/boringssl&quot; cargo build --examples
```

Alternatively you can use [OpenSSL/quictls]. To enable quiche to use this vendor
the ``openssl`` feature can be added to the ``--feature`` list. Be aware that
``0-RTT`` is not supported if this vendor is used.

[BoringSSL]: https://boringssl.googlesource.com/boringssl/

[OpenSSL/quictls]: https://github.com/quictls/openssl

### Building for Android

Building quiche for Android (NDK version 19 or higher, 21 recommended), can be
done using [cargo-ndk] (v2.0 or later).

First the [Android NDK] needs to be installed, either using Android Studio or
directly, and the `ANDROID_NDK_HOME` environment variable needs to be set to the
NDK installation path, e.g.:

```bash
 $ export ANDROID_NDK_HOME=/usr/local/share/android-ndk
```

Then the Rust toolchain for the Android architectures needed can be installed as
follows:

```bash
 $ rustup target add aarch64-linux-android armv7-linux-androideabi i686-linux-android x86_64-linux-android
```

Note that the minimum API level is 21 for all target architectures.

[cargo-ndk] (v2.0 or later) also needs to be installed:

```bash
 $ cargo install cargo-ndk
```

Finally the quiche library can be built using the following procedure. Note that
the `-t &lt;architecture&gt;` and `-p &lt;NDK version&gt;` options are mandatory.

```bash
 $ cargo ndk -t arm64-v8a -p 21 -- build --features ffi
```

See [build_android_ndk19.sh] for more information.

[Android NDK]: https://developer.android.com/ndk
[cargo-ndk]: https://docs.rs/crate/cargo-ndk
[build_android_ndk19.sh]: https://github.com/cloudflare/quiche/blob/master/tools/android/build_android_ndk19.sh

### Building for iOS

To build quiche for iOS, you need the following:

- Install Xcode command-line tools. You can install them with Xcode or with the
  following command:

```bash
 $ xcode-select --install
```

- Install the Rust toolchain for iOS architectures:

```bash
 $ rustup target add aarch64-apple-ios x86_64-apple-ios
```

- Install `cargo-lipo`:

```bash
 $ cargo install cargo-lipo
```

To build libquiche, run the following command:

```bash
 $ cargo lipo --features ffi
```

or

```bash
 $ cargo lipo --features ffi --release
```

iOS build is tested in Xcode 10.1 and Xcode 11.2.

### Building Docker images

In order to build the Docker images, simply run the following command:

```bash
 $ make docker-build
```

You can find the quiche Docker images on the following Docker Hub repositories:

- [cloudflare/quiche](https://hub.docker.com/repository/docker/cloudflare/quiche)
- [cloudflare/quiche-qns](https://hub.docker.com/repository/docker/cloudflare/quiche-qns)

The `latest` tag will be updated whenever quiche master branch updates.

**cloudflare/quiche**

Provides a server and client installed in /usr/local/bin.

**cloudflare/quiche-qns**

Provides the script to test quiche within the [quic-interop-runner](https://github.com/marten-seemann/quic-interop-runner).

Disclaimers and Notes
---------

‚ö†Ô∏è This repository includes a number of client and server example
applications that are provided to demonstrate simple usage of the quiche library
API. They are not intended to be used in production environments; no
performance, security or reliability guarantees are provided.


Copyright
---------

Copyright (C) 2018-2019, Cloudflare, Inc.

See [COPYING] for the license.

[COPYING]: https://github.com/cloudflare/quiche/tree/master/COPYING
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[tree-sitter/tree-sitter]]></title>
            <link>https://github.com/tree-sitter/tree-sitter</link>
            <guid>https://github.com/tree-sitter/tree-sitter</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:43 GMT</pubDate>
            <description><![CDATA[An incremental parsing system for programming tools]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/tree-sitter/tree-sitter">tree-sitter/tree-sitter</a></h1>
            <p>An incremental parsing system for programming tools</p>
            <p>Language: Rust</p>
            <p>Stars: 23,965</p>
            <p>Forks: 2,433</p>
            <p>Stars today: 22 stars today</p>
            <h2>README</h2><pre># tree-sitter

[![DOI](https://zenodo.org/badge/14164618.svg)](https://zenodo.org/badge/latestdoi/14164618)
[![discord][discord]](https://discord.gg/w7nTvsVJhm)
[![matrix][matrix]](https://matrix.to/#/#tree-sitter-chat:matrix.org)

Tree-sitter is a parser generator tool and an incremental parsing library. It can build a concrete syntax tree for a source file and efficiently update the syntax tree as the source file is edited. Tree-sitter aims to be:

- **General** enough to parse any programming language
- **Fast** enough to parse on every keystroke in a text editor
- **Robust** enough to provide useful results even in the presence of syntax errors
- **Dependency-free** so that the runtime library (which is written in pure C) can be embedded in any application

## Links
- [Documentation](https://tree-sitter.github.io)
- [Rust binding](lib/binding_rust/README.md)
- [Wasm binding](lib/binding_web/README.md)
- [Command-line interface](crates/cli/README.md)

[discord]: https://img.shields.io/discord/1063097320771698699?logo=discord&amp;label=discord
[matrix]: https://img.shields.io/matrix/tree-sitter-chat%3Amatrix.org?logo=matrix&amp;label=matrix
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[apache/datafusion]]></title>
            <link>https://github.com/apache/datafusion</link>
            <guid>https://github.com/apache/datafusion</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:42 GMT</pubDate>
            <description><![CDATA[Apache DataFusion SQL Query Engine]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/apache/datafusion">apache/datafusion</a></h1>
            <p>Apache DataFusion SQL Query Engine</p>
            <p>Language: Rust</p>
            <p>Stars: 8,453</p>
            <p>Forks: 1,970</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre>&lt;!---
  Licensed to the Apache Software Foundation (ASF) under one
  or more contributor license agreements.  See the NOTICE file
  distributed with this work for additional information
  regarding copyright ownership.  The ASF licenses this file
  to you under the Apache License, Version 2.0 (the
  &quot;License&quot;); you may not use this file except in compliance
  with the License.  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing,
  software distributed under the License is distributed on an
  &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
  KIND, either express or implied.  See the License for the
  specific language governing permissions and limitations
  under the License.
--&gt;

# Apache DataFusion

[![Crates.io][crates-badge]][crates-url]
[![Apache licensed][license-badge]][license-url]
[![Build Status][actions-badge]][actions-url]
![Commit Activity][commit-activity-badge]
[![Open Issues][open-issues-badge]][open-issues-url]
[![Pending PRs][pending-pr-badge]][pending-pr-url]
[![Discord chat][discord-badge]][discord-url]
[![Linkedin][linkedin-badge]][linkedin-url]
![Crates.io MSRV][msrv-badge]

[crates-badge]: https://img.shields.io/crates/v/datafusion.svg
[crates-url]: https://crates.io/crates/datafusion
[license-badge]: https://img.shields.io/badge/license-Apache%20v2-blue.svg
[license-url]: https://github.com/apache/datafusion/blob/main/LICENSE.txt
[actions-badge]: https://github.com/apache/datafusion/actions/workflows/rust.yml/badge.svg
[actions-url]: https://github.com/apache/datafusion/actions?query=branch%3Amain
[discord-badge]: https://img.shields.io/badge/Chat-Discord-purple
[discord-url]: https://discord.com/invite/Qw5gKqHxUM
[commit-activity-badge]: https://img.shields.io/github/commit-activity/m/apache/datafusion
[open-issues-badge]: https://img.shields.io/github/issues-raw/apache/datafusion
[open-issues-url]: https://github.com/apache/datafusion/issues
[pending-pr-badge]: https://img.shields.io/github/issues-search/apache/datafusion?query=is%3Apr+is%3Aopen+draft%3Afalse+review%3Arequired+status%3Asuccess&amp;label=Pending%20PRs&amp;logo=github
[pending-pr-url]: https://github.com/apache/datafusion/pulls?q=is%3Apr+is%3Aopen+draft%3Afalse+review%3Arequired+status%3Asuccess+sort%3Aupdated-desc
[linkedin-badge]: https://img.shields.io/badge/Follow-Linkedin-blue
[linkedin-url]: https://www.linkedin.com/company/apache-datafusion/
[msrv-badge]: https://img.shields.io/crates/msrv/datafusion?label=Min%20Rust%20Version

[Website](https://datafusion.apache.org/) |
[API Docs](https://docs.rs/datafusion/latest/datafusion/) |
[Chat](https://discord.com/channels/885562378132000778/885562378132000781)

&lt;a href=&quot;https://datafusion.apache.org/&quot;&gt;
  &lt;img src=&quot;https://github.com/apache/datafusion/raw/HEAD/docs/source/_static/images/2x_bgwhite_original.png&quot; width=&quot;512&quot; alt=&quot;logo&quot;/&gt;
&lt;/a&gt;

DataFusion is an extensible query engine written in [Rust] that
uses [Apache Arrow] as its in-memory format.

This crate provides libraries and binaries for developers building fast and
feature-rich database and analytic systems, customized for particular workloads.
See [use cases] for examples. The following related subprojects target end users:

- [DataFusion Python](https://github.com/apache/datafusion-python/) offers a Python interface for SQL and DataFrame
  queries.
- [DataFusion Comet](https://github.com/apache/datafusion-comet/) is an accelerator for Apache Spark based on
  DataFusion.

&quot;Out of the box,&quot;
DataFusion offers [SQL](https://datafusion.apache.org/user-guide/sql/index.html) and [DataFrame](https://datafusion.apache.org/user-guide/dataframe.html) APIs, excellent [performance],
built-in support for CSV, Parquet, JSON, and Avro, extensive customization, and
a great community.

DataFusion features a full query planner, a columnar, streaming, multi-threaded,
vectorized execution engine, and partitioned data sources. You can
customize DataFusion at almost all points including additional data sources,
query languages, functions, custom operators and more.
See the [Architecture] section for more details.

[rust]: http://rustlang.org
[apache arrow]: https://arrow.apache.org
[use cases]: https://datafusion.apache.org/user-guide/introduction.html#use-cases
[python bindings]: https://github.com/apache/datafusion-python
[performance]: https://benchmark.clickhouse.com/
[architecture]: https://datafusion.apache.org/contributor-guide/architecture.html

Here are links to important resources:

- [Project Site](https://datafusion.apache.org/)
- [Installation](https://datafusion.apache.org/user-guide/cli/installation.html)
- [Rust Getting Started](https://datafusion.apache.org/user-guide/example-usage.html)
- [Rust DataFrame API](https://datafusion.apache.org/user-guide/dataframe.html)
- [Rust API docs](https://docs.rs/datafusion/latest/datafusion)
- [Rust Examples](https://github.com/apache/datafusion/tree/main/datafusion-examples)
- [Python DataFrame API](https://arrow.apache.org/datafusion-python/)
- [Architecture](https://docs.rs/datafusion/latest/datafusion/index.html#architecture)

## What can you do with this crate?

DataFusion is great for building projects such as domain-specific query engines, new database platforms and data pipelines, query languages and more.
It lets you start quickly from a fully working engine, and then customize those features specific to your needs. See the [list of known users](https://datafusion.apache.org/user-guide/introduction.html#known-users).

## Contributing to DataFusion

Please see the [contributor guide] and [communication] pages for more information.

[contributor guide]: https://datafusion.apache.org/contributor-guide
[communication]: https://datafusion.apache.org/contributor-guide/communication.html

## Crate features

This crate has several [features] which can be specified in your `Cargo.toml`.

[features]: https://doc.rust-lang.org/cargo/reference/features.html

Default features:

- `nested_expressions`: functions for working with nested types such as `array_to_string`
- `compression`: reading files compressed with `xz2`, `bzip2`, `flate2`, and `zstd`
- `crypto_expressions`: cryptographic functions such as `md5` and `sha256`
- `datetime_expressions`: date and time functions such as `to_timestamp`
- `encoding_expressions`: `encode` and `decode` functions
- `parquet`: support for reading the [Apache Parquet] format
- `sql`: support for SQL parsing and planning
- `regex_expressions`: regular expression functions, such as `regexp_match`
- `unicode_expressions`: include Unicode-aware functions such as `character_length`
- `unparser`: enables support to reverse LogicalPlans back into SQL
- `recursive_protection`: uses [recursive](https://docs.rs/recursive/latest/recursive/) for stack overflow protection.

Optional features:

- `avro`: support for reading the [Apache Avro] format
- `backtrace`: include backtrace information in error messages
- `parquet_encryption`: support for using [Parquet Modular Encryption]
- `serde`: enable arrow-schema&#039;s `serde` feature

[apache avro]: https://avro.apache.org/
[apache parquet]: https://parquet.apache.org/
[parquet modular encryption]: https://parquet.apache.org/docs/file-format/data-pages/encryption/

## DataFusion API Evolution and Deprecation Guidelines

Public methods in Apache DataFusion evolve over time: while we try to maintain a
stable API, we also improve the API over time. As a result, we typically
deprecate methods before removing them, according to the [deprecation guidelines].

[deprecation guidelines]: https://datafusion.apache.org/contributor-guide/api-health.html

## Dependencies and `Cargo.lock`

Following the [guidance] on committing `Cargo.lock` files, this project commits
its `Cargo.lock` file.

CI uses the committed `Cargo.lock` file, and dependencies are updated regularly
using [Dependabot] PRs.

[guidance]: https://blog.rust-lang.org/2023/08/29/committing-lockfiles.html
[dependabot]: https://docs.github.com/en/code-security/dependabot/working-with-dependabot
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
    </channel>
</rss>