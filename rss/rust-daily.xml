<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for rust - Rust Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for rust.</description>
        <lastBuildDate>Wed, 11 Jun 2025 00:05:46 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[tensorzero/tensorzero]]></title>
            <link>https://github.com/tensorzero/tensorzero</link>
            <guid>https://github.com/tensorzero/tensorzero</guid>
            <pubDate>Wed, 11 Jun 2025 00:05:46 GMT</pubDate>
            <description><![CDATA[TensorZero creates a feedback loop for optimizing LLM applications ‚Äî turning production data into smarter, faster, and cheaper models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/tensorzero/tensorzero">tensorzero/tensorzero</a></h1>
            <p>TensorZero creates a feedback loop for optimizing LLM applications ‚Äî turning production data into smarter, faster, and cheaper models.</p>
            <p>Language: Rust</p>
            <p>Stars: 6,327</p>
            <p>Forks: 390</p>
            <p>Stars today: 373 stars today</p>
            <h2>README</h2><pre>&lt;img src=&quot;https://github.com/user-attachments/assets/47d67430-386d-4675-82ad-d4734d3262d9&quot; width=128 height=128&gt;

# TensorZero

&lt;p&gt;&lt;picture&gt;&lt;img src=&quot;https://www.tensorzero.com/github-trending-badge.svg&quot; alt=&quot;#1 Repository Of The Day&quot;&gt;&lt;/picture&gt;&lt;/p&gt;

**TensorZero creates a feedback loop for optimizing LLM applications ‚Äî turning production data into smarter, faster, and cheaper models.**

1. Integrate our model gateway
2. Send metrics or feedback
3. Optimize prompts, models, and inference strategies
4. Watch your LLMs improve over time

It provides a **data &amp; learning flywheel for LLMs** by unifying:

- [x] **Inference:** one API for all LLMs, with &lt;1ms P99 overhead
- [x] **Observability:** inference &amp; feedback ‚Üí your database
- [x] **Optimization:** from prompts to fine-tuning and RL
- [x] **Evaluations:** compare prompts, models, inference strategies
- [x] **Experimentation:** built-in A/B testing, routing, fallbacks

---

&lt;p align=&quot;center&quot;&gt;
  &lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/&quot; target=&quot;_blank&quot;&gt;Website&lt;/a&gt;&lt;/b&gt;
  ¬∑
  &lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs&quot; target=&quot;_blank&quot;&gt;Docs&lt;/a&gt;&lt;/b&gt;
  ¬∑
  &lt;b&gt;&lt;a href=&quot;https://www.x.com/tensorzero&quot; target=&quot;_blank&quot;&gt;Twitter&lt;/a&gt;&lt;/b&gt;
  ¬∑
  &lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/slack&quot; target=&quot;_blank&quot;&gt;Slack&lt;/a&gt;&lt;/b&gt;
  ¬∑
  &lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/discord&quot; target=&quot;_blank&quot;&gt;Discord&lt;/a&gt;&lt;/b&gt;
  &lt;br&gt;
  &lt;br&gt;
  &lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/quickstart&quot; target=&quot;_blank&quot;&gt;Quick Start (5min)&lt;/a&gt;&lt;/b&gt;
  ¬∑
  &lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/tutorial&quot; target=&quot;_blank&quot;&gt;Comprehensive Tutorial&lt;/a&gt;&lt;/b&gt;
  ¬∑
  &lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/deployment&quot; target=&quot;_blank&quot;&gt;Deployment Guide&lt;/a&gt;&lt;/b&gt;
  ¬∑
  &lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/api-reference&quot; target=&quot;_blank&quot;&gt;API Reference&lt;/a&gt;&lt;/b&gt;
  ¬∑
  &lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/deployment&quot; target=&quot;_blank&quot;&gt;Configuration Reference&lt;/a&gt;&lt;/b&gt;
&lt;/p&gt;

---

&lt;table&gt;
  &lt;tr&gt;
    &lt;td width=&quot;30%&quot; valign=&quot;top&quot;&gt;&lt;b&gt;What is TensorZero?&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;70%&quot; valign=&quot;top&quot;&gt;TensorZero is an open-source framework for building production-grade LLM applications. It unifies an LLM gateway, observability, optimization, evaluations, and experimentation.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;30%&quot; valign=&quot;top&quot;&gt;&lt;b&gt;How is TensorZero different from other LLM frameworks?&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;70%&quot; valign=&quot;top&quot;&gt;
      1. TensorZero enables you to optimize complex LLM applications based on production metrics and human feedback.&lt;br&gt;
      2. TensorZero supports the needs of industrial-scale LLM applications: low latency, high throughput, type safety, self-hosted, GitOps, customizability, etc.&lt;br&gt;
      3. TensorZero unifies the entire LLMOps stack, creating compounding benefits. For example, LLM evaluations can be used for fine-tuning models alongside AI judges.
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;30%&quot; valign=&quot;top&quot;&gt;&lt;b&gt;Can I use TensorZero with ___?&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;70%&quot; valign=&quot;top&quot;&gt;Yes. Every major programming language is supported. You can use TensorZero with our Python client, any OpenAI SDK, or our HTTP API.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;30%&quot; valign=&quot;top&quot;&gt;&lt;b&gt;Is TensorZero production-ready?&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;70%&quot; valign=&quot;top&quot;&gt;Yes. Here&#039;s a case study: &lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/blog/case-study-automating-code-changelogs-at-a-large-bank-with-llms&quot;&gt;Automating Code Changelogs at a Large Bank with LLMs&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;30%&quot; valign=&quot;top&quot;&gt;&lt;b&gt;How much does TensorZero cost?&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;70%&quot; valign=&quot;top&quot;&gt;Nothing. TensorZero is 100% self-hosted and open-source. There are no paid features.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;30%&quot; valign=&quot;top&quot;&gt;&lt;b&gt;Who is building TensorZero?&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;70%&quot; valign=&quot;top&quot;&gt;Our technical team includes a former Rust compiler maintainer, machine learning researchers (Stanford, CMU, Oxford, Columbia) with thousands of citations, and the chief product officer of a decacorn startup. We&#039;re backed by the same investors as leading open-source projects (e.g. ClickHouse, CockroachDB) and AI labs (e.g. OpenAI, Anthropic).&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;30%&quot; valign=&quot;top&quot;&gt;&lt;b&gt;How do I get started?&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;70%&quot; valign=&quot;top&quot;&gt;You can adopt TensorZero incrementally. Our &lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/quickstart&quot;&gt;Quick Start&lt;/a&gt;&lt;/b&gt; goes from a vanilla OpenAI wrapper to a production-ready LLM application with observability and fine-tuning in just 5 minutes.&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

---

## Features

### üåê LLM Gateway

&gt; **Integrate with TensorZero once and access every major LLM provider.**

&lt;table&gt;
  &lt;tr&gt;&lt;/tr&gt; &lt;!-- flip highlight order --&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;Model Providers&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;Features&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;left&quot; valign=&quot;top&quot;&gt;
      &lt;p&gt;
        The TensorZero Gateway natively supports:
      &lt;/p&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/anthropic&quot;&gt;Anthropic&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/aws-bedrock&quot;&gt;AWS Bedrock&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/aws-sagemaker&quot;&gt;AWS SageMaker&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/azure&quot;&gt;Azure OpenAI Service&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/deepseek&quot;&gt;DeepSeek&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/fireworks&quot;&gt;Fireworks&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/gcp-vertex-ai-anthropic&quot;&gt;GCP Vertex AI Anthropic&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/gcp-vertex-ai-gemini&quot;&gt;GCP Vertex AI Gemini&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/google-ai-studio-gemini&quot;&gt;Google AI Studio (Gemini API)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/hyperbolic&quot;&gt;Hyperbolic&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/mistral&quot;&gt;Mistral&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/openai&quot;&gt;OpenAI&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/together&quot;&gt;Together&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/vllm&quot;&gt;vLLM&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/xai&quot;&gt;xAI&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
      &lt;/ul&gt;
      &lt;p&gt;
        &lt;em&gt;
          Need something else?
          Your provider is most likely supported because TensorZero integrates with &lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/providers/openai-compatible&quot;&gt;any OpenAI-compatible API (e.g. Ollama)&lt;/a&gt;&lt;/b&gt;.
          &lt;/em&gt;
      &lt;/p&gt;
    &lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;left&quot; valign=&quot;top&quot;&gt;
      &lt;p&gt;
        The TensorZero Gateway supports advanced features like:
      &lt;/p&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/retries-fallbacks&quot;&gt;Retries &amp; Fallbacks&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations&quot;&gt;Inference-Time Optimizations&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/prompt-templates-schemas&quot;&gt;Prompt Templates &amp; Schemas&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/tutorial#experimentation&quot;&gt;Experimentation (A/B Testing)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/configuration-reference&quot;&gt;Configuration-as-Code (GitOps)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/batch-inference&quot;&gt;Batch Inference&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/multimodal-inference&quot;&gt;Multimodal Inference (VLMs)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/inference-caching&quot;&gt;Inference Caching&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/metrics-feedback&quot;&gt;Metrics &amp; Feedback&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/episodes&quot;&gt;Multi-Step LLM Workflows (Episodes)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt;
        &lt;li&gt;&lt;em&gt;&amp; a lot more...&lt;/em&gt;&lt;/li&gt;
      &lt;/ul&gt;
      &lt;p&gt;
        The TensorZero Gateway is written in Rust ü¶Ä with &lt;b&gt;performance&lt;/b&gt; in mind (&amp;lt;1ms p99 latency overhead @ 10k QPS).
        See &lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/benchmarks&quot;&gt;Benchmarks&lt;/a&gt;&lt;/b&gt;.&lt;br&gt;
      &lt;/p&gt;
      &lt;p&gt;
        You can run inference using the &lt;b&gt;TensorZero client&lt;/b&gt; (recommended), the &lt;b&gt;OpenAI client&lt;/b&gt;, or the &lt;b&gt;HTTP API&lt;/b&gt;.
      &lt;/p&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;br&gt;

&lt;details open&gt;
&lt;summary&gt;&lt;b&gt;Usage: Python &amp;mdash; TensorZero Client (Recommended)&lt;/b&gt;&lt;/summary&gt;

You can access any provider using the TensorZero Python client.

1. `pip install tensorzero`
2. Optional: Set up the TensorZero configuration.
3. Run inference:

```python
from tensorzero import TensorZeroGateway  # or AsyncTensorZeroGateway


with TensorZeroGateway.build_embedded(clickhouse_url=&quot;...&quot;, config_file=&quot;...&quot;) as client:
    response = client.inference(
        model_name=&quot;openai::gpt-4o-mini&quot;,
        # Try other providers easily: &quot;anthropic::claude-3-7-sonnet-20250219&quot;
        input={
            &quot;messages&quot;: [
                {
                    &quot;role&quot;: &quot;user&quot;,
                    &quot;content&quot;: &quot;Write a haiku about artificial intelligence.&quot;,
                }
            ]
        },
    )
```

See **[Quick Start](https://www.tensorzero.com/docs/quickstart)** for more information.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Usage: Python &amp;mdash; OpenAI Client&lt;/b&gt;&lt;/summary&gt;

You can access any provider using the OpenAI Python client with TensorZero.

1. `pip install tensorzero`
2. Optional: Set up the TensorZero configuration.
3. Run inference:

```python
from openai import OpenAI  # or AsyncOpenAI
from tensorzero import patch_openai_client

client = OpenAI()

patch_openai_client(
    client,
    clickhouse_url=&quot;http://chuser:chpassword@localhost:8123/tensorzero&quot;,
    config_file=&quot;config/tensorzero.toml&quot;,
    async_setup=False,
)

response = client.chat.completions.create(
    model=&quot;tensorzero::model_name::openai::gpt-4o-mini&quot;,
    # Try other providers easily: &quot;tensorzero::model_name::anthropic::claude-3-7-sonnet-20250219&quot;
    messages=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;Write a haiku about artificial intelligence.&quot;,
        }
    ],
)
```

See **[Quick Start](https://www.tensorzero.com/docs/quickstart)** for more information.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Usage: JavaScript / TypeScript (Node) &amp;mdash; OpenAI Client&lt;/b&gt;&lt;/summary&gt;

You can access any provider using the OpenAI Node client with TensorZero.

1. Deploy `tensorzero/gateway` using Docker.
   **[Detailed instructions ‚Üí](https://www.tensorzero.com/docs/gateway/deployment)**
2. Set up the TensorZero configuration.
3. Run inference:

```ts
import OpenAI from &quot;openai&quot;;

const client = new OpenAI({
  baseURL: &quot;http://localhost:3000/openai/v1&quot;,
});

const response = await client.chat.completions.create({
  model: &quot;tensorzero::model_name::openai::gpt-4o-mini&quot;,
  // Try other providers easily: &quot;tensorzero::model_name::anthropic::claude-3-7-sonnet-20250219&quot;
  messages: [
    {
      role: &quot;user&quot;,
      content: &quot;Write a haiku about artificial intelligence.&quot;,
    },
  ],
});
```

See **[Quick Start](https://www.tensorzero.com/docs/quickstart)** for more information.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Usage: Other Languages &amp; Platforms &amp;mdash; HTTP API&lt;/b&gt;&lt;/summary&gt;

TensorZero supports virtually any programming language or platform via its HTTP API.

1. Deploy `tensorzero/gateway` using Docker.
   **[Detailed instructions ‚Üí](https://www.tensorzero.com/docs/gateway/deployment)**
2. Optional: Set up the TensorZero configuration.
3. Run inference:

```bash
curl -X POST &quot;http://localhost:3000/inference&quot; \
  -H &quot;Content-Type: application/json&quot; \
  -d &#039;{
    &quot;model_name&quot;: &quot;openai::gpt-4o-mini&quot;,
    &quot;input&quot;: {
      &quot;messages&quot;: [
        {
          &quot;role&quot;: &quot;user&quot;,
          &quot;content&quot;: &quot;Write a haiku about artificial intelligence.&quot;
        }
      ]
    }
  }&#039;
```

See **[Quick Start](https://www.tensorzero.com/docs/quickstart)** for more information.

&lt;/details&gt;

&lt;br&gt;

### üìà LLM Optimization

&gt; **Send production metrics and human feedback to easily optimize your prompts, models, and inference strategies &amp;mdash; using the UI or programmatically.**

#### Model Optimization

Optimize closed-source and open-source models using supervised fine-tuning (SFT) and preference fine-tuning (DPO).

&lt;table&gt;
  &lt;tr&gt;&lt;/tr&gt; &lt;!-- flip highlight order --&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;Supervised Fine-tuning &amp;mdash; UI&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;Preference Fine-tuning (DPO) &amp;mdash; Jupyter Notebook&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/cf7acf66-732b-43b3-af2a-5eba1ce40f6f&quot;&gt;&lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/a67a0634-04a7-42b0-b934-9130cb7cdf51&quot;&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

#### Inference-Time Optimization

Boost performance by dynamically updating your prompts with relevant examples, combining responses from multiple inferences, and more.

&lt;table&gt;
  &lt;tr&gt;&lt;/tr&gt; &lt;!-- flip highlight order --&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#best-of-n-sampling&quot;&gt;Best-of-N Sampling&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#mixture-of-n-sampling&quot;&gt;Mixture-of-N Sampling&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/c0edfa4c-713c-4996-9964-50c0d26e6970&quot;&gt;&lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/75b5bf05-4c1f-43c4-b158-d69d1b8d05be&quot;&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#dynamic-in-context-learning-dicl&quot;&gt;Dynamic In-Context Learning (DICL)&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#chain-of-thought-cot&quot;&gt;Chain-of-Thought (CoT)&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/d8489e92-ce93-46ac-9aab-289ce19bb67d&quot;&gt;&lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/ea13d73c-76a4-4e0c-a35b-0c648f898311&quot; height=&quot;320&quot;&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

_More coming soon..._

&lt;br&gt;

#### Prompt Optimization

Optimize your prompts programmatically using research-driven optimization techniques.

&lt;table&gt;
  &lt;tr&gt;&lt;/tr&gt; &lt;!-- flip highlight order --&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;&lt;a href=&quot;https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#best-of-n-sampling&quot;&gt;MIPROv2&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;&lt;a href=&quot;https://github.com/tensorzero/tensorzero/tree/main/examples/gsm8k-custom-recipe-dspy&quot;&gt;DSPy Integration&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/d81a7c37-382f-4c46-840f-e6c2593301db&quot; alt=&quot;MIPROv2 diagram&quot;&gt;&lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;
      TensorZero comes with several optimization recipes, but you can also easily create your own.
      This example shows how to optimize a TensorZero function using an arbitrary tool ‚Äî here, DSPy, a popular library for automated prompt engineering.
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

_More coming soon..._

&lt;br&gt;

### üîç LLM Observability

&gt; **Zoom in to debug individual API calls, or zoom out to monitor metrics across models and prompts over time &amp;mdash; all using the open-source TensorZero UI.**

&lt;table&gt;
  &lt;tr&gt;&lt;/tr&gt; &lt;!-- flip highlight order --&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;Observability ¬ª Inference&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;Observability ¬ª Function&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/2cc3cc9a-f33f-4e94-b8de-07522326f80a&quot;&gt;&lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/00ae6605-8fa0-4efd-8238-ae8ea589860f&quot;&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;br&gt;

### üìä LLM Evaluations

&gt; **Compare prompts, models, and inference strategies using TensorZero Evaluations &amp;mdash; with support for heuristics and LLM judges.**

&lt;table&gt;
  &lt;tr&gt;&lt;/tr&gt; &lt;!-- flip highlight order --&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;Evaluation ¬ª UI&lt;/b&gt;&lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;b&gt;Evaluation ¬ª CLI&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; valign=&quot;middle&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/f4bf54e3-1b63-46c8-be12-2eaabf615699&quot;&gt;&lt;/td&gt;
    &lt;td width=&quot;50%&quot; align=&quot;left&quot; valign=&quot;middle&quot;&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker compose run --rm evaluations \
  --evaluation-name extract_data \
  --dataset-name hard_test_cases \
  --variant-name gpt_4o \
  --concurrency 5&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;Run ID: 01961de9-c8a4-7c60-ab8d-15491a9708e4
Number of datapoints: 100
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100/100
exact_match: 0.83 ¬± 0.03
semantic_match: 0.98 ¬± 0.01
item_count: 7.15 ¬± 0.39&lt;/code&gt;&lt;/pre&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## Demo

&gt; **Watch LLMs get better at data extraction in real-time with TensorZero!**
&gt;
&gt; **[Dynamic in-context learning (DICL)](https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#dynamic-in-context-learning-dicl)** is a powerful inference-time optimization available out of the box with TensorZero.
&gt; It enhances LLM performance by automatically incorporating relevant historical examples into the prompt, without the need for model fine-tuning.

https://github.com/user-attachments/assets/4df1022e-886e-48c2-8f79-6af3cdad79cb

## LLM Engineering with TensorZero

&lt;br&gt;
&lt;p align=&quot;center&quot; &gt;
  &lt;a href=&quot;https://www.tensorzero.com/docs&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/user-attachments/assets/34a92c18-242e-4d76-a99c-861283de68a6&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/user-attachments/assets/e8bc699b-6378-4c2a-9cc1-6d189025e270&quot;&gt;
      &lt;img alt=&quot;TensorZero Flywheel&quot; src=&quot;https://github.com/user-attachments/assets/34a92c18-242e-4d76-a99c-861283de68a6&quot; width=720&gt;
    &lt;/picture&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;br&gt;

1. The **[TensorZero Gateway](https://www.tensorzero.com/docs/gateway/)** is a high-performance model gateway written in Rust ü¶Ä that provides a unified API interface for all major LLM providers, allowing for seamless cross-platform integration and fallbacks.
2. It handles structured schema-based inference with &amp;lt;1ms P99 latency overhead (see **[Benchmarks](https://www.tensorzero.com/docs/gateway/benchmarks)**) and built-in observability, experimentation, and **[inference-time optimizations](https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations)**.
3. It also collects downstream metrics and feedback associated with these inferences, with first-class support for multi-step LLM sy

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[eythaann/Seelen-UI]]></title>
            <link>https://github.com/eythaann/Seelen-UI</link>
            <guid>https://github.com/eythaann/Seelen-UI</guid>
            <pubDate>Wed, 11 Jun 2025 00:05:45 GMT</pubDate>
            <description><![CDATA[The Fully Customizable Desktop Environment for Windows 10/11.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/eythaann/Seelen-UI">eythaann/Seelen-UI</a></h1>
            <p>The Fully Customizable Desktop Environment for Windows 10/11.</p>
            <p>Language: Rust</p>
            <p>Stars: 8,106</p>
            <p>Forks: 259</p>
            <p>Stars today: 605 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[YaLTeR/niri]]></title>
            <link>https://github.com/YaLTeR/niri</link>
            <guid>https://github.com/YaLTeR/niri</guid>
            <pubDate>Wed, 11 Jun 2025 00:05:44 GMT</pubDate>
            <description><![CDATA[A scrollable-tiling Wayland compositor.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/YaLTeR/niri">YaLTeR/niri</a></h1>
            <p>A scrollable-tiling Wayland compositor.</p>
            <p>Language: Rust</p>
            <p>Stars: 8,073</p>
            <p>Forks: 264</p>
            <p>Stars today: 80 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;niri&lt;/h1&gt;
&lt;p align=&quot;center&quot;&gt;A scrollable-tiling Wayland compositor.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://matrix.to/#/#niri:matrix.org&quot;&gt;&lt;img alt=&quot;Matrix&quot; src=&quot;https://img.shields.io/badge/matrix-%23niri-blue?logo=matrix&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/YaLTeR/niri/blob/main/LICENSE&quot;&gt;&lt;img alt=&quot;GitHub License&quot; src=&quot;https://img.shields.io/github/license/YaLTeR/niri&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/YaLTeR/niri/releases&quot;&gt;&lt;img alt=&quot;GitHub Release&quot; src=&quot;https://img.shields.io/github/v/release/YaLTeR/niri?logo=github&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://github.com/YaLTeR/niri/wiki/Getting-Started&quot;&gt;Getting Started&lt;/a&gt; | &lt;a href=&quot;https://github.com/YaLTeR/niri/wiki/Configuration:-Introduction&quot;&gt;Configuration&lt;/a&gt; | &lt;a href=&quot;https://github.com/YaLTeR/niri/discussions/325&quot;&gt;Setup&amp;nbsp;Showcase&lt;/a&gt;
&lt;/p&gt;

![niri with a few windows open](https://github.com/user-attachments/assets/535e6530-2f44-4b84-a883-1240a3eee6e9)

## About

Windows are arranged in columns on an infinite strip going to the right.
Opening a new window never causes existing windows to resize.

Every monitor has its own separate window strip.
Windows can never &quot;overflow&quot; onto an adjacent monitor.

Workspaces are dynamic and arranged vertically.
Every monitor has an independent set of workspaces, and there&#039;s always one empty workspace present all the way down.

The workspace arrangement is preserved across disconnecting and connecting monitors where it makes sense.
When a monitor disconnects, its workspaces will move to another monitor, but upon reconnection they will move back to the original monitor.

## Features

- Built from the ground up for scrollable tiling
- [Dynamic workspaces](https://github.com/YaLTeR/niri/wiki/Workspaces) like in GNOME
- An [Overview](https://github.com/user-attachments/assets/379a5d1f-acdb-4c11-b36c-e85fd91f0995) that zooms out workspaces and windows
- Built-in screenshot UI
- Monitor and window screencasting through xdg-desktop-portal-gnome
    - You can [block out](https://github.com/YaLTeR/niri/wiki/Configuration:-Window-Rules#block-out-from) sensitive windows from screencasts
    - [Dynamic cast target](https://github.com/YaLTeR/niri/wiki/Screencasting#dynamic-screencast-target) that can change what it shows on the go
- [Touchpad](https://github.com/YaLTeR/niri/assets/1794388/946a910e-9bec-4cd1-a923-4a9421707515) and [mouse](https://github.com/YaLTeR/niri/assets/1794388/8464e65d-4bf2-44fa-8c8e-5883355bd000) gestures
- Group windows into [tabs](https://github.com/YaLTeR/niri/wiki/Tabs)
- Configurable layout: gaps, borders, struts, window sizes
- [Gradient borders](https://github.com/YaLTeR/niri/wiki/Configuration:-Layout#gradients) with Oklab and Oklch support
- [Animations](https://github.com/YaLTeR/niri/assets/1794388/ce178da2-af9e-4c51-876f-8709c241d95e) with support for [custom shaders](https://github.com/YaLTeR/niri/assets/1794388/27a238d6-0a22-4692-b794-30dc7a626fad)
- Live-reloading config

## Video Demo

https://github.com/YaLTeR/niri/assets/1794388/bce834b0-f205-434e-a027-b373495f9729

## Status

Niri is stable for day-to-day use and does most things expected of a Wayland compositor.
Many people are daily-driving niri, and are happy to help in our [Matrix channel].

Give it a try!
Follow the instructions on the [Getting Started](https://github.com/YaLTeR/niri/wiki/Getting-Started) wiki page.
Have your [waybar]s and [fuzzel]s ready: niri is not a complete desktop environment.

Here are some points you may have questions about:

- **Multi-monitor**: yes, a core part of the design from the very start. Mixed DPI works.
- **Fractional scaling**: yes, plus all niri UI stays pixel-perfect.
- **NVIDIA**: seems to work fine.
- **Floating windows**: yes, starting from niri 25.01.
- **Input devices**: niri supports tablets, touchpads, and touchscreens.
You can map the tablet to a specific monitor, or use [OpenTabletDriver].
We have touchpad gestures, but no touchscreen gestures yet.
- **Wlr protocols**: yes, we have most of the important ones like layer-shell, gamma-control, screencopy.
You can check on [wayland.app](https://wayland.app) at the bottom of each protocol&#039;s page.
- **Performance**: while I run niri on beefy machines, I try to stay conscious of performance.
I&#039;ve seen someone use it fine on an Eee¬†PC¬†900 from¬†2008, of all things.
- **Xwayland**: no built-in support, but xwayland-satellite is [easy to set up](https://github.com/YaLTeR/niri/wiki/Xwayland#using-xwayland-satellite) and works very well.
    - Steam and games, including Proton: work perfectly through xwayland-satellite.
    - JetBrains IDEs, Ghidra: work well through xwayland-satellite.
    - Discord and other Electron apps: work well through xwayland-satellite.
    - Chromium and VSCode: work perfectly natively on Wayland with the right flags.
    - X11 apps that want to position windows or bars at specific screen coordinates: won&#039;t work well; you can run them in a nested compositor like [labwc](https://github.com/YaLTeR/niri/wiki/Xwayland#using-the-labwc-wayland-compositor) or [rootful Xwayland](https://github.com/YaLTeR/niri/wiki/Xwayland#directly-running-xwayland-in-rootful-mode).
    - Display scaling (integer or fractional) keeps X11 apps crisp, but you need the latest xwayland-satellite.
    For games, you can run them in [gamescope] at native resolution, even with display scaling.

## Inspiration

Niri is heavily inspired by [PaperWM] which implements scrollable tiling on top of GNOME Shell.

One of the reasons that prompted me to try writing my own compositor is being able to properly separate the monitors.
Being a GNOME Shell extension, PaperWM has to work against Shell&#039;s global window coordinate space to prevent windows from overflowing.

## Tile Scrollably Elsewhere

Here are some other projects which implement a similar workflow:

- [PaperWM]: scrollable tiling on top of GNOME Shell.
- [karousel]: scrollable tiling on top of KDE.
- [scroll](https://github.com/dawsers/scroll) and [papersway]: scrollable tiling on top of sway/i3.
- [hyprscrolling] and [hyprslidr]: scrollable tiling on top of Hyprland.
- [PaperWM.spoon]: scrollable tiling on top of macOS.

## Media

[niri: Making a Wayland compositor in Rust](https://youtu.be/Kmz8ODolnDg?list=PLRdS-n5seLRqrmWDQY4KDqtRMfIwU0U3T)

My talk from the 2024 Moscow RustCon about niri, and how I do randomized property testing and profiling, and measure input latency.
The talk is in Russian, but I prepared full English subtitles that you can find in YouTube&#039;s subtitle language selector.

## Contact

We have a Matrix chat, feel free to join and ask a question: https://matrix.to/#/#niri:matrix.org

[PaperWM]: https://github.com/paperwm/PaperWM
[waybar]: https://github.com/Alexays/Waybar
[fuzzel]: https://codeberg.org/dnkl/fuzzel
[karousel]: https://github.com/peterfajdiga/karousel
[papersway]: https://spwhitton.name/tech/code/papersway/
[hyprscrolling]: https://github.com/hyprwm/hyprland-plugins/tree/main/hyprscrolling
[hyprslidr]: https://gitlab.com/magus/hyprslidr
[PaperWM.spoon]: https://github.com/mogenson/PaperWM.spoon
[Matrix channel]: https://matrix.to/#/#niri:matrix.org
[OpenTabletDriver]: https://opentabletdriver.net/
[gamescope]: https://github.com/ValveSoftware/gamescope
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[GraphiteEditor/Graphite]]></title>
            <link>https://github.com/GraphiteEditor/Graphite</link>
            <guid>https://github.com/GraphiteEditor/Graphite</guid>
            <pubDate>Wed, 11 Jun 2025 00:05:43 GMT</pubDate>
            <description><![CDATA[2D vector & raster editor that melds traditional layers & tools with a modern node-based, non-destructive, procedural workflow.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/GraphiteEditor/Graphite">GraphiteEditor/Graphite</a></h1>
            <p>2D vector & raster editor that melds traditional layers & tools with a modern node-based, non-destructive, procedural workflow.</p>
            <p>Language: Rust</p>
            <p>Stars: 12,180</p>
            <p>Forks: 598</p>
            <p>Stars today: 194 stars today</p>
            <h2>README</h2><pre>

&lt;a href=&quot;https://graphite.rs/&quot;&gt;
&lt;picture&gt;
&lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/user-attachments/assets/9366c148-4405-484f-909a-9a3526eb9209&quot;&gt;
&lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/user-attachments/assets/791508ab-bcd5-4e31-a3b9-1187cfd7a2f6&quot;&gt;
&lt;img alt=&quot;Graphite logo&quot; src=&quot;https://github.com/user-attachments/assets/791508ab-bcd5-4e31-a3b9-1187cfd7a2f6&quot;&gt;
&lt;/picture&gt;
&lt;/a&gt;

# Your procedural toolbox for 2D content creation

**Graphite is a free, open source vector and raster graphics engine, [available now](https://editor.graphite.rs) in alpha. Get creative with a fully nondestructive editing workflow that combines layer-based compositing with node-based generative design.**

Having begun life as a vector editor, Graphite continues evolving into a generalized, all-in-one graphics toolbox that&#039;s built more like a game engine than a conventional creative app. The editor&#039;s tools wrap its node graph core, providing user-friendly workflows for vector, raster, and beyond. Photo editing, motion graphics, digital painting, desktop publishing, and VFX compositing are additional competencies from the [roadmap](https://graphite.rs/features/#roadmap) making Graphite into a highly versatile content creation tool.

Learn more from the [website](https://graphite.rs/), subscribe to the [newsletter](https://graphite.rs/#newsletter), consider [volunteering](https://graphite.rs/volunteer/) or [donating](https://graphite.rs/donate/), and remember to give this repository a ‚≠ê!

&lt;br /&gt;
&lt;a href=&quot;https://discord.graphite.rs/&quot;&gt;
&lt;picture&gt;
&lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/user-attachments/assets/ad185fac-3b48-446d-863c-2bcb0724abee&quot;&gt;
&lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/user-attachments/assets/aa23f503-f3bf-444a-9080-8eaa19fa2fa8&quot;&gt;
&lt;img alt=&quot;Discord&quot; src=&quot;https://github.com/user-attachments/assets/aa23f503-f3bf-444a-9080-8eaa19fa2fa8&quot; width=&quot;48&quot; height=&quot;48&quot;&gt;
&lt;/picture&gt;
&lt;/a&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;a href=&quot;https://www.reddit.com/r/graphite/&quot;&gt;
&lt;picture&gt;
&lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/user-attachments/assets/d8c05686-2eb9-4ac1-8149-728c12b4e71a&quot;&gt;
&lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/user-attachments/assets/6f32329a-4d6f-42d8-9a2f-42977c0b3c05&quot;&gt;
&lt;img alt=&quot;Reddit&quot; src=&quot;https://github.com/user-attachments/assets/6f32329a-4d6f-42d8-9a2f-42977c0b3c05&quot; width=&quot;48&quot; height=&quot;48&quot;&gt;
&lt;/picture&gt;
&lt;/a&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;a href=&quot;https://bsky.app/profile/graphiteeditor.bsky.social&quot;&gt;
&lt;picture&gt;
&lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/user-attachments/assets/c736d80c-e9bf-4591-a7e0-a7723057a906&quot;&gt;
&lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/user-attachments/assets/3db9b0a1-5ab7-4bff-bfd3-8a4ade7b98bd&quot;&gt;
&lt;img alt=&quot;Bluesky&quot; src=&quot;https://github.com/user-attachments/assets/3db9b0a1-5ab7-4bff-bfd3-8a4ade7b98bd&quot; width=&quot;48&quot; height=&quot;48&quot;&gt;
&lt;/picture&gt;
&lt;/a&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;a href=&quot;https://twitter.com/graphiteeditor&quot;&gt;
&lt;picture&gt;
&lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/user-attachments/assets/115f04cc-e3c2-4f90-ac35-eb9edd3ca9be&quot;&gt;
&lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/user-attachments/assets/4ed4185d-a622-418c-bbf4-a0419e690ca9&quot;&gt;
&lt;img alt=&quot;Twitter&quot; src=&quot;https://github.com/user-attachments/assets/4ed4185d-a622-418c-bbf4-a0419e690ca9&quot; width=&quot;48&quot; height=&quot;48&quot;&gt;
&lt;/picture&gt;
&lt;/a&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;a href=&quot;https://www.youtube.com/@GraphiteEditor&quot;&gt;
&lt;picture&gt;
&lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/user-attachments/assets/cbc02fad-5cbc-4715-a8e5-860198e989c7&quot;&gt;
&lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/user-attachments/assets/d13b484d-97a8-4d9e-bbe4-c60348b3f676&quot;&gt;
&lt;img alt=&quot;YouTube&quot; src=&quot;https://github.com/user-attachments/assets/d13b484d-97a8-4d9e-bbe4-c60348b3f676&quot; width=&quot;48&quot; height=&quot;48&quot;&gt;
&lt;/picture&gt;
&lt;/a&gt;
&lt;br /&gt;&lt;br /&gt;

## Screenshots

![&quot;Isometric Fountain&quot; vector artwork](https://static.graphite.rs/content/index/gui-demo-node-graph-isometric-fountain.png)

![&quot;Marbled Mandelbrot&quot; fractal raster artwork](https://static.graphite.rs/content/index/gui-demo-fractal__3.png)

## Contributing/building the code

Are you a graphics programmer or Rust developer? Graphite aims to be one of the most approachable projects for putting your engineering skills to use in the world of open source. See [instructions here](https://graphite.rs/volunteer/guide/) for setting up the project and getting started.

*By submitting code for inclusion in the project, you are agreeing to license your changes under the Apache 2.0 license, and that you have the authority to do so. Some directories may have other licenses, like dual-licensed MIT/Apache 2.0, and code submissions to those directories mean you agree to the applicable license(s).*

## Support our mission ‚ù§Ô∏è

Graphite is 100% community built and funded. Please become a part of keeping our project alive and thriving with a [donation](https://graphite.rs/donate/) if you share a belief in our mission:

&gt; Graphite strives to unshackle the creativity of every budding artist and seasoned professional by building the best comprehensive art and design tool that&#039;s accessible to all.
&gt; 
&gt; Mission success will come when Graphite is an industry standard. A cohesive product vision and focus on innovation over imitation is the strategy that will make that possible.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[kata-containers/kata-containers]]></title>
            <link>https://github.com/kata-containers/kata-containers</link>
            <guid>https://github.com/kata-containers/kata-containers</guid>
            <pubDate>Wed, 11 Jun 2025 00:05:42 GMT</pubDate>
            <description><![CDATA[Kata Containers is an open source project and community working to build a standard implementation of lightweight Virtual Machines (VMs) that feel and perform like containers, but provide the workload isolation and security advantages of VMs. https://katacontainers.io/]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/kata-containers/kata-containers">kata-containers/kata-containers</a></h1>
            <p>Kata Containers is an open source project and community working to build a standard implementation of lightweight Virtual Machines (VMs) that feel and perform like containers, but provide the workload isolation and security advantages of VMs. https://katacontainers.io/</p>
            <p>Language: Rust</p>
            <p>Stars: 6,256</p>
            <p>Forks: 1,115</p>
            <p>Stars today: 46 stars today</p>
            <h2>README</h2><pre>&lt;img src=&quot;https://object-storage-ca-ymq-1.vexxhost.net/swift/v1/6e4619c416ff4bd19e1c087f27a43eea/www-images-prod/openstack-logo/kata/SVG/kata-1.svg&quot; width=&quot;900&quot;&gt;

[![CI | Publish Kata Containers payload](https://github.com/kata-containers/kata-containers/actions/workflows/payload-after-push.yaml/badge.svg)](https://github.com/kata-containers/kata-containers/actions/workflows/payload-after-push.yaml) [![Kata Containers Nightly CI](https://github.com/kata-containers/kata-containers/actions/workflows/ci-nightly.yaml/badge.svg)](https://github.com/kata-containers/kata-containers/actions/workflows/ci-nightly.yaml)

# Kata Containers

Welcome to Kata Containers!

This repository is the home of the Kata Containers code for the 2.0 and newer
releases.

If you want to learn about Kata Containers, visit the main
[Kata Containers website](https://katacontainers.io).

## Introduction

Kata Containers is an open source project and community working to build a
standard implementation of lightweight Virtual Machines (VMs) that feel and
perform like containers, but provide the workload isolation and security
advantages of VMs.

## License

The code is licensed under the Apache 2.0 license.
See [the license file](LICENSE) for further details.

## Platform support

Kata Containers currently runs on 64-bit systems supporting the following
technologies:

| Architecture | Virtualization technology |
|-|-|
| `x86_64`, `amd64` | [Intel](https://www.intel.com) VT-x, AMD SVM |
| `aarch64` (&quot;`arm64`&quot;)| [ARM](https://www.arm.com) Hyp |
| `ppc64le` | [IBM](https://www.ibm.com) Power |
| `s390x` | [IBM](https://www.ibm.com) Z &amp; LinuxONE SIE |

### Hardware requirements

The [Kata Containers runtime](src/runtime) provides a command to
determine if your host system is capable of running and creating a
Kata Container:

```bash
$ kata-runtime check
```

&gt; **Notes:**
&gt;
&gt; - This command runs a number of checks including connecting to the
&gt;   network to determine if a newer release of Kata Containers is
&gt;   available on GitHub. If you do not wish this to check to run, add
&gt;   the `--no-network-checks` option.
&gt;
&gt; - By default, only a brief success / failure message is printed.
&gt;   If more details are needed, the `--verbose` flag can be used to display the
&gt;   list of all the checks performed.
&gt;
&gt; - If the command is run as the `root` user additional checks are
&gt;   run (including checking if another incompatible hypervisor is running).
&gt;   When running as `root`, network checks are automatically disabled.

## Getting started

See the [installation documentation](docs/install).

## Documentation

See the [official documentation](docs) including:

- [Installation guides](docs/install)
- [Developer guide](docs/Developer-Guide.md)
- [Design documents](docs/design)
  - [Architecture overview](docs/design/architecture)
  - [Architecture 3.0 overview](docs/design/architecture_3.0/)

## Configuration

Kata Containers uses a single
[configuration file](src/runtime/README.md#configuration)
which contains a number of sections for various parts of the Kata
Containers system including the [runtime](src/runtime), the
[agent](src/agent) and the [hypervisor](#hypervisors).

## Hypervisors

See the [hypervisors document](docs/hypervisors.md) and the
[Hypervisor specific configuration details](src/runtime/README.md#hypervisor-specific-configuration).

## Community

To learn more about the project, its community and governance, see the
[community repository](https://github.com/kata-containers/community). This is
the first place to go if you wish to contribute to the project.

## Getting help

See the [community](#community) section for ways to contact us.

### Raising issues

Please raise an issue
[in this repository](https://github.com/kata-containers/kata-containers/issues).

&gt; **Note:**
&gt; If you are reporting a security issue, please follow the [vulnerability reporting process](https://github.com/kata-containers/community#vulnerability-handling)

## Developers

See the [developer guide](docs/Developer-Guide.md).

### Components

### Main components

The table below lists the core parts of the project:

| Component | Type | Description |
|-|-|-|
| [runtime](src/runtime) | core | Main component run by a container manager and providing a containerd shimv2 runtime implementation. |
| [runtime-rs](src/runtime-rs) | core | The Rust version runtime. |
| [agent](src/agent) | core | Management process running inside the virtual machine / POD that sets up the container environment. |
| [`dragonball`](src/dragonball) | core | An optional built-in VMM brings out-of-the-box Kata Containers experience with optimizations on container workloads |
| [documentation](docs) | documentation | Documentation common to all components (such as design and install documentation). |
| [tests](tests) | tests | Excludes unit tests which live with the main code. |

### Additional components

The table below lists the remaining parts of the project:

| Component | Type | Description |
|-|-|-|
| [packaging](tools/packaging) | infrastructure | Scripts and metadata for producing packaged binaries&lt;br/&gt;(components, hypervisors, kernel and rootfs). |
| [kernel](https://www.kernel.org) | kernel | Linux kernel used by the hypervisor to boot the guest image. Patches are stored [here](tools/packaging/kernel). |
| [osbuilder](tools/osbuilder) | infrastructure | Tool to create &quot;mini O/S&quot; rootfs and initrd images and kernel for the hypervisor. |
| [kata-debug](tools/packaging/kata-debug/README.md) | infrastructure | Utility tool to gather Kata Containers debug information from Kubernetes clusters. |
| [`agent-ctl`](src/tools/agent-ctl) | utility | Tool that provides low-level access for testing the agent. |
| [`kata-ctl`](src/tools/kata-ctl) | utility | Tool that provides advanced commands and debug facilities. |
| [`trace-forwarder`](src/tools/trace-forwarder) | utility | Agent tracing helper. |
| [`runk`](src/tools/runk) | utility | Standard OCI container runtime based on the agent. |
| [`ci`](.github/workflows) | CI | Continuous Integration configuration files and scripts. |
| [`ocp-ci`](ci/openshift-ci/README.md) | CI | Continuous Integration configuration for the OpenShift pipelines. |
| [`katacontainers.io`](https://github.com/kata-containers/www.katacontainers.io) | Source for the [`katacontainers.io`](https://www.katacontainers.io) site. |
| [`Webhook`](tools/testing/kata-webhook/README.md) | utility | Example of a simple admission controller webhook to annotate pods with the Kata runtime class |

### Packaging and releases

Kata Containers is now
[available natively for most distributions](docs/install/README.md#packaged-installation-methods).

## General tests

See the [tests documentation](tests/README.md).

## Metrics tests

See the [metrics documentation](tests/metrics/README.md).

## Glossary of Terms

See the [glossary of terms](https://github.com/kata-containers/kata-containers/wiki/Glossary) related to Kata Containers.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[openobserve/openobserve]]></title>
            <link>https://github.com/openobserve/openobserve</link>
            <guid>https://github.com/openobserve/openobserve</guid>
            <pubDate>Wed, 11 Jun 2025 00:05:41 GMT</pubDate>
            <description><![CDATA[üöÄ 10x easier, üöÄ 140x lower storage cost, üöÄ high performance, üöÄ petabyte scale - Elasticsearch/Splunk/Datadog alternative for üöÄ (logs, metrics, traces, RUM, Error tracking, Session replay).]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/openobserve/openobserve">openobserve/openobserve</a></h1>
            <p>üöÄ 10x easier, üöÄ 140x lower storage cost, üöÄ high performance, üöÄ petabyte scale - Elasticsearch/Splunk/Datadog alternative for üöÄ (logs, metrics, traces, RUM, Error tracking, Session replay).</p>
            <p>Language: Rust</p>
            <p>Stars: 15,481</p>
            <p>Forks: 588</p>
            <p>Stars today: 34 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://openobserve.ai&quot;&gt;&lt;img src=&quot;https://openobserve.ai/img/logo/logo_horizontal.svg&quot; alt=&quot;OpenObserve&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;em&gt;üöÄ 10x easier, üöÄ 140x lower storage cost, üöÄ high performance, üöÄ petabyte scale - Elasticsearch/Splunk/Datadog alternative for üöÄ (logs, metrics, traces).&lt;/em&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/openobserve/openobserve&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/last-commit/openobserve/openobserve&quot; alt=&quot;Last Commit&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/openobserve/openobserve/stargazers&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/openobserve/openobserve&quot; alt=&quot;GitHub Stars&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/openobserve/openobserve/issues&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/issues/openobserve/openobserve&quot; alt=&quot;GitHub Issues&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/openobserve/openobserve/graphs/contributors&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/contributors/openobserve/openobserve&quot; alt=&quot;Contributors&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/openobserve/openobserve/releases&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/v/release/openobserve/openobserve&quot; alt=&quot;GitHub Release&quot;&gt;
&lt;/a&gt;
&lt;/p&gt;

OpenObserve (O2 for short) is a cloud-native observability platform built specifically for logs, metrics, traces, analytics, RUM (Real User Monitoring - Performance, Errors, Session Replay) designed to work at petabyte scale.

It is straightforward and easy to operate, in contrast to Elasticsearch, which requires understanding and tuning numerous settings. Get OpenObserve up and running in under 2 minutes.

OpenObserve serves as a seamless replacement for Elasticsearch for users who ingest data using APIs and perform searches. OpenObserve comes with its own user interface, eliminating the need for separate installation.

You can reduce your log storage costs by ~140x compared to Elasticsearch by using OpenObserve. Below, we present the results from pushing logs from our production Kubernetes cluster to both Elasticsearch and OpenObserve using Fluent Bit.

![OpenObserve Vs Elasticsearch](./screenshots/zo_vs_es.png)

## üé• Introduction Video

[![OpenObserve Introduction](./screenshots/o2_intro.webp)](https://www.youtube.com/watch?v=4VwuC1tpRP4)

## üåü Features:

- **Logs, Metrics, Traces**: Comprehensive support for various data types.
- **OpenTelemetry Support**: Full compatibility with OTLP for logs, metrics, and traces.
- **Real User Monitoring (RUM)**: Includes performance tracking, error logging, and session replay.
- **Dashboards, Reports, Alerts**: Features over 18 different chart types for comprehensive data visualization for on-the-fly analysis and reporting along with alerting.
- **Pipelines**: Enrich, redact, reduce, normalize data on the fly. Stream processing for logs to metrics and more.
- **Advanced Embedded GUI**: Intuitive and user-friendly interface.
- **SQL and PromQL Support**: Query logs and traces with SQL, and metrics with SQL and PromQL.
- **Single Binary or HA Installation**: Install using a single binary for small deployments or in HA mode for large deployments.
- **Versatile Storage Options**: Supports local disk, S3, MinIO, GCS, Azure Blob Storage.
- **High Availability and Clustering**: Ensures reliable and scalable performance.
- **Dynamic Schema**: Adapts to your data structure seamlessly.
- **Built-in Authentication**: Secure and ready to use.
- **Ease of Operation**: Designed for simplicity and efficiency.
- **Seamless Upgrades**: Hassle-free updates.
- **Multilingual UI**: Supports 11 languages, including English, Spanish, German, French, Chinese, and more.

For a full list of features, check the [documentation](https://openobserve.ai/docs/#project-status-features-and-roadmap).

## ‚ö°Ô∏è Quick start

### üê≥ Docker:
```bash
docker run -d \
      --name openobserve \
      -v $PWD/data:/data \
      -p 5080:5080 \
      -e ZO_ROOT_USER_EMAIL=&quot;root@example.com&quot; \
      -e ZO_ROOT_USER_PASSWORD=&quot;Complexpass#123&quot; \
      public.ecr.aws/zinclabs/openobserve:latest
```

### üêô Docker Compose:
```yaml
services:
  openobserve:
    image: public.ecr.aws/zinclabs/openobserve:latest
    restart: unless-stopped
    environment:
      ZO_ROOT_USER_EMAIL: &quot;root@example.com&quot;
      ZO_ROOT_USER_PASSWORD: &quot;Complexpass#123&quot;
    ports:
      - &quot;5080:5080&quot;
    volumes:
      - data:/data
volumes:
  data:
```

For other ways to quickly install OpenObserve or use OpenObserve cloud, check [quickstart documentation](https://openobserve.ai/docs/quickstart).

For installing OpenObserve in HA mode, check [HA deployment documentation](https://openobserve.ai/docs/ha_deployment/).

&lt;!-- ## Enterprise Vs Open source Vs Cloud edition

OpenObserve is available in three different editions:


| Feature | Open Source (Self hosted) | Enterprise (Self hosted) | Cloud |
| --- | --- | --- | --- | 
| Logs | ‚úÖ | ‚úÖ | ‚úÖ |
| Metrics | ‚úÖ | ‚úÖ | ‚úÖ |
| Traces | ‚úÖ | ‚úÖ | ‚úÖ |
| RUM | ‚úÖ | ‚úÖ | ‚úÖ |
| Alerts | ‚úÖ | ‚úÖ | ‚úÖ |
| Dashboards | ‚úÖ | ‚úÖ | ‚úÖ |
| Reports | ‚úÖ | ‚úÖ | ‚úÖ |
| VRL functions | ‚úÖ | ‚úÖ | ‚úÖ |
| Pipelines | ‚úÖ | ‚úÖ | ‚úÖ |
| High Availability | ‚úÖ | ‚úÖ | ‚úÖ |
| Multitenancy (Organizations) | ‚úÖ | ‚úÖ | ‚úÖ |
| Dynamic schema and schema evolution | ‚úÖ | ‚úÖ | ‚úÖ |
| Advanced multilingual GUI | ‚úÖ | ‚úÖ | ‚úÖ |
| Single Sign On | ‚ùå | ‚úÖ | ‚úÖ |
| Role Based Access Control (RBAC) | ‚ùå | ‚úÖ | ‚úÖ |
| Federated search / Super cluster | ‚ùå | ‚úÖ | ‚ùå |
| Query management | ‚ùå | ‚úÖ | ‚ùå |
| Workload management (QoS) | ‚ùå | ‚úÖ | ‚ùå |
| Audit trail | ‚ùå | ‚úÖ | ‚ùå |
| Ability to influence roadmap | ‚ùå | ‚úÖ | ‚úÖ on enterprise plan |
| License | AGPL | Enterprise | Cloud |
| Support | Community | Enterprise | Cloud |
| Cost | Free | If self hosted, free for up to 200 GB/Day data ingested &lt;br&gt; Paid thereafter  | Free 200 GB/Month data ingested &lt;br&gt; Paid thereafter | --&gt;


## üì∑ Screenshots

### Home

![Home](./screenshots/zo_home.png)

### Logs

![Logs](./screenshots/logs.png)

### Traces (OpenTelemetry)

Trace details page
![Traces using OpenTelemetry](./screenshots/traces.png)

Golden metrics based on traces
![Traces golden metrics](./screenshots/traces-overall.png)

### Visualizations and Dashboards

![Dashboard](./screenshots/dashboard.png)
![Dashboard](./screenshots/dashboard2.png)
![Create panel](./screenshots/create-panel.png)
![Map](./screenshots/map.png)

### Front end monitoring

Performance analytics
![Performance](./screenshots/performance.png)

Session replay
![Session replay](./screenshots/session-replay.png)

Error tracking
![Error tracking](./screenshots/error-tracking.png)


### Alerts

![Alerts](./screenshots/alerts.png)


### Streams

![Streams](./screenshots/streams.png)

### Ingestion

![Ingestion](./screenshots/ingestion1.png)

### Pipeline

Pipeline
![Pipeline](./screenshots/pipeline.png)

Function
![Function](./screenshots/function.png)


### IAM

SSO (Single Sign On)
![SSO](./screenshots/sso.png)

RBAC (Role Based Access Control)
![RBAC](./screenshots/iam_rbac.png)


### SBOM

Software Bill of Materials for OpenObserve

#### Rust

SBOM can be found [here](./openobserve.cdx.xml). You can analyze it using [dependency track](https://dependencytrack.org/).

In order to generate the SBOM, you can use the following commands:

Install cargo-cyclonedx:

````bash
cargo install cargo-cyclonedx
````

Generate the SBOM:
```bash
cargo-cyclonedx cyclonedx
```

#### JavaScript

SBOM can be found [here](./web/sbom.json ). You can analyze it using [dependency track](https://dependencytrack.org/).

In order to generate the SBOM, you can use the following commands:

Install cyclonedx-npm:

````bash
npm install --global @cyclonedx/cyclonedx-npm
````

Generate the SBOM:
```bash
cd web
cyclonedx-npm &gt; sbom.json         
```


## ‚öñÔ∏è License

OpenObserve is licensed under the AGPL-3.0 license. For more details, see the [LICENSE](https://github.com/openobserve/openobserve/blob/main/LICENSE).

## üåç Community

### üîó Join OpenObserve community on Slack

[![Slack](./screenshots/slack.png)](https://short.openobserve.ai/community)

Easiest way to get support is to join the [Slack channel](https://short.openobserve.ai/community).

### üì± Join OpenObserve community on WeChat

&lt;img src=&quot;./screenshots/wechat_qr.jpg&quot; width=&quot;300&quot;&gt;
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[gfx-rs/wgpu]]></title>
            <link>https://github.com/gfx-rs/wgpu</link>
            <guid>https://github.com/gfx-rs/wgpu</guid>
            <pubDate>Wed, 11 Jun 2025 00:05:40 GMT</pubDate>
            <description><![CDATA[A cross-platform, safe, pure-Rust graphics API.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/gfx-rs/wgpu">gfx-rs/wgpu</a></h1>
            <p>A cross-platform, safe, pure-Rust graphics API.</p>
            <p>Language: Rust</p>
            <p>Stars: 14,331</p>
            <p>Forks: 1,067</p>
            <p>Stars today: 14 stars today</p>
            <h2>README</h2><pre># wgpu
&lt;img align=&quot;right&quot; width=&quot;20%&quot; src=&quot;logo.png&quot;&gt;

[![Matrix Space](https://img.shields.io/static/v1?label=Space&amp;message=%23Wgpu&amp;color=blue&amp;logo=matrix)](https://matrix.to/#/#Wgpu:matrix.org)
[![Dev Matrix](https://img.shields.io/static/v1?label=devs&amp;message=%23wgpu&amp;color=blueviolet&amp;logo=matrix)](https://matrix.to/#/#wgpu:matrix.org)
[![User Matrix](https://img.shields.io/static/v1?label=users&amp;message=%23wgpu-users&amp;color=blueviolet&amp;logo=matrix)](https://matrix.to/#/#wgpu-users:matrix.org)
[![Build Status](https://img.shields.io/github/actions/workflow/status/gfx-rs/wgpu/ci.yml?branch=trunk&amp;logo=github&amp;label=CI)](https://github.com/gfx-rs/wgpu/actions)
[![codecov.io](https://img.shields.io/codecov/c/github/gfx-rs/wgpu?logo=codecov&amp;logoColor=fff&amp;label=codecov&amp;token=84qJTesmeS)](https://codecov.io/gh/gfx-rs/wgpu)

`wgpu` is a cross-platform, safe, pure-rust graphics API. It runs natively on Vulkan, Metal, D3D12, and OpenGL; and on top of WebGL2 and WebGPU on wasm.

The API is based on the [WebGPU standard][webgpu]. It serves as the core of the WebGPU integration in Firefox, Servo, and Deno.

[webgpu]: https://gpuweb.github.io/gpuweb/

## Quick Links

| Docs                  | Examples                  | Changelog               |
|:---------------------:|:-------------------------:|:-----------------------:|
| [v25][rel-docs]       | [v25][rel-examples]       | [v25][rel-change]       |
| [`trunk`][trunk-docs] | [`trunk`][trunk-examples] | [`trunk`][trunk-change] |

Contributors are welcome! See [CONTRIBUTING.md][contrib] for more information.

[rel-docs]: https://docs.rs/wgpu/
[rel-examples]: https://github.com/gfx-rs/wgpu/tree/v25/examples#readme
[rel-change]: https://github.com/gfx-rs/wgpu/releases
[trunk-docs]: https://wgpu.rs/doc/wgpu/
[trunk-examples]: https://github.com/gfx-rs/wgpu/tree/trunk/examples#readme
[trunk-change]: https://github.com/gfx-rs/wgpu/blob/trunk/CHANGELOG.md#unreleased
[contrib]: CONTRIBUTING.md

## Repo Overview

The repository hosts the following libraries:

- [![Crates.io](https://img.shields.io/crates/v/wgpu.svg?label=wgpu)](https://crates.io/crates/wgpu) [![docs.rs](https://docs.rs/wgpu/badge.svg)](https://docs.rs/wgpu/) - User facing Rust API.
- [![Crates.io](https://img.shields.io/crates/v/wgpu-core.svg?label=wgpu-core)](https://crates.io/crates/wgpu-core) [![docs.rs](https://docs.rs/wgpu-core/badge.svg)](https://docs.rs/wgpu-core/) - Internal safe implementation.
- [![Crates.io](https://img.shields.io/crates/v/wgpu-hal.svg?label=wgpu-hal)](https://crates.io/crates/wgpu-hal) [![docs.rs](https://docs.rs/wgpu-hal/badge.svg)](https://docs.rs/wgpu-hal/) - Internal unsafe GPU API abstraction layer.
- [![Crates.io](https://img.shields.io/crates/v/wgpu-types.svg?label=wgpu-types)](https://crates.io/crates/wgpu-types) [![docs.rs](https://docs.rs/wgpu-types/badge.svg)](https://docs.rs/wgpu-types/) - Rust types shared between all crates.
- [![Crates.io](https://img.shields.io/crates/v/naga.svg?label=naga)](https://crates.io/crates/naga) [![docs.rs](https://docs.rs/naga/badge.svg)](https://docs.rs/naga/) - Stand-alone shader translation library.
- [![Crates.io](https://img.shields.io/crates/v/deno_webgpu.svg?label=deno_webgpu)](https://crates.io/crates/deno_webgpu) - WebGPU implementation for the Deno JavaScript/TypeScript runtime

The following binaries:

- [![Crates.io](https://img.shields.io/crates/v/naga-cli.svg?label=naga-cli)](https://crates.io/crates/naga-cli) - Tool for translating shaders between different languages using `naga`.
- [![Crates.io](https://img.shields.io/crates/v/wgpu-info.svg?label=wgpu-info)](https://crates.io/crates/wgpu-info) - Tool for getting information on GPUs in the system.
- `cts_runner` - WebGPU Conformance Test Suite runner using `deno_webgpu`.
- `player` - standalone application for replaying the API traces.

For an overview of all the components in the gfx-rs ecosystem, see [the big picture](./docs/big-picture.png).

## Getting Started

### Play with our Examples

Go to &lt;https://wgpu.rs/examples/&gt; to play with our examples in your browser. Requires a browser supporting WebGPU for the WebGPU examples.

### Rust

Rust examples can be found at [examples](examples). You can run the examples natively with `cargo run --bin wgpu-examples &lt;example&gt;`.

If you are new to wgpu and graphics programming, we recommend starting with https://sotrh.github.io/learn-wgpu/.

To run the examples in a browser, run `cargo xtask run-wasm`.
Then open `http://localhost:8000` in your browser, and you can choose an example to run.
Naturally, in order to display any of the WebGPU based examples, you need to make sure your browser supports it.

### C/C++

To use wgpu in C/C++, you need [wgpu-native](https://github.com/gfx-rs/wgpu-native).

If you are looking for a wgpu C++ tutorial, look at the following:

- https://eliemichel.github.io/LearnWebGPU/

### Others

If you want to use wgpu in other languages, there are many bindings to wgpu-native from languages such as Python, D, Julia, Kotlin, and more. See [the list](https://github.com/gfx-rs/wgpu-native#bindings).

## Community

We have the Matrix space [![Matrix Space](https://img.shields.io/static/v1?label=Space&amp;message=%23Wgpu&amp;color=blue&amp;logo=matrix)](https://matrix.to/#/#Wgpu:matrix.org) with a few different rooms that form the wgpu community:

- [![Wgpu Matrix](https://img.shields.io/static/v1?label=wgpu-devs&amp;message=%23wgpu&amp;color=blueviolet&amp;logo=matrix)](https://matrix.to/#/#wgpu:matrix.org) - discussion of the wgpu&#039;s development.
- [![Naga Matrix](https://img.shields.io/static/v1?label=naga-devs&amp;message=%23naga&amp;color=blueviolet&amp;logo=matrix)](https://matrix.to/#/#naga:matrix.org) - discussion of the naga&#039;s development.
- [![User Matrix](https://img.shields.io/static/v1?label=wgpu-users&amp;message=%23wgpu-users&amp;color=blueviolet&amp;logo=matrix)](https://matrix.to/#/#wgpu-users:matrix.org) - discussion of using the library and the surrounding ecosystem.
- [![Random Matrix](https://img.shields.io/static/v1?label=random&amp;message=%23wgpu-random&amp;color=blueviolet&amp;logo=matrix)](https://matrix.to/#/#wgpu-random:matrix.org) - discussion of everything else.

## Wiki

We have a [wiki](https://github.com/gfx-rs/wgpu/wiki) that serves as a knowledge base.

## Extension Specifications

While the core of wgpu is based on the WebGPU standard, we also support extensions that allow for features that the standard does not have yet.
For high-level documentation on how to use these extensions, see the individual specifications:

üß™EXPERIMENTALüß™ APIs are subject to change and may allow undefined behavior if used incorrectly.

- üß™EXPERIMENTALüß™ [Ray Tracing](./docs/api-specs/ray_tracing.md).

## Supported Platforms

| API    | Windows            | Linux/Android      | macOS/iOS          | Web (wasm)         |
| ------ | ------------------ | ------------------ | ------------------ | ------------------ |
| Vulkan |         ‚úÖ         |         ‚úÖ         |         üåã         |                    |
| Metal  |                    |                    |         ‚úÖ         |                    |
| DX12   |         ‚úÖ         |                    |                    |                    |
| OpenGL |    üÜó (GL 3.3+)    |  üÜó (GL ES 3.0+)   |         üìê         |    üÜó (WebGL2)     |
| WebGPU |                    |                    |                    |         ‚úÖ         |

‚úÖ = First Class Support  
üÜó = Downlevel/Best Effort Support  
üìê = Requires the [ANGLE](#angle) translation layer (GL ES 3.0 only)  
üåã = Requires the [MoltenVK](https://vulkan.lunarg.com/sdk/home#mac) translation layer  
üõ†Ô∏è = Unsupported, though open to contributions

### Shader Support

wgpu supports shaders in [WGSL](https://gpuweb.github.io/gpuweb/wgsl/), SPIR-V, and GLSL.
Both [HLSL](https://github.com/Microsoft/DirectXShaderCompiler) and [GLSL](https://github.com/KhronosGroup/glslang)
have compilers to target SPIR-V. All of these shader languages can be used with any backend as we handle all of the conversions. Additionally, support for these shader inputs is not going away.

While WebGPU does not support any shading language other than WGSL, we will automatically convert your
non-WGSL shaders if you&#039;re running on WebGPU.

WGSL is always supported by default, but GLSL and SPIR-V need features enabled to compile in support.

Note that the WGSL specification is still under development,
so the [draft specification][wgsl spec] does not exactly describe what `wgpu` supports.
See [below](#tracking-the-webgpu-and-wgsl-draft-specifications) for details.

To enable SPIR-V shaders, enable the `spirv` feature of wgpu.
To enable GLSL shaders, enable the `glsl` feature of wgpu.

### Angle

[Angle](http://angleproject.org) is a translation layer from GLES to other backends developed by Google.
We support running our GLES3 backend over it in order to reach platforms DX11 support, which aren&#039;t accessible otherwise.
In order to run with Angle, the &quot;angle&quot; feature has to be enabled, and Angle libraries placed in a location visible to the application.
These binaries can be downloaded from [gfbuild-angle](https://github.com/DileSoft/gfbuild-angle) artifacts, [manual compilation](https://github.com/google/angle/blob/main/doc/DevSetup.md) may be required on Macs with Apple silicon.

On Windows, you generally need to copy them into the working directory, in the same directory as the executable, or somewhere in your path.
On Linux, you can point to them using `LD_LIBRARY_PATH` environment.

### MSRV policy

Due to complex dependants, we have two MSRV policies:

- `naga`, `wgpu-core`, `wgpu-hal`, and `wgpu-types`&#039;s MSRV is **1.76**.
- The rest of the workspace has an MSRV of **1.84**.

It is enforced on CI (in &quot;/.github/workflows/ci.yml&quot;) with the `CORE_MSRV` and `REPO_MSRV` variables.
This version can only be upgraded in breaking releases, though we release a breaking version every three months.

The `naga`, `wgpu-core`, `wgpu-hal`, and `wgpu-types` crates should never
require an MSRV ahead of Firefox&#039;s MSRV for nightly builds, as
determined by the value of `MINIMUM_RUST_VERSION` in
[`python/mozboot/mozboot/util.py`][util].

[util]: https://searchfox.org/mozilla-central/source/python/mozboot/mozboot/util.py

## Environment Variables

All testing and example infrastructure share the same set of environment variables that determine which Backend/GPU it will run on.

- `WGPU_ADAPTER_NAME` with a substring of the name of the adapter you want to use (ex. `1080` will match `NVIDIA GeForce 1080ti`).
- `WGPU_BACKEND` with a comma-separated list of the backends you want to use (`vulkan`, `metal`, `dx12`, or `gl`).
- `WGPU_POWER_PREF` with the power preference to choose when a specific adapter name isn&#039;t specified (`high`, `low` or `none`)
- `WGPU_DX12_COMPILER` with the DX12 shader compiler you wish to use (`dxc`, `static-dxc`, or `fxc`). Note that `dxc` requires `dxcompiler.dll` (min v1.8.2502) to be in the working directory, and `static-dxc` requires the `static-dxc` crate feature to be enabled. Otherwise, it will fall back to `fxc`.
- `WGPU_GLES_MINOR_VERSION` with the minor OpenGL ES 3 version number to request (`0`, `1`, `2` or `automatic`).
- `WGPU_ALLOW_UNDERLYING_NONCOMPLIANT_ADAPTER` with a boolean whether non-compliant drivers are enumerated (`0` for false, `1` for true).

When running the CTS, use the variables `DENO_WEBGPU_ADAPTER_NAME`, `DENO_WEBGPU_BACKEND`, `DENO_WEBGPU_POWER_PREFERENCE`.

## Testing

We have multiple methods of testing, each of which tests different qualities about wgpu. We automatically run our tests on CI. The current state of CI testing:

| Platform/Backend | Tests              | Notes                 |
| ---------------- | ------------------ | --------------------- |
| Windows/DX12     | :heavy_check_mark: | using WARP            |
| Windows/OpenGL   | :heavy_check_mark: | using llvmpipe        |
| MacOS/Metal      | :heavy_check_mark: | using hardware runner |
| Linux/Vulkan     | :heavy_check_mark: | using lavapipe        |
| Linux/OpenGL ES  | :heavy_check_mark: | using llvmpipe        |
| Chrome/WebGL     | :heavy_check_mark: | using swiftshader     |
| Chrome/WebGPU    | :x:                | not set up            |

### Core Test Infrastructure

We use a tool called [`cargo nextest`](https://github.com/nextest-rs/nextest) to run our tests.
To install it, run `cargo install cargo-nextest`.

To run the test suite:

```
cargo xtask test
```

To run the test suite on WebGL (currently incomplete):

```
cd wgpu
wasm-pack test --headless --chrome --no-default-features --features webgl --workspace
```

This will automatically run the tests using a packaged browser. Remove `--headless` to run the tests with whatever browser you wish at `http://localhost:8000`.

If you are a user and want a way to help contribute to wgpu, we always need more help writing test cases.

### WebGPU Conformance Test Suite

WebGPU includes a Conformance Test Suite to validate that implementations are
working correctly. We run cases from the CTS against wgpu using
[Deno](https://deno.com/). A [default list of enabled
tests](./cts_runner/test.lst) is automatically run on pull requests in CI.

To run the default set of CTS tests locally, run:

```
cargo xtask cts
```

You can also specify a test selector on the command line:

```
cargo xtask cts &#039;webgpu:api,operation,command_buffer,basic:*&#039;
```

Or supply your own test list in a file:

```
cargo xtask cts -f your_tests.lst
```

To find the full list of tests, go to the
[web version of the CTS](https://gpuweb.github.io/cts/standalone/?runnow=0&amp;worker=0&amp;debug=0&amp;q=webgpu:*).

The version of the CTS used by `cargo xtask cts` is specified in
[`cts_runner/revision.txt`](./cts_runner/revision.txt).

## Tracking the WebGPU and WGSL draft specifications

The `wgpu` crate is meant to be an idiomatic Rust translation of the [WebGPU API][webgpu spec].
That specification, along with its shading language, [WGSL][wgsl spec],
are both still in the &quot;Working Draft&quot; phase,
and while the general outlines are stable,
details change frequently.
Until the specification is stabilized, the `wgpu` crate and the version of WGSL it implements
will likely differ from what is specified,
as the implementation catches up.

Exactly which WGSL features `wgpu` supports depends on how you are using it:

- When running as native code, `wgpu` uses the [Naga][naga] crate
  to translate WGSL code into the shading language of your platform&#039;s native GPU API.
  Naga has [a milestone][naga wgsl milestone]
  for catching up to the WGSL specification,
  but in general, there is no up-to-date summary
  of the differences between Naga and the WGSL spec.

- When running in a web browser (by compilation to WebAssembly)
  without the `&quot;webgl&quot;` feature enabled,
  `wgpu` relies on the browser&#039;s own WebGPU implementation.
  WGSL shaders are simply passed through to the browser,
  so that determines which WGSL features you can use.

- When running in a web browser with `wgpu`&#039;s `&quot;webgl&quot;` feature enabled,
  `wgpu` uses Naga to translate WGSL programs into GLSL.
  This uses the same version of Naga as if you were running `wgpu` as native code.

[webgpu spec]: https://www.w3.org/TR/webgpu/
[wgsl spec]: https://gpuweb.github.io/gpuweb/wgsl/
[naga]: https://github.com/gfx-rs/naga/
[naga wgsl milestone]: https://github.com/gfx-rs/naga/milestone/4

## Coordinate Systems

wgpu uses the coordinate systems of D3D and Metal:

| Render                                              | Texture                                               |
| --------------------------------------------------- | ----------------------------------------------------- |
| ![render_coordinates](./docs/render_coordinates.png) | ![texture_coordinates](./docs/texture_coordinates.png) |
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[ArthurBrussee/brush]]></title>
            <link>https://github.com/ArthurBrussee/brush</link>
            <guid>https://github.com/ArthurBrussee/brush</guid>
            <pubDate>Wed, 11 Jun 2025 00:05:39 GMT</pubDate>
            <description><![CDATA[3D Reconstruction for all]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ArthurBrussee/brush">ArthurBrussee/brush</a></h1>
            <p>3D Reconstruction for all</p>
            <p>Language: Rust</p>
            <p>Stars: 1,872</p>
            <p>Forks: 85</p>
            <p>Stars today: 4 stars today</p>
            <h2>README</h2><pre># Brush - 3D reconstruction for all

https://github.com/user-attachments/assets/b7f55b9c-8632-49f9-b34b-d5de52a7a8b0

Brush is a 3D reconstruction engine using [Gaussian splatting](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/). It works on a wide range of systems: **macOS/windows/linux**, **AMD/Nvidia/Intel** cards, **Android**, and in a **browser**. To achieve this, it uses WebGPU compatible tech and the [Burn](https://github.com/tracel-ai/burn) machine learning framework, which has a portable [`wgpu`](https://github.com/gfx-rs/wgpu) backend.

[**Try the (experimental) web demo** &lt;img src=&quot;https://cdn-icons-png.flaticon.com/256/888/888846.png&quot; alt=&quot;chrome logo&quot; width=&quot;24&quot;/&gt;
](https://arthurbrussee.github.io/brush-demo)
_NOTE: Only works on Chrome 131+ as of Jan 2025. Firefox and Safari are hopefully supported [soon](https://caniuse.com/webgpu)_

[![](https://dcbadge.limes.pink/api/server/https://discord.gg/TbxJST2BbC)](https://discord.gg/TbxJST2BbC)

https://github.com/user-attachments/assets/4c70f892-cfd2-419f-8098-b0e20dba23c7

Training &amp; Viewing on the web

https://github.com/user-attachments/assets/d6751cb3-ff58-45a4-8321-77d3b0a7b051

Training on a pixel 7

# Why

Machine learning for real time rendering has tons of potential, but most ML tools don&#039;t align well with it: Rendering requires realtime interactivity, usually involves dynamic shapes, and it&#039;s cumbersome to ship apps with large PyTorch/Jax/CUDA deps. The usual fix is to write a separate training and inference application. Brush on the other hand, written in `rust` using `wgpu` and `burn`, can produce simple dependency free binaries, run on nearly all devices, and doesn&#039;t require any cumbersome setup.

# Features

## Training

Brush works with _posed_ image data. It can load COLMAP data or datasets in the Nerfstudio format with a transforms.json. Training is fully supported natively, on mobile, and in a browser*.

It also supports masking images:
- Images with transparency. This will force the final splat to match the transparency of the input.
- A folder of images called &#039;masks&#039;. This ignores parts of the image that are masked out.

While training you can interact with the scene and see the training dynamics live, and compare the current rendering to training or eval views as the training progresses.

(*To train in your browser, you have to load your dataset a zip).

## Viewer
Brush also works well as a splat viewer, including on the web. It can load normal .ply files. It can also stream in data from a URL (for a web app, simply append `?url=`). There&#039;s both orbit and flythrough controls.

Brush also can load .zip of splat files to display them as an animation, or a special ply that includes delta frames. This was used for [cat-4D](https://cat-4d.github.io/) and [Cap4D](https://felixtaubner.github.io/cap4d/)!

## CLI
Brush can be used as a CLI. Run `brush --help` to get an overview. Every CLI command can work with `--with-viewer` which also opens the UI, for easy debugging.

## Rerun

https://github.com/user-attachments/assets/f679fec0-935d-4dd2-87e1-c301db9cdc2c

While training, additional data can be visualized with the excellent [rerun](https://rerun.io/). To install rerun on your machine, please follow their [instructions](https://rerun.io/docs/getting-started/installing-viewer). Open the ./brush_blueprint.rbl in the viewer for best results.

## Building Brush
First install rust 1.85+. You can run tests with `cargo test --all`. Brush uses the wonderful [rerun](https://rerun.io/) for additional visualizations while training, run `cargo install rerun-cli` if you want to use it.

### Windows/macOS/Linux
Simply `cargo run` or `cargo run --release` from the workspace root. Brush can also be used as a CLI, run `cargo run --release -- --help` to use the CLI directly from source. See the notes about the CLI in the features section.

### Web
This project uses [`trunk`](https://github.com/trunk-rs/trunk) to build for the web. Install trunk, and then run `trunk serve` or `trunk serve --release` to run a development server.

WebGPU is still a new standard, and as such, only Chrome 134+ work currently.

### Android

As a one time setup, make sure you have the Android SDK &amp; NDK installed.
- Check if ANDROID_NDK_HOME and ANDROID_HOME are set
- Add the Android target to rust `rustup target add aarch64-linux-android`
- Install cargo-ndk to manage building a lib `cargo install cargo-ndk`

Each time you change the rust code, run
- `cargo ndk -t arm64-v8a -o crates/brush-app/app/src/main/jniLibs/ build`
- Nb:  Nb, for best performance, build in release mode. This is separate
  from the Android Studio app build configuration.
- `cargo ndk -t arm64-v8a -o crates/brush-app/app/src/main/jniLibs/  build --release`

You can now either run the project from Android Studio (Android Studio does NOT build the rust code), or run it from the command line:
```
./gradlew build
./gradlew installDebug
adb shell am start -n com.splats.app/.MainActivity
```

You can also open this folder as a project in Android Studio and run things from there.

Nb: Running in Android Studio does _not_ rebuild the rust code automatically.

## Results

| Metric | bicycle | garden | stump | room | counter | kitchen | bonsai | Average |
|--------|---------|---------|--------|-------|----------|----------|---------|----------|
| **PSNR ‚Üë** |
| inria 30K | 25.25 | 27.41 | 26.55 | 30.63 | 28.70 | 30.32 | 31.98 | 28.69 |
| gsplat 30K | 25.22 | 27.32 | 26.53 | 31.36 | 29.02 | **31.16**‚≠ê | **32.06**‚≠ê | 28.95 |
| brush 30K | **25.55**‚≠ê | **27.42**‚≠ê | **26.88**‚≠ê | **31.45**‚≠ê | **29.17**‚≠ê | 30.55 | 32.02 | **29.01**‚≠ê |
| **SSIM ‚Üë** |
| inria 30k | 0.763 | 0.863 | 0.771 | **0.918**‚≠ê | 0.906 | 0.925 | 0.941 | 0.870 |
| gsplat | 0.764 | 0.865 | 0.768 | **0.918**‚≠ê | 0.907 | **0.926**‚≠ê | 0.941 | 0.870 |
| brush | **0.781**‚≠ê | **0.869**‚≠ê | **0.791**‚≠ê | 0.916 | **0.909**‚≠ê | 0.920 | **0.942**‚≠ê | **0.875**‚≠ê |
| **Splat Count (millions) ‚Üì** |
| inira | 6.06 | 5.71 | 4.82 | 1.55 | 1.19 | 1.78 | 1.24 | 3.19 |
| gsplat | 6.26 | 5.84 | 4.81 | 1.59 | 1.21 | 1.79 | 1.25 | 3.25 |
| brush | **3.30**‚≠ê | **2.90**‚≠ê | **2.55**‚≠ê | **0.75**‚≠ê | **0.60**‚≠ê | **0.79**‚≠ê | **0.68**‚≠ê | **1.65**‚≠ê |
| **Minutes (4070 ti)** |
| brush | 35 | 35 | 28 | 18 | 19 | 18 | 18 | 24.43 |

Numbers taken from [here](https://docs.gsplat.studio/main/tests/eval.html). Note that Brush by default regularizes opacity slightly.

## Benchmarks

Rendering is generally faster than gsplat, while end-to-end training speeds are similar. You can run benchmarks of some of the kernels using `cargo bench`. For additional profiling, you can use [tracy](https://github.com/wolfpld/tracy) and run with `cargo run --release --feature=tracy`.

# Acknowledgements

[**gSplat**](https://github.com/nerfstudio-project/gsplat), for their reference version of the kernels

**Peter Hedman, George Kopanas &amp; Bernhard Kerbl**, for the many discussions &amp; pointers.

**The Burn team**, for help &amp; improvements to Burn along the way

**Raph Levien**, for the [original version](https://github.com/googlefonts/compute-shader-101/pull/31) of the GPU radix sort.

# Disclaimer

This is *not* an official Google product. This repository is a forked public version of [the google-research repository](https://github.com/google-research/google-research/tree/master/brush_splat)
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[risingwavelabs/risingwave]]></title>
            <link>https://github.com/risingwavelabs/risingwave</link>
            <guid>https://github.com/risingwavelabs/risingwave</guid>
            <pubDate>Wed, 11 Jun 2025 00:05:38 GMT</pubDate>
            <description><![CDATA[Stream processing and management platform.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/risingwavelabs/risingwave">risingwavelabs/risingwave</a></h1>
            <p>Stream processing and management platform.</p>
            <p>Language: Rust</p>
            <p>Stars: 7,869</p>
            <p>Forks: 650</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre>
&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source srcset=&quot;.github/RisingWave-logo-dark.svg&quot; width=&quot;500px&quot; media=&quot;(prefers-color-scheme: dark)&quot;&gt;
    &lt;img src=&quot;.github/RisingWave-logo-light.svg&quot; width=&quot;500px&quot;&gt;
  &lt;/picture&gt;
&lt;/p&gt;


&lt;div align=&quot;center&quot;&gt;

### üåä Ride the Wave of Streaming Data.

&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://docs.risingwave.com/&quot;&gt;Docs&lt;/a&gt; | &lt;a href=&quot;https://docs.risingwave.com/get-started/rw-benchmarks-stream-processing&quot;&gt;Benchmarks&lt;/a&gt; | &lt;a href=&quot;https://docs.risingwave.com/demos/overview&quot;&gt;Demos&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a
    href=&quot;https://github.com/risingwavelabs/risingwave/releases/latest&quot;
    target=&quot;_blank&quot;
  &gt;
    &lt;img alt=&quot;Release&quot; src=&quot;https://img.shields.io/github/v/release/risingwavelabs/risingwave.svg?sort=semver&quot; /&gt;
  &lt;/a&gt;
  &lt;a
    href=&quot;https://go.risingwave.com/slack&quot;
    target=&quot;_blank&quot;
  &gt;
    &lt;img alt=&quot;Slack&quot; src=&quot;https://badgen.net/badge/Slack/Join%20RisingWave/0abd59?icon=slack&quot; /&gt;
  &lt;/a&gt;
  &lt;a
    href=&quot;https://x.com/risingwavelabs&quot;
    target=&quot;_blank&quot;
  &gt;
    &lt;img alt=&quot;X&quot; src=&quot;https://img.shields.io/twitter/follow/risingwavelabs&quot; /&gt;
  &lt;/a&gt;
  &lt;a
    href=&quot;https://www.youtube.com/@risingwave-labs&quot;
    target=&quot;_blank&quot;
  &gt;
    &lt;img alt=&quot;YouTube&quot; src=&quot;https://img.shields.io/youtube/channel/views/UCsHwdyBRxBpmkA5RRd0YNEA&quot; /&gt;
  &lt;/a&gt;
&lt;/div&gt;

RisingWave is a stream processing and management platform designed to offer the &lt;i&gt;&lt;b&gt;simplest&lt;/b&gt;&lt;/i&gt; and &lt;i&gt;&lt;b&gt;most cost-effective&lt;/b&gt;&lt;/i&gt; way to &lt;b&gt;process&lt;/b&gt;, &lt;b&gt;analyze&lt;/b&gt;, and &lt;b&gt;manage&lt;/b&gt; real-time event data ‚Äî with built-in support for the [Apache Iceberg‚Ñ¢](https://iceberg.apache.org/) open table format. It provides both a Postgres-compatible [SQL interface](https://docs.risingwave.com/sql/overview) and a DataFrame-style [Python interface](https://docs.risingwave.com/python-sdk/intro).

RisingWave can &lt;b&gt;ingest&lt;/b&gt; millions of events per second, continuously &lt;b&gt;join and analyze&lt;/b&gt; live streams with historical data, &lt;b&gt;serve&lt;/b&gt; ad-hoc queries at low latency, and &lt;b&gt;persist&lt;/b&gt; fresh, consistent results to Apache Iceberg‚Ñ¢ or any other downstream system.

![RisingWave](./docs/dev/src/images/architecture_20250609.jpg)

## Try it out in 60 seconds

Install RisingWave standalone mode:
```shell
curl -L https://risingwave.com/sh | sh
```

To learn about other installation options, such as using a Docker image, see [Quick Start](https://docs.risingwave.com/docs/current/get-started/).

## Stream, Store, and Query ‚Äî All in One

RisingWave delivers a full **end-to-end streaming data platform** ‚Äî combining real-time processing with built-in storage and open-format persistence.

It supports:

* **Ingestion:** Ingest millions of events per second from streaming and batch sources.
* **Stream processing:** Perform real-time incremental processing to join and analyze live data with historical tables.
* **Delivery:** Deliver fresh, consistent results to data lakes (e.g., Apache Iceberg‚Ñ¢) or any destination.

What sets RisingWave apart is its integrated storage engine:

* **Online serving:** Row-based storage optimized for point and range queries with single-digit millisecond latency.
* **Offline persistence:** Built-in Apache Iceberg‚Ñ¢ integration for low-cost, durable storage with open access for external query engines.

With RisingWave, real-time data isn‚Äôt just processed ‚Äî it‚Äôs stored, queried, and shared across your entire stack.

## Key design decisions

RisingWave is designed to be easier to use and more cost-efficient:

### PostgreSQL compatibility

* **Seamless integration:** Connects via the PostgreSQL wire protocol, working with psql, JDBC, and any Postgres tool.
* **Expressive SQL:** Supports structured, semi-structured, and unstructured data with a familiar SQL dialect.
* **No manual state tuning:** Eliminates complex state management configurations.

### S3 as primary storage

RisingWave stores tables, materialized views, and internal states of stream processing jobs in S3 (or equivalent object storage), providing:
- **High performance:** Optimized for complex queries, including joins and time windowing.
- **Fast recovery:** Restores from system failures within seconds.
- **[Dynamic scaling](https://docs.risingwave.com/deploy/k8s-cluster-scaling):** Instantly adjusts resources to handle workload spikes.

### Elastic disk cache

Beyond caching hot data in memory, RisingWave supports [**elastic disk cache**](https://docs.risingwave.com/get-started/disk-cache), a powerful performance optimization that uses local disks or EBS for efficient data caching. This minimizes access to S3, lowering processing latency and cutting S3 access costs.

### Apache Iceberg‚Ñ¢ native support
RisingWave [**natively integrates with Apache Iceberg‚Ñ¢**](https://docs.risingwave.com/iceberg/overview), enabling continuous ingestion of streaming data into Iceberg tables. It can also read directly from Iceberg, perform automatic compaction, and maintain table health over time. Since Iceberg is an open table format, results are accessible by other query engines ‚Äî making storage not only cost-efficient, but interoperable by design.

## In what use cases does RisingWave excel?
RisingWave is particularly effective for the following use cases:

* **Streaming analytics**: Achieve sub-second data freshness in live dashboards, ideal for high-stakes scenarios like stock trading, sports betting, and IoT monitoring.
* **Event-driven applications**: Develop sophisticated monitoring and alerting systems for critical applications such as fraud and anomaly detection.
* **Real-time data enrichment**: Continuously ingest data from diverse sources, conduct real-time data enrichment, and efficiently deliver the results to downstream systems.
* **Feature engineering**: Transform batch and streaming data into features in your machine learning models using a unified codebase, ensuring seamless integration and consistency.

## Production deployments

[**RisingWave Cloud**](https://cloud.risingwave.com) offers the easiest way to run RisingWave in production.

For **Docker deployment**, please refer to [Docker Compose](https://docs.risingwave.com/docs/current/risingwave-docker-compose/).

For **Kubernetes deployment**, please refer to [Kubernetes with Helm](https://docs.risingwave.com/docs/current/risingwave-k8s-helm/) or [Kubernetes with Operator](https://docs.risingwave.com/docs/current/risingwave-kubernetes/).

## Community

Looking for help, discussions, collaboration opportunities, or a casual afternoon chat with our fellow engineers and community members? Join our [Slack workspace](https://risingwave.com/slack)!

## Notes on telemetry


RisingWave uses [Scarf](https://scarf.sh/) to collect anonymized installation analytics. These analytics help support us understand and improve the distribution of our package. The privacy policy of Scarf is available at [https://about.scarf.sh/privacy-policy](https://about.scarf.sh/privacy-policy).

RisingWave also collects anonymous usage statistics to better understand how the community is using RisingWave. The sole intention of this exercise is to help improve the product. Users may opt out easily at any time. Please refer to the [user documentation](https://docs.risingwave.com/docs/current/telemetry/) for more details.

## License

RisingWave is distributed under the Apache License (Version 2.0). Please refer to [LICENSE](LICENSE) for more information.

## Contributing

Thanks for your interest in contributing to the project! Please refer to [RisingWave Developer Guide](https://risingwavelabs.github.io/risingwave/) for more information.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[antinomyhq/forge]]></title>
            <link>https://github.com/antinomyhq/forge</link>
            <guid>https://github.com/antinomyhq/forge</guid>
            <pubDate>Wed, 11 Jun 2025 00:05:37 GMT</pubDate>
            <description><![CDATA[AI enabled pair programmer for Claude, GPT, O Series, Grok, Deepseek, Gemini and 300+ models]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/antinomyhq/forge">antinomyhq/forge</a></h1>
            <p>AI enabled pair programmer for Claude, GPT, O Series, Grok, Deepseek, Gemini and 300+ models</p>
            <p>Language: Rust</p>
            <p>Stars: 3,134</p>
            <p>Forks: 1,074</p>
            <p>Stars today: 534 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;‚öíÔ∏è Forge: AI-Enhanced Terminal Development Environment&lt;/h1&gt;
&lt;p align=&quot;center&quot;&gt;A comprehensive coding agent that integrates AI capabilities with your development environment&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;code&gt;npm install -g @antinomyhq/forge&lt;/code&gt;&lt;/p&gt;

[![CI Status](https://img.shields.io/github/actions/workflow/status/antinomyhq/forge/ci.yml?style=for-the-badge)](https://github.com/antinomyhq/forge/actions)
[![GitHub Release](https://img.shields.io/github/v/release/antinomyhq/forge?style=for-the-badge)](https://github.com/antinomyhq/forge/releases)
[![Discord](https://img.shields.io/discord/1044859667798568962?style=for-the-badge&amp;cacheSeconds=120&amp;logo=discord)](https://discord.gg/kRZBPpkgwq)
[![CLA assistant](https://cla-assistant.io/readme/badge/antinomyhq/forge?style=for-the-badge)](https://cla-assistant.io/antinomyhq/forge)

![Code-Forge Demo](https://assets.antinomy.ai/images/forge_demo_2x.gif)

---

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Table&amp;nbsp;of&amp;nbsp;Contents&lt;/strong&gt;&lt;/summary&gt;

- [Quickstart](#quickstart)
- [Usage Examples](#usage-examples)
- [Interactive Mode Examples](#interactive-mode-examples)
- [Why Forge?](#why-forge)
- [Command-Line Options](#command-line-options)
- [Advanced Configuration](#advanced-configuration)
  - [Provider Configuration](#provider-configuration)
  - [forge.yaml Configuration Options](#forgeyaml-configuration-options)
- [Documentation](#documentation)
- [Community](#community)
- [Support Us](#support-us)

&lt;/details&gt;

---

## Quickstart

Install globally:

```bash
npm install -g @antinomyhq/forge
```

Sign up at [Antinomy.ai](https://app.antinomy.ai/app/) to enable the Forge provider.

Then set up your Forge provider key:

```bash
# .env
FORGE_KEY=ForgeKey
```

Run Forge in interactive mode:

```bash
forge
```

That&#039;s it! Forge is now ready to assist you with your development tasks.

## Usage Examples

Forge can be used in different ways depending on your needs. Here are some common usage patterns:

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Code Understanding&lt;/strong&gt;&lt;/summary&gt;

```
&gt; Can you explain how the authentication system works in this codebase?
```

Forge will analyze your project&#039;s structure, identify authentication-related files, and provide a detailed explanation of the authentication flow, including the relationships between different components.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Implementing New Features&lt;/strong&gt;&lt;/summary&gt;

```
&gt; I need to add a dark mode toggle to our React application. How should I approach this?
```

Forge will suggest the best approach based on your current codebase, explain the steps needed, and even scaffold the necessary components and styles for you.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Debugging Assistance&lt;/strong&gt;&lt;/summary&gt;

```
&gt; I&#039;m getting this error: &quot;TypeError: Cannot read property &#039;map&#039; of undefined&quot;. What might be causing it?
```

Forge will analyze the error, suggest potential causes based on your code, and propose different solutions to fix the issue.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Code Reviews&lt;/strong&gt;&lt;/summary&gt;

```
&gt; Please review the code in src/components/UserProfile.js and suggest improvements
```

Forge will analyze the code, identify potential issues, and suggest improvements for readability, performance, security, and maintainability.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Learning New Technologies&lt;/strong&gt;&lt;/summary&gt;

```
&gt; I want to integrate GraphQL into this Express application. Can you explain how to get started?
```

Forge will provide a tailored tutorial on integrating GraphQL with Express, using your specific project structure as context.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Database Schema Design&lt;/strong&gt;&lt;/summary&gt;

```
&gt; I need to design a database schema for a blog with users, posts, comments, and categories
```

Forge will suggest an appropriate schema design, including tables/collections, relationships, indexes, and constraints based on your project&#039;s existing database technology.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Refactoring Legacy Code&lt;/strong&gt;&lt;/summary&gt;

```
&gt; Help me refactor this class-based component to use React Hooks
```

Forge can help modernize your codebase by walking you through refactoring steps and implementing them with your approval.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Git Operations&lt;/strong&gt;&lt;/summary&gt;

```
&gt; I need to merge branch &#039;feature/user-profile&#039; into main but there are conflicts
```

Forge can guide you through resolving git conflicts, explaining the differences and suggesting the best way to reconcile them.
&lt;/details&gt;

## Why Forge?

Forge is designed for developers who want to enhance their workflow with AI assistance while maintaining full control over their development environment.

- **Zero configuration** - Just add your API key and you&#039;re ready to go
- **Seamless integration** - Works right in your terminal, where you already work
- **Multi-provider support** - Use OpenAI, Anthropic, or other LLM providers
- **Secure by design** - Your code stays on your machine
- **Open-source** - Transparent, extensible, and community-driven

Forge helps you code faster, solve complex problems, and learn new technologies without leaving your terminal.

## Command-Line Options

Here&#039;s a quick reference of Forge&#039;s command-line options:

| Option                          | Description                                                |
| ------------------------------- | ---------------------------------------------------------- |
| `-p, --prompt &lt;PROMPT&gt;`         | Direct prompt to process without entering interactive mode |
| `-c, --command &lt;COMMAND&gt;`       | Path to a file containing initial commands to execute      |
| `-w, --workflow &lt;WORKFLOW&gt;`     | Path to a file containing the workflow to execute          |
| `-e, --event &lt;EVENT&gt;`           | Dispatch an event to the workflow                          |
| `--conversation &lt;CONVERSATION&gt;` | Path to a file containing the conversation to execute      |
| `-r, --restricted`              | Enable restricted shell mode for enhanced security         |
| `--verbose`                     | Enable verbose output mode                                 |
| `-h, --help`                    | Print help information                                     |
| `-V, --version`                 | Print version                                              |

## Advanced Configuration

### Provider Configuration

Forge supports multiple AI providers. Below are setup instructions for each supported provider:

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Antinomy.ai (Recommended)&lt;/strong&gt;&lt;/summary&gt;

```bash
# .env
FORGE_KEY=ForgeKey
```

To use Antinomy&#039;s provider with Forge:
1. Visit [https://app.antinomy.ai/](https://app.antinomy.ai/)
2. Login with your existing credentials or create a new account
3. Once logged in, your account will automatically enable the Forge Provider

_No changes in `forge.yaml` required_

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;OpenRouter&lt;/strong&gt;&lt;/summary&gt;

```bash
# .env
OPENROUTER_API_KEY=&lt;your_openrouter_api_key&gt;
```

_No changes in `forge.yaml` required_

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;&lt;/summary&gt;

```bash
# .env
OPENAI_API_KEY=&lt;your_openai_api_key&gt;
```

```yaml
# forge.yaml
model: o3-mini-high
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;&lt;/summary&gt;

```bash
# .env
ANTHROPIC_API_KEY=&lt;your_anthropic_api_key&gt;
```

```yaml
# forge.yaml
model: claude-3.7-sonnet
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Google Vertex AI&lt;/strong&gt;&lt;/summary&gt;

```bash
# .env
PROJECT_ID=&lt;your_project_id&gt;
LOCATION=&lt;your_location&gt;
OPENAI_API_KEY=&lt;vertex_ai_key&gt;
OPENAI_URL=https://${LOCATION}-aiplatform.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/${LOCATION}/endpoints/openapi
```

```yaml
# forge.yaml
model: publishers/anthropic/models/claude-3-7-sonnet
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;OpenAI-Compatible Providers&lt;/strong&gt;&lt;/summary&gt;

```bash
# .env
OPENAI_API_KEY=&lt;your_provider_api_key&gt;
OPENAI_URL=&lt;your_provider_url&gt;
```

```yaml
# forge.yaml
model: &lt;provider-specific-model&gt;
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Groq&lt;/strong&gt;&lt;/summary&gt;

```bash
# .env
OPENAI_API_KEY=&lt;your_groq_api_key&gt;
OPENAI_URL=https://api.groq.com/openai/v1
```

```yaml
# forge.yaml
model: deepseek-r1-distill-llama-70b
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Amazon Bedrock&lt;/strong&gt;&lt;/summary&gt;

To use Amazon Bedrock models with Forge, you&#039;ll need to first set up the [Bedrock Access Gateway](https://github.com/aws-samples/bedrock-access-gateway):

1. **Set up Bedrock Access Gateway**:

   - Follow the deployment steps in the [Bedrock Access Gateway repo](https://github.com/aws-samples/bedrock-access-gateway)
   - Create your own API key in Secrets Manager
   - Deploy the CloudFormation stack
   - Note your API Base URL from the CloudFormation outputs

2. **Create these files in your project directory**:

   ```bash
   # .env
   OPENAI_API_KEY=&lt;your_bedrock_gateway_api_key&gt;
   OPENAI_URL=&lt;your_bedrock_gateway_base_url&gt;
   ```

   ```yaml
   # forge.yaml
   model: anthropic.claude-3-opus
   ```

   &lt;/details&gt;

### forge.yaml Configuration Options

The `forge.yaml` file supports several advanced configuration options that let you customize Forge&#039;s behavior.

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Custom Rules&lt;/strong&gt;&lt;/summary&gt;

Add your own guidelines that all agents should follow when generating responses.

```yaml
# forge.yaml
custom_rules: |
  1. Always add comprehensive error handling to any code you write.
  2. Include unit tests for all new functions.
  3. Follow our team&#039;s naming convention: camelCase for variables, PascalCase for classes.
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Commands&lt;/strong&gt;&lt;/summary&gt;

Define custom commands as shortcuts for repetitive prompts:

```yaml
# forge.yaml
commands:
  - name: &quot;refactor&quot;
    description: &quot;Refactor selected code&quot;
    prompt: &quot;Please refactor this code to improve readability and performance&quot;
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/summary&gt;

Specify the default AI model to use for all agents in the workflow.

```yaml
# forge.yaml
model: &quot;claude-3.7-sonnet&quot;
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Max Walker Depth&lt;/strong&gt;&lt;/summary&gt;

Control how deeply Forge traverses your project directory structure when gathering context.

```yaml
# forge.yaml
max_walker_depth: 3 # Limit directory traversal to 3 levels deep
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Temperature&lt;/strong&gt;&lt;/summary&gt;

Adjust the creativity and randomness in AI responses. Lower values (0.0-0.3) produce more focused, deterministic outputs, while higher values (0.7-2.0) generate more diverse and creative results.

```yaml
# forge.yaml
temperature: 0.7 # Balanced creativity and focus
```

&lt;/details&gt;

---

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt;&lt;/summary&gt;

The MCP feature allows AI agents to communicate with external tools and services. This implementation follows Anthropic&#039;s [Model Context Protocol](https://docs.anthropic.com/en/docs/claude-code/tutorials#set-up-model-context-protocol-mcp) design.

### MCP Configuration

Configure MCP servers using the CLI:

```bash
# List all MCP servers
forge mcp list

# Add a new server
forge mcp add

# Add a server using JSON format
forge mcp add-json

# Get server details
forge mcp get

# Remove a server
forge mcp remove
```

Or manually create a `.mcp.json` file with the following structure:

```json
{
  &quot;mcp_servers&quot;: {
    &quot;server_name&quot;: {
      &quot;command&quot;: &quot;command_to_execute&quot;,
      &quot;args&quot;: [&quot;arg1&quot;, &quot;arg2&quot;],
      &quot;env&quot;: {&quot;ENV_VAR&quot;: &quot;value&quot;}
    },
    &quot;another_server&quot;: {
      &quot;url&quot;: &quot;http://localhost:3000/events&quot;
    }
  }
}
```

MCP configurations are read from two locations (in order of precedence):

1. Local configuration (project-specific)
2. User configuration (user-specific)

### Example Use Cases

MCP can be used for various integrations:

- Web browser automation
- External API interactions
- Tool integration
- Custom service connections

### Usage in Multi-Agent Workflows

MCP tools can be used as part of multi-agent workflows, allowing specialized agents to interact with external systems as part of a collaborative problem-solving approach.

&lt;/details&gt;

---

## Documentation

For comprehensive documentation on all features and capabilities, please visit the [documentation site](https://github.com/antinomyhq/forge/tree/main/docs).

---

## Community

Join our vibrant Discord community to connect with other Forge users and contributors, get help with your projects, share ideas, and provide feedback!

[![Discord](https://img.shields.io/discord/1044859667798568962?style=for-the-badge&amp;cacheSeconds=120&amp;logo=discord)](https://discord.gg/kRZBPpkgwq)

---

## Support Us

Your support drives Forge&#039;s continued evolution! By starring our GitHub repository, you:

- Help others discover this powerful tool üîç
- Motivate our development team üí™
- Enable us to prioritize new features üõ†Ô∏è
- Strengthen our open-source community üå±
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[vectordotdev/vector]]></title>
            <link>https://github.com/vectordotdev/vector</link>
            <guid>https://github.com/vectordotdev/vector</guid>
            <pubDate>Wed, 11 Jun 2025 00:05:36 GMT</pubDate>
            <description><![CDATA[A high-performance observability data pipeline.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/vectordotdev/vector">vectordotdev/vector</a></h1>
            <p>A high-performance observability data pipeline.</p>
            <p>Language: Rust</p>
            <p>Stars: 19,685</p>
            <p>Forks: 1,761</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;
    &lt;a href=&quot;https://vector.dev/docs/setup/quickstart/&quot;&gt;Quickstart&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://vector.dev/docs/&quot;&gt;Docs&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://vector.dev/guides/&quot;&gt;Guides&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://vector.dev/components/&quot;&gt;Integrations&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://chat.vector.dev&quot;&gt;Chat&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://vector.dev/releases/latest/download/&quot;&gt;Download&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://rust-doc.vector.dev/&quot;&gt;Rust Crate Docs&lt;/a&gt;
  &lt;/strong&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;website/static/img/diagram.svg&quot; alt=&quot;Vector&quot;&gt;
&lt;/p&gt;

## What is Vector?

Vector is a high-performance, end-to-end (agent &amp; aggregator) observability data
pipeline that puts you in control of your observability data.
[Collect][docs.sources], [transform][docs.transforms], and [route][docs.sinks]
all your logs and metrics to any vendors you want today and any other
vendors you may want tomorrow. Vector enables dramatic cost reduction, novel
data enrichment, and data security where you need it, not where it is most
convenient for your vendors. Additionally, it is open source and up to 10x
faster than every alternative in the space.

To get started, follow our [**quickstart guide**][docs.quickstart] or [**install
Vector**][docs.installation].

### Principles

* **Reliable** - Built in [Rust][urls.rust], Vector&#039;s primary design goal is reliability.
* **End-to-end** - Deploys as an [agent][docs.roles#agent] or [aggregator][docs.roles#aggregator]. Vector is a complete platform.
* **Unified** - [Logs][docs.data-model.log], [metrics][docs.data-model.metric] (beta), and traces (coming soon). One tool for all of your data.

### Use cases

* Reduce total observability costs.
* Transition vendors without disrupting workflows.
* Enhance data quality and improve insights.
* Consolidate agents and eliminate agent fatigue.
* Improve overall observability performance and reliability.

### Community

* Vector is relied on by startups and enterprises like **Atlassian**, **T-Mobile**,
  **Comcast**, **Zendesk**, **Discord**, **Fastly**, **CVS**, **Trivago**,
  **Tuple**, **Douban**, **Visa**, **Mambu**, **Blockfi**, **Claranet**,
  **Instacart**, **Forcepoint**, and [many more][urls.production_users].
* Vector is **downloaded over 100,000 times per day**.
* Vector&#039;s largest user **processes over 30TB daily**.
* Vector has **over 100 contributors** and growing.

## [Documentation](https://vector.dev/docs/)

### About

* [**Concepts**][docs.about.concepts]
* [**Under the hood**][docs.about.under-the-hood]
  * [**Architecture**][docs.under-the-hood.architecture] - [data model][docs.architecture.data-model] ([log][docs.data-model.log], [metric][docs.data-model.metric]), [pipeline model][docs.architecture.pipeline-model], [concurrency model][docs.architecture.concurrency-model], [runtime model][docs.architecture.runtime-model]
  * [**Networking**][docs.under-the-hood.networking] - [ARC][docs.networking.adaptive-request-concurrency]
  * [**Guarantees**][docs.under-the-hood.guarantees]

### Setup

* [**Quickstart**][docs.setup.quickstart]
* [**Installation**][docs.setup.installation] - [operating systems][docs.installation.operating_systems], [package managers][docs.installation.package_managers], [platforms][docs.installation.platforms] ([Kubernetes][docs.platforms.kubernetes]), [manual][docs.installation.manual]
* [**Deployment**][docs.deployment] - [roles][docs.deployment.roles], [topologies][docs.deployment.topologies]

### Reference

* **Configuration**
  * [**Sources**][docs.configuration.sources] - [docker_logs][docs.sources.docker_logs], [file][docs.sources.file], [http][docs.sources.http], [journald][docs.sources.journald], [kafka][docs.sources.kafka], [socket][docs.sources.socket], and [many more...][docs.sources]
  * [**Transforms**][docs.configuration.transforms] - [dedupe][docs.transforms.dedupe], [filter][docs.transforms.filter], [log_to_metric][docs.transforms.log_to_metric], [lua][docs.transforms.lua], [remap][docs.transforms.remap],
    and [many more...][docs.transforms]
  * [**Sinks**][docs.configuration.sinks] - [aws_cloudwatch_logs][docs.sinks.aws_cloudwatch_logs], [aws_s3][docs.sinks.aws_s3], [clickhouse][docs.sinks.clickhouse], [elasticsearch][docs.sinks.elasticsearch], [gcp_cloud_storage][docs.sinks.gcp_cloud_storage], and [many more...][docs.sinks]
  * [**Enrichment Tables**][docs.configuration.enrichment_tables]
  * [**Unit tests**][docs.configuration.tests]
* [**Remap Language**][docs.reference.vrl]
* [**API**][docs.reference.api]
* [**CLI**][docs.reference.cli]

### Administration

* [**Management**][docs.administration.management]
* [**Monitoring &amp; observing**][docs.administration.monitoring]
* [**Upgrading**][docs.administration.upgrading]
* [**Validating**][docs.administration.validating]

### Resources

* [**Community**][urls.vector_community] - [chat][urls.vector_chat], [calendar][urls.vector_calendar], [@vectordotdev][urls.vector_twitter]
* [**Releases**][urls.vector_releases]
* **Policies** - [Code of Conduct][urls.vector_code_of_conduct], [Privacy][urls.vector_privacy_policy], [Releases][urls.vector_releases_policy], [Security][urls.vector_security_policy], [Versioning][urls.vector_versioning_policy]

## Comparisons

### Performance

The following performance tests demonstrate baseline performance between
common protocols with the exception of the Regex Parsing test.

| Test                                                                                                                   | Vector          | Filebeat | FluentBit       | FluentD   | Logstash  | SplunkUF        | SplunkHF |
| ---------------------------------------------------------------------------------------------------------------------- | --------------- | -------- | --------------- | --------- | --------- | --------------- | -------- |
| [TCP to Blackhole](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/tcp_to_blackhole_performance) | _**86mib/s**_   | n/a      | 64.4mib/s       | 27.7mib/s | 40.6mib/s | n/a             | n/a      |
| [File to TCP](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_to_tcp_performance)           | _**76.7mib/s**_ | 7.8mib/s | 35mib/s         | 26.1mib/s | 3.1mib/s  | 40.1mib/s       | 39mib/s  |
| [Regex Parsing](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/regex_parsing_performance)       | 13.2mib/s       | n/a      | _**20.5mib/s**_ | 2.6mib/s  | 4.6mib/s  | n/a             | 7.8mib/s |
| [TCP to HTTP](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/tcp_to_http_performance)           | _**26.7mib/s**_ | n/a      | 19.6mib/s       | &lt;1mib/s   | 2.7mib/s  | n/a             | n/a      |
| [TCP to TCP](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/tcp_to_tcp_performance)             | 69.9mib/s       | 5mib/s   | 67.1mib/s       | 3.9mib/s  | 10mib/s   | _**70.4mib/s**_ | 7.6mib/s |

To learn more about our performance tests, please see the [Vector test harness][urls.vector_test_harness].

### Correctness

The following correctness tests are not exhaustive, but they demonstrate
fundamental differences in quality and attention to detail:

| Test                                                                                                                                 | Vector | Filebeat | FluentBit | FluentD | Logstash | Splunk UF | Splunk HF |
| ------------------------------------------------------------------------------------------------------------------------------------ | ------ | -------- | --------- | ------- | -------- | --------- | --------- |
| [Disk Buffer Persistence](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/disk_buffer_persistence_correctness) | **‚úì**  | ‚úì        |           |         | ‚ö†        | ‚úì         | ‚úì         |
| [File Rotate (create)](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_rotate_create_correctness)         | **‚úì**  | ‚úì        | ‚úì         | ‚úì       | ‚úì        | ‚úì         | ‚úì         |
| [File Rotate (copytruncate)](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_rotate_truncate_correctness) | **‚úì**  |          |           |         |          | ‚úì         | ‚úì         |
| [File Truncation](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_truncate_correctness)                   | **‚úì**  | ‚úì        | ‚úì         | ‚úì       | ‚úì        | ‚úì         | ‚úì         |
| [Process (SIGHUP)](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/sighup_correctness)                         | **‚úì**  |          |           |         | ‚ö†        | ‚úì         | ‚úì         |
| [JSON (wrapped)](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/wrapped_json_correctness)                     | **‚úì**  | ‚úì        | ‚úì         | ‚úì       | ‚úì        | ‚úì         | ‚úì         |

To learn more about our correctness tests, please see the [Vector test harness][urls.vector_test_harness].

### Features

Vector is an end-to-end, unified, open data platform.

|                     | **Vector** | Beats | Fluentbit | Fluentd | Logstash | Splunk UF | Splunk HF | Telegraf |
| ------------------- | ---------- | ----- | --------- | ------- | -------- | --------- | --------- | -------- |
| **End-to-end**      | **‚úì**      |       |           |         |          |           |           | ‚úì        |
| Agent               | **‚úì**      | ‚úì     | ‚úì         |         |          | ‚úì         |           | ‚úì        |
| Aggregator          | **‚úì**      |       |           | ‚úì       | ‚úì        |           | ‚úì         | ‚úì        |
| **Unified**         | **‚úì**      |       |           |         |          |           |           | ‚úì        |
| Logs                | **‚úì**      | ‚úì     | ‚úì         | ‚úì       | ‚úì        | ‚úì         | ‚úì         | ‚úì        |
| Metrics             | **‚úì**      | ‚ö†     | ‚ö†         | ‚ö†       | ‚ö†        | ‚ö†         | ‚ö†         | ‚úì        |
| Traces              | üöß         |       |           |         |          |           |           |          |
| **Open**            | **‚úì**      |       | ‚úì         | ‚úì       |          |           |           | ‚úì        |
| Open-source         | **‚úì**      | ‚úì     | ‚úì         | ‚úì       | ‚úì        |           |           | ‚úì        |
| Vendor-neutral      | **‚úì**      |       | ‚úì         | ‚úì       |          |           |           | ‚úì        |
| **Reliability**     | **‚úì**      |       |           |         |          |           |           |          |
| Memory-safe         | **‚úì**      |       |           |         |          |           |           | ‚úì        |
| Delivery guarantees | **‚úì**      |       |           |         |          | ‚úì         | ‚úì         |          |
| Multi-core          | **‚úì**      | ‚úì     | ‚úì         | ‚úì       | ‚úì        | ‚úì         | ‚úì         | ‚úì        |


‚ö† = Not interoperable, metrics are represented as structured logs

---

&lt;p align=&quot;center&quot;&gt;
  Developed with ‚ù§Ô∏è by &lt;strong&gt;&lt;a href=&quot;https://datadoghq.com&quot;&gt;Datadog&lt;/a&gt;&lt;/strong&gt; - &lt;a href=&quot;https://github.com/vectordotdev/vector/security/policy&quot;&gt;Security Policy&lt;/a&gt; - &lt;a href=&quot;https://github.com/vectordotdev/vector/blob/master/PRIVACY.md&quot;&gt;Privacy Policy&lt;/a&gt;
&lt;/p&gt;

[docs.about.concepts]: https://vector.dev/docs/about/concepts/
[docs.about.under-the-hood]: https://vector.dev/docs/about/under-the-hood/
[docs.administration.monitoring]: https://vector.dev/docs/administration/monitoring/
[docs.administration.management]: https://vector.dev/docs/administration/management/
[docs.administration.upgrading]: https://vector.dev/docs/administration/upgrading/
[docs.administration.validating]: https://vector.dev/docs/administration/validating/
[docs.architecture.concurrency-model]: https://vector.dev/docs/about/under-the-hood/architecture/concurrency-model/
[docs.architecture.data-model]: https://vector.dev/docs/about/under-the-hood/architecture/data-model/
[docs.architecture.pipeline-model]: https://vector.dev/docs/about/under-the-hood/architecture/pipeline-model/
[docs.architecture.runtime-model]: https://vector.dev/docs/about/under-the-hood/architecture/runtime-model/
[docs.configuration.sinks]: https://vector.dev/docs/reference/configuration/sinks/
[docs.configuration.sources]: https://vector.dev/docs/reference/configuration/sources/
[docs.configuration.tests]: https://vector.dev/docs/reference/configuration/tests/
[docs.configuration.transforms]: https://vector.dev/docs/reference/configuration/transforms/
[docs.configuration.enrichment_tables]: https://vector.dev/docs/reference/configuration/global-options/#enrichment_tables
[docs.data-model.log]: https://vector.dev/docs/about/under-the-hood/architecture/data-model/log/
[docs.data-model.metric]: https://vector.dev/docs/about/under-the-hood/architecture/data-model/metric/
[docs.deployment.roles]: https://vector.dev/docs/setup/deployment/roles/
[docs.deployment.topologies]: https://vector.dev/docs/setup/deployment/topologies/
[docs.deployment]: https://vector.dev/docs/setup/deployment/
[docs.installation.manual]: https://vector.dev/docs/setup/installation/manual/
[docs.installation.operating_systems]: https://vector.dev/docs/setup/installation/operating-systems/
[docs.installation.package_managers]: https://vector.dev/docs/setup/installation/package-managers/
[docs.installation.platforms]: https://vector.dev/docs/setup/installation/platforms/
[docs.installation]: https://vector.dev/docs/setup/installation/
[docs.networking.adaptive-request-concurrency]: https://vector.dev/docs/about/under-the-hood/networking/arc/
[docs.platforms.kubernetes]: https://vector.dev/docs/setup/installation/platforms/kubernetes/
[docs.quickstart]: https://vector.dev/docs/setup/quickstart/
[docs.reference.api]: https://vector.dev/docs/reference/api/
[docs.reference.cli]: https://vector.dev/docs/reference/cli/
[docs.reference.vrl]: https://vector.dev/docs/reference/vrl/
[docs.roles#agent]: https://vector.dev/docs/setup/deployment/roles/#agent
[docs.roles#aggregator]: https://vector.dev/docs/setup/deployment/roles/#aggregator
[docs.setup.installation]: https://vector.dev/docs/setup/installation/
[docs.setup.quickstart]: https://vector.dev/docs/setup/quickstart/
[docs.sinks.aws_cloudwatch_logs]: https://vector.dev/docs/reference/configuration/sinks/aws_cloudwatch_logs/
[docs.sinks.aws_s3]: https://vector.dev/docs/reference/configuration/sinks/aws_s3/
[docs.sinks.clickhouse]: https://vector.dev/docs/reference/configuration/sinks/clickhouse/
[docs.sinks.elasticsearch]: https://vector.dev/docs/reference/configuration/sinks/elasticsearch/
[docs.sinks.gcp_cloud_storage]: https://vector.dev/docs/reference/configuration/sinks/gcp_cloud_storage/
[docs.sinks]: https://vector.dev/docs/reference/configuration/sinks/
[docs.sources.docker_logs]: https://vector.dev/docs/reference/configuration/sources/docker_logs/
[docs.sources.file]: https://vector.dev/docs/reference/configuration/sources/file/
[docs.sources.http]: https://vector.dev/docs/reference/configuration/sources/http/
[docs.sources.journald]: https://vector.dev/docs/reference/configuration/sources/journald/
[docs.sources.kafka]: https://vector.dev/docs/reference/configuration/sources/kafka/
[docs.sources.socket]: https://vector.dev/docs/reference/configuration/sources/socket/
[docs.sources]: https://vector.dev/docs/reference/configuration/sources/
[docs.transforms.dedupe]: https://vector.dev/docs/reference/configuration/transforms/dedupe/
[docs.transforms.filter]: https://vector.dev/docs/reference/configuration/transforms/filter/
[docs.transforms.log_to_metric]: https://vector.dev/docs/reference/configuration/transforms/log_to_metric/
[docs.transforms.lua]: https://vector.dev/docs/reference/configuration/transforms/lua/
[docs.transforms.remap]: https://vector.dev/docs/reference/configuration/transforms/remap/
[docs.transforms]: https://vector.dev/docs/reference/configuration/transforms/
[docs.under-the-hood.architecture]: https://vector.dev/docs/about/under-the-hood/architecture/
[docs.under-the-hood.guarantees]: https://vector.dev/docs/about/under-the-hood/guarantees/
[docs.under-the-hood.networking]: https://vector.dev/docs/about/under-the-hood/networking/
[urls.production_users]: https://github.com/vectordotdev/vector/issues/790
[urls.rust]: https://www.rust-lang.org/
[urls.vector_calendar]: https://calendar.vector.dev
[urls.vector_chat]: https://chat.vector.dev
[urls.vector_code_of_conduct]: https://github.com/vectordotdev/vector/blob/master/CODE_OF_CONDUCT.md
[urls.vector_community]: https://vector.dev/community/
[urls.vector_privacy_policy]: https://github.com/vectordotdev/vector/blob/master/PRIVACY.md
[urls.vector_release_policy]: https://github.com/vectordotdev/vector/blob/master/RELEASING.md
[urls.vector_releases]: https://vector.dev/releases/
[urls.vector_releases_policy]: https://github.com/vectordotdev/vector/blob/master/RELEASES.md
[urls.vector_security_policy]: https://github.com/vectordotdev/vector/security/policy
[urls.vector_test_harness]: https://github.com/vectordotdev/vector-test-harness/
[urls.vector_twitter]: https://twitter.com/vectordotdev
[urls.vector_versioning_policy]: https://github.com/vectordotdev/vector/blob/master/VERSIONING.md
[urls.vote_feature]: https://github.com/vectordotdev/vector/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc+label%3A%22type%3A+feature%22

</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[slint-ui/slint]]></title>
            <link>https://github.com/slint-ui/slint</link>
            <guid>https://github.com/slint-ui/slint</guid>
            <pubDate>Wed, 11 Jun 2025 00:05:35 GMT</pubDate>
            <description><![CDATA[Slint is an open-source declarative GUI toolkit to build native user interfaces for Rust, C++, JavaScript, or Python apps.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/slint-ui/slint">slint-ui/slint</a></h1>
            <p>Slint is an open-source declarative GUI toolkit to build native user interfaces for Rust, C++, JavaScript, or Python apps.</p>
            <p>Language: Rust</p>
            <p>Stars: 19,363</p>
            <p>Forks: 684</p>
            <p>Stars today: 19 stars today</p>
            <h2>README</h2><pre>&lt;!-- Copyright ¬© SixtyFPS GmbH &lt;info@slint.dev&gt; ; SPDX-License-Identifier: GPL-3.0-only OR LicenseRef-Slint-Royalty-free-2.0 OR LicenseRef-Slint-Software-3.0 --&gt;

![Slint](./logo/slint-logo-full-light.svg#gh-light-mode-only) ![Slint](./logo/slint-logo-full-dark.svg#gh-dark-mode-only)

[![Build Status](https://github.com/slint-ui/slint/workflows/CI/badge.svg)](https://github.com/slint-ui/slint/actions)
[![REUSE status](https://api.reuse.software/badge/github.com/slint-ui/slint)](https://api.reuse.software/info/github.com/slint-ui/slint)
[![Discussions](https://img.shields.io/github/discussions/slint-ui/slint)](https://github.com/slint-ui/slint/discussions)

**Slint** is an open-source declarative GUI toolkit for building native user interfaces for embedded systems, desktops, and mobile platforms.

Write your UI once in `.slint`, a simple markup language. Connect it to business logic written in Rust, C++, JavaScript, or Python.

## Why Slint?

The name *Slint* is derived from our design goals:

- **Scalable**: Slint should support responsive UI design, allow cross-platform
    usage across operating systems and processor architectures and support
    multiple programming languages.
- **Lightweight**: Slint should require minimal resources, in terms of memory
    and processing power, and yet deliver a smooth, smartphone-like user
    experience on any device.
- **Intuitive**: Designers and developers should feel productive while enjoying
    the GUI design and development process. The design creation tools should be
    intuitive to use for the designers. Similarly for the developers, the APIs
    should be consistent and easy to use, no matter which programming language
    they choose.
- **Native**: GUI built with Slint should match the end users&#039; expectations of a
    native application irrespective of the platform - desktop, mobile, web or
    embedded system. The UI design should be compiled to machine code and provide
    flexibility that only a native application can offer: Access full operating
    system APIs, utilize all CPU and GPU cores, connect to any peripheral.

Beyond the design goals, here‚Äôs what makes Slint stand out:

- **Independent UI Design**: Use a declarative language similar to separate your UI from business logic. Designers can work in parallel with developers.
- **Tooling**: Iterate quickly with our Live Preview &amp; editor integrations. Integrate from Figma with the [Slint To Figma plugin](https://www.figma.com/community/plugin/1474418299182276871/figma-to-slint).
- **Stable APIs**: Slint follows a stable 1.x API. We evolve carefully without breaking your code.

See what others have built: [#MadeWithSlint](https://madewithslint.com)

## Examples

### Embedded

| RaspberryPi                          | STM32                         | RP2040                         |
| ------------------------------------ | ----------------------------- | ------------------------------ |
| [Video of Slint on Raspberry Pi][#1] | [Video of Slint on STM32][#2] | [Video of Slint on RP2040][#3] |

### Desktop

| Windows                                     | macOS                                     | Linux                                     |
| ------------------------------------------- | ----------------------------------------- | ----------------------------------------- |
| ![Screenshot of the Gallery on Windows][#4] | ![Screenshot of the Gallery on macOS][#5] | ![Screenshot of the Gallery on Linux][#6] |

### Web using WebAssembly

| Printer Demo                                | Slide Puzzle                                 | Energy Monitor                                       | Widget Gallery                                | Weather demo                                  |
| ------------------------------------------- | -------------------------------------------- | ---------------------------------------------------- | --------------------------------------------- | --------------------------------------------- |
| [![Screenshot of the Printer Demo][#7]][#8] | [![Screenshot of the Slide Puzzle][#9]][#10] | [![Screenshot of the Energy Monitor Demo][#11]][#12] | [![Screenshot of the Gallery Demo][#13]][#14] | [![Screenshot of the weather Demo][#29]][#30] |

More examples and demos in the [examples folder](examples#examples)

## Get Started

### Hello World

The UI is defined in a Domain Specific Language that is declarative, easy to use,
intuitive, and provides a powerful way to describe graphical elements, their
placement, their hierarchy, property bindings, and the flow of data through the
different states.

Here&#039;s the obligatory &quot;Hello World&quot;:

```slint
export component HelloWorld inherits Window {
    width: 400px;
    height: 400px;

    Text {
       y: parent.width / 2;
       x: parent.x + 200px;
       text: &quot;Hello, world&quot;;
       color: blue;
    }
}
```

### Documentation

For more details, check out the [Slint Language Documentation](https://slint.dev/docs/slint).

The [examples](examples) folder contains examples and demos, showing how to
use the Slint markup language and how to interact with a Slint user interface
from supported programming languages.

The `docs` folder contains a lot more information, including
[build instructions](docs/building.md), and
[internal developer docs](docs/development.md).

Refer to the README of each language directory in the `api` folder:

- [C++](api/cpp) ([Documentation][#15] | [Getting Started Template][#17])
- [Rust](api/rs/slint) [![Crates.io][#18]][#19] ([Documentation][#20] | [Tutorial Video][#22] | [Getting Started Template][#23])
- [JavaScript/NodeJS (Beta)](api/node) [![npm][#24]][#25] ([Documentation][#26] | [Getting Started Template][#28])
- [Python (Beta)](api/python) [![pypi][#31]][#32] ([Documentation][#33] | [Getting Started Template][#34])

## Architecture

An application is composed of the business logic written in Rust, C++, or
JavaScript and the `.slint` user interface design markup, which is compiled to
native code.

![Architecture Overview](https://slint.dev/resources/architecture.drawio.svg)

### Compiler

The `.slint` files are compiled ahead of time. The expressions in the `.slint`
are pure functions that the compiler can optimize. For example, the compiler
could choose to &quot;inline&quot; properties and remove those that are constant or
unchanged.

The compiler uses the typical compiler phases of lexing, parsing, optimization,
and finally code generation. It provides different back-ends for code generation
in the target language. The C++ code generator produces a C++ header file, the
Rust generator produces Rust code, and so on. An interpreter for dynamic
languages is also included.

### Runtime

The runtime library consists of an engine that supports properties declared in
the `.slint` language. Components with their elements, items, and properties are
laid out in a single memory region, to reduce memory allocations.

Rendering backends and styles are configurable at compile time:

- The `femtovg` renderer uses OpenGL ES 2.0 for rendering.
- The `skia` renderer uses [Skia](https://skia.org) for rendering.
- The `software` renderer uses the CPU with no additional dependencies.

NOTE: When Qt is installed on the system, the `qt` style becomes available,
using Qt&#039;s QStyle to achieve native looking widgets.

### Tooling

We have a few tools to help with the development of .slint files:

- A [**LSP Server**](./tools/lsp) that adds features like auto-complete and live
  preview of the .slint files to many editors.
- It is bundled in a [**Visual Studio Code Extension**](./editors/vscode)
  available from the market place.
- A [**slint-viewer**](./tools/viewer) tool which displays the .slint files. The
  `--auto-reload` argument makes it easy to preview your UI while you are
  working on it (when using the LSP preview is not possible).
- [**SlintPad**](https://slintpad.com/), an online editor to try out .slint syntax
  without installing anything ([sources](./tools/slintpad)).
- A [**Figma to Slint**](https://www.figma.com/community/plugin/1474418299182276871/figma-to-slint) plugin.

Please check our [Editors README](./editors/README.md) for tips on how to
configure your favorite editor to work well with Slint.

## License

You can use Slint under ***any*** of the following licenses, at your choice:

1. Build proprietary desktop, mobile, or web applications for free with the [Royalty-free License](LICENSES/LicenseRef-Slint-Royalty-free-2.0.md),
2. Build open source embedded, desktop, mobile, or web applications for free with the [GNU GPLv3](LICENSES/GPL-3.0-only.txt),
3. Build proprietary embedded, desktop, mobile, or web applications with the [Paid license](LICENSES/LicenseRef-Slint-Software-3.0.md).

See the [Slint licensing options on the website](https://slint.dev/pricing.html) and the [Licensing FAQ](FAQ.md#licensing).

## Contributions

We welcome your contributions: in the form of code, bug reports or feedback.
For contribution guidelines see [CONTRIBUTING.md](CONTRIBUTING.md).

## Frequently Asked Questions

Please see our separate [FAQ](FAQ.md).

## About us (SixtyFPS GmbH)

We are passionate about software - API design, cross-platform software
development and user interface components. Our aim is to make developing user
interfaces fun for everyone: from Python, JavaScript, C++, or Rust developers all the
way to UI/UX designers. We believe that software grows organically and keeping
it open source is the best way to sustain that growth. Our team members are
located remotely in Germany, Finland, and US.

### Stay up to date

- Follow [@slint_ui](https://twitter.com/slint_ui) on X/Twitter.
- Follow [@slint@fosstodon.org](https://mastodon.social/@slint@fosstodon.org) on Mastodon.
- Follow [@slint-ui](https://www.linkedin.com/company/slint-ui/) on LinkedIn.
- Follow [@slint.dev](https://bsky.app/profile/slint.dev) on Bluesky
- Subscribe to our [YouTube channel](https://www.youtube.com/@Slint-UI)

### Contact us

Feel free to join [Github discussions](https://github.com/slint-ui/slint/discussions)
for general chat or questions. Use [Github issues](https://github.com/slint-ui/slint/issues)
to report public suggestions or bugs.

We chat in [our Mattermost instance](https://chat.slint.dev) where you are
welcome to listen in or ask your questions.

You can of course also contact us privately via email to [info@slint.dev](mailto://info@slint.dev).

[#1]: https://www.youtube.com/watch?v=_BDbNHrjK7g
[#2]: https://www.youtube.com/watch?v=NNNOJJsOAis
[#3]: https://www.youtube.com/watch?v=dkBwNocItGs
[#4]: https://slint.dev/resources/gallery_win_screenshot.png &quot;Gallery&quot;
[#5]: https://slint.dev/resources/gallery_mac_screenshot.png &quot;Gallery&quot;
[#6]: https://slint.dev/resources/gallery_linux_screenshot.png &quot;Gallery&quot;
[#7]: https://slint.dev/resources/printerdemo_screenshot.png &quot;Printer Demo&quot;
[#8]: https://slint.dev/demos/printerdemo/
[#9]: https://slint.dev/resources/puzzle_screenshot.png &quot;Slide Puzzle&quot;
[#10]: https://slint.dev/demos/slide_puzzle/
[#11]: https://slint.dev/resources/energy-monitor-screenshot.png &quot;Energy Monitor Demo&quot;
[#12]: https://slint.dev/demos/energy-monitor/
[#13]: https://slint.dev/resources/gallery_screenshot.png &quot;Gallery Demo&quot;
[#14]: https://slint.dev/demos/gallery/
[#15]: https://slint.dev/latest/docs/cpp
[#17]: https://github.com/slint-ui/slint-cpp-template
[#18]: https://img.shields.io/crates/v/slint
[#19]: https://crates.io/crates/slint
[#20]: https://slint.dev/latest/docs/rust/slint/
[#22]: https://youtu.be/WBcv4V-whHk
[#23]: https://github.com/slint-ui/slint-rust-template
[#24]: https://img.shields.io/npm/v/slint-ui
[#25]: https://www.npmjs.com/package/slint-ui
[#26]: https://slint.dev/latest/docs/node
[#28]: https://github.com/slint-ui/slint-nodejs-template
[#29]: ./demos/weather-demo/docs/img/desktop-preview.png &quot;Weather Demo&quot;
[#30]: https://slint.dev/demos/weather-demo/
[#31]: https://img.shields.io/pypi/v/slint
[#32]: https://pypi.org/project/slint/
[#33]: http://snapshots.slint.dev/master/docs/python/
[#34]: https://github.com/slint-ui/slint-python-template
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[tracel-ai/burn]]></title>
            <link>https://github.com/tracel-ai/burn</link>
            <guid>https://github.com/tracel-ai/burn</guid>
            <pubDate>Wed, 11 Jun 2025 00:05:34 GMT</pubDate>
            <description><![CDATA[Burn is a next generation Deep Learning Framework that doesn't compromise on flexibility, efficiency and portability.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/tracel-ai/burn">tracel-ai/burn</a></h1>
            <p>Burn is a next generation Deep Learning Framework that doesn't compromise on flexibility, efficiency and portability.</p>
            <p>Language: Rust</p>
            <p>Stars: 11,336</p>
            <p>Forks: 587</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/tracel-ai/burn/main/assets/logo-burn-neutral.webp&quot; width=&quot;350px&quot;/&gt;

[![Discord](https://img.shields.io/discord/1038839012602941528.svg?color=7289da&amp;&amp;logo=discord)](https://discord.gg/uPEBbYYDB6)
[![Current Crates.io Version](https://img.shields.io/crates/v/burn.svg)](https://crates.io/crates/burn)
[![Minimum Supported Rust Version](https://img.shields.io/crates/msrv/burn)](https://crates.io/crates/burn)
[![Documentation](https://img.shields.io/badge/docs-latest-blue)](https://burn.dev/docs/burn)
[![Test Status](https://github.com/tracel-ai/burn/actions/workflows/test.yml/badge.svg)](https://github.com/tracel-ai/burn/actions/workflows/test.yml)
[![CodeCov](https://codecov.io/gh/tracel-ai/burn/branch/main/graph/badge.svg)](https://codecov.io/gh/tracel-ai/burn)
![license](https://shields.io/badge/license-MIT%2FApache--2.0-blue)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/tracel-ai/burn)

[&lt;img src=&quot;https://www.runblaze.dev/ci-blaze-powered.png&quot; width=&quot;125px&quot;/&gt;](https://www.runblaze.dev)

---

**Burn is a next generation Deep Learning Framework that doesn&#039;t compromise on &lt;br /&gt;
flexibility, efficiency and portability.**

&lt;br/&gt;
&lt;/div&gt;

&lt;div align=&quot;left&quot;&gt;

## Performance

&lt;div align=&quot;left&quot;&gt;
&lt;img align=&quot;right&quot; src=&quot;https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-blazingly-fast.png&quot; height=&quot;96px&quot;/&gt;

Because we believe the goal of a deep learning framework is to convert computation into useful
intelligence, we have made performance a core pillar of Burn. We strive to achieve top efficiency by
leveraging multiple optimization techniques described below.

**Click on each section for more details** üëá

&lt;/div&gt;

&lt;br /&gt;

&lt;details&gt;
&lt;summary&gt;
Automatic kernel fusion üí•
&lt;/summary&gt;
&lt;br /&gt;

Using Burn means having your models optimized on any backend. When possible, we provide a way to
automatically and dynamically create custom kernels that minimize data relocation between different
memory spaces, extremely useful when moving memory is the bottleneck.

As an example, you could write your own GELU activation function with the high level tensor api (see
Rust code snippet below).

```rust
fn gelu_custom&lt;B: Backend, const D: usize&gt;(x: Tensor&lt;B, D&gt;) -&gt; Tensor&lt;B, D&gt; {
    let x = x.clone() * ((x / SQRT_2).erf() + 1);
    x / 2
}
```

Then, at runtime, a custom low-level kernel will be automatically created for your specific
implementation and will rival a handcrafted GPU implementation. The kernel consists of about 60
lines of WGSL [WebGPU Shading Language](&quot;https://www.w3.org/TR/WGSL/https://www.w3.org/TR/WGSL/&quot;),
an extremely verbose lower level shader language you probably don&#039;t want to program your deep
learning models in!

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Asynchronous execution ‚ù§Ô∏è‚Äçüî•
&lt;/summary&gt;
&lt;br /&gt;

For [first-party backends](#backends), an asynchronous execution style
is used, which allows to perform various optimizations, such as the previously mentioned automatic
kernel fusion.

Asynchronous execution also ensures that the normal execution of the framework does not block the
model computations, which implies that the framework overhead won&#039;t impact the speed of execution
significantly. Conversely, the intense computations in the model do not interfere with the
responsiveness of the framework. For more information about our asynchronous backends, see
[this blog post](https://burn.dev/blog/creating-high-performance-asynchronous-backends-with-burn-compute).

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Thread-safe building blocks ü¶û
&lt;/summary&gt;
&lt;br /&gt;

Burn emphasizes thread safety by leveraging the
[ownership system of Rust](https://doc.rust-lang.org/book/ch04-00-understanding-ownership.html).
With Burn, each module is the owner of its weights. It is therefore possible to send a module to
another thread for computing the gradients, then send the gradients to the main thread that can
aggregate them, and _voil√†_, you get multi-device training.

This is a very different approach from what PyTorch does, where backpropagation actually mutates the
_grad_ attribute of each tensor parameter. This is not a thread-safe operation and therefore
requires lower level synchronization primitives, see
[distributed training](https://pytorch.org/docs/stable/distributed.html) for reference. Note that
this is still very fast, but not compatible across different backends and quite hard to implement.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Intelligent memory management ü¶Ä
&lt;/summary&gt;
&lt;br /&gt;

One of the main roles of a deep learning framework is to reduce the amount of memory necessary to
run models. The naive way of handling memory is that each tensor has its own memory space, which is
allocated when the tensor is created then deallocated as the tensor gets out of scope. However,
allocating and deallocating data is very costly, so a memory pool is often required to achieve good
throughput. Burn offers an infrastructure that allows for easily creating and selecting memory
management strategies for backends. For more details on memory management in Burn, see
[this blog post](https://burn.dev/blog/creating-high-performance-asynchronous-backends-with-burn-compute).

Another very important memory optimization of Burn is that we keep track of when a tensor can be
mutated in-place just by using the ownership system well. Even though it is a rather small memory
optimization on its own, it adds up considerably when training or running inference with larger
models and contributes to reduce the memory usage even more. For more information, see
[this blog post about tensor handling](https://burn.dev/blog/burn-rusty-approach-to-tensor-handling).

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Automatic kernel selection üéØ
&lt;/summary&gt;
&lt;br /&gt;

A good deep learning framework should ensure that models run smoothly on all hardware. However, not
all hardware share the same behavior in terms of execution speed. For instance, a matrix
multiplication kernel can be launched with many different parameters, which are highly sensitive to
the size of the matrices and the hardware. Using the wrong configuration could reduce the speed of
execution by a large factor (10 times or even more in extreme cases), so choosing the right kernels
becomes a priority.

With our home-made backends, we run benchmarks automatically and choose the best configuration for
the current hardware and matrix sizes with a reasonable caching strategy.

This adds a small overhead by increasing the warmup execution time, but stabilizes quickly after a
few forward and backward passes, saving lots of time in the long run. Note that this feature isn&#039;t
mandatory, and can be disabled when cold starts are a priority over optimized throughput.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Hardware specific features üî•
&lt;/summary&gt;
&lt;br /&gt;

It is no secret that deep learning is mostly relying on matrix multiplication as its core operation,
since this is how fully-connected neural networks are modeled.

More and more, hardware manufacturers optimize their chips specifically for matrix multiplication
workloads. For instance, Nvidia has its _Tensor Cores_ and today most cellphones have AI specialized
chips. As of this moment, we support Tensor Cores with our LibTorch, Candle, CUDA, Metal and WGPU/SPIR-V
backends, but not other accelerators yet. We hope
[this issue](https://github.com/gpuweb/gpuweb/issues/4195) gets resolved at some point to bring
support to our WGPU backend.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Custom Backend Extension üéí
&lt;/summary&gt;
&lt;br /&gt;

Burn aims to be the most flexible deep learning framework. While it&#039;s crucial to maintain
compatibility with a wide variety of backends, Burn also provides the ability to extend the
functionalities of a backend implementation to suit your personal modeling requirements.

This versatility is advantageous in numerous ways, such as supporting custom operations like flash
attention or manually writing your own kernel for a specific backend to enhance performance. See
[this section](https://burn.dev/burn-book/advanced/backend-extension/index.html) in the Burn Book üî•
for more details.

&lt;/details&gt;

&lt;br /&gt;

## Backend

&lt;div align=&quot;left&quot;&gt;
&lt;img align=&quot;right&quot; src=&quot;https://raw.githubusercontent.com/tracel-ai/burn/main/assets/backend-chip.png&quot; height=&quot;96px&quot;/&gt;

Burn strives to be as fast as possible on as many hardwares as possible, with robust implementations.
We believe this flexibility is crucial for modern needs where you may train your models in the cloud,
then deploy on customer hardwares, which vary from user to user.

&lt;/div&gt;

&lt;br /&gt;

**Supported Backends**

| Backend  | Devices                      | Class       |
| -------- | ---------------------------- | ----------- |
| CUDA     | NVIDIA GPUs                  | First-Party |
| ROCm     | AMD GPUs                     | First-Party |
| Metal    | Apple GPUs                   | First-Party |
| Vulkan   | Most GPUs on Linux &amp; Windows | First-Party |
| Wgpu     | Most GPUs                    | First-Party |
| NdArray  | Most CPUs                    | Third-Party |
| LibTorch | Most GPUs &amp; CPUs             | Third-Party |
| Candle   | Nvidia, Apple GPUs &amp; CPUs    | Third-Party |

&lt;br /&gt;

Compared to other frameworks, Burn has a very different approach to supporting many backends. By
design, most code is generic over the Backend trait, which allows us to build Burn with swappable
backends. This makes composing backend possible, augmenting them with additional functionalities
such as autodifferentiation and automatic kernel fusion.

&lt;details&gt;
&lt;summary&gt;
Autodiff: Backend decorator that brings backpropagation to any backend üîÑ
&lt;/summary&gt;
&lt;br /&gt;

Contrary to the aforementioned backends, Autodiff is actually a backend _decorator_. This means that
it cannot exist by itself; it must encapsulate another backend.

The simple act of wrapping a base backend with Autodiff transparently equips it with
autodifferentiation support, making it possible to call backward on your model.

```rust
use burn::backend::{Autodiff, Wgpu};
use burn::tensor::{Distribution, Tensor};

fn main() {
    type Backend = Autodiff&lt;Wgpu&gt;;

    let x: Tensor&lt;Backend, 2&gt; = Tensor::random([32, 32], Distribution::Default);
    let y: Tensor&lt;Backend, 2&gt; = Tensor::random([32, 32], Distribution::Default).require_grad();

    let tmp = x.clone() + y.clone();
    let tmp = tmp.matmul(x);
    let tmp = tmp.exp();

    let grads = tmp.backward();
    let y_grad = y.grad(&amp;grads).unwrap();
    println!(&quot;{y_grad}&quot;);
}
```

Of note, it is impossible to make the mistake of calling backward on a model that runs on a backend
that does not support autodiff (for inference), as this method is only offered by an Autodiff
backend.

See the [Autodiff Backend README](./crates/burn-autodiff/README.md) for more details.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Fusion: Backend decorator that brings kernel fusion to all first-party backends
&lt;/summary&gt;
&lt;br /&gt;

This backend decorator enhances a backend with kernel fusion, provided that the inner backend
supports it. Note that you can compose this backend with other backend decorators such as Autodiff.
For now, only the WGPU and CUDA backends have support for fused kernels.

```rust
use burn::backend::{Autodiff, Fusion, Wgpu};
use burn::tensor::{Distribution, Tensor};

fn main() {
    type Backend = Autodiff&lt;Fusion&lt;Wgpu&gt;&gt;;

    let x: Tensor&lt;Backend, 2&gt; = Tensor::random([32, 32], Distribution::Default);
    let y: Tensor&lt;Backend, 2&gt; = Tensor::random([32, 32], Distribution::Default).require_grad();

    let tmp = x.clone() + y.clone();
    let tmp = tmp.matmul(x);
    let tmp = tmp.exp();

    let grads = tmp.backward();
    let y_grad = y.grad(&amp;grads).unwrap();
    println!(&quot;{y_grad}&quot;);
}

```

Of note, we plan to implement automatic gradient checkpointing based on compute bound and memory
bound operations, which will work gracefully with the fusion backend to make your code run even
faster during training, see [this issue](https://github.com/tracel-ai/burn/issues/936).

See the [Fusion Backend README](./crates/burn-fusion/README.md) for more details.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Router (Beta): Backend decorator that composes multiple backends into a single one
&lt;/summary&gt;
&lt;br /&gt;

That backend simplifies hardware operability, if for instance you want to execute some operations on the CPU and other operations on the GPU.

```rust
use burn::tensor::{Distribution, Tensor};
use burn::backend::{
    NdArray, Router, Wgpu, ndarray::NdArrayDevice, router::duo::MultiDevice, wgpu::WgpuDevice,
};

fn main() {
    type Backend = Router&lt;(Wgpu, NdArray)&gt;;

    let device_0 = MultiDevice::B1(WgpuDevice::DiscreteGpu(0));
    let device_1 = MultiDevice::B2(NdArrayDevice::Cpu);

    let tensor_gpu =
        Tensor::&lt;Backend, 2&gt;::random([3, 3], burn::tensor::Distribution::Default, &amp;device_0);
    let tensor_cpu =
        Tensor::&lt;Backend, 2&gt;::random([3, 3], burn::tensor::Distribution::Default, &amp;device_1);
}

```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Remote (Beta): Backend decorator for remote backend execution, useful for distributed computations
&lt;/summary&gt;
&lt;br /&gt;

That backend has two parts, one client and one server.
The client sends tensor operations over the network to a remote compute backend.
You can use any first-party backend as server in a single line of code:

```rust
fn main_server() {
    // Start a server on port 3000.
    burn::server::start::&lt;burn::backend::Cuda&gt;(Default::default(), 3000);
}

fn main_client() {
    // Create a client that communicate with the server on port 3000.
    use burn::backend::{Autodiff, RemoteBackend};

    type Backend = Autodiff&lt;RemoteDevice&gt;;

    let device = RemoteDevice::new(&quot;ws://localhost:3000&quot;);
    let tensor_gpu =
        Tensor::&lt;Backend, 2&gt;::random([3, 3], Distribution::Default, &amp;device);
}

```

&lt;/details&gt;

&lt;br /&gt;

## Training &amp; Inference

&lt;div align=&quot;left&quot;&gt;
&lt;img align=&quot;right&quot; src=&quot;https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-wall.png&quot; height=&quot;96px&quot;/&gt;

The whole deep learning workflow is made easy with Burn, as you can monitor your training progress
with an ergonomic dashboard, and run inference everywhere from embedded devices to large GPU
clusters.

Burn was built from the ground up with training and inference in mind. It&#039;s also worth noting how
Burn, in comparison to frameworks like PyTorch, simplifies the transition from training to
deployment, eliminating the need for code changes.

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

&lt;br /&gt;

&lt;a href=&quot;https://www.youtube.com/watch?v=N9RM5CQbNQc&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/tracel-ai/burn/main/assets/burn-train-tui.png&quot; alt=&quot;Burn Train TUI&quot; width=&quot;75%&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

&lt;br /&gt;

**Click on the following sections to expand üëá**

&lt;details&gt;
&lt;summary&gt;
Training Dashboard üìà
&lt;/summary&gt;
&lt;br /&gt;

As you can see in the previous video (click on the picture!), a new terminal UI dashboard based on
the [Ratatui](https://github.com/ratatui-org/ratatui) crate allows users to follow their training
with ease without having to connect to any external application.

You can visualize your training and validation metrics updating in real-time and analyze the
lifelong progression or recent history of any registered metrics using only the arrow keys. Break
from the training loop without crashing, allowing potential checkpoints to be fully written or
important pieces of code to complete without interruption üõ°

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
ONNX Support üê´
&lt;/summary&gt;
&lt;br /&gt;

ONNX (Open Neural Network Exchange) is an open-standard format that exports both the architecture
and the weights of a deep learning model.

Burn supports the importation of models that follow the ONNX standard so you can easily port a model
you have written in another framework like TensorFlow or PyTorch to Burn to benefit from all the
advantages our framework offers.

Our ONNX support is further described in
[this section of the Burn Book üî•](https://burn.dev/burn-book/import/onnx-model.html).

&gt; **Note**: This crate is in active development and currently supports a
&gt; [limited set of ONNX operators](./crates/burn-import/SUPPORTED-ONNX-OPS.md).

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Importing PyTorch or Safetensors Models üöö
&lt;/summary&gt;
&lt;br /&gt;

You can load weights from PyTorch or Safetensors formats directly into your Burn-defined models. This makes it easy to reuse existing models while benefiting from Burn&#039;s performance and deployment features.

Learn more:

- [Import pre-trained PyTorch models into Burn](https://burn.dev/burn-book/import/pytorch-model.html)
- [Load models from Safetensors format](https://burn.dev/burn-book/import/safetensors-model.html)

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Inference in the Browser üåê
&lt;/summary&gt;
&lt;br /&gt;

Several of our backends can compile to Web Assembly: Candle and NdArray for CPU, and WGPU for GPU.
This means that you can run inference directly within a browser. We provide several examples of
this:

- [MNIST](./examples/mnist-inference-web) where you can draw digits and a small convnet tries to
  find which one it is! 2Ô∏è‚É£ 7Ô∏è‚É£ üò∞
- [Image Classification](./examples/image-classification-web) where you can upload images and
  classify them! üåÑ

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Embedded: &lt;i&gt;no_std&lt;/i&gt; support ‚öôÔ∏è
&lt;/summary&gt;
&lt;br /&gt;

Burn&#039;s core components support [no_std](https://docs.rust-embedded.org/book/intro/no-std.html). This
means it can run in bare metal environment such as embedded devices without an operating system.

&gt; As of now, only the NdArray backend can be used in a _no_std_ environment.

&lt;/details&gt;

&lt;br /&gt;

### Benchmarks

To evaluate performance across different backends and track improvements over time, we provide a
dedicated benchmarking suite.

Run and compare benchmarks using [burn-bench](https://github.com/tracel-ai/burn-bench).

&gt; ‚ö†Ô∏è **Warning**
&gt; When using one of the `wgpu` backends, you may encounter compilation errors related to recursive type evaluation. This is due to complex type nesting within the `wgpu` dependency chain.
&gt; To resolve this issue, add the following line at the top of your `main.rs` or `lib.rs` file:
&gt;
&gt; ```rust
&gt; #![recursion_limit = &quot;256&quot;]
&gt; ```
&gt;
&gt; The default recursion limit (128) is often just below the required depth (typically 130-150) due to deeply nested associated types and trait bounds.

## Getting Started

&lt;div align=&quot;left&quot;&gt;
&lt;img align=&quot;right&quot; src=&quot;https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-walking.png&quot; height=&quot;96px&quot;/&gt;

Just heard of Burn? You are at the right place! Just continue reading this section and we hope you
can get on board really quickly.

&lt;/div&gt;

&lt;details&gt;
&lt;summary&gt;
The Burn Book üî•
&lt;/summary&gt;
&lt;br /&gt;

To begin working effectively with Burn, it is crucial to understand its key components and
philosophy. This is why we highly recommend new users to read the first sections of
[The Burn Book üî•](https://burn.dev/burn-book/). It provides detailed examples and explanations
covering every facet of the framework, including building blocks like tensors, modules, and
optimizers, all the way to advanced usage, like coding your own GPU kernels.

&gt; The project is constantly evolving, and we try as much as possible to keep the book up to date
&gt; with new additions. However, we might miss some details sometimes, so if you see something weird,
&gt; let us know! We also gladly accept Pull Requests üòÑ

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Examples üôè
&lt;/summary&gt;
&lt;br /&gt;

Let&#039;s start with a code snippet that shows how intuitive the framework is to use! In the following,
we declare a neural network module with some parameters along with its forward pass.

```rust
use burn::nn;
use burn::module::Module;
use burn::tensor::backend::Backend;

#[derive(Module, Debug)]
pub struct PositionWiseFeedForward&lt;B: Backend&gt; {
    linear_inner: nn::Linear&lt;B&gt;,
    linear_outer: nn::Linear&lt;B&gt;,
    dropout: nn::Dropout,
    gelu: nn::Gelu,
}

impl&lt;B: Backend&gt; PositionWiseFeedForward&lt;B&gt; {
    pub fn forward&lt;const D: usize&gt;(&amp;self, input: Tensor&lt;B, D&gt;) -&gt; Tensor&lt;B, D&gt;

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[libp2p/rust-libp2p]]></title>
            <link>https://github.com/libp2p/rust-libp2p</link>
            <guid>https://github.com/libp2p/rust-libp2p</guid>
            <pubDate>Wed, 11 Jun 2025 00:05:33 GMT</pubDate>
            <description><![CDATA[The Rust Implementation of the libp2p networking stack.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/libp2p/rust-libp2p">libp2p/rust-libp2p</a></h1>
            <p>The Rust Implementation of the libp2p networking stack.</p>
            <p>Language: Rust</p>
            <p>Stars: 5,028</p>
            <p>Forks: 1,078</p>
            <p>Stars today: 2 stars today</p>
            <h2>README</h2><pre># Central repository for work on libp2p

&lt;a href=&quot;http://libp2p.io/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/project-libp2p-yellow.svg?style=flat-square&quot; /&gt;&lt;/a&gt;
[![dependency status](https://deps.rs/repo/github/libp2p/rust-libp2p/status.svg?style=flat-square)](https://deps.rs/repo/github/libp2p/rust-libp2p)
[![Crates.io](https://img.shields.io/crates/v/libp2p.svg)](https://crates.io/crates/libp2p)
[![docs.rs](https://img.shields.io/badge/api-rustdoc-blue.svg)](https://docs.rs/libp2p)
[![docs.rs master](https://img.shields.io/badge/docs-master-blueviolet)](https://libp2p.github.io/rust-libp2p/libp2p/)

This repository is the central place for Rust development of the [libp2p](https://libp2p.io) spec.

## Getting started

- **Main documentation** can be found on https://docs.rs/libp2p.

- The **[examples](examples)** folder contains small binaries showcasing the
  many protocols in this repository.

- For **security related issues** please [file a private security vulnerability
  report](https://github.com/libp2p/rust-libp2p/security/advisories/new) . Please do not file a
  public issue on GitHub.

- To **report bugs, suggest improvements or request new features** please open a
  GitHub issue on this repository.

- For **rust-libp2p specific questions** please use the GitHub _Discussions_
  forum https://github.com/libp2p/rust-libp2p/discussions.

- For **discussions and questions related to multiple libp2p implementations**
  please use the libp2p _Discourse_ forum https://discuss.libp2p.io.

- For synchronous discussions join the [open rust-libp2p maintainer
  calls](https://github.com/libp2p/rust-libp2p/discussions?discussions_q=open+maintainers+call+)
  or the [biweekly libp2p community calls](https://discuss.libp2p.io/t/libp2p-community-calls/1157).

## Repository Structure

The main components of this repository are structured as follows:

  * `core/`: The implementation of `libp2p-core` with its `Transport` and
    `StreamMuxer` API on which almost all other crates depend.

  * `transports/`: Implementations of transport protocols (e.g. TCP) and protocol upgrades
    (e.g. for authenticated encryption, compression, ...) based on the `libp2p-core` `Transport`
    API.

  * `muxers/`: Implementations of the `StreamMuxer` interface of `libp2p-core`,
    e.g. (sub)stream multiplexing protocols on top of (typically TCP) connections.
    Multiplexing protocols are (mandatory) `Transport` upgrades.

  * `swarm/`: The implementation of `libp2p-swarm` building on `libp2p-core`
    with the central interfaces `NetworkBehaviour` and `ConnectionHandler` used
    to implement application protocols (see `protocols/`).

  * `protocols/`: Implementations of application protocols based on the
    `libp2p-swarm` APIs.

  * `misc/`: Utility libraries.

  * `libp2p/examples/`: Worked examples of built-in application protocols (see `protocols/`)
    with common `Transport` configurations.

## Community Guidelines

The libp2p project operates under the [IPFS Code of
Conduct](https://github.com/ipfs/community/blob/master/code-of-conduct.md).

&gt; tl;dr
&gt;
&gt; - Be respectful.
&gt; - We&#039;re here to help: abuse@ipfs.io
&gt; - Abusive behavior is never tolerated.
&gt; - Violations of this code may result in swift and permanent expulsion from the
&gt;   IPFS [and libp2p] community.
&gt; - &quot;Too long, didn&#039;t read&quot; is not a valid excuse for not knowing what is in
&gt;   this document.

## Maintainers

(In alphabetical order.)

- Jo√£o Oliveira ([@jxs](https://github.com/jxs))

## Notable users

(open a pull request if you want your project to be added here)

- [COMIT](https://github.com/comit-network/xmr-btc-swap) - Bitcoin‚ÄìMonero Cross-chain Atomic Swap.
- [Forest](https://github.com/ChainSafe/forest) - An implementation of Filecoin written in Rust.
- [fuel-core](https://github.com/FuelLabs/fuel-core) - A Rust implementation of the Fuel protocol.
- [HotShot](https://github.com/EspressoSystems/HotShot) - Decentralized sequencer in Rust developed by [Espresso Systems](https://www.espressosys.com/).
- [ipfs-embed](https://github.com/ipfs-rust/ipfs-embed) - A small embeddable ipfs implementation used and maintained by [Actyx](https://www.actyx.com).
- [Homestar](https://github.com/ipvm-wg/homestar) - An InterPlanetary Virtual Machine (IPVM) implementation used and maintained by Fission.
- [beetle](https://github.com/n0-computer/beetle) - Next-generation implementation of IPFS for Cloud &amp; Mobile platforms.
- [Lighthouse](https://github.com/sigp/lighthouse) - Ethereum consensus client in Rust.
- [Locutus](https://github.com/freenet/locutus) - Global, observable, decentralized key-value store.
- [OpenMina](https://github.com/openmina/openmina) - In-browser Mina Rust implementation.
- [qaul ŸÇŸàŸÑ](https://github.com/qaul/qaul.net) - Internet Independent Wireless Mesh Communication App
- [rust-ipfs](https://github.com/rs-ipfs/rust-ipfs) - IPFS implementation in Rust.
- [Safe Network](https://github.com/maidsafe/safe_network) - Safe Network implementation in Rust.
- [SQD Network](https://github.com/subsquid/sqd-network) - A decentralized storage for Web3 data.
- [Starcoin](https://github.com/starcoinorg/starcoin) - A smart contract blockchain network that scales by layering.
- [Subspace](https://github.com/subspace/subspace) - Subspace Network reference implementation
- [Substrate](https://github.com/paritytech/substrate) - Framework for blockchain innovation,
used by [Polkadot](https://www.parity.io/technologies/polkadot/).
- [Swarm NL](https://github.com/algorealmInc/SwarmNL) - A library that makes it easy to configure the networking requirements for any distributed application.
- [Taple](https://github.com/opencanarias/taple-core) - Sustainable DLT for asset and process traceability by [OpenCanarias](https://www.opencanarias.com/en/).
- [Ceylon](https://github.com/ceylonai/ceylon) - A Multi-Agent System (MAS) Development Framework.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[rustdesk/rustdesk]]></title>
            <link>https://github.com/rustdesk/rustdesk</link>
            <guid>https://github.com/rustdesk/rustdesk</guid>
            <pubDate>Wed, 11 Jun 2025 00:05:32 GMT</pubDate>
            <description><![CDATA[An open-source remote desktop application designed for self-hosting, as an alternative to TeamViewer.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/rustdesk/rustdesk">rustdesk/rustdesk</a></h1>
            <p>An open-source remote desktop application designed for self-hosting, as an alternative to TeamViewer.</p>
            <p>Language: Rust</p>
            <p>Stars: 90,372</p>
            <p>Forks: 12,968</p>
            <p>Stars today: 118 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;res/logo-header.svg&quot; alt=&quot;RustDesk - Your remote desktop&quot;&gt;&lt;br&gt;
  &lt;a href=&quot;#raw-steps-to-build&quot;&gt;Build&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;#how-to-build-with-docker&quot;&gt;Docker&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;#file-structure&quot;&gt;Structure&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;#snapshot&quot;&gt;Snapshot&lt;/a&gt;&lt;br&gt;
  [&lt;a href=&quot;docs/README-UA.md&quot;&gt;–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞&lt;/a&gt;] | [&lt;a href=&quot;docs/README-CS.md&quot;&gt;ƒçesky&lt;/a&gt;] | [&lt;a href=&quot;docs/README-ZH.md&quot;&gt;‰∏≠Êñá&lt;/a&gt;] | [&lt;a href=&quot;docs/README-HU.md&quot;&gt;Magyar&lt;/a&gt;] | [&lt;a href=&quot;docs/README-ES.md&quot;&gt;Espa√±ol&lt;/a&gt;] | [&lt;a href=&quot;docs/README-FA.md&quot;&gt;ŸÅÿßÿ±ÿ≥€å&lt;/a&gt;] | [&lt;a href=&quot;docs/README-FR.md&quot;&gt;Fran√ßais&lt;/a&gt;] | [&lt;a href=&quot;docs/README-DE.md&quot;&gt;Deutsch&lt;/a&gt;] | [&lt;a href=&quot;docs/README-PL.md&quot;&gt;Polski&lt;/a&gt;] | [&lt;a href=&quot;docs/README-ID.md&quot;&gt;Indonesian&lt;/a&gt;] | [&lt;a href=&quot;docs/README-FI.md&quot;&gt;Suomi&lt;/a&gt;] | [&lt;a href=&quot;docs/README-ML.md&quot;&gt;‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç&lt;/a&gt;] | [&lt;a href=&quot;docs/README-JP.md&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt;] | [&lt;a href=&quot;docs/README-NL.md&quot;&gt;Nederlands&lt;/a&gt;] | [&lt;a href=&quot;docs/README-IT.md&quot;&gt;Italiano&lt;/a&gt;] | [&lt;a href=&quot;docs/README-RU.md&quot;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt;] | [&lt;a href=&quot;docs/README-PTBR.md&quot;&gt;Portugu√™s (Brasil)&lt;/a&gt;] | [&lt;a href=&quot;docs/README-EO.md&quot;&gt;Esperanto&lt;/a&gt;] | [&lt;a href=&quot;docs/README-KR.md&quot;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt;] | [&lt;a href=&quot;docs/README-AR.md&quot;&gt;ÿßŸÑÿπÿ±ÿ®Ÿä&lt;/a&gt;] | [&lt;a href=&quot;docs/README-VN.md&quot;&gt;Ti·∫øng Vi·ªát&lt;/a&gt;] | [&lt;a href=&quot;docs/README-DA.md&quot;&gt;Dansk&lt;/a&gt;] | [&lt;a href=&quot;docs/README-GR.md&quot;&gt;ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨&lt;/a&gt;] | [&lt;a href=&quot;docs/README-TR.md&quot;&gt;T√ºrk√ße&lt;/a&gt;] | [&lt;a href=&quot;docs/README-NO.md&quot;&gt;Norsk&lt;/a&gt;]&lt;br&gt;
  &lt;b&gt;We need your help to translate this README, &lt;a href=&quot;https://github.com/rustdesk/rustdesk/tree/master/src/lang&quot;&gt;RustDesk UI&lt;/a&gt; and &lt;a href=&quot;https://github.com/rustdesk/doc.rustdesk.com&quot;&gt;RustDesk Doc&lt;/a&gt; to your native language&lt;/b&gt;
&lt;/p&gt;

&gt; [!Caution]
&gt; **Misuse Disclaimer:** &lt;br&gt;
&gt; The developers of RustDesk do not condone or support any unethical or illegal use of this software. Misuse, such as unauthorized access, control or invasion of privacy, is strictly against our guidelines. The authors are not responsible for any misuse of the application.


Chat with us: [Discord](https://discord.gg/nDceKgxnkV) | [Twitter](https://twitter.com/rustdesk) | [Reddit](https://www.reddit.com/r/rustdesk) | [YouTube](https://www.youtube.com/@rustdesk)

[![ko-fi](https://ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/I2I04VU09)

Yet another remote desktop software, written in Rust. Works out of the box, no configuration required. You have full control of your data, with no concerns about security. You can use our rendezvous/relay server, [set up your own](https://rustdesk.com/server), or [write your own rendezvous/relay server](https://github.com/rustdesk/rustdesk-server-demo).

![image](https://user-images.githubusercontent.com/71636191/171661982-430285f0-2e12-4b1d-9957-4a58e375304d.png)

RustDesk welcomes contribution from everyone. See [CONTRIBUTING.md](docs/CONTRIBUTING.md) for help getting started.

[**FAQ**](https://github.com/rustdesk/rustdesk/wiki/FAQ)

[**BINARY DOWNLOAD**](https://github.com/rustdesk/rustdesk/releases)

[**NIGHTLY BUILD**](https://github.com/rustdesk/rustdesk/releases/tag/nightly)

[&lt;img src=&quot;https://f-droid.org/badge/get-it-on.png&quot;
    alt=&quot;Get it on F-Droid&quot;
    height=&quot;80&quot;&gt;](https://f-droid.org/en/packages/com.carriez.flutter_hbb)
[&lt;img src=&quot;https://flathub.org/api/badge?svg&amp;locale=en&quot;
    alt=&quot;Get it on Flathub&quot;
    height=&quot;80&quot;&gt;](https://flathub.org/apps/com.rustdesk.RustDesk)

## Dependencies

Desktop versions use Flutter or Sciter (deprecated) for GUI, this tutorial is for Sciter only, since it is easier and more friendly to start. Check out our [CI](https://github.com/rustdesk/rustdesk/blob/master/.github/workflows/flutter-build.yml) for building Flutter version.

Please download Sciter dynamic library yourself.

[Windows](https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.win/x64/sciter.dll) |
[Linux](https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.lnx/x64/libsciter-gtk.so) |
[macOS](https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.osx/libsciter.dylib)

## Raw steps to build

- Prepare your Rust development env and C++ build env

- Install [vcpkg](https://github.com/microsoft/vcpkg), and set `VCPKG_ROOT` env variable correctly

  - Windows: vcpkg install libvpx:x64-windows-static libyuv:x64-windows-static opus:x64-windows-static aom:x64-windows-static
  - Linux/macOS: vcpkg install libvpx libyuv opus aom

- run `cargo run`

## [Build](https://rustdesk.com/docs/en/dev/build/)

## How to build on Linux

### Ubuntu 18 (Debian 10)

```sh
sudo apt install -y zip g++ gcc git curl wget nasm yasm libgtk-3-dev clang libxcb-randr0-dev libxdo-dev \
        libxfixes-dev libxcb-shape0-dev libxcb-xfixes0-dev libasound2-dev libpulse-dev cmake make \
        libclang-dev ninja-build libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev libpam0g-dev
```

### openSUSE Tumbleweed

```sh
sudo zypper install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libXfixes-devel cmake alsa-lib-devel gstreamer-devel gstreamer-plugins-base-devel xdotool-devel pam-devel
```

### Fedora 28 (CentOS 8)

```sh
sudo yum -y install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libxdo-devel libXfixes-devel pulseaudio-libs-devel cmake alsa-lib-devel gstreamer1-devel gstreamer1-plugins-base-devel pam-devel
```

### Arch (Manjaro)

```sh
sudo pacman -Syu --needed unzip git cmake gcc curl wget yasm nasm zip make pkg-config clang gtk3 xdotool libxcb libxfixes alsa-lib pipewire
```

### Install vcpkg

```sh
git clone https://github.com/microsoft/vcpkg
cd vcpkg
git checkout 2023.04.15
cd ..
vcpkg/bootstrap-vcpkg.sh
export VCPKG_ROOT=$HOME/vcpkg
vcpkg/vcpkg install libvpx libyuv opus aom
```

### Fix libvpx (For Fedora)

```sh
cd vcpkg/buildtrees/libvpx/src
cd *
./configure
sed -i &#039;s/CFLAGS+=-I/CFLAGS+=-fPIC -I/g&#039; Makefile
sed -i &#039;s/CXXFLAGS+=-I/CXXFLAGS+=-fPIC -I/g&#039; Makefile
make
cp libvpx.a $HOME/vcpkg/installed/x64-linux/lib/
cd
```

### Build

```sh
curl --proto &#039;=https&#039; --tlsv1.2 -sSf https://sh.rustup.rs | sh
source $HOME/.cargo/env
git clone --recurse-submodules https://github.com/rustdesk/rustdesk
cd rustdesk
mkdir -p target/debug
wget https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.lnx/x64/libsciter-gtk.so
mv libsciter-gtk.so target/debug
VCPKG_ROOT=$HOME/vcpkg cargo run
```

## How to build with Docker

Begin by cloning the repository and building the Docker container:

```sh
git clone https://github.com/rustdesk/rustdesk
cd rustdesk
git submodule update --init --recursive
docker build -t &quot;rustdesk-builder&quot; .
```

Then, each time you need to build the application, run the following command:

```sh
docker run --rm -it -v $PWD:/home/user/rustdesk -v rustdesk-git-cache:/home/user/.cargo/git -v rustdesk-registry-cache:/home/user/.cargo/registry -e PUID=&quot;$(id -u)&quot; -e PGID=&quot;$(id -g)&quot; rustdesk-builder
```

Note that the first build may take longer before dependencies are cached, subsequent builds will be faster. Additionally, if you need to specify different arguments to the build command, you may do so at the end of the command in the `&lt;OPTIONAL-ARGS&gt;` position. For instance, if you wanted to build an optimized release version, you would run the command above followed by `--release`. The resulting executable will be available in the target folder on your system, and can be run with:

```sh
target/debug/rustdesk
```

Or, if you&#039;re running a release executable:

```sh
target/release/rustdesk
```

Please ensure that you are running these commands from the root of the RustDesk repository, otherwise the application might not be able to find the required resources. Also note that other cargo subcommands such as `install` or `run` are not currently supported via this method as they would install or run the program inside the container instead of the host.

## File Structure

- **[libs/hbb_common](https://github.com/rustdesk/rustdesk/tree/master/libs/hbb_common)**: video codec, config, tcp/udp wrapper, protobuf, fs functions for file transfer, and some other utility functions
- **[libs/scrap](https://github.com/rustdesk/rustdesk/tree/master/libs/scrap)**: screen capture
- **[libs/enigo](https://github.com/rustdesk/rustdesk/tree/master/libs/enigo)**: platform specific keyboard/mouse control
- **[libs/clipboard](https://github.com/rustdesk/rustdesk/tree/master/libs/clipboard)**: file copy and paste implementation for Windows, Linux, macOS.
- **[src/ui](https://github.com/rustdesk/rustdesk/tree/master/src/ui)**: obsolete Sciter UI (deprecated)
- **[src/server](https://github.com/rustdesk/rustdesk/tree/master/src/server)**: audio/clipboard/input/video services, and network connections
- **[src/client.rs](https://github.com/rustdesk/rustdesk/tree/master/src/client.rs)**: start a peer connection
- **[src/rendezvous_mediator.rs](https://github.com/rustdesk/rustdesk/tree/master/src/rendezvous_mediator.rs)**: Communicate with [rustdesk-server](https://github.com/rustdesk/rustdesk-server), wait for remote direct (TCP hole punching) or relayed connection
- **[src/platform](https://github.com/rustdesk/rustdesk/tree/master/src/platform)**: platform specific code
- **[flutter](https://github.com/rustdesk/rustdesk/tree/master/flutter)**: Flutter code for desktop and mobile
- **[flutter/web/js](https://github.com/rustdesk/rustdesk/tree/master/flutter/web/v1/js)**: JavaScript for Flutter web client

## Screenshots

![Connection Manager](https://github.com/rustdesk/rustdesk/assets/28412477/db82d4e7-c4bc-4823-8e6f-6af7eadf7651)

![Connected to a Windows PC](https://github.com/rustdesk/rustdesk/assets/28412477/9baa91e9-3362-4d06-aa1a-7518edcbd7ea)

![File Transfer](https://github.com/rustdesk/rustdesk/assets/28412477/39511ad3-aa9a-4f8c-8947-1cce286a46ad)

![TCP Tunneling](https://github.com/rustdesk/rustdesk/assets/28412477/78e8708f-e87e-4570-8373-1360033ea6c5)

</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[unionlabs/union]]></title>
            <link>https://github.com/unionlabs/union</link>
            <guid>https://github.com/unionlabs/union</guid>
            <pubDate>Wed, 11 Jun 2025 00:05:31 GMT</pubDate>
            <description><![CDATA[The trust-minimized, zero-knowledge bridging protocol, designed for censorship resistance, extremely high security, and usage in decentralized finance.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/unionlabs/union">unionlabs/union</a></h1>
            <p>The trust-minimized, zero-knowledge bridging protocol, designed for censorship resistance, extremely high security, and usage in decentralized finance.</p>
            <p>Language: Rust</p>
            <p>Stars: 66,947</p>
            <p>Forks: 3,241</p>
            <p>Stars today: 263 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;./.github/images/union-logo-white.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;./.github/images/union-logo-black.svg&quot;&gt;
    &lt;img alt=&quot;Union&quot;
         src=&quot;./.github/images/union-logo-black.svg&quot;
         width=&quot;100%&quot;&gt;
  &lt;/picture&gt;
&lt;/div&gt;

&lt;br/&gt;

&lt;div align=&quot;center&quot;&gt;

[![built with garnix](https://img.shields.io/endpoint.svg?url=https%3A%2F%2Fgarnix.io%2Fapi%2Fbadges%2Funionlabs%2Funion%3Fbranch%3Dmain)](https://garnix.io)
[![Docs](https://img.shields.io/badge/docs-main-blue)][docs]
[![Discord badge]](https://discord.union.build)
[![Twitter handle]][twitter badge]

&lt;/div&gt;

Union is the hyper-efficient zero-knowledge infrastructure layer for general message passing, asset transfers, NFTs, and DeFi. Its based on [Consensus Verification] and has no dependencies on trusted third parties, oracles, multi-signatures or MPC. It implements [IBC] for compatibility with [Cosmos] chains and connects to EVM chains like [Ethereum], [Berachain (beacon-kit)](https://github.com/berachain/beacon-kit), [Arbitrum], and more.

The upgradability of contracts on other chains, connections, token configurations, and evolution of the protocol will all be controlled by decentralized governance, aligning the priorities of Union with its users, validators, and operators.

## Components

| Component                                             | Description                                          | Language(s)           |
| ----------------------------------------------------- | ---------------------------------------------------- | --------------------- |
| [`uniond`](./uniond/README.md)                        | The Union node implementation, using [`CometBLS`]    | [Go]                  |
| [`galoisd`](./galoisd)                                | The zero-knowledge prover implementation             | [Go] [Gnark]          |
| [`voyager`](./voyager)                                | Modular hyper-performant cross-ecosystem relayer     | [Rust]                |
| [`hubble`](./hubble)                                  | Multi-ecosystem, GMP-enabled chain indexer           | [Rust]                |
| [`cosmwasm`](./cosmwasm)                              | [CosmWasm] smart contract stack                      | [Rust]                |
| [`light-clients`](./cosmwasm/ibc-union/lightclient)   | [Light Clients] for various ecosystems               | [Rust]                |
| [`unionvisor`](./unionvisor/README.md)                | Node supervisor intended for production usage        | [Rust]                |
| [`drip`](./drip)                                      | Faucet for [Cosmos] chains: [app.union.build/faucet] | [Rust]                |
| [`evm`](./evm)                                        | [EVM] smart contract stack                           | [Solidity]            |
| [`app`](./app2)                                       | [app.union.build]                                    | [TypeScript] [Svelte] |
| [`site`](./site)                                      | [union.build]                                        | [TypeScript] [Astro]  |
| [`TypeScript SDK`](./typescript-sdk)                  | TypeScript SDK for interacting with Union            | [TypeScript]          |

## Quickstart

Install [Nix] to _[reproducibly build](https://en.wikipedia.org/wiki/Reproducible_builds) any component_, and to enter a dev shell with _all dependencies_:

```sh
curl --proto &#039;=https&#039; --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install
```

_(Note that some components can only be built on Linux. If you are using macOS, we recommend using [OrbStack] to easily set up a [NixOS] VM within two minutes. Most Union developers use macOS with [OrbStack], and there is no need to install Nix inside of the [NixOS] VM.)_

You can now _reproducibly_ build any of Union&#039;s components from source:

```sh
nix build .#uniond -L
nix build .#voyager -L
nix build .#app -L

# to see all packages, run:
nix flake show
```

The result of whatever you build will be in `result/`

You can now also enter our dev shell, which has all of the dependencies (`cargo`, `rustc`, `node`, `go`, etc.) you need to work on any component:
_(Don&#039;t worry, this will not affect your system outside of this repo)_

```sh
nix develop
```

Run the following to format the entire repo and check your spelling before each PR:

```sh
nix run .#pre-commit -L
```

Check the `#developers` channel on [Union&#039;s discord](https://discord.union.build) if you need any help with this.

## Docs

The official docs are hosted [here][docs]. Each individual component also has accompanying developer documentation for contributors, which you can find in each `README.md`.

[app.union.build]: https://app.union.build
[app.union.build/faucet]: https://app.union.build/faucet
[arbitrum]: https://github.com/OffchainLabs/arbitrum
[astro]: https://astro.build
[consensus verification]: https://union.build/docs/concepts/consensus-verification/
[cosmos]: https://cosmos.network
[cosmwasm]: https://cosmwasm.com/
[discord badge]: https://img.shields.io/discord/1158939416870522930?logo=discord
[docs]: https://docs.union.build &quot;Official Union Docs&quot;
[ethereum]: https://ethereum.org
[evm]: https://ethereum.org/en/developers/docs/evm/
[gnark]: https://github.com/ConsenSys/gnark
[go]: https://go.dev/
[ibc]: https://github.com/cosmos/ibc &quot;cosmos/ibc&quot;
[light clients]: https://a16zcrypto.com/posts/article/an-introduction-to-light-clients/
[nix]: https://zero-to-nix.com/
[nixos]: https://nixos.org
[orbstack]: https://orbstack.dev/
[rust]: https://www.rust-lang.org/
[solidity]: https://soliditylang.org/
[svelte]: https://svelte.dev
[twitter badge]: https://twitter.com/intent/follow?screen_name=union_build
[twitter handle]: https://img.shields.io/twitter/follow/union_build.svg?style=social&amp;label=Follow
[typescript]: https://www.typescriptlang.org/
[union.build]: https://union.build
[`cometbls`]: https://github.com/unionlabs/cometbls
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[mainmatter/100-exercises-to-learn-rust]]></title>
            <link>https://github.com/mainmatter/100-exercises-to-learn-rust</link>
            <guid>https://github.com/mainmatter/100-exercises-to-learn-rust</guid>
            <pubDate>Wed, 11 Jun 2025 00:05:30 GMT</pubDate>
            <description><![CDATA[A self-paced course to learn Rust, one exercise at a time.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mainmatter/100-exercises-to-learn-rust">mainmatter/100-exercises-to-learn-rust</a></h1>
            <p>A self-paced course to learn Rust, one exercise at a time.</p>
            <p>Language: Rust</p>
            <p>Stars: 7,846</p>
            <p>Forks: 1,559</p>
            <p>Stars today: 21 stars today</p>
            <h2>README</h2><pre># Learn Rust, one exercise at a time

You&#039;ve heard about Rust, but you never had the chance to try it out?\
This course is for you!

You&#039;ll learn Rust by solving 100 exercises.\
You&#039;ll go from knowing nothing about Rust to being able to start
writing your own programs, one exercise at a time.

&gt; [!NOTE]
&gt; This course has been written by [Mainmatter](https://mainmatter.com/rust-consulting/).\
&gt; It&#039;s one of the trainings in [our portfolio of Rust workshops](https://mainmatter.com/services/workshops/rust/).\
&gt; Check out our [landing page](https://mainmatter.com/rust-consulting/) if you&#039;re looking for Rust consulting or
&gt; training!

## Getting started

Go to [rust-exercises.com](https://rust-exercises.com) and follow the instructions there
to get started with the course.

## Requirements

- **Rust** (follow instructions [here](https://www.rust-lang.org/tools/install)).\
  If `rustup` is already installed on your system, run `rustup update` (or another appropriate command depending on how
  you installed Rust on your system)
  to make sure you&#039;re running on the latest stable version.
- _(Optional but recommended)_ An IDE with Rust autocompletion support.
  We recommend one of the following:
  - [RustRover](https://www.jetbrains.com/rust/);
  - [Visual Studio Code](https://code.visualstudio.com) with
    the [`rust-analyzer`](https://marketplace.visualstudio.com/items?itemName=matklad.rust-analyzer) extension.

## Solutions

You can find the solutions to the exercises in
the [`solutions` branch](https://github.com/mainmatter/100-exercises-to-learn-rust/tree/solutions) of this repository.

# License

Copyright ¬© 2024- Mainmatter GmbH (https://mainmatter.com), released under the
[Creative Commons Attribution-NonCommercial 4.0 International license](https://creativecommons.org/licenses/by-nc/4.0/).
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[jtroo/kanata]]></title>
            <link>https://github.com/jtroo/kanata</link>
            <guid>https://github.com/jtroo/kanata</guid>
            <pubDate>Wed, 11 Jun 2025 00:05:29 GMT</pubDate>
            <description><![CDATA[Improve keyboard comfort and usability with advanced customization]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jtroo/kanata">jtroo/kanata</a></h1>
            <p>Improve keyboard comfort and usability with advanced customization</p>
            <p>Language: Rust</p>
            <p>Stars: 5,186</p>
            <p>Forks: 185</p>
            <p>Stars today: 23 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;Kanata&lt;/h1&gt;

&lt;h3 align=&quot;center&quot;&gt;
  &lt;img
    alt=&quot;Image of a keycap with the letter K on it in pink tones&quot;
    title=&quot;Kanata&quot;
    height=&quot;160&quot;
    src=&quot;assets/kanata-icon.svg&quot;
  /&gt;
&lt;/h3&gt;

&lt;div align=&quot;center&quot;&gt;
  Improve your keyboard comfort
&lt;/div&gt;

## What does this do?

This is a cross-platform software keyboard remapper for Linux, macOS and Windows.
A short summary of the features:

- multiple layers of key functionality
- advanced key behaviour customization (e.g. tap-hold, macros, unicode)

To see all of the features, see the [configuration guide](./docs/config.adoc).

You can find pre-built binaries in the [releases page](https://github.com/jtroo/kanata/releases)
or read on for build instructions.

You can see a [list of known issues here](./docs/platform-known-issues.adoc).

### Demo

#### Demo video
[Showcase of multi-layer functionality (30s, 1.7 MB)](https://user-images.githubusercontent.com/6634136/183001314-f64a7e26-4129-4f20-bf26-7165a6e02c38.mp4).

#### Online simulator

You can check out the [online simulator](https://jtroo.github.io)
to test configuration validity and test input simulation.

## Why is this useful?

Imagine if, instead of pressing Shift to type uppercase letters, we had giant
keyboards with separate keys for lowercase and uppercase letters. I hope we can
all agree: that would be a terrible user experience!

A way to think of how Shift keys work is that they switch your input to another
layer of functionality where you now type uppercase letters and symbols
instead of lowercase letters and numbers.

What kanata allows you to do is take this alternate layer concept that Shift
keys have and apply it to any key. You can then customize what those layers do
to suit your exact needs and workflows.

## Usage

Running kanata currently does not start it in a background process.
You will need to keep the window that starts kanata running to keep kanata active.
Some tips for running kanata in the background:

- Windows: https://github.com/jtroo/kanata/discussions/193
- Linux: https://github.com/jtroo/kanata/discussions/130#discussioncomment-10227272
- Run from tray icon: [kanata-tray](https://github.com/rszyma/kanata-tray)

### Pre-built executables

See the
[releases page](https://github.com/jtroo/kanata/releases)
for executables and instructions.

### Build it yourself

This project uses the latest Rust stable toolchain. If you installed the
Rust toolchain using `rustup`, e.g. by using the instructions from the
[official website](https://www.rust-lang.org/learn/get-started),
you can get the latest stable toolchain with `rustup update stable`.

&lt;details&gt;
&lt;summary&gt;Instructions&lt;/summary&gt;

Using `cargo install`:

    cargo install kanata

    # On Linux and macOS, this may not work without `sudo`, see below
    kanata --cfg &lt;your_configuration_file&gt;

Build and run yourself in Linux:

    git clone https://github.com/jtroo/kanata &amp;&amp; cd kanata
    cargo build   # --release optional, not really perf sensitive

    # sudo is used because kanata opens /dev/ files
    #
    # See below if you want to avoid needing sudo:
    # https://github.com/jtroo/kanata/wiki/Avoid-using-sudo-on-Linux
    sudo target/debug/kanata --cfg &lt;your_configuration_file&gt;

Build and run yourself in Windows.

    git clone https://github.com/jtroo/kanata; cd kanata
    cargo build   # --release optional, not really perf sensitive
    target\debug\kanata --cfg &lt;your_configuration_file&gt;

Build and run yourself in macOS:

First install the Karabiner driver by following the macOS documentation
in the [releases page](https://github.com/jtroo/kanata/releases/).

Then you can compile and run with the instructions below:

    git clone https://github.com/jtroo/kanata &amp;&amp; cd kanata
    cargo build   # --release optional, not really perf sensitive

    # sudo is needed to gain permission to intercept the keyboard

    sudo target/debug/kanata --cfg &lt;your_configuration_file&gt;

The full configuration guide is [found here](./docs/config.adoc).

Sample configuration files are found in [cfg_samples](./cfg_samples). The
[simple.kbd](./cfg_samples/simple.kbd) file contains a basic configuration file
that is hopefully easy to understand but does not contain all features. The
`kanata.kbd` contains an example of all features with documentation. The
release assets also have a `kanata.kbd` file that is tested to work with that
release. All key names can be found in the [keys module](./src/keys/mod.rs),
and you can also define your own key names.

&lt;/details&gt;

### Feature flags

When either building yourself or using `cargo install`,
you can add feature flags that
enable functionality that is turned off by default.

&lt;details&gt;
&lt;summary&gt;Instructions&lt;/summary&gt;

If you want to enable the `cmd` actions,
add the flag `--features cmd`.
For example:

```
cargo build --release --features cmd
cargo install --features cmd
```

On Windows,
if you want to compile a binary that uses the Interception driver,
you should add the flag `--features interception_driver`.
For example:

```
cargo build --release --features interception_driver
cargo install --features interception_driver
```

To combine multiple flags,
use a single `--features` flag
and use a comma to separate the features.
For example:

```
cargo build --release --features cmd,interception_driver
cargo install --features cmd,interception_driver
```
&lt;/details&gt;

## Other installation methods

[![Packaging status](https://repology.org/badge/vertical-allrepos/kanata.svg)](https://repology.org/project/kanata/versions)

## Notable features

- Human-readable configuration file.
  - [Minimal example](./cfg_samples/minimal.kbd)
  - [Full guide](./docs/config.adoc)
  - [Simple example with explanations](./cfg_samples/simple.kbd)
  - [All features showcase](./cfg_samples/kanata.kbd)
- Live reloading of the configuration for easy testing of your changes.
- Multiple layers of key functionality
- Advanced actions such as tap-hold, unicode output, dynamic and static macros
- Vim-like leader sequences to execute other actions
- Optionally run a TCP server to interact with other programs
  - Other programs can respond to [layer changes or trigger layer changes](https://github.com/jtroo/kanata/issues/47)
- [Interception driver](https://web.archive.org/web/20240209172129/http://www.oblita.com/interception) support (use `kanata_wintercept.exe`)
  - Note that this issue exists, which is outside the control of this project:
    https://github.com/oblitum/Interception/issues/25

## Contributing

Contributions are welcome!

Unless explicitly stated otherwise, your contributions to kanata will be made
under the LGPL-3.0-only[*] license.

Some directories are exceptions:
- [keyberon](./keyberon): MIT License
- [interception](./interception): MIT or Apache-2.0 Licenses

[Here&#039;s a basic low-effort design doc of kanata](./docs/design.md)

[*]: https://www.gnu.org/licenses/identify-licenses-clearly.html

## How you can help

- Try it out and let me know what you think. Feel free to file an issue or
  start a discussion.
- Usability issues and unhelpful error messages are considered bugs that should
  be fixed. If you encounter any, I would be thankful if you file an issue.
- Browse the open issues and help out if you are able and/or would like to. If
  you want to try contributing, feel free to ping jtroo for some pointers.
- If you know anything about writing a keyboard driver for Windows, starting an
  open-source alternative to the Interception driver would be lovely.

## Community projects related to kanata

- [vscode-kanata](https://github.com/rszyma/vscode-kanata): Language support for kanata configuration files in VS Code
- [komokana](https://github.com/LGUG2Z/komokana): Automatic application-aware layer switching for [`komorebi`](https://github.com/LGUG2Z/komorebi) (Windows)
- [kanata-tray](https://github.com/rszyma/kanata-tray): Control kanata from a tray icon
- [OverKeys](https://github.com/conventoangelo/overkeys): Visual layer display for kanata - see your active layers and keymaps in real-time (Windows)
- Application-aware layer switching:
   - [qanata (Linux)](https://github.com/veyxov/qanata)
   - [kanawin (Windows)](https://github.com/Aqaao/kanawin)
   - [window_tools (Windows)](https://github.com/reidprichard/window_tools)
   - [nata (Linux)](https://github.com/mdSlash/nata)
   - [kanata-vk-agent (macOS)](https://github.com/devsunb/kanata-vk-agent)
   - [hyprkan (Linux)](https://github.com/mdSlash/hyprkan)

## What does the name mean?

I wanted a &quot;k&quot; word since this relates to keyboards. According to Wikipedia,
kanata is an indigenous Iroquoian word meaning &quot;village&quot; or &quot;settlement&quot; and is
the origin of Canada&#039;s name.

There&#039;s also PPT‚úß.

## Motivation

TLDR: QMK features but for any keyboard, not just fancy mechanical ones.

&lt;details&gt;
  &lt;summary&gt;Long version&lt;/summary&gt;

I have a few keyboards that run [QMK](https://docs.qmk.fm/#/). QMK allows the
user to customize the functionality of their keyboard to their heart&#039;s content.

One great use case of QMK is its ability map keys so that they overlap with the
home row keys but are accessible on another layer. I won&#039;t comment on
productivity, but I find this greatly helps with my keyboard comfort.

For example, these keys are on the right side of the keyboard:

    7 8 9
    u i o
    j k l
    m , .

On one layer I have arrow keys in the same position, and on another layer I
have a numpad.

    arrows:       numpad:
    - - -         7 8 9
    - ‚Üë -         4 5 6
    ‚Üê ‚Üì ‚Üí         1 2 3
    - - -         0 * .

One could add as many customizations as one likes to improve comfort, speed,
etc. Personally my main motivator is comfort due to a repetitive strain injury
in the past.

However, QMK doesn&#039;t run everywhere. In fact, it doesn&#039;t run on **most**
hardware you can get. You can&#039;t get it to run on a laptop keyboard or any
mainstream office keyboard. I believe that the comfort and empowerment QMK
provides should be available to anyone with a computer on their existing
hardware, instead of having to purchase an enthusiast mechanical keyboard
(which are admittedly very nice ‚Äî I own a few ‚Äî but can be costly).

The best alternative solution that I found for keyboards that don&#039;t run QMK was
[kmonad](https://github.com/kmonad/kmonad). This is an excellent project
and I recommend it if you want to try something similar.

The reason for this project&#039;s existence is that kmonad is written in Haskell
and I have no idea how to begin contributing to a Haskell project. From an
outsider&#039;s perspective I think Haskell is a great language but I really can&#039;t
wrap my head around it. And there are a few [outstanding issues](./docs/kmonad_comparison.md)
at the time of writing that make kmonad suboptimal for my personal workflows.

This project is written in Rust because Rust is my favourite programming
language and the prior work of the awesome [keyberon crate](https://github.com/TeXitoi/keyberon)
exists.
&lt;/details&gt;

## Similar Projects

The most similar project is [kmonad](https://github.com/kmonad/kmonad),
which served as the inspiration for kanata. [Here&#039;s a comparison document](./docs/kmonad_comparison.md).
Other similar projects:

- [QMK](https://docs.qmk.fm/#/): Open source keyboard firmware
- [keyberon](https://github.com/TeXitoi/keyberon): Rust `#[no_std]` library intended for keyboard firmware
- [ktrl](https://github.com/ItayGarin/ktrl): Linux-only keyboard customizer with layers, a TCP server, and audio support
- [kbremap](https://github.com/timokroeger/kbremap): Windows-only keyboard customizer with layers and unicode
- [xcape](https://github.com/alols/xcape): Linux-only tap-hold modifiers
- [karabiner-elements](https://karabiner-elements.pqrs.org/): Mac-only keyboard customizer
- [capsicain](https://github.com/cajhin/capsicain): Windows-only key remapper with driver-level key interception
- [keyd](https://github.com/rvaiya/keyd): Linux-only key remapper very similar to QMK, kmonad, and kanata
- [xremap](https://github.com/k0kubun/xremap): Linux-only application-aware key remapper inspired more by Emacs key sequences vs. QMK layers/Vim modes
- [keymapper](https://github.com/houmain/keymapper): Context-aware cross-platform key remapper with a different transformation model (Linux, Windows, Mac)
- [mouseless](https://github.com/jbensmann/mouseless): Linux-only mouse-focused key remapper that also has layers, key combo and tap-hold capabilities

### Why the list?

While kanata is the best tool for some, it may not be the best tool for
you. I&#039;m happy to introduce you to tools that may better suit your needs. This
list is also useful as reference/inspiration for functionality that could be
added to kanata.

## Donations/Support?

The author (jtroo) will not accept monetary donations for work on kanata.
Please instead donate your time and/or money to charity.

Some links are below. These links are provided for learning and as interesting
reads. They are **not** an endorsement.

- https://www.effectivealtruism.org/
- https://www.givewell.org/
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[brave/adblock-rust]]></title>
            <link>https://github.com/brave/adblock-rust</link>
            <guid>https://github.com/brave/adblock-rust</guid>
            <pubDate>Wed, 11 Jun 2025 00:05:28 GMT</pubDate>
            <description><![CDATA[Brave's Rust-based adblock engine]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/brave/adblock-rust">brave/adblock-rust</a></h1>
            <p>Brave's Rust-based adblock engine</p>
            <p>Language: Rust</p>
            <p>Stars: 1,735</p>
            <p>Forks: 144</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre># adblock-rust

[![crates.io](https://img.shields.io/crates/v/adblock.svg)](https://crates.io/crates/adblock)
[![npmjs.com](https://img.shields.io/npm/v/adblock-rs.svg)](https://www.npmjs.com/package/adblock-rs)
[![docs.rs](https://docs.rs/adblock/badge.svg)](https://docs.rs/adblock)
![Build Status](https://github.com/brave/adblock-rust/actions/workflows/ci.yml/badge.svg)
[![License](https://img.shields.io/badge/License-MPL--2.0-blue)](LICENSE)

### _Putting you back in control of your browsing experience._

`adblock-rust` is the engine powering [Brave](https://brave.com)&#039;s native adblocker, available as a library for anyone to use. It features:

- Network blocking
- Cosmetic filtering
- Resource replacements
- Hosts syntax
- uBlock Origin syntax extensions
- iOS content-blocking syntax conversion
- Compiling to native code or WASM
- Rust bindings ([crates](https://crates.io/crates/adblock))
- JS bindings ([npm](https://npmjs.com/adblock-rs))
- Community-maintained Python bindings ([pypi](https://pypi.org/project/adblock/))
- High performance!

## Getting started

`adblock-rust` is used in several projects, including browsers, research tools, and proxies.
It may be a good fit for yours, too!

See [docs.rs](https://docs.rs/adblock) for detailed API documentation.

Also check the [Rust example](./examples/example.rs) or the [NodeJS example](./js/example.mjs).

### Optional features

The following `cargo` [features](https://doc.rust-lang.org/cargo/reference/features.html) can be used to tweak `adblock-rust` to best fit your use-case.

#### CSS validation during rule parsing (`css-validation`)

When parsing cosmetic filter rules, it&#039;s possible to include a built-in implementation of CSS validation (through the [selectors](https://crates.io/crates/selectors) and [cssparser](https://crates.io/crates/cssparser) crates) by enabling the `css-validation` feature. This will cause `adblock-rust` to reject cosmetic filter rules with invalid CSS syntax.

#### Content blocking format translation (`content-blocking`)

Enabling the `content-blocking` feature gives `adblock-rust` support for conversion of standard ABP-style rules into Apple&#039;s [content-blocking format](https://developer.apple.com/documentation/safariservices/creating_a_content_blocker), which can be exported for use on iOS and macOS platforms.

#### External domain resolution (`embedded-domain-resolver`)

By default, `adblock-rust` ships with a built-in domain resolution implementation (through the [addr](https://crates.io/crates/addr) crate) that will generally suffice for standalone use-cases. For more advanced use-cases, disabling the `embedded-domain-resolver` feature will allow `adblock-rust` to use an external domain resolution implementation instead. This is extremely useful to reduce binary bloat and improve consistency when embedding `adblock-rust` within a browser.

#### Parsing resources from uBlock Origin&#039;s formats (`resource-assembler`)

`adblock-rust` uses uBlock Origin-compatible resources for scriptlet injection and redirect rules.
The `resource-assembler` feature allows `adblock-rust` to parse these resources directly from the file formats used by the uBlock Origin repository.

#### Thread safety (`unsync-regex-caching`)

The `unsync-regex-caching` feature enables optimizations for rule matching speed and the amount of memory used by the engine.
This feature can be disabled to make the engine `Send + Sync`, although it is recommended to only access the engine on a single thread to maintain optimal performance.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[dani-garcia/vaultwarden]]></title>
            <link>https://github.com/dani-garcia/vaultwarden</link>
            <guid>https://github.com/dani-garcia/vaultwarden</guid>
            <pubDate>Wed, 11 Jun 2025 00:05:27 GMT</pubDate>
            <description><![CDATA[Unofficial Bitwarden compatible server written in Rust, formerly known as bitwarden_rs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/dani-garcia/vaultwarden">dani-garcia/vaultwarden</a></h1>
            <p>Unofficial Bitwarden compatible server written in Rust, formerly known as bitwarden_rs</p>
            <p>Language: Rust</p>
            <p>Stars: 45,316</p>
            <p>Forks: 2,147</p>
            <p>Stars today: 29 stars today</p>
            <h2>README</h2><pre>![Vaultwarden Logo](./resources/vaultwarden-logo-auto.svg)

An alternative server implementation of the Bitwarden Client API, written in Rust and compatible with [official Bitwarden clients](https://bitwarden.com/download/) [[disclaimer](#disclaimer)], perfect for self-hosted deployment where running the official resource-heavy service might not be ideal.

---

[![GitHub Release](https://img.shields.io/github/release/dani-garcia/vaultwarden.svg?style=for-the-badge&amp;logo=vaultwarden&amp;color=005AA4)](https://github.com/dani-garcia/vaultwarden/releases/latest)
[![ghcr.io Pulls](https://img.shields.io/badge/dynamic/json?style=for-the-badge&amp;logo=github&amp;logoColor=fff&amp;color=005AA4&amp;url=https%3A%2F%2Fipitio.github.io%2Fbackage%2Fdani-garcia%2Fvaultwarden%2Fvaultwarden.json&amp;query=%24.downloads&amp;label=ghcr.io%20pulls&amp;cacheSeconds=14400)](https://github.com/dani-garcia/vaultwarden/pkgs/container/vaultwarden)
[![Docker Pulls](https://img.shields.io/docker/pulls/vaultwarden/server.svg?style=for-the-badge&amp;logo=docker&amp;logoColor=fff&amp;color=005AA4&amp;label=docker.io%20pulls)](https://hub.docker.com/r/vaultwarden/server)
[![Quay.io](https://img.shields.io/badge/quay.io-download-005AA4?style=for-the-badge&amp;logo=redhat&amp;cacheSeconds=14400)](https://quay.io/repository/vaultwarden/server) &lt;br&gt;
[![Contributors](https://img.shields.io/github/contributors-anon/dani-garcia/vaultwarden.svg?style=flat-square&amp;logo=vaultwarden&amp;color=005AA4)](https://github.com/dani-garcia/vaultwarden/graphs/contributors)
[![Forks](https://img.shields.io/github/forks/dani-garcia/vaultwarden.svg?style=flat-square&amp;logo=github&amp;logoColor=fff&amp;color=005AA4)](https://github.com/dani-garcia/vaultwarden/network/members)
[![Stars](https://img.shields.io/github/stars/dani-garcia/vaultwarden.svg?style=flat-square&amp;logo=github&amp;logoColor=fff&amp;color=005AA4)](https://github.com/dani-garcia/vaultwarden/stargazers)
[![Issues Open](https://img.shields.io/github/issues/dani-garcia/vaultwarden.svg?style=flat-square&amp;logo=github&amp;logoColor=fff&amp;color=005AA4&amp;cacheSeconds=300)](https://github.com/dani-garcia/vaultwarden/issues)
[![Issues Closed](https://img.shields.io/github/issues-closed/dani-garcia/vaultwarden.svg?style=flat-square&amp;logo=github&amp;logoColor=fff&amp;color=005AA4&amp;cacheSeconds=300)](https://github.com/dani-garcia/vaultwarden/issues?q=is%3Aissue+is%3Aclosed)
[![AGPL-3.0 Licensed](https://img.shields.io/github/license/dani-garcia/vaultwarden.svg?style=flat-square&amp;logo=vaultwarden&amp;color=944000&amp;cacheSeconds=14400)](https://github.com/dani-garcia/vaultwarden/blob/main/LICENSE.txt) &lt;br&gt;
[![Dependency Status](https://img.shields.io/badge/dynamic/xml?url=https%3A%2F%2Fdeps.rs%2Frepo%2Fgithub%2Fdani-garcia%2Fvaultwarden%2Fstatus.svg&amp;query=%2F*%5Blocal-name()%3D&#039;svg&#039;%5D%2F*%5Blocal-name()%3D&#039;g&#039;%5D%5B2%5D%2F*%5Blocal-name()%3D&#039;text&#039;%5D%5B4%5D&amp;style=flat-square&amp;logo=rust&amp;label=dependencies&amp;color=005AA4)](https://deps.rs/repo/github/dani-garcia/vaultwarden)
[![GHA Release](https://img.shields.io/github/actions/workflow/status/dani-garcia/vaultwarden/release.yml?style=flat-square&amp;logo=github&amp;logoColor=fff&amp;label=Release%20Workflow)](https://github.com/dani-garcia/vaultwarden/actions/workflows/release.yml)
[![GHA Build](https://img.shields.io/github/actions/workflow/status/dani-garcia/vaultwarden/build.yml?style=flat-square&amp;logo=github&amp;logoColor=fff&amp;label=Build%20Workflow)](https://github.com/dani-garcia/vaultwarden/actions/workflows/build.yml) &lt;br&gt;
[![Matrix Chat](https://img.shields.io/matrix/vaultwarden:matrix.org.svg?style=flat-square&amp;logo=matrix&amp;logoColor=fff&amp;color=953B00&amp;cacheSeconds=14400)](https://matrix.to/#/#vaultwarden:matrix.org)
[![GitHub Discussions](https://img.shields.io/github/discussions/dani-garcia/vaultwarden?style=flat-square&amp;logo=github&amp;logoColor=fff&amp;color=953B00&amp;cacheSeconds=300)](https://github.com/dani-garcia/vaultwarden/discussions)
[![Discourse Discussions](https://img.shields.io/discourse/topics?server=https%3A%2F%2Fvaultwarden.discourse.group%2F&amp;style=flat-square&amp;logo=discourse&amp;color=953B00)](https://vaultwarden.discourse.group/)

&gt; [!IMPORTANT]
&gt; **When using this server, please report any bugs or suggestions directly to us (see [Get in touch](#get-in-touch)), regardless of whatever clients you are using (mobile, desktop, browser...). DO NOT use the official Bitwarden support channels.**

&lt;br&gt;

## Features

A nearly complete implementation of the Bitwarden Client API is provided, including:

 * [Personal Vault](https://bitwarden.com/help/managing-items/)
 * [Send](https://bitwarden.com/help/about-send/)
 * [Attachments](https://bitwarden.com/help/attachments/)
 * [Website icons](https://bitwarden.com/help/website-icons/)
 * [Personal API Key](https://bitwarden.com/help/personal-api-key/)
 * [Organizations](https://bitwarden.com/help/getting-started-organizations/)
   - [Collections](https://bitwarden.com/help/about-collections/),
     [Password Sharing](https://bitwarden.com/help/sharing/),
     [Member Roles](https://bitwarden.com/help/user-types-access-control/),
     [Groups](https://bitwarden.com/help/about-groups/),
     [Event Logs](https://bitwarden.com/help/event-logs/),
     [Admin Password Reset](https://bitwarden.com/help/admin-reset/),
     [Directory Connector](https://bitwarden.com/help/directory-sync/),
     [Policies](https://bitwarden.com/help/policies/)
 * [Multi/Two Factor Authentication](https://bitwarden.com/help/bitwarden-field-guide-two-step-login/)
   - [Authenticator](https://bitwarden.com/help/setup-two-step-login-authenticator/),
     [Email](https://bitwarden.com/help/setup-two-step-login-email/),
     [FIDO2 WebAuthn](https://bitwarden.com/help/setup-two-step-login-fido/),
     [YubiKey](https://bitwarden.com/help/setup-two-step-login-yubikey/),
     [Duo](https://bitwarden.com/help/setup-two-step-login-duo/)
 * [Emergency Access](https://bitwarden.com/help/emergency-access/)
 * [Vaultwarden Admin Backend](https://github.com/dani-garcia/vaultwarden/wiki/Enabling-admin-page)
 * [Modified Web Vault client](https://github.com/dani-garcia/bw_web_builds) (Bundled within our containers)

&lt;br&gt;

## Usage

&gt; [!IMPORTANT]
&gt; Most modern web browsers disallow the use of Web Crypto APIs in insecure contexts. In this case, you might get an error like `Cannot read property &#039;importKey&#039;`. To solve this problem, you need to access the web vault via HTTPS or localhost.
&gt;
&gt;This can be configured in [Vaultwarden directly](https://github.com/dani-garcia/vaultwarden/wiki/Enabling-HTTPS) or using a third-party reverse proxy ([some examples](https://github.com/dani-garcia/vaultwarden/wiki/Proxy-examples)).
&gt;
&gt;If you have an available domain name, you can get HTTPS certificates with [Let&#039;s Encrypt](https://letsencrypt.org/), or you can generate self-signed certificates with utilities like [mkcert](https://github.com/FiloSottile/mkcert). Some proxies automatically do this step, like Caddy or Traefik (see examples linked above).

&gt; [!TIP]
&gt;**For more detailed examples on how to install, use and configure Vaultwarden you can check our [Wiki](https://github.com/dani-garcia/vaultwarden/wiki).**

The main way to use Vaultwarden is via our container images which are published to [ghcr.io](https://github.com/dani-garcia/vaultwarden/pkgs/container/vaultwarden), [docker.io](https://hub.docker.com/r/vaultwarden/server) and [quay.io](https://quay.io/repository/vaultwarden/server).

There are also [community driven packages](https://github.com/dani-garcia/vaultwarden/wiki/Third-party-packages) which can be used, but those might be lagging behind the latest version or might deviate in the way Vaultwarden is configured, as described in our [Wiki](https://github.com/dani-garcia/vaultwarden/wiki).

### Docker/Podman CLI

Pull the container image and mount a volume from the host for persistent storage.&lt;br&gt;
You can replace `docker` with `podman` if you prefer to use podman.

```shell
docker pull vaultwarden/server:latest
docker run --detach --name vaultwarden \
  --env DOMAIN=&quot;https://vw.domain.tld&quot; \
  --volume /vw-data/:/data/ \
  --restart unless-stopped \
  --publish 80:80 \
  vaultwarden/server:latest
```

This will preserve any persistent data under `/vw-data/`, you can adapt the path to whatever suits you.

### Docker Compose

To use Docker compose you need to create a `compose.yaml` which will hold the configuration to run the Vaultwarden container.

```yaml
services:
  vaultwarden:
    image: vaultwarden/server:latest
    container_name: vaultwarden
    restart: unless-stopped
    environment:
      DOMAIN: &quot;https://vw.domain.tld&quot;
    volumes:
      - ./vw-data/:/data/
    ports:
      - 80:80
```

&lt;br&gt;

## Get in touch

Have a question, suggestion or need help? Join our community on [Matrix](https://matrix.to/#/#vaultwarden:matrix.org), [GitHub Discussions](https://github.com/dani-garcia/vaultwarden/discussions) or [Discourse Forums](https://vaultwarden.discourse.group/).

Encountered a bug or crash? Please search our issue tracker and discussions to see if it&#039;s already been reported. If not, please [start a new discussion](https://github.com/dani-garcia/vaultwarden/discussions) or [create a new issue](https://github.com/dani-garcia/vaultwarden/issues/). Ensure you&#039;re using the latest version of Vaultwarden and there aren&#039;t any similar issues open or closed!

&lt;br&gt;

## Contributors

Thanks for your contribution to the project!

[![Contributors Count](https://img.shields.io/github/contributors-anon/dani-garcia/vaultwarden?style=for-the-badge&amp;logo=vaultwarden&amp;color=005AA4)](https://github.com/dani-garcia/vaultwarden/graphs/contributors)&lt;br&gt;
[![Contributors Avatars](https://contributors-img.web.app/image?repo=dani-garcia/vaultwarden)](https://github.com/dani-garcia/vaultwarden/graphs/contributors)

&lt;br&gt;

## Disclaimer

**This project is not associated with [Bitwarden](https://bitwarden.com/) or Bitwarden, Inc.**

However, one of the active maintainers for Vaultwarden is employed by Bitwarden and is allowed to contribute to the project on their own time. These contributions are independent of Bitwarden and are reviewed by other maintainers.

The maintainers work together to set the direction for the project, focusing on serving the self-hosting community, including individuals, families, and small organizations, while ensuring the project&#039;s sustainability.

**Please note:** We cannot be held liable for any data loss that may occur while using Vaultwarden. This includes passwords, attachments, and other information handled by the application. We highly recommend performing regular backups of your files and database. However, should you experience data loss, we encourage you to contact us immediately.

&lt;br&gt;

## Bitwarden_RS

This project was known as Bitwarden_RS and has been renamed to separate itself from the official Bitwarden server in the hopes of avoiding confusion and trademark/branding issues.&lt;br&gt;
Please see [#1642 - v1.21.0 release and project rename to Vaultwarden](https://github.com/dani-garcia/vaultwarden/discussions/1642) for more explanation.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[Eventual-Inc/Daft]]></title>
            <link>https://github.com/Eventual-Inc/Daft</link>
            <guid>https://github.com/Eventual-Inc/Daft</guid>
            <pubDate>Wed, 11 Jun 2025 00:05:26 GMT</pubDate>
            <description><![CDATA[Distributed query engine providing simple and reliable data processing for any modality and scale]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Eventual-Inc/Daft">Eventual-Inc/Daft</a></h1>
            <p>Distributed query engine providing simple and reliable data processing for any modality and scale</p>
            <p>Language: Rust</p>
            <p>Stars: 2,905</p>
            <p>Forks: 216</p>
            <p>Stars today: 8 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[bevyengine/bevy]]></title>
            <link>https://github.com/bevyengine/bevy</link>
            <guid>https://github.com/bevyengine/bevy</guid>
            <pubDate>Wed, 11 Jun 2025 00:05:25 GMT</pubDate>
            <description><![CDATA[A refreshingly simple data-driven game engine built in Rust]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/bevyengine/bevy">bevyengine/bevy</a></h1>
            <p>A refreshingly simple data-driven game engine built in Rust</p>
            <p>Language: Rust</p>
            <p>Stars: 40,016</p>
            <p>Forks: 3,944</p>
            <p>Stars today: 25 stars today</p>
            <h2>README</h2><pre># [![Bevy](assets/branding/bevy_logo_light_dark_and_dimmed.svg)](https://bevy.org)

[![License](https://img.shields.io/badge/license-MIT%2FApache-blue.svg)](https://github.com/bevyengine/bevy#license)
[![Crates.io](https://img.shields.io/crates/v/bevy.svg)](https://crates.io/crates/bevy)
[![Downloads](https://img.shields.io/crates/d/bevy.svg)](https://crates.io/crates/bevy)
[![Docs](https://docs.rs/bevy/badge.svg)](https://docs.rs/bevy/latest/bevy/)
[![CI](https://github.com/bevyengine/bevy/workflows/CI/badge.svg)](https://github.com/bevyengine/bevy/actions)
[![Discord](https://img.shields.io/discord/691052431525675048.svg?label=&amp;logo=discord&amp;logoColor=ffffff&amp;color=7389D8&amp;labelColor=6A7EC2)](https://discord.gg/bevy)

## What is Bevy?

Bevy is a refreshingly simple data-driven game engine built in Rust. It is free and open-source forever!

## WARNING

Bevy is still in the early stages of development. Important features are missing. Documentation is sparse. A new version of Bevy containing breaking changes to the API is released [approximately once every 3 months](https://bevy.org/news/bevy-0-6/#the-train-release-schedule). We provide [migration guides](https://bevy.org/learn/migration-guides/), but we can&#039;t guarantee migrations will always be easy. Use only if you are willing to work in this environment.

**MSRV:** Bevy relies heavily on improvements in the Rust language and compiler.
As a result, the Minimum Supported Rust Version (MSRV) is generally close to &quot;the latest stable release&quot; of Rust.

## Design Goals

* **Capable**: Offer a complete 2D and 3D feature set
* **Simple**: Easy for newbies to pick up, but infinitely flexible for power users
* **Data Focused**: Data-oriented architecture using the Entity Component System paradigm
* **Modular**: Use only what you need. Replace what you don&#039;t like
* **Fast**: App logic should run quickly, and when possible, in parallel
* **Productive**: Changes should compile quickly ... waiting isn&#039;t fun

## About

* **[Features](https://bevy.org):** A quick overview of Bevy&#039;s features.
* **[News](https://bevy.org/news/)**: A development blog that covers our progress, plans and shiny new features.

## Docs

* **[Quick Start Guide](https://bevy.org/learn/quick-start/introduction):** Bevy&#039;s official Quick Start Guide. The best place to start learning Bevy.
* **[Bevy Rust API Docs](https://docs.rs/bevy):** Bevy&#039;s Rust API docs, which are automatically generated from the doc comments in this repo.
* **[Official Examples](https://github.com/bevyengine/bevy/tree/latest/examples):** Bevy&#039;s dedicated, runnable examples, which are great for digging into specific concepts.
* **[Community-Made Learning Resources](https://bevy.org/assets/#learning)**: More tutorials, documentation, and examples made by the Bevy community.

## Community

Before contributing or participating in discussions with the community, you should familiarize yourself with our [**Code of Conduct**](./CODE_OF_CONDUCT.md).

* **[Discord](https://discord.gg/bevy):** Bevy&#039;s official discord server.
* **[Reddit](https://reddit.com/r/bevy):** Bevy&#039;s official subreddit.
* **[GitHub Discussions](https://github.com/bevyengine/bevy/discussions):** The best place for questions about Bevy, answered right here!
* **[Bevy Assets](https://bevy.org/assets/):** A collection of awesome Bevy projects, tools, plugins and learning materials.

### Contributing

If you&#039;d like to help build Bevy, check out the **[Contributor&#039;s Guide](https://bevy.org/learn/contribute/introduction)**.
For simple problems, feel free to [open an issue](https://github.com/bevyengine/bevy/issues) or
[PR](https://github.com/bevyengine/bevy/pulls) and tackle it yourself!

For more complex architecture decisions and experimental mad science, please open an [RFC](https://github.com/bevyengine/rfcs) (Request For Comments) so we can brainstorm together effectively!

## Getting Started

We recommend checking out the [Quick Start Guide](https://bevy.org/learn/quick-start/introduction) for a brief introduction.

Follow the [Setup guide](https://bevy.org/learn/quick-start/getting-started/setup) to ensure your development environment is set up correctly.
Once set up, you can quickly try out the [examples](https://github.com/bevyengine/bevy/tree/latest/examples) by cloning this repo and running the following commands:

```sh
# Switch to the correct version (latest release, default is main development branch)
git checkout latest
# Runs the &quot;breakout&quot; example
cargo run --example breakout
```

To draw a window with standard functionality enabled, use:

```rust
use bevy::prelude::*;

fn main() {
  App::new()
    .add_plugins(DefaultPlugins)
    .run();
}
```

### Fast Compiles

Bevy can be built just fine using default configuration on stable Rust. However for really fast iterative compiles, you should enable the &quot;fast compiles&quot; setup by [following the instructions here](https://bevy.org/learn/quick-start/getting-started/setup).

## [Bevy Cargo Features][cargo_features]

This [list][cargo_features] outlines the different cargo features supported by Bevy. These allow you to customize the Bevy feature set for your use-case.

[cargo_features]: docs/cargo_features.md

## Thanks

Bevy is the result of the hard work of many people. A huge thanks to all Bevy contributors, the many open source projects that have come before us, the [Rust gamedev ecosystem](https://arewegameyet.rs/), and the many libraries we build on.

A huge thanks to Bevy&#039;s [generous sponsors](https://bevy.org). Bevy will always be free and open source, but it isn&#039;t free to make. Please consider [sponsoring our work](https://bevy.org/donate/) if you like what we&#039;re building.

&lt;!-- This next line need to stay exactly as is. It is required for BrowserStack sponsorship. --&gt;
This project is tested with BrowserStack.

## License

Bevy is free, open source and permissively licensed!
Except where noted (below and/or in individual files), all code in this repository is dual-licensed under either:

* MIT License ([LICENSE-MIT](LICENSE-MIT) or [http://opensource.org/licenses/MIT](http://opensource.org/licenses/MIT))
* Apache License, Version 2.0 ([LICENSE-APACHE](LICENSE-APACHE) or [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0))

at your option.
This means you can select the license you prefer!
This dual-licensing approach is the de-facto standard in the Rust ecosystem and there are [very good reasons](https://github.com/bevyengine/bevy/issues/2373) to include both.

Some of the engine&#039;s code carries additional copyright notices and license terms due to their external origins.
These are generally BSD-like, but exact details vary by crate:
If the README of a crate contains a &#039;License&#039; header (or similar), the additional copyright notices and license terms applicable to that crate will be listed.
The above licensing requirement still applies to contributions to those crates, and sections of those crates will carry those license terms.
The [license](https://doc.rust-lang.org/cargo/reference/manifest.html#the-license-and-license-file-fields) field of each crate will also reflect this.
For example, [`bevy_mikktspace`](./crates/bevy_mikktspace/README.md#license-agreement) has code under the Zlib license (as well as a copyright notice when choosing the MIT license).

The [assets](assets) included in this repository (for our [examples](./examples/README.md)) typically fall under different open licenses.
These will not be included in your game (unless copied in by you), and they are not distributed in the published bevy crates.
See [CREDITS.md](CREDITS.md) for the details of the licenses of those files.

### Your contributions

Unless you explicitly state otherwise,
any contribution intentionally submitted for inclusion in the work by you,
as defined in the Apache-2.0 license,
shall be dual licensed as above,
without any additional terms or conditions.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
    </channel>
</rss>