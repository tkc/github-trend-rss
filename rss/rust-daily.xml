<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for rust - Rust Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for rust.</description>
        <lastBuildDate>Fri, 04 Jul 2025 00:05:21 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[EasyTier/EasyTier]]></title>
            <link>https://github.com/EasyTier/EasyTier</link>
            <guid>https://github.com/EasyTier/EasyTier</guid>
            <pubDate>Fri, 04 Jul 2025 00:05:21 GMT</pubDate>
            <description><![CDATA[A simple, decentralized mesh VPN with WireGuard support.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/EasyTier/EasyTier">EasyTier/EasyTier</a></h1>
            <p>A simple, decentralized mesh VPN with WireGuard support.</p>
            <p>Language: Rust</p>
            <p>Stars: 5,394</p>
            <p>Forks: 463</p>
            <p>Stars today: 28 stars today</p>
            <h2>README</h2><pre># EasyTier

[![Github release](https://img.shields.io/github/v/tag/EasyTier/EasyTier)](https://github.com/EasyTier/EasyTier/releases)
[![GitHub](https://img.shields.io/github/license/EasyTier/EasyTier)](https://github.com/EasyTier/EasyTier/blob/main/LICENSE)
[![GitHub last commit](https://img.shields.io/github/last-commit/EasyTier/EasyTier)](https://github.com/EasyTier/EasyTier/commits/main)
[![GitHub issues](https://img.shields.io/github/issues/EasyTier/EasyTier)](https://github.com/EasyTier/EasyTier/issues)
[![GitHub Core Actions](https://github.com/EasyTier/EasyTier/actions/workflows/core.yml/badge.svg)](https://github.com/EasyTier/EasyTier/actions/workflows/core.yml)
[![GitHub GUI Actions](https://github.com/EasyTier/EasyTier/actions/workflows/gui.yml/badge.svg)](https://github.com/EasyTier/EasyTier/actions/workflows/gui.yml)
[![GitHub Test Actions](https://github.com/EasyTier/EasyTier/actions/workflows/test.yml/badge.svg)](https://github.com/EasyTier/EasyTier/actions/workflows/test.yml)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/EasyTier/EasyTier)

[ÁÆÄ‰Ωì‰∏≠Êñá](/README_CN.md) | [English](/README.md)

**Please visit the [EasyTier Official Website](https://easytier.cn/en/) to view the full documentation.**

EasyTier is a simple, safe and decentralized VPN networking solution implemented with the Rust language and Tokio framework.

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;assets/image-5.png&quot; width=&quot;300&quot;&gt;
&lt;img src=&quot;assets/image-4.png&quot; width=&quot;300&quot;&gt;
&lt;/p&gt;

## Features

- **Decentralized**: No need to rely on centralized services, nodes are equal and independent.
- **Safe**: Use WireGuard protocol to encrypt data.
- **High Performance**: Full-link zero-copy, with performance comparable to mainstream networking software.
- **Cross-platform**: Supports MacOS/Linux/Windows/Android, will support IOS in the future. The executable file is statically linked, making deployment simple.
- **Networking without public IP**: Supports networking using shared public nodes, refer to [Configuration Guide](#Networking-without-public-IP)
- **NAT traversal**: Supports UDP-based NAT traversal, able to establish stable connections even in complex network environments.
- **Subnet Proxy (Point-to-Network)**: Nodes can expose accessible network segments as proxies to the VPN subnet, allowing other nodes to access these subnets through the node.
- **Smart Routing**: Selects links based on traffic to reduce latency and increase throughput.
- **TCP Support**: Provides reliable data transmission through concurrent TCP links when UDP is limited, optimizing performance.
- **High Availability**: Supports multi-path and switches to healthy paths when high packet loss or network errors are detected.
- **IPv6 Support**: Supports networking using IPv6.
- **Multiple Protocol Types**: Supports communication between nodes using protocols such as WebSocket and QUIC.
- **Web Management Interface**: Provides a [web-based management](https://easytier.cn/web) interface for easy configuration and monitoring.

## Installation

1. **Download the precompiled binary file**

    Visit the [GitHub Release page](https://github.com/EasyTier/EasyTier/releases) to download the binary file suitable for your operating system. Release includes both command-line programs and GUI programs in the compressed package.

2. **Install via crates.io**

    ```sh
    cargo install easytier
    ```

3. **Install from source code**

    ```sh
    cargo install --git https://github.com/EasyTier/EasyTier.git easytier
    ```

4. **Install by Docker Compose**

    Please visit the [EasyTier Official Website](https://easytier.cn/en/) to view the full documentation.

5. **Install by script (For Linux Only)**

    ```sh
    wget -O /tmp/easytier.sh &quot;https://raw.githubusercontent.com/EasyTier/EasyTier/main/script/install.sh&quot; &amp;&amp; bash /tmp/easytier.sh install
    ```

    The script supports the following commands and options:

    Commands:
    - `install`: Install EasyTier
    - `uninstall`: Uninstall EasyTier
    - `update`: Update EasyTier to the latest version
    - `help`: Show help message

    Options:
    - `--skip-folder-verify`: Skip folder verification during installation
    - `--skip-folder-fix`: Skip automatic folder path fixing
    - `--no-gh-proxy`: Disable GitHub proxy
    - `--gh-proxy`: Set custom GitHub proxy URL (default: https://ghfast.top/)

    Examples:
    ```sh
    # Show help
    bash /tmp/easytier.sh help

    # Install with options
    bash /tmp/easytier.sh install --skip-folder-verify
    bash /tmp/easytier.sh install --no-gh-proxy
    bash /tmp/easytier.sh install --gh-proxy https://your-proxy.com/

    # Update EasyTier
    bash /tmp/easytier.sh update

    # Uninstall EasyTier
    bash /tmp/easytier.sh uninstall
    ```

6. **Install by Homebrew (For MacOS Only)**

    ```sh
    brew tap brewforge/chinese
    brew install --cask easytier-gui
    ```

## Quick Start

&gt; The following text only describes the use of the command-line tool; the GUI program can be configured by referring to the following concepts.

Make sure EasyTier is installed according to the [Installation Guide](#Installation), and both easytier-core and easytier-cli commands are available.

### Two-node Networking

Assuming the network topology of the two nodes is as follows

```mermaid
flowchart LR

subgraph Node A IP 22.1.1.1
nodea[EasyTier\n10.144.144.1]
end

subgraph Node B
nodeb[EasyTier\n10.144.144.2]
end

nodea &lt;-----&gt; nodeb

```

1. Execute on Node A:

    ```sh
    sudo easytier-core --ipv4 10.144.144.1
    ```

    Successful execution of the command will print the following.

    ![alt text](/assets/image-2.png)

2. Execute on Node B

    ```sh
    sudo easytier-core --ipv4 10.144.144.2 --peers udp://22.1.1.1:11010
    ```

3. Test Connectivity

    The two nodes should connect successfully and be able to communicate within the virtual subnet

    ```sh
    ping 10.144.144.2
    ```

    Use easytier-cli to view node information in the subnet

    ```sh
    easytier-cli peer
    ```

    ![alt text](/assets/image.png)

    ```sh
    easytier-cli route
    ```

    ![alt text](/assets/image-1.png)


    ```sh
    easytier-cli node
    ```

    ![alt text](assets/image-10.png)

---

### Multi-node Networking

Based on the two-node networking example just now, if more nodes need to join the virtual network, you can use the following command.

```sh
sudo easytier-core --ipv4 10.144.144.2 --peers udp://22.1.1.1:11010
```

The `--peers` parameter can fill in the listening address of any node already in the virtual network.

---

### Subnet Proxy (Point-to-Network) Configuration

Assuming the network topology is as follows, Node B wants to share its accessible subnet 10.1.1.0/24 with other nodes.

```mermaid
flowchart LR

subgraph Node A IP 22.1.1.1
nodea[EasyTier\n10.144.144.1]
end

subgraph Node B
nodeb[EasyTier\n10.144.144.2]
end

id1[[10.1.1.0/24]]

nodea &lt;--&gt; nodeb &lt;-.-&gt; id1

```

Then the startup parameters for Node B&#039;s easytier are (new -n parameter)

```sh
sudo easytier-core --ipv4 10.144.144.2 -n 10.1.1.0/24
```

Subnet proxy information will automatically sync to each node in the virtual network, and each node will automatically configure the corresponding route. Node A can check whether the subnet proxy is effective through the following command.

1. Check whether the routing information has been synchronized, the proxy_cidrs column shows the proxied subnets.

    ```sh
    easytier-cli route
    ```

   ![alt text](/assets/image-3.png)

2. Test whether Node A can access nodes under the proxied subnet

    ```sh
    ping 10.1.1.2
    ```

---

### Networking without Public IP

EasyTier supports networking using shared public nodes. The currently deployed shared public node is ``tcp://public.easytier.cn:11010``.

When using shared nodes, each node entering the network needs to provide the same ``--network-name`` and ``--network-secret`` parameters as the unique identifier of the network.

Taking two nodes as an example, Node A executes:

```sh
sudo easytier-core -i 10.144.144.1 --network-name abc --network-secret abc -p tcp://public.easytier.cn:11010
```

Node B executes

```sh
sudo easytier-core --ipv4 10.144.144.2 --network-name abc --network-secret abc -p tcp://public.easytier.cn:11010
```

After the command is successfully executed, Node A can access Node B through the virtual IP 10.144.144.2.

### Use EasyTier with WireGuard Client

EasyTier can be used as a WireGuard server to allow any device with WireGuard client installed to access the EasyTier network. For platforms currently unsupported by EasyTier (such as iOS, Android, etc.), this method can be used to connect to the EasyTier network.

Assuming the network topology is as follows:

```mermaid
flowchart LR

ios[[iPhone \n WireGuard Installed]]

subgraph Node A IP 22.1.1.1
nodea[EasyTier\n10.144.144.1]
end

subgraph Node B
nodeb[EasyTier\n10.144.144.2]
end

id1[[10.1.1.0/24]]

ios &lt;-.-&gt; nodea &lt;--&gt; nodeb &lt;-.-&gt; id1
```

To enable an iPhone to access the EasyTier network through Node A, the following configuration can be applied:

Include the --vpn-portal parameter in the easytier-core command on Node A to specify the port that the WireGuard service listens on and the subnet used by the WireGuard network.

```sh
# The following parameters mean: listen on port 0.0.0.0:11013, and use the 10.14.14.0/24 subnet for WireGuard
sudo easytier-core --ipv4 10.144.144.1 --vpn-portal wg://0.0.0.0:11013/10.14.14.0/24
```

After successfully starting easytier-core, use easytier-cli to obtain the WireGuard client configuration.

```sh
$&gt; easytier-cli vpn-portal
portal_name: wireguard

############### client_config_start ###############

[Interface]
PrivateKey = 9VDvlaIC9XHUvRuE06hD2CEDrtGF+0lDthgr9SZfIho=
Address = 10.14.14.0/32 # should assign an ip from this cidr manually

[Peer]
PublicKey = zhrZQg4QdPZs8CajT3r4fmzcNsWpBL9ImQCUsnlXyGM=
AllowedIPs = 10.144.144.0/24,10.14.14.0/24
Endpoint = 0.0.0.0:11013 # should be the public ip(or domain) of the vpn server
PersistentKeepalive = 25

############### client_config_end ###############

connected_clients:
[]
```

Before using the Client Config, you need to modify the Interface Address and Peer Endpoint to the client&#039;s IP and the IP of the EasyTier node, respectively. Import the configuration file into the WireGuard client to access the EasyTier network.

### Self-Hosted Public Server

Every virtual network (with same network name and secret) can act as a public server cluster. Nodes of other network can connect to arbitrary nodes in public server cluster to discover each other without public IP.

Run you own public server cluster is exactly same as running an virtual network, except that you can skip config the ipv4 addr.

You can also join the official public server cluster with following command:

```
sudo easytier-core --network-name easytier --network-secret easytier -p tcp://public.easytier.cn:11010
```


### Configurations

You can use ``easytier-core --help`` to view all configuration items

## Roadmap

- [ ] Support features such TCP hole punching, KCP, FEC etc.
- [ ] Support iOS.

## Community and Contribution

We welcome and encourage community contributions! If you want to get involved, please submit a [GitHub PR](https://github.com/EasyTier/EasyTier/pulls). Detailed contribution guidelines can be found in [CONTRIBUTING.md](https://github.com/EasyTier/EasyTier/blob/main/CONTRIBUTING.md).

## Related Projects and Resources

- [ZeroTier](https://www.zerotier.com/): A global virtual network for connecting devices.
- [TailScale](https://tailscale.com/): A VPN solution aimed at simplifying network configuration.
- [vpncloud](https://github.com/dswd/vpncloud): A P2P Mesh VPN
- [Candy](https://github.com/lanthora/candy): A reliable, low-latency, and anti-censorship virtual private network

## License

EasyTier is released under the [Apache License 2.0](https://github.com/EasyTier/EasyTier/blob/main/LICENSE).

## Contact

- Ask questions or report problems: [GitHub Issues](https://github.com/EasyTier/EasyTier/issues)
- Discussion and exchange: [GitHub Discussions](https://github.com/EasyTier/EasyTier/discussions)
- TelegramÔºöhttps://t.me/easytier
- QQ Group: 949700262

## Sponsor

&lt;img src=&quot;assets/image-8.png&quot; width=&quot;300&quot;&gt;
&lt;img src=&quot;assets/image-9.png&quot; width=&quot;300&quot;&gt;
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[BoundaryML/baml]]></title>
            <link>https://github.com/BoundaryML/baml</link>
            <guid>https://github.com/BoundaryML/baml</guid>
            <pubDate>Fri, 04 Jul 2025 00:05:20 GMT</pubDate>
            <description><![CDATA[The AI framework that adds the engineering to prompt engineering (Python/TS/Ruby/Java/C#/Rust/Go compatible)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/BoundaryML/baml">BoundaryML/baml</a></h1>
            <p>The AI framework that adds the engineering to prompt engineering (Python/TS/Ruby/Java/C#/Rust/Go compatible)</p>
            <p>Language: Rust</p>
            <p>Stars: 4,317</p>
            <p>Forks: 191</p>
            <p>Stars today: 34 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://boundaryml.com?utm_source=github&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;fern/assets/baml-lamb-white.png&quot;&gt;
    &lt;img src=&quot;fern/assets/baml-lamb-white.png&quot; height=&quot;64&quot; id=&quot;top&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt;

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

[![BAML Version](https://img.shields.io/pypi/v/baml-py?color=006dad&amp;label=BAML%20Version)](https://pypi.org/project/baml-py/)


## BAML: Basically a Made-up Language
&lt;h4&gt;

[Homepage](https://www.boundaryml.com/) | [Docs](https://docs.boundaryml.com) | [BAML AI Chat](https://www.boundaryml.com/chat) | [Discord](https://discord.gg/BTNBeXGuaS)



&lt;/h4&gt;


&lt;/div&gt;

BAML is a simple prompting language for building reliable **AI workflows and agents**.

BAML makes prompt engineering easy by turning it into _schema engineering_ -- where you mostly focus on the models of your prompt -- to get more reliable outputs. 
You don&#039;t need to write your whole app in BAML, only the prompts! You can wire-up your LLM Functions in any language of your choice! See our quickstarts for [Python](https://docs.boundaryml.com/guide/installation-language/python), [TypeScript](https://docs.boundaryml.com/guide/installation-language/typescript), [Ruby](https://docs.boundaryml.com/guide/installation-language/ruby) and [Go, and more](https://docs.boundaryml.com/guide/installation-language/rest-api-other-languages).

BAML comes with all batteries included -- with full typesafety, streaming, retries, wide model support, even when they don&#039;t support native [tool-calling APIs](#enable-reliable-tool-calling-with-any-model-even-when-they-dont-support-it)

**Try BAML**: [Prompt Fiddle](https://www.promptfiddle.com) ‚Ä¢ [Interactive App Examples](https://baml-examples.vercel.app/)


## The core BAML principle: LLM Prompts are functions

The fundamental building block in BAML is a function. Every prompt is a function that takes in parameters and returns a type.

```rust
function ChatAgent(message: Message[], tone: &quot;happy&quot; | &quot;sad&quot;) -&gt; string
```

Every function additionally defines which models it uses and what its prompt is.

```rust
function ChatAgent(message: Message[], tone: &quot;happy&quot; | &quot;sad&quot;) -&gt; StopTool | ReplyTool {
    client &quot;openai/gpt-4o-mini&quot;

    prompt #&quot;
        Be a {{ tone }} bot.

        {{ ctx.output_format }}

        {% for m in message %}
        {{ _.role(m.role) }}
        {{ m.content }}
        {% endfor %}
    &quot;#
}

class Message {
    role string
    content string
}

class ReplyTool {
  response string
}

class StopTool {
  action &quot;stop&quot; @description(#&quot;
    when it might be a good time to end the conversation
  &quot;#)
}
```

## BAML Functions can be called from any language
Below we call the ChatAgent function we defined in BAML through Python. BAML&#039;s Rust compiler generates a &quot;baml_client&quot; to access and call them.

```python
from baml_client import b
from baml_client.types import Message, StopTool

messages = [Message(role=&quot;assistant&quot;, content=&quot;How can I help?&quot;)]

while True:
  print(messages[-1].content)
  user_reply = input()
  messages.append(Message(role=&quot;user&quot;, content=user_reply))
  tool = b.ChatAgent(messages, &quot;happy&quot;)
  if isinstance(tool, StopTool):
    print(&quot;Goodbye!&quot;)
    break
  else:
    messages.append(Message(role=&quot;assistant&quot;, content=tool.response))
```
You can write any kind of agent or workflow using chained BAML functions. An agent is a while loop that calls a Chat BAML Function with some state.

And if you need to stream, add a couple more lines:
```python
stream = b.stream.ChatAgent(messages, &quot;happy&quot;)
# partial is a Partial type with all Optional fields
for tool in stream:
    if isinstance(tool, StopTool):
      ...
    
final = stream.get_final_response()
```
And get fully type-safe outputs for each chunk in the stream.

## Test prompts 10x faster, right in your IDE
BAML comes with native tooling for VSCode (jetbrains + neovim coming soon). 

**Visualize full prompt (including any multi-modal assets), and the API request**. BAML gives you full transparency and control of the prompt.

![raw-curl](https://github.com/user-attachments/assets/c0b34db9-80cd-45a7-a356-6b5ab4a9c5b7)

**Using AI is all about iteration speed.**

If testing your pipeline takes 2 minutes, you can only test 10 ideas in 20 minutes.

If you reduce it to 5 seconds, you can test 240 ideas in the same amount of time.
![resume-attempt2-smaller2](https://github.com/user-attachments/assets/6fc6b8a6-ffed-4cfc-80b8-78bc8a3d66a6)

The playground also allows you to run tests in parallel -- for even faster iteration speeds üöÄ.

No need to login to websites, and no need to manually define json schemas.

## Enable reliable tool-calling with any model
BAML works even when the models don&#039;t support native tool-calling APIs. We created the SAP (schema-aligned parsing) algorithm to support the flexible outputs LLMs can provide, like markdown within a JSON blob or chain-of-thought prior to answering. [Read more about SAP](https://www.boundaryml.com/blog/schema-aligned-parsing)

With BAML, your structured outputs work in Day-1 of a model release. No need to figure out whether a model supports parallel tool calls, or whether it supports recursive schemas, or `anyOf` or `oneOf` etc.

See it in action with: **[Deepseek-R1](https://www.boundaryml.com/blog/deepseek-r1-function-calling)** and [OpenAI O1](https://www.boundaryml.com/blog/openai-o1).



## Switch from 100s of models in a couple lines
```diff
function Extract() -&gt; Resume {
+  client openai/o3-mini
  prompt #&quot;
    ....
  &quot;#
}
```
[Retry policies](https://docs.boundaryml.com/ref/llm-client-strategies/retry-policy) ‚Ä¢ [fallbacks](https://docs.boundaryml.com/ref/llm-client-strategies/fallback) ‚Ä¢ [model rotations](https://docs.boundaryml.com/ref/llm-client-strategies/round-robin). All statically defined.
![Fallback Retry](https://www.boundaryml.com/blog/2025-01-24-ai-agents-need-a-new-syntax/06-fallback-retry.gif)
Want to do pick models at runtime? Check out the [Client Registry](https://docs.boundaryml.com/guide/baml-advanced/llm-client-registry).

We support: [OpenAI](https://docs.boundaryml.com/ref/llm-client-providers/open-ai) ‚Ä¢ [Anthropic](https://docs.boundaryml.com/ref/llm-client-providers/anthropic) ‚Ä¢ [Gemini](https://docs.boundaryml.com/ref/llm-client-providers/google-ai-gemini) ‚Ä¢ [Vertex](https://docs.boundaryml.com/ref/llm-client-providers/google-vertex) ‚Ä¢ [Bedrock](https://docs.boundaryml.com/ref/llm-client-providers/aws-bedrock) ‚Ä¢ [Azure OpenAI](https://docs.boundaryml.com/ref/llm-client-providers/open-ai-from-azure) ‚Ä¢ [Anything OpenAI Compatible](https://docs.boundaryml.com/ref/llm-client-providers/openai-generic) ([Ollama](https://docs.boundaryml.com/ref/llm-client-providers/openai-generic-ollama), [OpenRouter](https://docs.boundaryml.com/ref/llm-client-providers/openai-generic-open-router), [VLLM](https://docs.boundaryml.com/ref/llm-client-providers/openai-generic-v-llm), [LMStudio](https://docs.boundaryml.com/ref/llm-client-providers/openai-generic-lm-studio), [TogetherAI](https://docs.boundaryml.com/ref/llm-client-providers/openai-generic-together-ai), and more)

## Build beautiful streaming UIs
BAML generates a ton of utilities for NextJS, Python (and any language) to make streaming UIs easy.
![recipe-generator](https://github.com/user-attachments/assets/cf82495b-21fc-40bf-ae98-93eef923d620)

BAML&#039;s streaming interfaces are fully type-safe. Check out the [Streaming Docs](https://docs.boundaryml.com/guide/baml-basics/streaming), and our [React hooks](https://docs.boundaryml.com/guide/framework-integration/react-next-js/quick-start)

## Fully Open-Source, and offline
- 100% open-source (Apache 2)
- 100% private. AGI will not require an internet connection, neither will BAML
    - No network requests beyond model calls you explicitly set
    - Not stored or used for any training data
- BAML files can be saved locally on your machine and checked into Github for easy diffs.
- Built in Rust. So fast, you can&#039;t even tell it&#039;s there.

## BAML&#039;s Design Philosophy

Everything is fair game when making new syntax. If you can code it, it can be yours. This is our design philosophy to help restrict ideas:

- **1:** Avoid invention when possible
    - Yes, prompts need versioning ‚Äî we have a great versioning tool: git
    - Yes, you need to save prompts ‚Äî we have a great storage tool: filesystems
- **2:** Any file editor and any terminal should be enough to use it
- **3:** Be fast
- **4:** A first year university student should be able to understand it

## Why a new programming language

We used to write websites like this:

```python
def home():
    return &quot;&lt;button onclick=\&quot;() =&gt; alert(\\\&quot;hello!\\\&quot;)\&quot;&gt;Click&lt;/button&gt;&quot;
```

And now we do this:

```jsx
function Home() {
  return &lt;button onClick={() =&gt; setCount(prev =&gt; prev + 1)}&gt;
          {count} clicks!
         &lt;/button&gt;
}
```

New syntax can be incredible at expressing new ideas. Plus the idea of maintaining hundreds of f-strings for prompts kind of disgusts us ü§Æ. Strings are bad for maintainable codebases. We prefer structured strings.

The goal of BAML is to give you the expressiveness of English, but the structure of code.

Full [blog post](https://www.boundaryml.com/blog/ai-agents-need-new-syntax) by us.


## Conclusion

As models get better, we&#039;ll continue expecting even more out of them. But what will never change is that we&#039;ll want a way to write maintainable code that uses those models. The current way we all just assemble strings is very reminiscent of the early days PHP/HTML soup in web development. We hope some of the ideas we shared today can make a tiny dent in helping us all shape the way we all code tomorrow.

## FAQ
|   |   |
| - | - |
| Do I need to write my whole app in BAML? | Nope, only the prompts! BAML translates definitions into the language of your choice! [Python](https://docs.boundaryml.com/guide/installation-language/python), [TypeScript](https://docs.boundaryml.com/guide/installation-language/typescript), [Ruby](https://docs.boundaryml.com/guide/installation-language/ruby) and [more](https://docs.boundaryml.com/guide/installation-language/rest-api-other-languages). |
| Is BAML stable? | Yes, many companies use it in production! We ship updates weekly! |
| Why a new language? | [Jump to section](#why-a-new-programming-language) |


## Contributing
Checkout our [guide on getting started](/CONTRIBUTING.md)

## Citation

You can cite the BAML repo as follows:
```bibtex
@software{baml,
  author = {Vaibhav Gupta, Aaron Villalpando and Boundary ML team},
  title = {BAML},
  url = {https://github.com/boundaryml/baml},
  year = {2024}
}
```

---

Made with ‚ù§Ô∏è by Boundary

HQ in Seattle, WA

P.S. We&#039;re hiring for software engineers that love rust. [Email us](founders@boundaryml.com) or reach out on [discord](https://discord.gg/ENtBB6kkXH)!

&lt;div align=&quot;left&quot; style=&quot;align-items: left;&quot;&gt;
        &lt;a href=&quot;#top&quot;&gt;
            &lt;img src=&quot;https://img.shields.io/badge/Back%20to%20Top-000000?style=for-the-badge&amp;logo=github&amp;logoColor=white&quot; alt=&quot;Back to Top&quot;&gt;
        &lt;/a&gt;
&lt;/div&gt;

&lt;img src=&quot;https://imgs.xkcd.com/comics/standards.png&quot; alt_text=&quot;hi&quot; /&gt;
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[get-convex/convex-backend]]></title>
            <link>https://github.com/get-convex/convex-backend</link>
            <guid>https://github.com/get-convex/convex-backend</guid>
            <pubDate>Fri, 04 Jul 2025 00:05:19 GMT</pubDate>
            <description><![CDATA[The open-source reactive database for app developers]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/get-convex/convex-backend">get-convex/convex-backend</a></h1>
            <p>The open-source reactive database for app developers</p>
            <p>Language: Rust</p>
            <p>Stars: 5,468</p>
            <p>Forks: 284</p>
            <p>Stars today: 42 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://static.convex.dev/logo/convex-logo-light.svg&quot; width=&quot;600&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://static.convex.dev/logo/convex-logo.svg&quot; width=&quot;600&quot;&gt;
  &lt;img alt=&quot;Convex logo&quot; src=&quot;https://static.convex.dev/logo/convex-logo.svg&quot; width=&quot;600&quot;&gt;
&lt;/picture&gt;
&lt;/p&gt;

[Convex](https://convex.dev) is the open-source reactive database designed to
make life easy for web app developers, whether human or LLM. Fetch data and
perform business logic with strong consistency by writing pure TypeScript.

Convex provides a database, a place to write your server functions, and client
libraries. It makes it easy to build and scale dynamic live-updating apps.
[Read the docs to learn more](https://docs.convex.dev/understanding/).

Development of the Convex backend is led by the Convex team. We
[welcome bug fixes](./CONTRIBUTING.md) and
[love receiving feedback](https://discord.gg/convex). We keep this repository
synced with any internal development work within a handful of days.

## Getting Started

Visit our [documentation](https://docs.convex.dev/) to learn more about Convex
and follow our getting started guides.

The easiest way to build with Convex is through our
[cloud platform](https://www.convex.dev/plans), which includes a generous free
tier and lets you focus on building your application without worrying about
infrastructure. Many small applications and side-projects can operate entirely
on the free tier with zero cost and zero maintenance.

## Self Hosting

The self-hosted product includes most features of the cloud product, including
the dashboard and CLI. Self-hosted Convex works well with a variety of tools
including Neon, Fly.io, Vercel, Netlify, RDS, Sqlite, Postgres, and more.

You can either use Docker (recommended) or a prebuilt binary to self host
Convex. Check out our [self-hosting guide](./self-hosted/README.md) for detailed
instructions. Community support for self-hosting is available in the
`#self-hosted` channel on [Discord](https://discord.gg/convex).

## Community &amp; Support

- Join our [Discord community](https://discord.gg/convex) for help and
  discussions.
- Report issues when building and using the open source Convex backend through
  [GitHub Issues](https://github.com/get-convex/convex-backend/issues)

## Building from source

See [BUILD.md](./BUILD.md).

## Disclaimers

- If you choose to self-host, we recommend following the self-hosting guide. If
  you are instead building from source, make sure to change your instance secret
  and admin key from the defaults in the repo.
- Convex is battle tested most thoroughly on Linux and Mac. On Windows, it has
  less experience. If you run into issues, please message us on
  [Discord](https://convex.dev/community) in the `#self-hosted` channel.
- Convex self-hosted builds contain a beacon to help Convex improve the product.
  The information is minimal and anonymous and helpful to Convex, but if you
  really want to disable it, you can set the `--disable-beacon` flag on the
  backend binary. The beacon&#039;s messages print in the log and only include
  - A random identifier for your deployment (not used elsewhere)
  - Migration version of your database
  - Git rev of the backend
  - Uptime of the backend

## Repository layout

- `crates/` contains Rust code

  - Main binary
    - `local_backend/` is an application server on top of the `Runtime`. This is
      the serving edge for the Convex cloud.

- `npm-packages/` contains both our public and internal TypeScript packages.
  - Internal packages
    - `udf-runtime/` sets up the user-defined functions JS environment for
      queries and mutations
    - `udf-tests/` is a collection of functions used in testing the isolate
      layer
    - `system-udfs/` contains functions used by the Convex system e.g. the CLI
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[zama-ai/tfhe-rs]]></title>
            <link>https://github.com/zama-ai/tfhe-rs</link>
            <guid>https://github.com/zama-ai/tfhe-rs</guid>
            <pubDate>Fri, 04 Jul 2025 00:05:18 GMT</pubDate>
            <description><![CDATA[TFHE-rs: A Pure Rust implementation of the TFHE Scheme for Boolean and Integer Arithmetics Over Encrypted Data.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/zama-ai/tfhe-rs">zama-ai/tfhe-rs</a></h1>
            <p>TFHE-rs: A Pure Rust implementation of the TFHE Scheme for Boolean and Integer Arithmetics Over Encrypted Data.</p>
            <p>Language: Rust</p>
            <p>Stars: 1,269</p>
            <p>Forks: 201</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
&lt;!-- product name logo --&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/zama-ai/tfhe-rs/assets/157474013/5283e0ba-da1e-43af-9f2a-c5221367a12b&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/zama-ai/tfhe-rs/assets/157474013/b94a8c96-7595-400b-9311-70765c706955&quot;&gt;
  &lt;img width=600 alt=&quot;Zama TFHE-rs&quot;&gt;
&lt;/picture&gt;
&lt;/p&gt;

&lt;hr/&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/zama-ai/tfhe-rs-handbook/blob/main/tfhe-rs-handbook.pdf&quot;&gt; üìÉ Read Handbook&lt;/a&gt; |&lt;a href=&quot;https://docs.zama.ai/tfhe-rs&quot;&gt; üìí Documentation&lt;/a&gt; | &lt;a href=&quot;https://zama.ai/community&quot;&gt; üíõ Community support&lt;/a&gt; | &lt;a href=&quot;https://github.com/zama-ai/awesome-zama&quot;&gt; üìö FHE resources by Zama&lt;/a&gt;
&lt;/p&gt;


&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/zama-ai/tfhe-rs/releases&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/zama-ai/tfhe-rs?style=flat-square&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-BSD--3--Clause--Clear-%23ffb243?style=flat-square&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/zama-ai/bounty-program&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Contribute-Zama%20Bounty%20Program-%23ffd208?style=flat-square&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://slsa.dev&quot;&gt;&lt;img alt=&quot;SLSA 3&quot; src=&quot;https://slsa.dev/images/gh-badge-level3.svg&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;

## About

### What is TFHE-rs

**TFHE-rs** is a pure Rust implementation of TFHE for boolean and integer arithmetics over encrypted data.

It includes:
- a **Rust** API
- a **C** API
- and a **client-side WASM** API

TFHE-rs is designed for developers and researchers who want full control over
what they can do with TFHE, while not having to worry about the low-level
implementation. The goal is to have a stable, simple, high-performance, and
production-ready library for all the advanced features of TFHE.
&lt;br&gt;&lt;/br&gt;

### Main features

- **Low-level cryptographic library** that implements Zama‚Äôs variant of TFHE, including programmable bootstrapping
- **Implementation of the original TFHE boolean API** that can be used as a drop-in replacement for other TFHE libraries
- **Short integer API** that enables exact, unbounded FHE integer arithmetics with up to 8 bits of message space
- **Size-efficient public key encryption**
- **Ciphertext and server key compression** for efficient data transfer
- **Full Rust API, C bindings to the Rust High-Level API, and client-side Javascript API using WASM**.

*Learn more about TFHE-rs features in the [documentation](https://docs.zama.ai/tfhe-rs/readme).*
&lt;br&gt;&lt;/br&gt;

## Table of Contents
- **[Getting started](#getting-started)**
   - [Cargo.toml configuration](#cargotoml-configuration)
   - [A simple example](#a-simple-example)
- **[Resources](#resources)**
   - [TFHE deep dive](#tfhe-deep-dive)
   - [Tutorials](#tutorials)
   - [Documentation](#documentation)
- **[Working with TFHE-rs](#working-with-tfhe-rs)**
   - [Disclaimers](#disclaimers)
   - [Citations](#citations)
   - [Contributing](#contributing)
   - [License](#license)
- **[Support](#support)**
&lt;br&gt;&lt;/br&gt;

## Getting started

&gt; [!Important]
&gt; **TFHE-rs** released its first stable version v1.0.0 in February 2025, stabilizing the high-level API for the x86 CPU backend.

### Cargo.toml configuration
To use the latest version of `TFHE-rs` in your project, you first need to add it as a dependency in your `Cargo.toml`:

```toml
tfhe = { version = &quot;*&quot;, features = [&quot;boolean&quot;, &quot;shortint&quot;, &quot;integer&quot;] }
```

&gt; [!Note]
&gt; Note: You need to use Rust version &gt;= 1.84 to compile TFHE-rs.

&gt; [!Note]
&gt; Note: AArch64-based machines are not supported for Windows as it&#039;s currently missing an entropy source to be able to seed the [CSPRNGs](https://en.wikipedia.org/wiki/Cryptographically_secure_pseudorandom_number_generator) used in TFHE-rs.

&lt;p align=&quot;right&quot;&gt;
  &lt;a href=&quot;#about&quot; &gt; ‚Üë Back to top &lt;/a&gt;
&lt;/p&gt;

### A simple example

Here is a full example:

``` rust
use tfhe::prelude::*;
use tfhe::{generate_keys, set_server_key, ConfigBuilder, FheUint32, FheUint8};

fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    // Basic configuration to use homomorphic integers
    let config = ConfigBuilder::default().build();

    // Key generation
    let (client_key, server_keys) = generate_keys(config);

    let clear_a = 1344u32;
    let clear_b = 5u32;
    let clear_c = 7u8;

    // Encrypting the input data using the (private) client_key
    // FheUint32: Encrypted equivalent to u32
    let mut encrypted_a = FheUint32::try_encrypt(clear_a, &amp;client_key)?;
    let encrypted_b = FheUint32::try_encrypt(clear_b, &amp;client_key)?;

    // FheUint8: Encrypted equivalent to u8
    let encrypted_c = FheUint8::try_encrypt(clear_c, &amp;client_key)?;

    // On the server side:
    set_server_key(server_keys);

    // Clear equivalent computations: 1344 * 5 = 6720
    let encrypted_res_mul = &amp;encrypted_a * &amp;encrypted_b;

    // Clear equivalent computations: 6720 &gt;&gt; 5 = 210
    encrypted_a = &amp;encrypted_res_mul &gt;&gt; &amp;encrypted_b;

    // Clear equivalent computations: let casted_a = a as u8;
    let casted_a: FheUint8 = encrypted_a.cast_into();

    // Clear equivalent computations: min(210, 7) = 7
    let encrypted_res_min = &amp;casted_a.min(&amp;encrypted_c);

    // Operation between clear and encrypted data:
    // Clear equivalent computations: 7 &amp; 1 = 1
    let encrypted_res = encrypted_res_min &amp; 1_u8;

    // Decrypting on the client side:
    let clear_res: u8 = encrypted_res.decrypt(&amp;client_key);
    assert_eq!(clear_res, 1_u8);

    Ok(())
}
```

To run this code, use the following command:
&lt;p align=&quot;center&quot;&gt; &lt;code&gt; cargo run --release &lt;/code&gt; &lt;/p&gt;

&gt; [!Note]
&gt; Note that when running code that uses `TFHE-rs`, it is highly recommended
to run in release mode with cargo&#039;s `--release` flag to have the best performances possible.

*Find an example with more explanations in [this part of the documentation](https://docs.zama.ai/tfhe-rs/get-started/quick_start)*

&lt;p align=&quot;right&quot;&gt;
  &lt;a href=&quot;#about&quot; &gt; ‚Üë Back to top &lt;/a&gt;
&lt;/p&gt;



## Resources

### TFHE-rs Handbook
A document containing scientific and technical details about algorithms implemented into the library is available here: [TFHE-rs: A (Practical) Handbook](https://github.com/zama-ai/tfhe-rs-handbook/blob/main/tfhe-rs-handbook.pdf).

### TFHE deep dive
- [TFHE Deep Dive - Part I - Ciphertext types](https://www.zama.ai/post/tfhe-deep-dive-part-1)
- [TFHE Deep Dive - Part II - Encodings and linear leveled operations](https://www.zama.ai/post/tfhe-deep-dive-part-2)
- [TFHE Deep Dive - Part III - Key switching and leveled multiplications](https://www.zama.ai/post/tfhe-deep-dive-part-3)
- [TFHE Deep Dive - Part IV - Programmable Bootstrapping](https://www.zama.ai/post/tfhe-deep-dive-part-4)
&lt;br&gt;&lt;/br&gt;

### Tutorials
- [[Video tutorial] Implement signed integers using TFHE-rs ](https://www.zama.ai/post/video-tutorial-implement-signed-integers-ssing-tfhe-rs)
- [Homomorphic parity bit](https://docs.zama.ai/tfhe-rs/tutorials/parity_bit)
- [Homomorphic case changing on Ascii string](https://docs.zama.ai/tfhe-rs/tutorials/ascii_fhe_string)
- [Boolean SHA256 with TFHE-rs](https://www.zama.ai/post/boolean-sha256-tfhe-rs)
- [Dark market with TFHE-rs](https://www.zama.ai/post/dark-market-tfhe-rs)
- [Regular expression engine with TFHE-rs](https://www.zama.ai/post/regex-engine-tfhe-rs)

*Explore more useful resources in [TFHE-rs tutorials](https://docs.zama.ai/tfhe-rs/tutorials) and [Awesome Zama repo](https://github.com/zama-ai/awesome-zama)*
&lt;br&gt;&lt;/br&gt;
### Documentation

Full, comprehensive documentation is available here: [https://docs.zama.ai/tfhe-rs](https://docs.zama.ai/tfhe-rs).
&lt;p align=&quot;right&quot;&gt;
  &lt;a href=&quot;#about&quot; &gt; ‚Üë Back to top &lt;/a&gt;
&lt;/p&gt;


## Working with TFHE-rs

### Disclaimers

#### Security estimation

Security estimations are done using the
[Lattice Estimator](https://github.com/malb/lattice-estimator)
with `red_cost_model = reduction.RC.BDGL16`.

When a new update is published in the Lattice Estimator, we update parameters accordingly.

### Security model

By default, the parameter sets used in the High-Level API with the x86 CPU backend have a failure probability $\le 2^{128}$ to securely work in the IND-CPA^D model using the algorithmic techniques provided in our code base [1].
If you want to work within the IND-CPA security model, which is less strict than the IND-CPA-D model, the parameter sets can easily be changed and would have slightly better performance. More details can be found in the [TFHE-rs documentation](https://docs.zama.ai/tfhe-rs).

The default parameters used in the High-Level API with the GPU backend are chosen considering the IND-CPA security model, and are selected with a bootstrapping failure probability fixed at $p_{error} \le 2^{-64}$. In particular, it is assumed that the results of decrypted computations are not shared by the secret key owner with any third parties, as such an action can lead to leakage of the secret encryption key. If you are designing an application where decryptions must be shared, you will need to craft custom encryption parameters which are chosen in consideration of the IND-CPA^D security model [2].

[1] Bernard, Olivier, et al. &quot;Drifting Towards Better Error Probabilities in Fully Homomorphic Encryption Schemes&quot;. https://eprint.iacr.org/2024/1718.pdf

[2] Li, Baiyu, et al. &quot;Securing approximate homomorphic encryption using differential privacy.&quot; Annual International Cryptology Conference. Cham: Springer Nature Switzerland, 2022. https://eprint.iacr.org/2022/816.pdf

#### Side-channel attacks

Mitigation for side-channel attacks has not yet been implemented in TFHE-rs,
and will be released in upcoming versions.
&lt;br&gt;&lt;/br&gt;

### Citations
To cite TFHE-rs in academic papers, please use the following entry:

```text
@Misc{TFHE-rs,
  title={{TFHE-rs: A Pure Rust Implementation of the TFHE Scheme for Boolean and Integer Arithmetics Over Encrypted Data}},
  author={Zama},
  year={2022},
  note={\url{https://github.com/zama-ai/tfhe-rs}},
}
```

### Contributing

There are two ways to contribute to TFHE-rs:

- [Open issues](https://github.com/zama-ai/tfhe-rs/issues/new/choose) to report bugs and typos, or to suggest new ideas
- Request to become an official contributor by emailing [hello@zama.ai](mailto:hello@zama.ai).

Becoming an approved contributor involves signing our Contributor License Agreement (CLA). Only approved contributors can send pull requests, so please make sure to get in touch before you do!
&lt;br&gt;&lt;/br&gt;

### License
This software is distributed under the **BSD-3-Clause-Clear** license. Read [this](LICENSE) for more details.

#### FAQ
**Is Zama‚Äôs technology free to use?**
&gt;Zama‚Äôs libraries are free to use under the BSD 3-Clause Clear license only for development, research, prototyping, and experimentation purposes. However, for any commercial use of Zama&#039;s open source code, companies must purchase Zama‚Äôs commercial patent license.
&gt;
&gt;Everything we do is open source and we are very transparent on what it means for our users, you can read more about how we monetize our open source products at Zama in [this blogpost](https://www.zama.ai/post/open-source).

**What do I need to do if I want to use Zama‚Äôs technology for commercial purposes?**
&gt;To commercially use Zama‚Äôs technology you need to be granted Zama‚Äôs patent license. Please contact us hello@zama.ai for more information.

**Do you file IP on your technology?**
&gt;Yes, all Zama‚Äôs technologies are patented.

**Can you customize a solution for my specific use case?**
&gt;We are open to collaborating and advancing the FHE space with our partners. If you have specific needs, please email us at hello@zama.ai.

&lt;p align=&quot;right&quot;&gt;
  &lt;a href=&quot;#about&quot; &gt; ‚Üë Back to top &lt;/a&gt;
&lt;/p&gt;


## Support

&lt;a target=&quot;_blank&quot; href=&quot;https://community.zama.ai&quot;&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/zama-ai/tfhe-rs/assets/157474013/08656d0a-3f44-4126-b8b6-8c601dff5380&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/zama-ai/tfhe-rs/assets/157474013/1c9c9308-50ac-4aab-a4b9-469bb8c536a4&quot;&gt;
  &lt;img alt=&quot;Support&quot;&gt;
&lt;/picture&gt;
&lt;/a&gt;

üåü If you find this project helpful or interesting, please consider giving it a star on GitHub! Your support helps to grow the community and motivates further development.

&lt;p align=&quot;right&quot;&gt;
  &lt;a href=&quot;#about&quot; &gt; ‚Üë Back to top &lt;/a&gt;
&lt;/p&gt;
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[astral-sh/uv]]></title>
            <link>https://github.com/astral-sh/uv</link>
            <guid>https://github.com/astral-sh/uv</guid>
            <pubDate>Fri, 04 Jul 2025 00:05:17 GMT</pubDate>
            <description><![CDATA[An extremely fast Python package and project manager, written in Rust.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/astral-sh/uv">astral-sh/uv</a></h1>
            <p>An extremely fast Python package and project manager, written in Rust.</p>
            <p>Language: Rust</p>
            <p>Stars: 60,207</p>
            <p>Forks: 1,717</p>
            <p>Stars today: 139 stars today</p>
            <h2>README</h2><pre># uv

[![uv](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/uv/main/assets/badge/v0.json)](https://github.com/astral-sh/uv)
[![image](https://img.shields.io/pypi/v/uv.svg)](https://pypi.python.org/pypi/uv)
[![image](https://img.shields.io/pypi/l/uv.svg)](https://pypi.python.org/pypi/uv)
[![image](https://img.shields.io/pypi/pyversions/uv.svg)](https://pypi.python.org/pypi/uv)
[![Actions status](https://github.com/astral-sh/uv/actions/workflows/ci.yml/badge.svg)](https://github.com/astral-sh/uv/actions)
[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;logoColor=white)](https://discord.gg/astral-sh)

An extremely fast Python package and project manager, written in Rust.

&lt;p align=&quot;center&quot;&gt;
  &lt;picture align=&quot;center&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/astral-sh/uv/assets/1309177/03aa9163-1c79-4a87-a31d-7a9311ed9310&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/astral-sh/uv/assets/1309177/629e59c0-9c6e-4013-9ad4-adb2bcf5080d&quot;&gt;
    &lt;img alt=&quot;Shows a bar chart with benchmark results.&quot; src=&quot;https://github.com/astral-sh/uv/assets/1309177/629e59c0-9c6e-4013-9ad4-adb2bcf5080d&quot;&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;i&gt;Installing &lt;a href=&quot;https://trio.readthedocs.io/&quot;&gt;Trio&lt;/a&gt;&#039;s dependencies with a warm cache.&lt;/i&gt;
&lt;/p&gt;

## Highlights

- üöÄ A single tool to replace `pip`, `pip-tools`, `pipx`, `poetry`, `pyenv`, `twine`, `virtualenv`,
  and more.
- ‚ö°Ô∏è [10-100x faster](https://github.com/astral-sh/uv/blob/main/BENCHMARKS.md) than `pip`.
- üóÇÔ∏è Provides [comprehensive project management](#projects), with a
  [universal lockfile](https://docs.astral.sh/uv/concepts/projects/layout#the-lockfile).
- ‚ùáÔ∏è [Runs scripts](#scripts), with support for
  [inline dependency metadata](https://docs.astral.sh/uv/guides/scripts#declaring-script-dependencies).
- üêç [Installs and manages](#python-versions) Python versions.
- üõ†Ô∏è [Runs and installs](#tools) tools published as Python packages.
- üî© Includes a [pip-compatible interface](#the-pip-interface) for a performance boost with a
  familiar CLI.
- üè¢ Supports Cargo-style [workspaces](https://docs.astral.sh/uv/concepts/projects/workspaces) for
  scalable projects.
- üíæ Disk-space efficient, with a [global cache](https://docs.astral.sh/uv/concepts/cache) for
  dependency deduplication.
- ‚è¨ Installable without Rust or Python via `curl` or `pip`.
- üñ•Ô∏è Supports macOS, Linux, and Windows.

uv is backed by [Astral](https://astral.sh), the creators of
[Ruff](https://github.com/astral-sh/ruff).

## Installation

Install uv with our standalone installers:

```bash
# On macOS and Linux.
curl -LsSf https://astral.sh/uv/install.sh | sh
```

```bash
# On Windows.
powershell -ExecutionPolicy ByPass -c &quot;irm https://astral.sh/uv/install.ps1 | iex&quot;
```

Or, from [PyPI](https://pypi.org/project/uv/):

```bash
# With pip.
pip install uv
```

```bash
# Or pipx.
pipx install uv
```

If installed via the standalone installer, uv can update itself to the latest version:

```bash
uv self update
```

See the [installation documentation](https://docs.astral.sh/uv/getting-started/installation/) for
details and alternative installation methods.

## Documentation

uv&#039;s documentation is available at [docs.astral.sh/uv](https://docs.astral.sh/uv).

Additionally, the command line reference documentation can be viewed with `uv help`.

## Features

### Projects

uv manages project dependencies and environments, with support for lockfiles, workspaces, and more,
similar to `rye` or `poetry`:

```console
$ uv init example
Initialized project `example` at `/home/user/example`

$ cd example

$ uv add ruff
Creating virtual environment at: .venv
Resolved 2 packages in 170ms
   Built example @ file:///home/user/example
Prepared 2 packages in 627ms
Installed 2 packages in 1ms
 + example==0.1.0 (from file:///home/user/example)
 + ruff==0.5.0

$ uv run ruff check
All checks passed!

$ uv lock
Resolved 2 packages in 0.33ms

$ uv sync
Resolved 2 packages in 0.70ms
Audited 1 package in 0.02ms
```

See the [project documentation](https://docs.astral.sh/uv/guides/projects/) to get started.

uv also supports building and publishing projects, even if they&#039;re not managed with uv. See the
[publish guide](https://docs.astral.sh/uv/guides/publish/) to learn more.

### Scripts

uv manages dependencies and environments for single-file scripts.

Create a new script and add inline metadata declaring its dependencies:

```console
$ echo &#039;import requests; print(requests.get(&quot;https://astral.sh&quot;))&#039; &gt; example.py

$ uv add --script example.py requests
Updated `example.py`
```

Then, run the script in an isolated virtual environment:

```console
$ uv run example.py
Reading inline script metadata from: example.py
Installed 5 packages in 12ms
&lt;Response [200]&gt;
```

See the [scripts documentation](https://docs.astral.sh/uv/guides/scripts/) to get started.

### Tools

uv executes and installs command-line tools provided by Python packages, similar to `pipx`.

Run a tool in an ephemeral environment using `uvx` (an alias for `uv tool run`):

```console
$ uvx pycowsay &#039;hello world!&#039;
Resolved 1 package in 167ms
Installed 1 package in 9ms
 + pycowsay==0.0.0.2
  &quot;&quot;&quot;

  ------------
&lt; hello world! &gt;
  ------------
   \   ^__^
    \  (oo)\_______
       (__)\       )\/\
           ||----w |
           ||     ||
```

Install a tool with `uv tool install`:

```console
$ uv tool install ruff
Resolved 1 package in 6ms
Installed 1 package in 2ms
 + ruff==0.5.0
Installed 1 executable: ruff

$ ruff --version
ruff 0.5.0
```

See the [tools documentation](https://docs.astral.sh/uv/guides/tools/) to get started.

### Python versions

uv installs Python and allows quickly switching between versions.

Install multiple Python versions:

```console
$ uv python install 3.10 3.11 3.12
Searching for Python versions matching: Python 3.10
Searching for Python versions matching: Python 3.11
Searching for Python versions matching: Python 3.12
Installed 3 versions in 3.42s
 + cpython-3.10.14-macos-aarch64-none
 + cpython-3.11.9-macos-aarch64-none
 + cpython-3.12.4-macos-aarch64-none
```

Download Python versions as needed:

```console
$ uv venv --python 3.12.0
Using Python 3.12.0
Creating virtual environment at: .venv
Activate with: source .venv/bin/activate

$ uv run --python pypy@3.8 -- python --version
Python 3.8.16 (a9dbdca6fc3286b0addd2240f11d97d8e8de187a, Dec 29 2022, 11:45:30)
[PyPy 7.3.11 with GCC Apple LLVM 13.1.6 (clang-1316.0.21.2.5)] on darwin
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt;&gt;
```

Use a specific Python version in the current directory:

```console
$ uv python pin 3.11
Pinned `.python-version` to `3.11`
```

See the [Python installation documentation](https://docs.astral.sh/uv/guides/install-python/) to get
started.

### The pip interface

uv provides a drop-in replacement for common `pip`, `pip-tools`, and `virtualenv` commands.

uv extends their interfaces with advanced features, such as dependency version overrides,
platform-independent resolutions, reproducible resolutions, alternative resolution strategies, and
more.

Migrate to uv without changing your existing workflows ‚Äî and experience a 10-100x speedup ‚Äî with the
`uv pip` interface.

Compile requirements into a platform-independent requirements file:

```console
$ uv pip compile docs/requirements.in \
   --universal \
   --output-file docs/requirements.txt
Resolved 43 packages in 12ms
```

Create a virtual environment:

```console
$ uv venv
Using Python 3.12.3
Creating virtual environment at: .venv
Activate with: source .venv/bin/activate
```

Install the locked requirements:

```console
$ uv pip sync docs/requirements.txt
Resolved 43 packages in 11ms
Installed 43 packages in 208ms
 + babel==2.15.0
 + black==24.4.2
 + certifi==2024.7.4
 ...
```

See the [pip interface documentation](https://docs.astral.sh/uv/pip/index/) to get started.

## Platform support

See uv&#039;s [platform support](https://docs.astral.sh/uv/reference/platforms/) document.

## Versioning policy

See uv&#039;s [versioning policy](https://docs.astral.sh/uv/reference/versioning/) document.

## Contributing

We are passionate about supporting contributors of all levels of experience and would love to see
you get involved in the project. See the
[contributing guide](https://github.com/astral-sh/uv/blob/main/CONTRIBUTING.md) to get started.

## FAQ

#### How do you pronounce uv?

It&#039;s pronounced as &quot;you - vee&quot; ([`/juÀê viÀê/`](https://en.wikipedia.org/wiki/Help:IPA/English#Key))

#### How should I stylize uv?

Just &quot;uv&quot;, please. See the [style guide](./STYLE.md#styling-uv) for details.

## Acknowledgements

uv&#039;s dependency resolver uses [PubGrub](https://github.com/pubgrub-rs/pubgrub) under the hood. We&#039;re
grateful to the PubGrub maintainers, especially [Jacob Finkelman](https://github.com/Eh2406), for
their support.

uv&#039;s Git implementation is based on [Cargo](https://github.com/rust-lang/cargo).

Some of uv&#039;s optimizations are inspired by the great work we&#039;ve seen in [pnpm](https://pnpm.io/),
[Orogene](https://github.com/orogene/orogene), and [Bun](https://github.com/oven-sh/bun). We&#039;ve also
learned a lot from Nathaniel J. Smith&#039;s [Posy](https://github.com/njsmith/posy) and adapted its
[trampoline](https://github.com/njsmith/posy/tree/main/src/trampolines/windows-trampolines/posy-trampoline)
for Windows support.

## License

uv is licensed under either of

- Apache License, Version 2.0, ([LICENSE-APACHE](LICENSE-APACHE) or
  &lt;https://www.apache.org/licenses/LICENSE-2.0&gt;)
- MIT license ([LICENSE-MIT](LICENSE-MIT) or &lt;https://opensource.org/licenses/MIT&gt;)

at your option.

Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in uv
by you, as defined in the Apache-2.0 license, shall be dually licensed as above, without any
additional terms or conditions.

&lt;div align=&quot;center&quot;&gt;
  &lt;a target=&quot;_blank&quot; href=&quot;https://astral.sh&quot; style=&quot;background:none&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/astral-sh/uv/main/assets/svg/Astral.svg&quot; alt=&quot;Made by Astral&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[BurntSushi/ripgrep]]></title>
            <link>https://github.com/BurntSushi/ripgrep</link>
            <guid>https://github.com/BurntSushi/ripgrep</guid>
            <pubDate>Fri, 04 Jul 2025 00:05:16 GMT</pubDate>
            <description><![CDATA[ripgrep recursively searches directories for a regex pattern while respecting your gitignore]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/BurntSushi/ripgrep">BurntSushi/ripgrep</a></h1>
            <p>ripgrep recursively searches directories for a regex pattern while respecting your gitignore</p>
            <p>Language: Rust</p>
            <p>Stars: 53,475</p>
            <p>Forks: 2,163</p>
            <p>Stars today: 26 stars today</p>
            <h2>README</h2><pre>ripgrep (rg)
------------
ripgrep is a line-oriented search tool that recursively searches the current
directory for a regex pattern. By default, ripgrep will respect gitignore rules
and automatically skip hidden files/directories and binary files. (To disable
all automatic filtering by default, use `rg -uuu`.) ripgrep has first class
support on Windows, macOS and Linux, with binary downloads available for [every
release](https://github.com/BurntSushi/ripgrep/releases). ripgrep is similar to
other popular search tools like The Silver Searcher, ack and grep.

[![Build status](https://github.com/BurntSushi/ripgrep/workflows/ci/badge.svg)](https://github.com/BurntSushi/ripgrep/actions)
[![Crates.io](https://img.shields.io/crates/v/ripgrep.svg)](https://crates.io/crates/ripgrep)
[![Packaging status](https://repology.org/badge/tiny-repos/ripgrep.svg)](https://repology.org/project/ripgrep/badges)

Dual-licensed under MIT or the [UNLICENSE](https://unlicense.org).


### CHANGELOG

Please see the [CHANGELOG](CHANGELOG.md) for a release history.

### Documentation quick links

* [Installation](#installation)
* [User Guide](GUIDE.md)
* [Frequently Asked Questions](FAQ.md)
* [Regex syntax](https://docs.rs/regex/1/regex/#syntax)
* [Configuration files](GUIDE.md#configuration-file)
* [Shell completions](FAQ.md#complete)
* [Building](#building)
* [Translations](#translations)


### Screenshot of search results

[![A screenshot of a sample search with ripgrep](https://burntsushi.net/stuff/ripgrep1.png)](https://burntsushi.net/stuff/ripgrep1.png)


### Quick examples comparing tools

This example searches the entire
[Linux kernel source tree](https://github.com/BurntSushi/linux)
(after running `make defconfig &amp;&amp; make -j8`) for `[A-Z]+_SUSPEND`, where
all matches must be words. Timings were collected on a system with an Intel
i9-12900K 5.2 GHz.

Please remember that a single benchmark is never enough! See my
[blog post on ripgrep](https://blog.burntsushi.net/ripgrep/)
for a very detailed comparison with more benchmarks and analysis.

| Tool | Command | Line count | Time |
| ---- | ------- | ---------- | ---- |
| ripgrep (Unicode) | `rg -n -w &#039;[A-Z]+_SUSPEND&#039;` | 536 | **0.082s** (1.00x) |
| [hypergrep](https://github.com/p-ranav/hypergrep) | `hgrep -n -w &#039;[A-Z]+_SUSPEND&#039;` | 536 | 0.167s (2.04x) |
| [git grep](https://www.kernel.org/pub/software/scm/git/docs/git-grep.html) | `git grep -P -n -w &#039;[A-Z]+_SUSPEND&#039;` | 536 | 0.273s (3.34x) |
| [The Silver Searcher](https://github.com/ggreer/the_silver_searcher) | `ag -w &#039;[A-Z]+_SUSPEND&#039;` | 534 | 0.443s (5.43x) |
| [ugrep](https://github.com/Genivia/ugrep) | `ugrep -r --ignore-files --no-hidden -I -w &#039;[A-Z]+_SUSPEND&#039;` | 536 | 0.639s (7.82x) |
| [git grep](https://www.kernel.org/pub/software/scm/git/docs/git-grep.html) | `LC_ALL=C git grep -E -n -w &#039;[A-Z]+_SUSPEND&#039;` | 536 | 0.727s (8.91x) |
| [git grep (Unicode)](https://www.kernel.org/pub/software/scm/git/docs/git-grep.html) | `LC_ALL=en_US.UTF-8 git grep -E -n -w &#039;[A-Z]+_SUSPEND&#039;` | 536 | 2.670s (32.70x) |
| [ack](https://github.com/beyondgrep/ack3) | `ack -w &#039;[A-Z]+_SUSPEND&#039;` | 2677 | 2.935s (35.94x) |

Here&#039;s another benchmark on the same corpus as above that disregards gitignore
files and searches with a whitelist instead. The corpus is the same as in the
previous benchmark, and the flags passed to each command ensure that they are
doing equivalent work:

| Tool | Command | Line count | Time |
| ---- | ------- | ---------- | ---- |
| ripgrep | `rg -uuu -tc -n -w &#039;[A-Z]+_SUSPEND&#039;` | 447 | **0.063s** (1.00x) |
| [ugrep](https://github.com/Genivia/ugrep) | `ugrep -r -n --include=&#039;*.c&#039; --include=&#039;*.h&#039; -w &#039;[A-Z]+_SUSPEND&#039;` | 447 | 0.607s (9.62x) |
| [GNU grep](https://www.gnu.org/software/grep/) | `grep -E -r -n --include=&#039;*.c&#039; --include=&#039;*.h&#039; -w &#039;[A-Z]+_SUSPEND&#039;` | 447 | 0.674s (10.69x) |

Now we&#039;ll move to searching on single large file. Here is a straight-up
comparison between ripgrep, ugrep and GNU grep on a file cached in memory
(~13GB, [`OpenSubtitles.raw.en.gz`](http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2018/mono/OpenSubtitles.raw.en.gz), decompressed):

| Tool | Command | Line count | Time |
| ---- | ------- | ---------- | ---- |
| ripgrep (Unicode) | `rg -w &#039;Sherlock [A-Z]\w+&#039;` | 7882 | **1.042s** (1.00x) |
| [ugrep](https://github.com/Genivia/ugrep) | `ugrep -w &#039;Sherlock [A-Z]\w+&#039;` | 7882 | 1.339s (1.28x) |
| [GNU grep (Unicode)](https://www.gnu.org/software/grep/) | `LC_ALL=en_US.UTF-8 egrep -w &#039;Sherlock [A-Z]\w+&#039;` | 7882 | 6.577s (6.31x) |

In the above benchmark, passing the `-n` flag (for showing line numbers)
increases the times to `1.664s` for ripgrep and `9.484s` for GNU grep. ugrep
times are unaffected by the presence or absence of `-n`.

Beware of performance cliffs though:

| Tool | Command | Line count | Time |
| ---- | ------- | ---------- | ---- |
| ripgrep (Unicode) | `rg -w &#039;[A-Z]\w+ Sherlock [A-Z]\w+&#039;` | 485 | **1.053s** (1.00x) |
| [GNU grep (Unicode)](https://www.gnu.org/software/grep/) | `LC_ALL=en_US.UTF-8 grep -E -w &#039;[A-Z]\w+ Sherlock [A-Z]\w+&#039;` | 485 | 6.234s (5.92x) |
| [ugrep](https://github.com/Genivia/ugrep) | `ugrep -w &#039;[A-Z]\w+ Sherlock [A-Z]\w+&#039;` | 485 | 28.973s (27.51x) |

And performance can drop precipitously across the board when searching big
files for patterns without any opportunities for literal optimizations:

| Tool | Command | Line count | Time |
| ---- | ------- | ---------- | ---- |
| ripgrep | `rg &#039;[A-Za-z]{30}&#039;` | 6749 | **15.569s** (1.00x) |
| [ugrep](https://github.com/Genivia/ugrep) | `ugrep -E &#039;[A-Za-z]{30}&#039;` | 6749 | 21.857s (1.40x) |
| [GNU grep](https://www.gnu.org/software/grep/) | `LC_ALL=C grep -E &#039;[A-Za-z]{30}&#039;` | 6749 | 32.409s (2.08x) |
| [GNU grep (Unicode)](https://www.gnu.org/software/grep/) | `LC_ALL=en_US.UTF-8 grep -E &#039;[A-Za-z]{30}&#039;` | 6795 | 8m30s (32.74x) |

Finally, high match counts also tend to both tank performance and smooth
out the differences between tools (because performance is dominated by how
quickly one can handle a match and not the algorithm used to detect the match,
generally speaking):

| Tool | Command | Line count | Time |
| ---- | ------- | ---------- | ---- |
| ripgrep | `rg the` | 83499915 | **6.948s** (1.00x) |
| [ugrep](https://github.com/Genivia/ugrep) | `ugrep the` | 83499915 | 11.721s (1.69x) |
| [GNU grep](https://www.gnu.org/software/grep/) | `LC_ALL=C grep the` | 83499915 | 15.217s (2.19x) |

### Why should I use ripgrep?

* It can replace many use cases served by other search tools
  because it contains most of their features and is generally faster. (See
  [the FAQ](FAQ.md#posix4ever) for more details on whether ripgrep can truly
  replace grep.)
* Like other tools specialized to code search, ripgrep defaults to
  [recursive search](GUIDE.md#recursive-search) and does [automatic
  filtering](GUIDE.md#automatic-filtering). Namely, ripgrep won&#039;t search files
  ignored by your `.gitignore`/`.ignore`/`.rgignore` files, it won&#039;t search
  hidden files and it won&#039;t search binary files. Automatic filtering can be
  disabled with `rg -uuu`.
* ripgrep can [search specific types of files](GUIDE.md#manual-filtering-file-types).
  For example, `rg -tpy foo` limits your search to Python files and `rg -Tjs
  foo` excludes JavaScript files from your search. ripgrep can be taught about
  new file types with custom matching rules.
* ripgrep supports many features found in `grep`, such as showing the context
  of search results, searching multiple patterns, highlighting matches with
  color and full Unicode support. Unlike GNU grep, ripgrep stays fast while
  supporting Unicode (which is always on).
* ripgrep has optional support for switching its regex engine to use PCRE2.
  Among other things, this makes it possible to use look-around and
  backreferences in your patterns, which are not supported in ripgrep&#039;s default
  regex engine. PCRE2 support can be enabled with `-P/--pcre2` (use PCRE2
  always) or `--auto-hybrid-regex` (use PCRE2 only if needed). An alternative
  syntax is provided via the `--engine (default|pcre2|auto)` option.
* ripgrep has [rudimentary support for replacements](GUIDE.md#replacements),
  which permit rewriting output based on what was matched.
* ripgrep supports [searching files in text encodings](GUIDE.md#file-encoding)
  other than UTF-8, such as UTF-16, latin-1, GBK, EUC-JP, Shift_JIS and more.
  (Some support for automatically detecting UTF-16 is provided. Other text
  encodings must be specifically specified with the `-E/--encoding` flag.)
* ripgrep supports searching files compressed in a common format (brotli,
  bzip2, gzip, lz4, lzma, xz, or zstandard) with the `-z/--search-zip` flag.
* ripgrep supports
  [arbitrary input preprocessing filters](GUIDE.md#preprocessor)
  which could be PDF text extraction, less supported decompression, decrypting,
  automatic encoding detection and so on.
* ripgrep can be configured via a
  [configuration file](GUIDE.md#configuration-file).

In other words, use ripgrep if you like speed, filtering by default, fewer
bugs and Unicode support.


### Why shouldn&#039;t I use ripgrep?

Despite initially not wanting to add every feature under the sun to ripgrep,
over time, ripgrep has grown support for most features found in other file
searching tools. This includes searching for results spanning across multiple
lines, and opt-in support for PCRE2, which provides look-around and
backreference support.

At this point, the primary reasons not to use ripgrep probably consist of one
or more of the following:

* You need a portable and ubiquitous tool. While ripgrep works on Windows,
  macOS and Linux, it is not ubiquitous and it does not conform to any
  standard such as POSIX. The best tool for this job is good old grep.
* There still exists some other feature (or bug) not listed in this README that
  you rely on that&#039;s in another tool that isn&#039;t in ripgrep.
* There is a performance edge case where ripgrep doesn&#039;t do well where another
  tool does do well. (Please file a bug report!)
* ripgrep isn&#039;t possible to install on your machine or isn&#039;t available for your
  platform. (Please file a bug report!)


### Is it really faster than everything else?

Generally, yes. A large number of benchmarks with detailed analysis for each is
[available on my blog](https://blog.burntsushi.net/ripgrep/).

Summarizing, ripgrep is fast because:

* It is built on top of
  [Rust&#039;s regex engine](https://github.com/rust-lang/regex).
  Rust&#039;s regex engine uses finite automata, SIMD and aggressive literal
  optimizations to make searching very fast. (PCRE2 support can be opted into
  with the `-P/--pcre2` flag.)
* Rust&#039;s regex library maintains performance with full Unicode support by
  building UTF-8 decoding directly into its deterministic finite automaton
  engine.
* It supports searching with either memory maps or by searching incrementally
  with an intermediate buffer. The former is better for single files and the
  latter is better for large directories. ripgrep chooses the best searching
  strategy for you automatically.
* Applies your ignore patterns in `.gitignore` files using a
  [`RegexSet`](https://docs.rs/regex/1/regex/struct.RegexSet.html).
  That means a single file path can be matched against multiple glob patterns
  simultaneously.
* It uses a lock-free parallel recursive directory iterator, courtesy of
  [`crossbeam`](https://docs.rs/crossbeam) and
  [`ignore`](https://docs.rs/ignore).


### Feature comparison

Andy Lester, author of [ack](https://beyondgrep.com/), has published an
excellent table comparing the features of ack, ag, git-grep, GNU grep and
ripgrep: https://beyondgrep.com/feature-comparison/

Note that ripgrep has grown a few significant new features recently that
are not yet present in Andy&#039;s table. This includes, but is not limited to,
configuration files, passthru, support for searching compressed files,
multiline search and opt-in fancy regex support via PCRE2.


### Playground

If you&#039;d like to try ripgrep before installing, there&#039;s an unofficial
[playground](https://codapi.org/ripgrep/) and an [interactive
tutorial](https://codapi.org/try/ripgrep/).

If you have any questions about these, please open an issue in the [tutorial
repo](https://github.com/nalgeon/tryxinyminutes).


### Installation

The binary name for ripgrep is `rg`.

**[Archives of precompiled binaries for ripgrep are available for Windows,
macOS and Linux.](https://github.com/BurntSushi/ripgrep/releases)** Linux and
Windows binaries are static executables. Users of platforms not explicitly
mentioned below are advised to download one of these archives.

If you&#039;re a **macOS Homebrew** or a **Linuxbrew** user, then you can install
ripgrep from homebrew-core:

```
$ brew install ripgrep
```

If you&#039;re a **MacPorts** user, then you can install ripgrep from the
[official ports](https://www.macports.org/ports.php?by=name&amp;substr=ripgrep):

```
$ sudo port install ripgrep
```

If you&#039;re a **Windows Chocolatey** user, then you can install ripgrep from the
[official repo](https://chocolatey.org/packages/ripgrep):

```
$ choco install ripgrep
```

If you&#039;re a **Windows Scoop** user, then you can install ripgrep from the
[official bucket](https://github.com/ScoopInstaller/Main/blob/master/bucket/ripgrep.json):

```
$ scoop install ripgrep
```

If you&#039;re a **Windows Winget** user, then you can install ripgrep from the
[winget-pkgs](https://github.com/microsoft/winget-pkgs/tree/master/manifests/b/BurntSushi/ripgrep)
repository:

```
$ winget install BurntSushi.ripgrep.MSVC
```

If you&#039;re an **Arch Linux** user, then you can install ripgrep from the official repos:

```
$ sudo pacman -S ripgrep
```

If you&#039;re a **Gentoo** user, you can install ripgrep from the
[official repo](https://packages.gentoo.org/packages/sys-apps/ripgrep):

```
$ sudo emerge sys-apps/ripgrep
```

If you&#039;re a **Fedora** user, you can install ripgrep from official
repositories.

```
$ sudo dnf install ripgrep
```

If you&#039;re an **openSUSE** user, ripgrep is included in **openSUSE Tumbleweed**
and **openSUSE Leap** since 15.1.

```
$ sudo zypper install ripgrep
```

If you&#039;re a **RHEL/CentOS 7/8** user, you can install ripgrep from
[copr](https://copr.fedorainfracloud.org/coprs/carlwgeorge/ripgrep/):

```
$ sudo yum install -y yum-utils
$ sudo yum-config-manager --add-repo=https://copr.fedorainfracloud.org/coprs/carlwgeorge/ripgrep/repo/epel-7/carlwgeorge-ripgrep-epel-7.repo
$ sudo yum install ripgrep
```

If you&#039;re a **Nix** user, you can install ripgrep from
[nixpkgs](https://github.com/NixOS/nixpkgs/blob/master/pkgs/tools/text/ripgrep/default.nix):

```
$ nix-env --install ripgrep
```

If you&#039;re a **Flox** user, you can install ripgrep as follows:

```
$ flox install ripgrep
```

If you&#039;re a **Guix** user, you can install ripgrep from the official
package collection:

```
$ guix install ripgrep
```

If you&#039;re a **Debian** user (or a user of a Debian derivative like **Ubuntu**),
then ripgrep can be installed using a binary `.deb` file provided in each
[ripgrep release](https://github.com/BurntSushi/ripgrep/releases).

```
$ curl -LO https://github.com/BurntSushi/ripgrep/releases/download/14.1.0/ripgrep_14.1.1-1_amd64.deb
$ sudo dpkg -i ripgrep_14.1.1-1_amd64.deb
```

If you run Debian stable, ripgrep is [officially maintained by
Debian](https://tracker.debian.org/pkg/rust-ripgrep), although its version may
be older than the `deb` package available in the previous step.

```
$ sudo apt-get install ripgrep
```

If you&#039;re an **Ubuntu Cosmic (18.10)** (or newer) user, ripgrep is
[available](https://launchpad.net/ubuntu/+source/rust-ripgrep) using the same
packaging as Debian:

```
$ sudo apt-get install ripgrep
```

(N.B. Various snaps for ripgrep on Ubuntu are also available, but none of them
seem to work right and generate a number of very strange bug reports that I
don&#039;t know how to fix and don&#039;t have the time to fix. Therefore, it is no
longer a recommended installation option.)

If you&#039;re an **ALT** user, you can install ripgrep from the
[official repo](https://packages.altlinux.org/en/search?name=ripgrep):

```
$ sudo apt-get install ripgrep
```

If you&#039;re a **FreeBSD** user, then you can install ripgrep from the
[official ports](https://www.freshports.org/textproc/ripgrep/):

```
$ sudo pkg install ripgrep
```

If you&#039;re an **OpenBSD** user, then you can install ripgrep from the
[official ports](https://openports.se/textproc/ripgrep):

```
$ doas pkg_add ripgrep
```

If you&#039;re a **NetBSD** user, then you can install ripgrep from
[pkgsrc](https://pkgsrc.se/textproc/ripgrep):

```
$ sudo pkgin install ripgrep
```

If you&#039;re a **Haiku x86_64** user, then you can install ripgrep from the
[official ports](https://github.com/haikuports/haikuports/tree/master/sys-apps/ripgrep):

```
$ sudo pkgman install ripgrep
```

If you&#039;re a **Haiku x86_gcc2** user, then you can install ripgrep from the
same port as Haiku x86_64 using the x86 secondary architecture build:

```
$ sudo pkgman install ripgrep_x86
```

If you&#039;re a **Void Linux** user, then you can install ripgrep from the
[official repository](https://voidlinux.org/packages/?arch=x86_64&amp;q=ripgrep):

```
$ sudo xbps-install -Syv ripgrep
```

If you&#039;re a **Rust programmer**, ripgrep can be installed with `cargo`.

* Note that the minimum supported version of Rust for ripgrep is **1.72.0**,
  although ripgrep may work with older versions.
* Note that the binary may be bigger than expected because it contains debug
  symbols. This is intentional. To remove debug symbols and therefore reduce
  the file size, run `strip` on the binary.

```
$ cargo install ripgrep
```

Alternatively, one can use [`cargo
binstall`](https://github.com/cargo-bins/cargo-binstall) to install a ripgrep
binary directly from GitHub:

```
$ cargo binstall ripgrep
```


### Building

ripgrep is written in Rust, so you&#039;ll need to grab a
[Rust installation](https://www.rust-lang.org/) in order to compile it.
ripgrep compiles with Rust 1.72.0 (stable) or newer. In general, ripgrep tracks
the latest stable release of the Rust compiler.

To build ripgrep:

```
$ git clone https://github.com/BurntSushi/ripgrep
$ cd ripgrep
$ cargo build --release
$ ./target/release/rg --version
0.1.3
```

**NOTE:** In the past, ripgrep supported a `simd-accel` Cargo feature when
using a Rust nightly compiler. This only benefited UTF-16 transcoding.
Since it required unstable features, this build mode was prone to breakage.
Because of that, support for it has been removed. If you want SIMD
optimizations for UTF-16 transcoding, then you&#039;ll have to petition the
[`encoding_rs`](https://github.com/hsivonen/encoding_rs) project to use stable
APIs.

Finally, optional PCRE2 support can be built with ripgrep by enabling the
`pcre2` feature:

```
$ cargo build --release --features &#039;pcre2&#039;
```

Enabling the PCRE2 feature works with a stable Rust compiler and will
attempt to automatically find and link with your system&#039;s PCRE2 library via
`pkg-config`. If one doesn&#039;t exist, then ripgrep will build PCRE2 from source
using your system&#039;s C compiler and then statically link it into the final
executable. Static linking can be forced even when there is an available PCRE2
system library by either building ripgrep with the MUSL target or by setting
`PCRE2_SYS_STATIC=1`.

ripgrep can be built with the MUSL target on Linux by first installing the MUSL
library on your system (consult your friendly neighborhood package manager).
Then you just need to add MUSL support to your Rust toolchain and rebuild
ripgrep, which yields a fully static executable:

```
$ rustup target add x86_64-unknown-linux-musl
$ cargo build --release --target x86_64-unknown-linux-musl
```

Applying the `--features` flag from above works as expected. If you want to
build a static executable with MUSL and with PCRE2, then you will need to have
`musl-gcc` installed, which might be in a separate package from the actual
MUSL library, depending on your Linux distribution.


### Running tests

ripgrep is relatively well-test

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[theopfr/somo]]></title>
            <link>https://github.com/theopfr/somo</link>
            <guid>https://github.com/theopfr/somo</guid>
            <pubDate>Fri, 04 Jul 2025 00:05:15 GMT</pubDate>
            <description><![CDATA[A human-friendly alternative to netstat for socket and port monitoring on Linux.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/theopfr/somo">theopfr/somo</a></h1>
            <p>A human-friendly alternative to netstat for socket and port monitoring on Linux.</p>
            <p>Language: Rust</p>
            <p>Stars: 1,741</p>
            <p>Forks: 46</p>
            <p>Stars today: 98 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img style=&quot;width: 250px&quot; src=&quot;./images/somo-logo.png&quot; /&gt;
&lt;/p&gt;


### A human-friendly alternative to netstat for socket and port monitoring on Linux.


## ‚ú® Highlights:
- pleasing to the eye thanks to a nice table view
- filterable and sortable output
- interactive killing of processes
- JSON and custom formattable output
- from ``netstat -tulpn`` to ``somo -l``
- you can find all features further down

&lt;br /&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;./images/somo-example.png&quot; /&gt;
&lt;/p&gt;

---

## ‚¨áÔ∏è Installation:

### Option 1 - Debian:
If you use a Debian OS go to [releases](https://github.com/theopfr/somo/releases) and download the latest .deb release.

### Option 2 - From crates.io:
```sh
cargo install somo
```
Most of the time you will want to run this in ``sudo`` mode to see all processes and ports. By default, this is not possible when installed via cargo. But you can create a symlink so the binary can be run as root:
```sh
sudo ln -s ~/.cargo/bin/somo /usr/local/bin/somo
sudo somo   # this works now
```
### Option 3 - Nix:

If you use Nix with Flakes, you can build and use the development version.

```sh
nix build &#039;github:theopfr/somo?dir=nix&#039;
sudo ./result/bin/somo
```

---

## üèÉ‚Äç‚ôÄÔ∏è Running somo:
To run somo just type: 
```sh
sudo somo
```

Somo supports the following features:

### ‚ú® Filtering:
You can use the following flags to filter based on different attributes:
| filter flag | description | value |
| :------------- |:------------- | :----- |
| ```--tcp, -t``` | filter by TCP connections | - |
| ```--udp, -u``` | filter by UDP connections  | - | 
| ```--proto``` | deprecated ‚Äì use ``--tcp`` / ``--udp`` instead | ``tcp`` or ``udp`` | 
| ```--port, -p``` | filter by a local port | port number, e.g ``5433`` |
| ```--remote-port``` | filter by a remote port | port number, e.g ``443`` |
| ```--ip``` | filter by a remote IP | IP address e.g ``0.0.0.0`` |
| ```--program``` | filter by a client program | program name e.g ``chrome`` |
| ```--pid``` | filter by a PID | PID number, e.g ``10000`` |
| ```--open, -o``` | filter by open connections | - |
| ```--listen, -l``` | filter by listening connections | - |
| ```--exclude-ipv6``` | don&#039;t list IPv6 connections | - |

### ‚ú® Compact table view:
To get a smaller, more compact table use the ``--compact, -c`` flag.

&lt;img style=&quot;width: 75%&quot; src=&quot;./images/somo-compact-example.png&quot; /&gt;

### ‚ú® Process killing:
With the ``--kill, -k`` flag you can choose to kill a process after inspecting the connections using an interactive selection option.

&lt;img style=&quot;width: 75%&quot; src=&quot;./images/somo-kill-example.png&quot; /&gt;

### ‚ú® JSON and custom output format:
Using the ``--json`` flag you can choose to retrieve the connection data in JSON format. &lt;br /&gt;
You can also define a custom output format using the ``--format`` flag, for example:
```sh
somo --format &quot;PID: {{pid}}, Protocol: {{proto}}, Remote Address: {{remote_address}}&quot; # attributes must be specified in snake_case
```
In the format-string, the attributes have to be specified in *snake_case*.

### ‚ú® Sorting by columns:
The ``--sort, -s`` flag can be used to sort the table after a specific column ascending. For example:
```sh
somo --sort pid   # column names must be specified in snake_case
```
To get a descending order, you can use the ``--reverse, -r`` flag.

---

## üêö Shell Completions:
Somo supports shell completions for bash, zsh, fish, and elvish. Choose your shell:

### Bash
```bash
mkdir -p ~/.local/share/bash-completion/completions
somo generate-completions bash &gt; ~/.local/share/bash-completion/completions/somo
```

### Zsh
```zsh
mkdir -p ~/.local/share/zsh/site-functions
somo generate-completions zsh &gt; ~/.local/share/zsh/site-functions/_somo
echo &#039;fpath=(~/.local/share/zsh/site-functions $fpath)&#039; &gt;&gt; ~/.zshrc
echo &#039;autoload -U compinit &amp;&amp; compinit&#039; &gt;&gt; ~/.zshrc
```

### Fish
```fish
mkdir -p ~/.config/fish/completions
somo generate-completions fish &gt; ~/.config/fish/completions/somo.fish
```

### Elvish
```bash
mkdir -p ~/.config/elvish/lib
somo generate-completions elvish &gt; ~/.config/elvish/lib/somo.elv
echo &#039;use somo&#039; &gt;&gt; ~/.config/elvish/rc.elv
```</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[juspay/hyperswitch]]></title>
            <link>https://github.com/juspay/hyperswitch</link>
            <guid>https://github.com/juspay/hyperswitch</guid>
            <pubDate>Fri, 04 Jul 2025 00:05:14 GMT</pubDate>
            <description><![CDATA[An open source payments switch written in Rust to make payments fast, reliable and affordable]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/juspay/hyperswitch">juspay/hyperswitch</a></h1>
            <p>An open source payments switch written in Rust to make payments fast, reliable and affordable</p>
            <p>Language: Rust</p>
            <p>Stars: 21,236</p>
            <p>Forks: 3,583</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;./docs/imgs/hyperswitch-logo-dark.svg#gh-dark-mode-only&quot; alt=&quot;Hyperswitch-Logo&quot; width=&quot;40%&quot; /&gt;
  &lt;img src=&quot;./docs/imgs/hyperswitch-logo-light.svg#gh-light-mode-only&quot; alt=&quot;Hyperswitch-Logo&quot; width=&quot;40%&quot; /&gt;
&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt;Composable Open-Source Payments Infrastructure&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/juspay/hyperswitch/main/docs/gifs/quickstart.gif&quot; alt=&quot;Quickstart demo&quot; /&gt;
&lt;/p&gt;


&lt;!-- @import &quot;[TOC]&quot; {cmd=&quot;toc&quot; depthFrom=1 depthTo=6 orderedList=false} --&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/juspay/hyperswitch/actions?query=workflow%3ACI+branch%3Amain&quot;&gt;
    &lt;img src=&quot;https://github.com/juspay/hyperswitch/workflows/CI-push/badge.svg&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/juspay/hyperswitch/blob/main/LICENSE&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/license/juspay/hyperswitch&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/juspay/hyperswitch/blob/main/LICENSE&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Made_in-Rust-orange&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.linkedin.com/company/hyperswitch/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/follow-hyperswitch-blue?logo=linkedin&amp;labelColor=grey&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://x.com/hyperswitchio&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/follow-%40hyperswitchio-white?logo=x&amp;labelColor=grey&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://inviter.co/hyperswitch-slack&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/chat-on_slack-blue?logo=slack&amp;labelColor=grey&amp;color=%233f0e40&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;hr/&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üìÅ Table of Contents&lt;/strong&gt;&lt;/summary&gt;

- [What Can I Do with Hyperswitch?](#-what-can-i-do-with-hyperswitch)
- [Quickstart (Local Setup)](#-quickstart-local-setup)
- [Cloud Deployment](#cloud-deployment)
- [Hosted Sandbox (No Setup Required)](#hosted-sandbox-no-setup-required)
- [Why Hyperswitch?](#-why-hyperswitch)
- [Architectural Overview](#architectural-overview)
- [Our Vision](#our-vision)
- [Community &amp; Contributions](#community--contributions)
- [Feature Requests &amp; Bugs](#feature-requests--bugs)
- [Versioning](#versioning)
- [License](#copyright-and-license)
- [Team Behind Hyperswitch](#team-behind-hyperswitch)

&lt;/details&gt;

&lt;summary&gt;&lt;h2&gt; What Can I Do with Hyperswitch?&lt;/h2&gt;&lt;/summary&gt;

Hyperswitch offers a modular, open-source payments infrastructure designed for flexibility and control. Apart from our Payment Suite offering, this solution allows businesses to pick and integrate only the modules they need on top of their existing payment stack ‚Äî without unnecessary complexity or vendor lock-in.

Each module is independent and purpose-built to optimize different aspects of payment processing.

&lt;h3&gt; Learn More About The Payment Modules &lt;/h3&gt;
&lt;details&gt;

- **Cost Observability**  
  Advanced observability tools to audit, monitor, and optimize your payment costs. Detect hidden fees, downgrades, and penalties with self-serve dashboards and actionable insights.  
  _[Read more](https://docs.hyperswitch.io/about-hyperswitch/payments-modules/ai-powered-cost-observability)_

- **Revenue Recovery**  
  Combat passive churn with intelligent retry strategies tuned by card bin, region, method, and more. Offers fine-grained control over retry algorithms, penalty budgets, and recovery transparency.  
  _[Read more](https://docs.hyperswitch.io/about-hyperswitch/payments-modules/revenue-recovery)_

- **Vault**  
  A PCI-compliant vault service to store cards, tokens, wallets, and bank credentials. Provides a unified, secure, and reusable store of customer-linked payment methods.  
  _[Read more](https://docs.hyperswitch.io/about-hyperswitch/payments-modules/vault)_

- **Intelligent Routing**  
  Route each transaction to the PSP with the highest predicted auth rate. Reduce retries, avoid downtime, and minimize latency while maximizing first attempt success.  
  _[Read more](https://docs.hyperswitch.io/about-hyperswitch/payments-modules/intelligent-routing)_

- **Reconciliation**  
  Automate 2-way and 3-way reconciliation with backdated support, staggered scheduling, and customizable outputs. Reduces manual ops effort and increases audit confidence.  
  _[Read more](https://docs.hyperswitch.io/about-hyperswitch/payments-modules/reconciliation)_

- **Alternate Payment Methods**  
  Drop-in widgets for PayPal, Apple Pay, Google Pay, Samsung Pay, Pay by Bank, and BNPL providers like Klarna. Maximizes conversions with seamless one-click checkout.  
  _[Read more](https://docs.hyperswitch.io/about-hyperswitch/payments-modules/enable-alternate-payment-method-widgets)_

&lt;/details&gt;

## Quickstart 

&lt;h3&gt; Local Setup via Docker &lt;/h3&gt;

```bash
# One-click local setup

git clone --depth 1 --branch latest https://github.com/juspay/hyperswitch

cd hyperswitch

scripts/setup.sh
```
&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;This script: &lt;/strong&gt;&lt;/summary&gt;

  - Detects Docker/Podman  
  - Offers multiple deployment profiles:
    - **Standard**: App server + Control Center  
    - **Full**: Includes monitoring + schedulers  
    - **Minimal**: Standalone App server  
  - Provides access links when done

  If you need further help, check out our [video tutorial](https://docs.hyperswitch.io/hyperswitch-open-source/overview/unified-local-setup-using-docker).  

  üëâ After setup, [configure a connector](https://docs.hyperswitch.io/hyperswitch-open-source/account-setup/using-hyperswitch-control-center#add-a-payment-processor) and [test a payment](https://docs.hyperswitch.io/hyperswitch-open-source/account-setup/test-a-payment).
&lt;/details&gt;


&lt;h3&gt;Hosted Sandbox (No Setup Required)&lt;/h3&gt;

Hyperswitch offers a fully hosted sandbox environment that requires no setup. You can explore the Control Center, configure payment connectors, and test payments directly from the UI.

   &lt;a href=&quot;https://app.hyperswitch.io&quot;&gt;
     &lt;img src=&quot;https://github.com/juspay/hyperswitch/blob/main/docs/imgs/try-the-sandbox.png?raw=true&quot; height=&quot;35&quot;&gt;
   &lt;/a&gt;


&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt; What you can do in the Hosted Sandbox&lt;/strong&gt;&lt;/summary&gt;

  - Access the full Control Center  
  - Configure payment connectors  
  - View logs, routing rules, and retry strategies  
  - Try payments directly from the UI  
&lt;/details&gt;

&lt;h3&gt;&lt;strong&gt;Cloud Deployment&lt;/strong&gt;&lt;/h3&gt;

You can deploy to AWS, GCP, or Azure using Helm or CDK scripts. Fastest path:

Click to deploy via AWS:

   &lt;a href=&quot;https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=HyperswitchBootstarp&amp;templateURL=https://hyperswitch-synth.s3.eu-central-1.amazonaws.com/hs-starter-config.yaml&quot;&gt;
     &lt;img src=&quot;https://github.com/juspay/hyperswitch/blob/main/docs/imgs/aws_button.png?raw=true&quot; height=&quot;35&quot;&gt;
   &lt;/a&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;Cloud Deployment Instructions&lt;/strong&gt;&lt;/summary&gt;

  1. Click the AWS deployment button above to launch the stack.  
  2. Follow the guided steps in the AWS Console (approx. 30‚Äì45 mins).  

  ‚úÖ This setup provisions Hyperswitch on your cloud account using CloudFormation.  

  üìò For full instructions and Helm-based deployments, check out the  
  &lt;a href=&quot;https://docs.hyperswitch.io/hyperswitch-open-source/deploy-on-kubernetes-using-helm&quot;&gt;Cloud Install Guide&lt;/a&gt;.
&lt;/details&gt;


&lt;a href=&quot;#architectural-overview&quot;&gt;
  &lt;h2 id=&quot;architectural-overview&quot;&gt;Architectural Overview&lt;/h2&gt;
&lt;/a&gt;
&lt;img src=&quot;./docs/imgs/features.png&quot; /&gt;
&lt;img src=&quot;./docs/imgs/non-functional-features.png&quot; /&gt;
&lt;img src=&quot;./docs/imgs/hyperswitch-architecture-v1.png&quot; /&gt;

## Why Hyperswitch?

Hyperswitch is a commercial open-source payments stack purpose-built for scale, flexibility, and developer experience. Designed with a modular architecture, Hyperswitch lets you pick only the components you need‚Äîwhether it‚Äôs routing, retries, vaulting, or observability‚Äîwithout vendor lock-in or bloated integrations.

Built in Rust for performance and reliability, Hyperswitch supports global payment methods (cards, wallets, BNPL, UPI, Pay by Bank), exposes smart routing and retry logic, and provides a visual workflow builder in the Control Center. Whether you&#039;re integrating a full payment suite or augmenting an existing stack with a single module, Hyperswitch meets you where you are.

&lt;strong&gt;‚ÄúLinux for Payments‚Äù&lt;/strong&gt; ‚Äî Hyperswitch is a well-architected reference for teams who want to own their payments stack.

We believe in:

- &lt;strong&gt; Embracing Payment Diversity:&lt;/strong&gt; Innovation comes from enabling choice‚Äîacross payment methods, processors, and flows.

- &lt;strong&gt; Open Source by Default:&lt;/strong&gt; Transparency drives trust and builds better, reusable software.

- &lt;strong&gt; Community-Driven Development:&lt;/strong&gt; Our roadmap is shaped by real-world use cases and contributors. 

- &lt;strong&gt; Systems-Level Engineering:&lt;/strong&gt; We hold ourselves to a high bar for reliability, security, and performance.

- &lt;strong&gt; Maximizing Value Creation:&lt;/strong&gt; For developers, customers, and partners alike.

- &lt;strong&gt; Community-Driven, Enterprise-Tested:&lt;/strong&gt; Hyperswitch is built in the open with real-world feedback from developers and contributors, and maintained by Juspay, the team powering payment infrastructure for 400+ leading enterprises worldwide.

## Contributing

We welcome contributors from around the world to help build Hyperswitch. Whether you&#039;re fixing bugs, improving documentation, or adding new features, your help is appreciated.

Please read our [contributing guidelines](https://github.com/juspay/hyperswitch/blob/main/docs/CONTRIBUTING.md) to get started.

Join the conversation on [Slack](https://inviter.co/hyperswitch-slack) or explore open issues on [GitHub](https://github.com/juspay/hyperswitch/issues).

&lt;a href=&quot;#feature-requests&quot;&gt;
  &lt;h2 id=&quot;feature-requests&quot;&gt;Feature requests &amp; Bugs&lt;/h2&gt;
&lt;/a&gt;

For new product features, enhancements, roadmap discussions, or to share queries and ideas, visit our [GitHub Discussions](https://github.com/juspay/hyperswitch/discussions)

For reporting a bug, please read the issue guidelines and search for [existing and closed issues](https://github.com/juspay/hyperswitch/issues). If your problem or idea is not addressed yet, please [open a new issue](https://github.com/juspay/hyperswitch/issues/new/choose).

&lt;a href=&quot;#versioning&quot;&gt;
  &lt;h2 id=&quot;versioning&quot;&gt;Versioning&lt;/h2&gt;
&lt;/a&gt;

Check the [CHANGELOG.md](./CHANGELOG.md) file for details.

&lt;a href=&quot;#copyright-and-license&quot;&gt;
  &lt;h2 id=&quot;copyright-and-license&quot;&gt;Copyright and License&lt;/h2&gt;
&lt;/a&gt;

This product is licensed under the [Apache 2.0 License](LICENSE).

&lt;a href=&quot;#team-behind-hyperswitch&quot;&gt;
  &lt;h2 id=&quot;team-behind-hyperswitch&quot;&gt;Team behind Hyperswitch&lt;/h2&gt;
&lt;/a&gt;

The core team of 150+ engineers building Hyperswitch. Keep up the great work! ü•Ç

&lt;a href=&quot;https://github.com/juspay/hyperswitch/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contributors-img.web.app/image?repo=juspay/hyperswitch&quot; alt=&quot;Contributors&quot;/&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[gitbutlerapp/gitbutler]]></title>
            <link>https://github.com/gitbutlerapp/gitbutler</link>
            <guid>https://github.com/gitbutlerapp/gitbutler</guid>
            <pubDate>Fri, 04 Jul 2025 00:05:13 GMT</pubDate>
            <description><![CDATA[The GitButler version control client, backed by Git, powered by Tauri/Rust/Svelte]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/gitbutlerapp/gitbutler">gitbutlerapp/gitbutler</a></h1>
            <p>The GitButler version control client, backed by Git, powered by Tauri/Rust/Svelte</p>
            <p>Language: Rust</p>
            <p>Stars: 15,428</p>
            <p>Forks: 620</p>
            <p>Stars today: 4 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
   &lt;img align=&quot;center&quot; width=&quot;128px&quot; src=&quot;crates/gitbutler-tauri/icons/128x128@2x.png&quot; /&gt;
	&lt;h1 align=&quot;center&quot;&gt;&lt;b&gt;GitButler&lt;/b&gt;&lt;/h1&gt;
	&lt;p align=&quot;center&quot;&gt;
		Git branch management tool, built from the ground up for modern workflows
    &lt;br /&gt;
    &lt;a href=&quot;https://gitbutler.com&quot;&gt;&lt;strong&gt;gitbutler.com ¬ª&lt;/strong&gt;&lt;/a&gt;
    &lt;br /&gt;
    &lt;br /&gt;
    &lt;b&gt;Download for &lt;/b&gt;
    macOS (&lt;a href=&quot;https://app.gitbutler.com/downloads/release/darwin/aarch64/dmg&quot;&gt;Apple Silicon&lt;/a&gt; |
      &lt;a href=&quot;https://app.gitbutler.com/downloads/release/darwin/x86_64/dmg&quot;&gt;Intel&lt;/a&gt;) ¬∑
		Linux (&lt;a href=&quot;https://app.gitbutler.com/downloads/release/linux/x86_64/gz&quot;&gt;AppImage&lt;/a&gt; |
       &lt;a href=&quot;https://app.gitbutler.com/downloads/release/linux/x86_64/deb&quot;&gt;deb&lt;/a&gt;)
      ¬∑
		Windows (&lt;a href=&quot;https://app.gitbutler.com/downloads/release/windows/x86_64/msi&quot;&gt;msi&lt;/a&gt;)
    &lt;br /&gt;
    &lt;br /&gt;
    (Unstable Nightly releases can be found &lt;a href=&quot;https://app.gitbutler.com/downloads&quot;&gt;here&lt;/a&gt;)
  &lt;/p&gt;
&lt;/div&gt;

&lt;br/&gt;

![gitbutler_client](https://github.com/user-attachments/assets/bf9bdb33-a979-47a0-b2b2-8fff5ea53afb)

[![CI][s0]][l0] [![BADGE][s6]][l6] [![TWEET][s1]][l1] [![DISCORD][s2]][l2] [![INSTA][s3]][l3] [![YOUTUBE][s5]][l5] [![DEEPWIKI][s7]][l7]

[s0]: https://github.com/gitbutlerapp/gitbutler/actions/workflows/push.yaml/badge.svg
[l0]: https://github.com/gitbutlerapp/gitbutler/actions/workflows/push.yaml
[s1]: https://img.shields.io/badge/Twitter-black?logo=x&amp;logoColor=white
[l1]: https://twitter.com/intent/follow?screen_name=gitbutler
[s2]: https://img.shields.io/discord/1060193121130000425?label=Discord&amp;color=5865F2
[l2]: https://discord.gg/MmFkmaJ42D
[s3]: https://img.shields.io/badge/Instagram-E4405F?logo=instagram&amp;logoColor=white
[l3]: https://www.instagram.com/gitbutler/
[s5]: https://img.shields.io/youtube/channel/subscribers/UCEwkZIHGqsTGYvX8wgD0LoQ
[l5]: https://www.youtube.com/@gitbutlerapp
[s6]: https://img.shields.io/badge/GitButler-%23B9F4F2?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB3aWR0aD0iMzkiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCAzOSAyOCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTI1LjIxNDUgMTIuMTk5N0wyLjg3MTA3IDEuMzg5MTJDMS41NDI5NSAwLjc0NjUzMiAwIDEuNzE0MDYgMCAzLjE4OTQ3VjI0LjgxMDVDMCAyNi4yODU5IDEuNTQyOTUgMjcuMjUzNSAyLjg3MTA3IDI2LjYxMDlMMjUuMjE0NSAxNS44MDAzQzI2LjcxOTcgMTUuMDcyMSAyNi43MTk3IDEyLjkyNzkgMjUuMjE0NSAxMi4xOTk3WiIgZmlsbD0iYmxhY2siLz4KPHBhdGggZD0iTTEzLjc4NTUgMTIuMTk5N0wzNi4xMjg5IDEuMzg5MTJDMzcuNDU3MSAwLjc0NjUzMiAzOSAxLjcxNDA2IDM5IDMuMTg5NDdWMjQuODEwNUMzOSAyNi4yODU5IDM3LjQ1NzEgMjcuMjUzNSAzNi4xMjg5IDI2LjYxMDlMMTMuNzg1NSAxNS44MDAzQzEyLjI4MDMgMTUuMDcyMSAxMi4yODAzIDEyLjkyNzkgMTMuNzg1NSAxMi4xOTk3WiIgZmlsbD0idXJsKCNwYWludDBfcmFkaWFsXzMxMF8xMjkpIi8%2BCjxkZWZzPgo8cmFkaWFsR3JhZGllbnQgaWQ9InBhaW50MF9yYWRpYWxfMzEwXzEyOSIgY3g9IjAiIGN5PSIwIiByPSIxIiBncmFkaWVudFVuaXRzPSJ1c2VyU3BhY2VPblVzZSIgZ3JhZGllbnRUcmFuc2Zvcm09InRyYW5zbGF0ZSgxNi41NzAxIDE0KSBzY2FsZSgxOS44NjQxIDE5LjgzODMpIj4KPHN0b3Agb2Zmc2V0PSIwLjMwMTA1NiIgc3RvcC1vcGFjaXR5PSIwIi8%2BCjxzdG9wIG9mZnNldD0iMSIvPgo8L3JhZGlhbEdyYWRpZW50Pgo8L2RlZnM%2BCjwvc3ZnPgo%3D
[l6]: https://gitbutler.com/
[s7]: https://deepwiki.com/badge.svg
[l7]: https://deepwiki.com/gitbutlerapp/gitbutler

![Alt](https://repobeats.axiom.co/api/embed/fb23382bcf57c609832661874d3019a43555d6ae.svg &#039;Repobeats analytics for GitButler&#039;)

GitButler is a git client that lets you work on multiple branches at the same time.
It allows you to quickly organize file changes into separate branches while still having them applied to your working directory.
You can then push branches individually to your remote, or directly create pull requests.

In a nutshell, it&#039;s a more flexible version of `git add -p` and `git rebase -i`, allowing you to efficiently multitask across branches.

## How Does It Work?

GitButler keeps track of uncommitted changes in a layer on top of Git. Changes to files or parts of files can be grouped into what we call virtual branches. Whenever you are happy with the contents of a virtual branch, you can push it to a remote. GitButler makes sure that the state of other virtual branches is kept separate.

## How Do GB&#039;s Virtual Branches Differ From Git Branches?

The branches that we know and love in Git are separate universes, and switching between them is a full context switch. GitButler allows you to work with multiple branches in parallel in the same working directory. This effectively means having the content of multiple branches available at the same time.

GitButler is aware of changes before they are committed. This allows it to keep a record of which virtual branch each individual diff belongs to. Effectively, this means that you can separate out individual branches with their content at any time to push them to a remote or to unapply them from your working directory.

And finally, while in Git it is preferable that you create your desired branch ahead of time, using GitButler you can move changes between virtual branches at any point during development.

## Why GitButler?

We love Git. Our own [@schacon](https://github.com/schacon) has even published the [Pro Git](https://git-scm.com/book/en/v2) book. At the same time, Git&#039;s user interface hasn&#039;t been fundamentally changed for 15 years. While it was written for Linux kernel devs sending patches to each other over mailing lists, most developers today have different workflows and needs.

Instead of trying to fit the semantics of the Git CLI into a graphical interface, we are starting with the developer workflow and mapping it back to Git.

## Tech

GitButler is a [Tauri](https://tauri.app/)-based application. Its UI is written in [Svelte](https://svelte.dev/) using [TypeScript](https://www.typescriptlang.org) and its backend is written in [Rust](https://www.rust-lang.org/).

## Main Features

- **Virtual Branches**
  - Organize work on multiple branches simultaneously, rather than constantly switching branches
  - Automatically create new branches when needed
- **Easy Commit Management**
  - Undo, Amend and Squash commits by dragging and dropping
- **Undo Timeline**
  - Logs all operations and changes and allows you to easily undo or revert any operation
- **GitHub Integration**
  - Authenticate to GitHub to open Pull Requests, list branches and statuses and more
- **Easy SSH Key Management**
  - GitButler can generate an SSH key to upload to GitHub automatically
- **AI Tooling**
  - Automatically write commit messages based on your work in progress
  - Automatically create descriptive branch names
- **Commit Signing**
  - Easy commit signing with GPG or SSH

## Example Uses

### Fixing a Bug While Working on a Feature

&gt; Say that while developing a feature, you encounter a bug that you wish to fix. It&#039;s often desirable that you ship the fix as a separate contribution (Pull request).

Using Git you can stash your changes and switch to another branch, where you can commit, and push your fix.

_With GitButler_ you simply assign your fix to a separate virtual branch, which you can individually push (or directly create a PR). An additional benefit is that you can retain the fix in your working directory while waiting for CI and/or code review.

### Trying Someone Else&#039;s Branch Together With My Work in Progress

&gt; Say you want to test a branch from someone else for the purpose of code review.

Using Git trying out someone else&#039;s branch is a full context switch away from your own work.
_With GitButler_ you can apply and unapply (add / remove) any remote branch directly into your working directory.

## Documentation

You can find our end user documentation at: https://docs.gitbutler.com

## Bugs and Feature Requests

If you have a bug or feature request, feel free to open an [issue](https://github.com/gitbutlerapp/gitbutler/issues/new),
or [join our Discord server](https://discord.gg/MmFkmaJ42D).

## AI Commit Message Generation

Commit message generation is an opt-in feature. You can enable it while adding your repository for the first time or later in the project settings.

Currently, GitButler uses OpenAI&#039;s API for diff summarization, which means that if enabled, code diffs would be sent to OpenAI&#039;s servers.

Our goal is to make this feature more modular such that in the future you can modify the prompt as well as plug a different LLM endpoints (including local ones).

## Contributing

So you want to help out? Please check out the [CONTRIBUTING.md](CONTRIBUTING.md)
document.

If you want to skip right to getting the code to actually compile, take a look
at the [DEVELOPMENT.md](DEVELOPMENT.md) file.

Want to show your support? Add a GitButler badge to your project&#039;s README:

```md
[![GitButler](https://img.shields.io/badge/GitButler-%23B9F4F2?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB3aWR0aD0iMzkiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCAzOSAyOCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTI1LjIxNDUgMTIuMTk5N0wyLjg3MTA3IDEuMzg5MTJDMS41NDI5NSAwLjc0NjUzMiAwIDEuNzE0MDYgMCAzLjE4OTQ3VjI0LjgxMDVDMCAyNi4yODU5IDEuNTQyOTUgMjcuMjUzNSAyLjg3MTA3IDI2LjYxMDlMMjUuMjE0NSAxNS44MDAzQzI2LjcxOTcgMTUuMDcyMSAyNi43MTk3IDEyLjkyNzkgMjUuMjE0NSAxMi4xOTk3WiIgZmlsbD0iYmxhY2siLz4KPHBhdGggZD0iTTEzLjc4NTUgMTIuMTk5N0wzNi4xMjg5IDEuMzg5MTJDMzcuNDU3MSAwLjc0NjUzMiAzOSAxLjcxNDA2IDM5IDMuMTg5NDdWMjQuODEwNUMzOSAyNi4yODU5IDM3LjQ1NzEgMjcuMjUzNSAzNi4xMjg5IDI2LjYxMDlMMTMuNzg1NSAxNS44MDAzQzEyLjI4MDMgMTUuMDcyMSAxMi4yODAzIDEyLjkyNzkgMTMuNzg1NSAxMi4xOTk3WiIgZmlsbD0idXJsKCNwYWludDBfcmFkaWFsXzMxMF8xMjkpIi8%2BCjxkZWZzPgo8cmFkaWFsR3JhZGllbnQgaWQ9InBhaW50MF9yYWRpYWxfMzEwXzEyOSIgY3g9IjAiIGN5PSIwIiByPSIxIiBncmFkaWVudFVuaXRzPSJ1c2VyU3BhY2VPblVzZSIgZ3JhZGllbnRUcmFuc2Zvcm09InRyYW5zbGF0ZSgxNi41NzAxIDE0KSBzY2FsZSgxOS44NjQxIDE5LjgzODMpIj4KPHN0b3Agb2Zmc2V0PSIwLjMwMTA1NiIgc3RvcC1vcGFjaXR5PSIwIi8%2BCjxzdG9wIG9mZnNldD0iMSIvPgo8L3JhZGlhbEdyYWRpZW50Pgo8L2RlZnM%2BCjwvc3ZnPgo%3D)](https://gitbutler.com/)
```

[![BADGE][s6]][l6]
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[influxdata/influxdb]]></title>
            <link>https://github.com/influxdata/influxdb</link>
            <guid>https://github.com/influxdata/influxdb</guid>
            <pubDate>Fri, 04 Jul 2025 00:05:12 GMT</pubDate>
            <description><![CDATA[Scalable datastore for metrics, events, and real-time analytics]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/influxdata/influxdb">influxdata/influxdb</a></h1>
            <p>Scalable datastore for metrics, events, and real-time analytics</p>
            <p>Language: Rust</p>
            <p>Stars: 30,261</p>
            <p>Forks: 3,621</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
 &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/influxdb-logo.png&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/influxdb-logo-dark.png&quot;&gt;
    &lt;img src=&quot;assets/influxdb-logo.png&quot; alt=&quot;InfluxDB Logo&quot; width=&quot;600&quot;&gt;
  &lt;/picture&gt;
 &lt;p&gt;
&lt;/div&gt;

InfluxDB Core is a database built to collect, process, transform, and store event and time series data. It is ideal for use cases that require real-time ingest and fast query response times to build user interfaces, monitoring, and automation solutions.

Common use cases include:

- Monitoring sensor data
- Server monitoring
- Application performance monitoring
- Network monitoring
- Financial market and trading analytics
- Behavioral analytics

InfluxDB is optimized for scenarios where near real-time data monitoring is essential and queries 
need to return quickly to support user experiences such as dashboards and interactive user interfaces.

InfluxDB 3 Core‚Äôs feature highlights include:

- Diskless architecture with object storage support (or local disk with no dependencies)
- Fast query response times (under 10ms for last-value queries, or 30ms for distinct metadata)
- Embedded Python VM for plugins and triggers
- Parquet file persistence
- Compatibility with InfluxDB 1.x and 2.x write APIs
- Compatability with InfluxDB 1.x query API (InfluxQL)
- SQL query engine with support for FlightSQL and HTTP query API

## Project Status

InfluxDB 3 Core is GA as of April 15, 2025! We plan to have monthly point releases for the following six months, with patch releases as needed. We will move to a quarterly cadence after that for 3-4 releases, after which we&#039;ll reevaluate our release schedule.

Join the [InfluxDB3 Discord](https://discord.gg/vZe2w2Ds8B) 
or the public channels below to share your feedback, feature requests, and bug reports.

See the [InfluxDB 3 Core &amp; Enterprise GA release announcement here](https://www.influxdata.com/blog/influxdb-3-oss-ga/) 
or dig into the [InfluxDB 3 getting started guide here](https://docs.influxdata.com/influxdb3/core/get-started/).

## Learn InfluxDB
[Documentation](https://docs.influxdata.com/) | [Community Forum](https://community.influxdata.com/) | [Community Slack](https://www.influxdata.com/slack/) | [Blog](https://www.influxdata.com/blog/) | [InfluxDB University](https://university.influxdata.com/) | [YouTube](https://www.youtube.com/@influxdata8893)

Try **InfluxDB Cloud** for free and get started fast with no local setup required. Click [here](https://cloud2.influxdata.com/signup) to start building your application on InfluxDB Cloud.


## Installation
We have nightly and versioned Docker images, Debian packages, RPM packages, and tarballs of InfluxDB available on the [InfluxData downloads page](https://portal.influxdata.com/downloads/). We also provide the InfluxDB command line interface (CLI) client as a separate binary available at the same location.

- For v1 installation, use the [main 1.x branch](https://github.com/influxdata/influxdb/tree/master-1.x) or [install InfluxDB OSS directly](https://docs.influxdata.com/influxdb/v1/introduction/install/#installing-influxdb-oss).
- For v2 installation, use the [main 2.x branch](https://github.com/influxdata/influxdb/tree/main-2.x).
- For InfluxDB 3 Core see the [InfluxDB 3 Core getting started guide](https://docs.influxdata.com/influxdb3/core/get-started/).
- For InfluxDB 3 Enterprise see the [InfluxDB 3 Enterprise getting started guide](https://docs.influxdata.com/influxdb3/enterprise/get-started/).

If you are interested in building from source, see the [building from source](CONTRIBUTING.md#building-from-source) guide for contributors.

To begin using InfluxDB, visit our [Getting Started with InfluxDB](https://docs.influxdata.com/influxdb/v1/introduction/get-started/) documentation.


## License
The open source software we build is licensed under the permissive MIT or Apache 2 licenses at the user&#039;s choosing. We‚Äôve long held the view that our open source code should be truly open and our commercial code should be separate and closed. 


## Interested in joining the team building InfluxDB?
Check out current job openings at [www.influxdata.com/careers](https://www.influxdata.com/careers) today!
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[block/goose]]></title>
            <link>https://github.com/block/goose</link>
            <guid>https://github.com/block/goose</guid>
            <pubDate>Fri, 04 Jul 2025 00:05:11 GMT</pubDate>
            <description><![CDATA[an open source, extensible AI agent that goes beyond code suggestions - install, execute, edit, and test with any LLM]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/block/goose">block/goose</a></h1>
            <p>an open source, extensible AI agent that goes beyond code suggestions - install, execute, edit, and test with any LLM</p>
            <p>Language: Rust</p>
            <p>Stars: 15,441</p>
            <p>Forks: 1,279</p>
            <p>Stars today: 38 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# codename goose

_a local, extensible, open source AI agent that automates engineering tasks_

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://opensource.org/licenses/Apache-2.0&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/License-Apache_2.0-blue.svg&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/7GaTvbDwga&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/discord/1287729918100246654?logo=discord&amp;logoColor=white&amp;label=Join+Us&amp;color=blueviolet&quot; alt=&quot;Discord&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/block/goose/actions/workflows/ci.yml&quot;&gt;
     &lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/block/goose/ci.yml?branch=main&quot; alt=&quot;CI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;

goose is your on-machine AI agent, capable of automating complex development tasks from start to finish. More than just code suggestions, goose can build entire projects from scratch, write and execute code, debug failures, orchestrate workflows, and interact with external APIs - _autonomously_.

Whether you&#039;re prototyping an idea, refining existing code, or managing intricate engineering pipelines, goose adapts to your workflow and executes tasks with precision.

Designed for maximum flexibility, goose works with any LLM and supports multi-model configuration to optimize performance and cost, seamlessly integrates with MCP servers, and is available as both a desktop app as well as CLI - making it the ultimate AI assistant for developers who want to move faster and focus on innovation.

# Quick Links
- [Quickstart](https://block.github.io/goose/docs/quickstart)
- [Installation](https://block.github.io/goose/docs/getting-started/installation)
- [Tutorials](https://block.github.io/goose/docs/category/tutorials)
- [Documentation](https://block.github.io/goose/docs/category/getting-started)


# Goose Around with Us
- [Discord](https://discord.gg/block-opensource)
- [YouTube](https://www.youtube.com/@blockopensource)
- [LinkedIn](https://www.linkedin.com/company/block-opensource)
- [Twitter/X](https://x.com/blockopensource)
- [Bluesky](https://bsky.app/profile/opensource.block.xyz)
- [Nostr](https://njump.me/opensource@block.xyz)
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[oxc-project/oxc]]></title>
            <link>https://github.com/oxc-project/oxc</link>
            <guid>https://github.com/oxc-project/oxc</guid>
            <pubDate>Fri, 04 Jul 2025 00:05:10 GMT</pubDate>
            <description><![CDATA[‚öì A collection of JavaScript tools written in Rust.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/oxc-project/oxc">oxc-project/oxc</a></h1>
            <p>‚öì A collection of JavaScript tools written in Rust.</p>
            <p>Language: Rust</p>
            <p>Stars: 15,496</p>
            <p>Forks: 604</p>
            <p>Stars today: 24 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img alt=&quot;OXC Logo&quot; src=&quot;https://cdn.jsdelivr.net/gh/oxc-project/oxc-assets/preview-universal.png&quot; width=&quot;700&quot;&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;

[![MIT licensed][license-badge]][license-url]
[![Build Status][ci-badge]][ci-url]
[![Code Coverage][code-coverage-badge]][code-coverage-url]
[![CodSpeed Badge](https://img.shields.io/endpoint?url=https://codspeed.io/badge.json)](https://codspeed.io/oxc-project/oxc)
[![Sponsors][sponsors-badge]][sponsors-url]

[![Discord chat][discord-badge]][discord-url]
[![Playground][playground-badge]][playground-url]
[![Website][website-badge]][website-url]

&lt;/div&gt;

## ‚öì Oxc

The Oxidation Compiler is creating a collection of high-performance tools for JavaScript and TypeScript.

Oxc is building a parser, linter, formatter, transformer, minifier, resolver ... all written in Rust.

See more at [oxc.rs](https://oxc.rs)!

## VoidZero Inc.

Oxc is a project of [VoidZero](https://voidzero.dev/), see our announcement [Announcing VoidZero - Next Generation Toolchain for JavaScript](https://voidzero.dev/blog).

If you have requirements for JavaScript tools at scale, please [get in touch](https://forms.gle/WQgjyzYJpwurpxWKA)!

## üôãWho&#039;s using Oxc?

- [Rolldown] uses the [oxc][docs-oxc-url] crate for parsing and transformation.
- [Nova engine](https://trynova.dev) uses the [oxc][docs-oxc-url] crate for parsing.
- [Rolldown][rolldown], [Biome][biome] and [swc-node](https://github.com/swc-project/swc-node) uses the [oxc_resolver][docs-resolver-url] crate for module resolution.
- Projects and companies like [Preact](https://github.com/preactjs/preact/blob/4c20c23c16dd60f380ce9fe98afc93041a7e1562/oxlint.json), [Shopify](https://oxc.rs/blog/2023-12-12-announcing-oxlint.html#_50-100-times-faster-than-eslint), ByteDance and Shopee uses oxlint for linting.
- ...[and many more](https://oxc.rs/docs/guide/projects.html)

## ‚ö°Ô∏è Linter Quick Start

The linter is ready to catch mistakes for you. It comes with 93 rules turned on by default (out of 430+ in total) and no configuration is required.

To get started, run [oxlint][npm-oxlint] or via `npx`:

```bash
npx oxlint@latest
```

To give you an idea of its capabilities, here is an example from the [vscode] repository, which finishes linting 4800+ files in 0.7 seconds.

&lt;p float=&quot;left&quot; align=&quot;left&quot;&gt;
  &lt;img src=&quot;https://cdn.jsdelivr.net/gh/oxc-project/oxc-assets/linter-screenshot.png&quot; width=&quot;60%&quot;&gt;
&lt;/p&gt;

## ‚ö°Ô∏è Performance

- The parser aims to be the fastest Rust-based ready-for-production parser.
- The linter is more than 50 times faster than [ESLint], and scales with the number of CPU cores.

&lt;p float=&quot;left&quot; align=&quot;middle&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/Boshen/bench-javascript-parser-written-in-rust/main/bar-graph.svg&quot; width=&quot;49%&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/Boshen/bench-javascript-linter/main/bar-graph.svg&quot; width=&quot;49%&quot;&gt;
&lt;/p&gt;

## ‚å®Ô∏è Rust, Node.js and Wasm Usage

### Rust

Individual crates are published, you may use them to build your own JavaScript tools.

- The umbrella crate [oxc][docs-oxc-url] exports all public crates from this repository.
- The AST and parser crates [oxc_ast][docs-ast-url] and [oxc_parser][docs-parser-url] are production ready.
- The resolver crate [oxc_resolver][docs-resolver-url] for module resolution is also production ready.
- Example usages of these crates can be found in their respective `crates/*/examples` directory.

While Rust has gained a reputation for its comparatively slower compilation speed,
we have dedicated significant effort to fine-tune the Rust compilation speed.
Our aim is to minimize any impact on your development workflow,
ensuring that developing your own Oxc based tools remains a smooth and efficient experience.

This is demonstrated by our [CI runs](https://github.com/oxc-project/oxc/actions/workflows/ci.yml?query=branch%3Amain),
where warm runs complete in 3 minutes.

### Node.js

- via napi: [oxc-parser][npm-napi-parser], [oxc-transform][npm-napi-transform]

### Wasm

- [@oxc-parser/wasm](https://www.npmjs.com/package/@oxc-parser/wasm)

---

## üéØ Tools

- [AST and Parser](#-ast-and-parser)
- [Linter](#-linter)
- [Resolver](#-resolver)
- [Minifier](#-minifier)
- [Formatter](#-formatter)
- [Transformer](#-transformer)

### üî∏ AST and Parser

Oxc maintains its own AST and parser, which is by far the fastest and most conformant JavaScript and TypeScript (including JSX and TSX) parser written in Rust.

As the parser often represents a key performance bottleneck in JavaScript tooling,
any minor improvements can have a cascading effect on our downstream tools.
By developing our parser, we have the opportunity to explore and implement well-researched performance techniques.

While many existing JavaScript tools rely on [estree] as their AST specification,
a notable drawback is its abundance of ambiguous nodes.
This ambiguity often leads to confusion during development with [estree].

The Oxc AST differs slightly from the [estree] AST by removing ambiguous nodes and introducing distinct types.
For example, instead of using a generic [estree] `Identifier`,
the Oxc AST provides specific types such as `BindingIdentifier`, `IdentifierReference`, and `IdentifierName`.
This clear distinction greatly enhances the development experience by aligning more closely with the ECMAScript specification.

#### üèÜ Parser Performance

Our [benchmark][parser-benchmark] reveals that the Oxc parser surpasses the speed of the [swc] parser by approximately 3 times and the [Biome][biome] parser by 5 times.

&lt;details&gt;
  &lt;summary&gt;How is it so fast?&lt;/summary&gt;
  &lt;ul&gt;
    &lt;li&gt;AST is allocated in a memory arena (&lt;a href=&quot;https://crates.io/crates/bumpalo&quot;&gt;bumpalo&lt;/a&gt;) for fast AST memory allocation and deallocation.&lt;/li&gt;
    &lt;li&gt;Short strings are inlined by &lt;a href=&quot;https://crates.io/crates/compact_str&quot;&gt;CompactString&lt;/a&gt;.&lt;/li&gt;
    &lt;li&gt;No other heap allocations are done except the above two.&lt;/li&gt;
    &lt;li&gt;Scope binding, symbol resolution and some syntax errors are not done in the parser, they are delegated to the semantic analyzer.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/details&gt;

### üî∏ Linter

The linter embraces convention over configuration, eliminating the need for extensive configuration and plugin setup.
Unlike other linters like [ESLint], which often require intricate configurations and plugin installations (e.g. [@typescript-eslint]),
our linter only requires a single command that you can immediately run on your codebase:

```bash
npx oxlint@latest
```

#### üèÜ Linter Performance

The linter is 50 - 100 times faster than [ESLint] depending on the number of rules and number of CPU cores used.
It completes in less than a second for most codebases with a few hundred files and completes in a few seconds for
larger monorepos. See [bench-javascript-linter](https://github.com/Boshen/bench-javascript-linter) for details.

As an upside, the binary is approximately 5MB, whereas [ESLint] and its associated plugin dependencies can easily exceed 100.

You may also download the linter binary from the [latest release tag](https://github.com/oxc-project/oxc/releases/latest) as a standalone binary,
this lets you run the linter without a Node.js installation in your CI.

&lt;details&gt;
  &lt;summary&gt;How is it so fast?&lt;/summary&gt;
  &lt;ul&gt;
    &lt;li&gt;Oxc parser is used.&lt;/li&gt;
    &lt;li&gt;AST visit is a fast operation due to linear memory scan from the memory arena.&lt;/li&gt;
    &lt;li&gt;Files are linted in a multi-threaded environment, so scales with the total number of CPU cores.&lt;/li&gt;
    &lt;li&gt;Every single lint rule is tuned for performance.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/details&gt;

### üî∏ Resolver

Module resolution plays a crucial role in JavaScript tooling, especially for tasks like multi-file analysis or bundling. However, it can often become a performance bottleneck.
To address this, we developed [oxc_resolver][docs-resolver-url].

The resolver is production-ready and is currently being used in [Rolldown][rolldown]. Usage and examples can be found in its own [repository](https://github.com/oxc-project/oxc_resolver).

### üî∏ Transformer

A transformer is responsible for turning higher versions of ECMAScript to a lower version that can be used in older browsers.

TypeScript and React transforms are complete. See [Milestone 2](https://github.com/oxc-project/oxc/issues/2859) for current goals.

[oxc-transform][npm-napi-transform] can be used for experimentation.

### üî∏ Isolated Declarations

[TypeScript Isolated Declarations Emit](https://devblogs.microsoft.com/typescript/announcing-typescript-5-5/#isolated-declarations) without using the TypeScript compiler.

Our [benchmark](https://github.com/oxc-project/bench-transformer) indicates that our implementation is at least 20 times faster than the TypeScript compiler.

The [npm package](https://www.npmjs.com/package/oxc-transform) or [crate](https://crates.io/crates/oxc_isolated_declarations) can be used for this task.

### üî∏ Minifier

JavaScript minification plays a crucial role in optimizing website performance as it reduces the amount of data sent to users,
resulting in faster page loads.
This holds tremendous economic value, particularly for e-commerce websites, where every second can equate to millions of dollars.

However, existing minifiers typically require a trade-off between compression quality and speed.
You have to choose between the slowest for the best compression or the fastest for less compression.
But what if we could develop a faster minifier without compromising on compression?

We are actively working on a prototype that aims to achieve this goal,
by porting all test cases from well-known minifiers such as [google-closure-compiler], [terser], [esbuild], and [tdewolff-minify].

Preliminary results indicate that we are on track to achieve our objectives.
With the Oxc minifier, you can expect faster minification times without sacrificing compression quality.

### üî∏ Formatter

While [prettier] has established itself as the de facto code formatter for JavaScript, there is a significant demand in the developer community for a less opinionated alternative. Recognizing this need, our ambition is to undertake research and development to create a new JavaScript formatter that offers increased flexibility and customization options.

The [prototype](https://github.com/oxc-project/oxc/tree/main/crates/oxc_formatter) is currently work in progress.

---

## üß™Test Infrastructure

In Oxc, correctness and reliability are taken extremely seriously.

We spend half of our time on strengthening the test infrastructure to prevent problems from propagating to downstream tools.

[Test Infrastructure](https://oxc.rs/docs/learn/architecture/test.html) documents our test procedures:

- Conformance suite on Test262, Babel, TypeScript
- Lots of fuzzing
- Linter snapshot diagnostics
- oxlint ecosystem ci
- Idempotency testing
- Code coverage
- End to end 3000 top npm packages

---

## ‚úçÔ∏è Contribute

See [CONTRIBUTING.md](./CONTRIBUTING.md) for guidance.

Check out some of the [good first issues](https://github.com/oxc-project/oxc/contribute) or ask us on [Discord][discord-url].

If you are unable to contribute by code, you can still participate by:

- Add a [GitHub Star](https://github.com/oxc-project/oxc/stargazers) to the project.
- Join us on [Discord][discord-url].
- [Follow me on twitter](https://twitter.com/boshen_c) and tweet about this project.

## üìö Learning Resources

- My small tutorial on [how to write a JavaScript Parser in Rust](https://oxc.rs/docs/learn/parser_in_rust/intro.html)
- My small article [Pursuit of Performance on Building a JavaScript Compiler](https://oxc.rs/docs/learn/performance.html)
- [And more](https://oxc.rs/docs/learn/references.html)

## ü§ù Credits

This project was incubated with the assistance of these exceptional mentors and their projects:

- [Biome][biome] - [@ematipico](https://github.com/ematipico)
- [Ruff][ruff] - [@charliermarsh](https://github.com/charliermarsh), [@MichaReiser](https://github.com/MichaReiser)
- [quick-lint-js](https://github.com/quick-lint/quick-lint-js) - [@strager](https://github.com/strager)
- [elm-review](https://package.elm-lang.org/packages/jfmengels/elm-review/latest) - [@jfmengels](https://github.com/jfmengels)

Special thanks go to

- [@domonji](https://github.com/domonji) for bootstrapping this project together, and also completing the TypeScript parser.
- [@tongtong-lu](https://github.com/tongtong-lu) and [@guan-wy](https://github.com/guan-wy) for designing the [project logo](https://github.com/oxc-project/oxc-assets).

## ‚ù§ Who&#039;s [Sponsoring Oxc](https://github.com/sponsors/Boshen)?

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/sponsors/Boshen&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/Boshen/sponsors/main/sponsors.svg&quot; alt=&quot;My sponsors&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

## üìñ License

Oxc is free and open-source software licensed under the [MIT License](./LICENSE).

Oxc ports or copies code from other open source projects, their licenses are listed in [**Third-party library licenses**](./THIRD-PARTY-LICENSE).

[discord-badge]: https://img.shields.io/discord/1079625926024900739?logo=discord&amp;label=Discord
[discord-url]: https://discord.gg/9uXCAwqQZW
[license-badge]: https://img.shields.io/badge/license-MIT-blue.svg
[license-url]: https://github.com/oxc-project/oxc/blob/main/LICENSE
[ci-badge]: https://github.com/oxc-project/oxc/actions/workflows/ci.yml/badge.svg?event=push&amp;branch=main
[ci-url]: https://github.com/oxc-project/oxc/actions/workflows/ci.yml?query=event%3Apush+branch%3Amain
[npm-badge]: https://img.shields.io/npm/v/oxlint/latest?color=brightgreen
[npm-url]: https://www.npmjs.com/package/oxlint/v/latest
[code-size-badge]: https://img.shields.io/github/languages/code-size/oxc-project/oxc
[code-size-url]: https://github.com/oxc-project/oxc
[code-coverage-badge]: https://codecov.io/gh/oxc-project/oxc/graph/badge.svg?token=FVHEH0BQLJ
[code-coverage-url]: https://codecov.io/gh/oxc-project/oxc
[sponsors-badge]: https://img.shields.io/github/sponsors/Boshen
[sponsors-url]: https://github.com/sponsors/Boshen
[playground-badge]: https://img.shields.io/badge/Playground-blue?color=9BE4E0
[playground-url]: https://playground.oxc.rs/
[website-badge]: https://img.shields.io/badge/Website-blue
[website-url]: https://oxc.rs
[crate-oxc-url]: https://crates.io/crates/oxc
[crate-ast-url]: https://crates.io/crates/oxc_ast
[crate-parser-url]: https://crates.io/crates/oxc_parser
[docs-oxc-url]: https://docs.rs/oxc
[docs-ast-url]: https://docs.rs/oxc_ast
[docs-parser-url]: https://docs.rs/oxc_parser
[docs-resolver-url]: https://docs.rs/oxc_resolver
[Boshen]: https://github.com/boshen
[CompactString]: https://github.com/ParkMyCar/compact_str
[ESLint]: https://eslint.org/
[acorn]: https://github.com/acornjs/acorn
[babel]: https://babel.dev
[bumpalo]: https://docs.rs/bumpalo
[contributors]: https://github.com/oxc-project/oxc/graphs/contributors
[enhanced-resolve]: https://github.com/webpack/enhanced-resolve
[esbuild]: https://esbuild.github.io/
[eslint-plugin-import]: https://www.npmjs.com/package/eslint-plugin-import
[eslint-plugin-jest]: https://www.npmjs.com/package/eslint-plugin-jest
[estree]: https://github.com/estree/estree
[google-closure-compiler]: https://github.com/google/closure-compiler
[minification-benchmarks]: https://github.com/privatenumber/minification-benchmarks
[npm-napi-parser]: https://www.npmjs.com/package/oxc-parser
[npm-napi-transform]: https://www.npmjs.com/package/oxc-transform
[npm-oxlint]: https://www.npmjs.com/package/oxlint
[parser-benchmark]: https://github.com/Boshen/bench-javascript-parser-written-in-rust
[prettier]: https://prettier.io
[biome]: https://biomejs.dev/
[ruff]: https://beta.ruff.rs
[swc]: https://swc.rs
[tdewolff-minify]: https://github.com/tdewolff/minify
[terser]: https://terser.org
[vscode]: https://github.com/microsoft/vscode
[@typescript-eslint]: https://typescript-eslint.io
[rolldown]: https://rolldown.rs
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[meilisearch/meilisearch]]></title>
            <link>https://github.com/meilisearch/meilisearch</link>
            <guid>https://github.com/meilisearch/meilisearch</guid>
            <pubDate>Fri, 04 Jul 2025 00:05:09 GMT</pubDate>
            <description><![CDATA[A lightning-fast search engine API bringing AI-powered hybrid search to your sites and applications.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/meilisearch/meilisearch">meilisearch/meilisearch</a></h1>
            <p>A lightning-fast search engine API bringing AI-powered hybrid search to your sites and applications.</p>
            <p>Language: Rust</p>
            <p>Stars: 52,159</p>
            <p>Forks: 2,091</p>
            <p>Stars today: 33 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=logo#gh-light-mode-only&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;assets/meilisearch-logo-light.svg?sanitize=true#gh-light-mode-only&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=logo#gh-dark-mode-only&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;assets/meilisearch-logo-dark.svg?sanitize=true#gh-dark-mode-only&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;h4 align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=nav&quot;&gt;Website&lt;/a&gt; |
  &lt;a href=&quot;https://roadmap.meilisearch.com/tabs/1-under-consideration&quot;&gt;Roadmap&lt;/a&gt; |
  &lt;a href=&quot;https://www.meilisearch.com/pricing?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=nav&quot;&gt;Meilisearch Cloud&lt;/a&gt; |
  &lt;a href=&quot;https://blog.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=nav&quot;&gt;Blog&lt;/a&gt; |
  &lt;a href=&quot;https://www.meilisearch.com/docs?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=nav&quot;&gt;Documentation&lt;/a&gt; |
  &lt;a href=&quot;https://www.meilisearch.com/docs/faq?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=nav&quot;&gt;FAQ&lt;/a&gt; |
  &lt;a href=&quot;https://discord.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=nav&quot;&gt;Discord&lt;/a&gt;
&lt;/h4&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://deps.rs/repo/github/meilisearch/meilisearch&quot;&gt;&lt;img src=&quot;https://deps.rs/repo/github/meilisearch/meilisearch/status.svg&quot; alt=&quot;Dependency status&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/meilisearch/meilisearch/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/license-MIT-informational&quot; alt=&quot;License&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/meilisearch/meilisearch/queue&quot;&gt;&lt;img alt=&quot;Merge Queues enabled&quot; src=&quot;https://img.shields.io/badge/Merge_Queues-enabled-%2357cf60?logo=github&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;‚ö° A lightning-fast search engine that fits effortlessly into your apps, websites, and workflow üîç&lt;/p&gt;

[Meilisearch](https://www.meilisearch.com?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=intro) helps you shape a delightful search experience in a snap, offering features that work out of the box to speed up your workflow.

&lt;p align=&quot;center&quot; name=&quot;demo&quot;&gt;
  &lt;a href=&quot;https://where2watch.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=demo-gif#gh-light-mode-only&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;assets/demo-light.gif#gh-light-mode-only&quot; alt=&quot;A bright colored application for finding movies screening near the user&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://where2watch.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=demo-gif#gh-dark-mode-only&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;assets/demo-dark.gif#gh-dark-mode-only&quot; alt=&quot;A dark colored application for finding movies screening near the user&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

## üñ• Examples

- [**Movies**](https://where2watch.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=organization) ‚Äî An application to help you find streaming platforms to watch movies using [hybrid search](https://www.meilisearch.com/solutions/hybrid-search?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=demos).
- [**Ecommerce**](https://ecommerce.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=demos) ‚Äî Ecommerce website using disjunctive [facets](https://www.meilisearch.com/docs/learn/fine_tuning_results/faceted_search?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=demos), range and rating filtering, and pagination.
- [**Songs**](https://music.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=demos) ‚Äî Search through 47 million of songs.
- [**SaaS**](https://saas.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=demos) ‚Äî Search for contacts, deals, and companies in this [multi-tenant](https://www.meilisearch.com/docs/learn/security/multitenancy_tenant_tokens?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=demos) CRM application.

See the list of all our example apps in our [demos repository](https://github.com/meilisearch/demos).

## ‚ú® Features
- **Hybrid search:** Combine the best of both [semantic](https://www.meilisearch.com/docs/learn/experimental/vector_search?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=features) &amp; full-text search to get the most relevant results
- **Search-as-you-type:** Find &amp; display results in less than 50 milliseconds to provide an intuitive experience
- **[Typo tolerance](https://www.meilisearch.com/docs/learn/relevancy/typo_tolerance_settings?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=features):** get relevant matches even when queries contain typos and misspellings
- **[Filtering](https://www.meilisearch.com/docs/learn/fine_tuning_results/filtering?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=features) and [faceted search](https://www.meilisearch.com/docs/learn/fine_tuning_results/faceted_search?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=features):** enhance your users&#039; search experience with custom filters and build a faceted search interface in a few lines of code
- **[Sorting](https://www.meilisearch.com/docs/learn/fine_tuning_results/sorting?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=features):** sort results based on price, date, or pretty much anything else your users need
- **[Synonym support](https://www.meilisearch.com/docs/learn/relevancy/synonyms?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=features):** configure synonyms to include more relevant content in your search results
- **[Geosearch](https://www.meilisearch.com/docs/learn/fine_tuning_results/geosearch?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=features):** filter and sort documents based on geographic data
- **[Extensive language support](https://www.meilisearch.com/docs/learn/what_is_meilisearch/language?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=features):** search datasets in any language, with optimized support for Chinese, Japanese, Hebrew, and languages using the Latin alphabet
- **[Security management](https://www.meilisearch.com/docs/learn/security/master_api_keys?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=features):** control which users can access what data with API keys that allow fine-grained permissions handling
- **[Multi-Tenancy](https://www.meilisearch.com/docs/learn/security/multitenancy_tenant_tokens?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=features):** personalize search results for any number of application tenants
- **Highly Customizable:** customize Meilisearch to your specific needs or use our out-of-the-box and hassle-free presets
- **[RESTful API](https://www.meilisearch.com/docs/reference/api/overview?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=features):** integrate Meilisearch in your technical stack with our plugins and SDKs
- **AI-ready:** works out of the box with [langchain](https://www.meilisearch.com/with/langchain) and the [model context protocol](https://github.com/meilisearch/meilisearch-mcp)
- **Easy to install, deploy, and maintain**

## üìñ Documentation

You can consult Meilisearch&#039;s documentation at [meilisearch.com/docs](https://www.meilisearch.com/docs/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=docs).

## üöÄ Getting started

For basic instructions on how to set up Meilisearch, add documents to an index, and search for documents, take a look at our [documentation](https://www.meilisearch.com/docs?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=get-started) guide.

## üåç Supercharge your Meilisearch experience

Say goodbye to server deployment and manual updates with [Meilisearch Cloud](https://www.meilisearch.com/cloud?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch). Additional features include analytics &amp; monitoring in many regions around the world. No credit card is required.

## üß∞ SDKs &amp; integration tools

Install one of our SDKs in your project for seamless integration between Meilisearch and your favorite language or framework!

Take a look at the complete [Meilisearch integration list](https://www.meilisearch.com/docs/learn/what_is_meilisearch/sdks?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=sdks-link).

[![Logos belonging to different languages and frameworks supported by Meilisearch, including React, Ruby on Rails, Go, Rust, and PHP](assets/integrations.png)](https://www.meilisearch.com/docs/learn/what_is_meilisearch/sdks?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=sdks-logos)

## ‚öôÔ∏è Advanced usage

Experienced users will want to keep our [API Reference](https://www.meilisearch.com/docs/reference/api/overview?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=advanced) close at hand.

We also offer a wide range of dedicated guides to all Meilisearch features, such as [filtering](https://www.meilisearch.com/docs/learn/fine_tuning_results/filtering?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=advanced), [sorting](https://www.meilisearch.com/docs/learn/fine_tuning_results/sorting?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=advanced), [geosearch](https://www.meilisearch.com/docs/learn/fine_tuning_results/geosearch?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=advanced), [API keys](https://www.meilisearch.com/docs/learn/security/master_api_keys?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=advanced), and [tenant tokens](https://www.meilisearch.com/docs/learn/security/tenant_tokens?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=advanced).

Finally, for more in-depth information, refer to our articles explaining fundamental Meilisearch concepts such as [documents](https://www.meilisearch.com/docs/learn/core_concepts/documents?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=advanced) and [indexes](https://www.meilisearch.com/docs/learn/core_concepts/indexes?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=advanced).

## üìä Telemetry

Meilisearch collects **anonymized** user data to help us improve our product. You can [deactivate this](https://www.meilisearch.com/docs/learn/what_is_meilisearch/telemetry?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=telemetry#how-to-disable-data-collection) whenever you want.

To request deletion of collected data, please write to us at [privacy@meilisearch.com](mailto:privacy@meilisearch.com). Remember to include your `Instance UID` in the message, as this helps us quickly find and delete your data.

If you want to know more about the kind of data we collect and what we use it for, check the [telemetry section](https://www.meilisearch.com/docs/learn/what_is_meilisearch/telemetry?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=telemetry#how-to-disable-data-collection) of our documentation.

## üì´ Get in touch!

Meilisearch is a search engine created by [Meili](https://www.meilisearch.com/careers), a software development company headquartered in France and with team members all over the world. Want to know more about us? [Check out our blog!](https://blog.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=contact)

üóû [Subscribe to our newsletter](https://meilisearch.us2.list-manage.com/subscribe?u=27870f7b71c908a8b359599fb&amp;id=79582d828e) if you don&#039;t want to miss any updates! We promise we won&#039;t clutter your mailbox: we only send one edition every two months.

üíå Want to make a suggestion or give feedback? Here are some of the channels where you can reach us:

- For feature requests, please visit our [product repository](https://github.com/meilisearch/product/discussions)
- Found a bug? Open an [issue](https://github.com/meilisearch/meilisearch/issues)!
- Want to be part of our Discord community? [Join us!](https://discord.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=contact)

Thank you for your support!

## üë©‚Äçüíª Contributing

Meilisearch is, and will always be, open-source! If you want to contribute to the project, please look at [our contribution guidelines](CONTRIBUTING.md).

## üì¶ Versioning

Meilisearch releases and their associated binaries are available on the project&#039;s [releases page](https://github.com/meilisearch/meilisearch/releases).

The binaries are versioned following [SemVer conventions](https://semver.org/). To know more, read our [versioning policy](https://github.com/meilisearch/engine-team/blob/main/resources/versioning-policy.md).

Differently from the binaries, crates in this repository are not currently available on [crates.io](https://crates.io/) and do not follow [SemVer conventions](https://semver.org).
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[vosen/ZLUDA]]></title>
            <link>https://github.com/vosen/ZLUDA</link>
            <guid>https://github.com/vosen/ZLUDA</guid>
            <pubDate>Fri, 04 Jul 2025 00:05:08 GMT</pubDate>
            <description><![CDATA[CUDA on non-NVIDIA GPUs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/vosen/ZLUDA">vosen/ZLUDA</a></h1>
            <p>CUDA on non-NVIDIA GPUs</p>
            <p>Language: Rust</p>
            <p>Stars: 11,641</p>
            <p>Forks: 743</p>
            <p>Stars today: 39 stars today</p>
            <h2>README</h2><pre>[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?style=for-the-badge&amp;logo=discord&amp;logoColor=white)](https://discord.gg/sg6BNzXuc7)

# ZLUDA

ZLUDA is a drop-in replacement for CUDA on non-NVIDIA GPU. ZLUDA allows to run unmodified CUDA applications using non-NVIDIA GPUs with near-native performance.

ZLUDA supports AMD Radeon RX 5000 series and newer GPUs (both desktop and integrated).

![GeekBench 5.5.1 chart](geekbench.svg)

ZLUDA is work in progress. Follow development here and say hi on [Discord](https://discord.gg/sg6BNzXuc7). For more details see the announcement: https://vosen.github.io/ZLUDA/blog/zludas-third-life/

## Usage
**Warning**: This version ZLUDA is under heavy development (more [here](https://vosen.github.io/ZLUDA/blog/zludas-third-life/)) and right now only supports Geekbench. ZLUDA probably will not work with your application just yet.

### Windows
You should have recent AMD GPU driver (&quot;AMD Software: Adrenalin Edition&quot;) installed.\
To run your application you should etiher:
* (Recommended approach) Copy ZLUDA-provided `nvcuda.dll` and `nvml.dll` from `target\release` (if built from sources) or `zluda` (if downloaded a zip package) into a path which your application uses to load CUDA. Paths vary application to application, but usually it&#039;s the directory where the .exe file is located
* Use ZLUDA launcher like below. ZLUDA launcher is known to be buggy and incomplete:
    ```
    &lt;ZLUDA_DIRECTORY&gt;\zluda_with.exe -- &lt;APPLICATION&gt; &lt;APPLICATIONS_ARGUMENTS&gt;
    ```

### Linux

Run your application like this:
```
LD_LIBRARY_PATH=&lt;ZLUDA_DIRECTORY&gt; &lt;APPLICATION&gt; &lt;APPLICATIONS_ARGUMENTS&gt;
```

where `&lt;ZLUDA_DIRECTORY&gt;` is the directory which contains ZLUDA-provided `libcuda.so`: `target/release` if you built from sources or `zluda` if you downloaded prebuilt package.

### MacOS

Not supported

## Building

### Dependencies

* Git
* CMake
* Python 3
* Rust compiler (recent version)
* C++ compiler
* (Optional, but recommended) [Ninja build system](https://ninja-build.org/)

### Build steps

* Git clone the repo (make sure to use `--recursive` option to fetch submodules):  
`git clone --recursive https://github.com/vosen/ZLUDA.git`  
* Enter freshly cloned `ZLUDA` directory and build with cargo (this takes a while):  
`cargo xtask --release`

## Contributing

ZLUDA project has a commercial backing and _does not_ accept donations.
ZLUDA project accepts pull requests and other non-monetary contributions.

If you want to contribute a code fix or documentation update feel free to open a Pull Request.

### Getting started

There&#039;s no architecture document (yet). Two most important crates in ZLUDA are `ptx` (PTX compiler) and `zluda` (AMD GPU runtime). A good starting point to tinkering the project is to run one of the `ptx` unit tests under a debugger and understand what it is doing. `cargo test -p ptx -- ::add_hip` is a simple test that adds two numbers.

Github issues tagged with [&quot;help wanted&quot;](https://github.com/vosen/ZLUDA/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22) are tasks that are self-containted. Their level of difficulty varies, they are not always good beginner tasks, but they defined unambiguously.

If you have questions feel free to ask on [#devtalk channel on Discord](https://discord.com/channels/1273316903783497778/1303329281409159270).


## License

This software is dual-licensed under either the Apache 2.0 license or the MIT license. See [LICENSE-APACHE](LICENSE-APACHE) or [LICENSE-MIT](LICENSE-MIT) for details
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[lancedb/lance]]></title>
            <link>https://github.com/lancedb/lance</link>
            <guid>https://github.com/lancedb/lance</guid>
            <pubDate>Fri, 04 Jul 2025 00:05:07 GMT</pubDate>
            <description><![CDATA[Modern columnar data format for ML and LLMs implemented in Rust. Convert from parquet in 2 lines of code for 100x faster random access, vector index, and data versioning. Compatible with Pandas, DuckDB, Polars, Pyarrow, and PyTorch with more integrations coming..]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lancedb/lance">lancedb/lance</a></h1>
            <p>Modern columnar data format for ML and LLMs implemented in Rust. Convert from parquet in 2 lines of code for 100x faster random access, vector index, and data versioning. Compatible with Pandas, DuckDB, Polars, Pyarrow, and PyTorch with more integrations coming..</p>
            <p>Language: Rust</p>
            <p>Stars: 4,954</p>
            <p>Forks: 370</p>
            <p>Stars today: 20 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;p align=&quot;center&quot;&gt;

&lt;img width=&quot;257&quot; alt=&quot;Lance Logo&quot; src=&quot;https://user-images.githubusercontent.com/917119/199353423-d3e202f7-0269-411d-8ff2-e747e419e492.png&quot;&gt;

**Modern columnar data format for ML. Convert from Parquet in 2-lines of code for 100x faster random access, zero-cost schema evolution, rich secondary indices, versioning, and more.&lt;br/&gt;**
**Compatible with Pandas, DuckDB, Polars, Pyarrow, and Ray with more integrations on the way.**

&lt;a href=&quot;https://lancedb.github.io/lance/&quot;&gt;Documentation&lt;/a&gt; ‚Ä¢
&lt;a href=&quot;https://blog.lancedb.com/&quot;&gt;Blog&lt;/a&gt; ‚Ä¢
&lt;a href=&quot;https://discord.gg/zMM32dvNtd&quot;&gt;Discord&lt;/a&gt; ‚Ä¢
&lt;a href=&quot;https://x.com/lancedb&quot;&gt;X&lt;/a&gt;

[CI]: https://github.com/lancedb/lance/actions/workflows/rust.yml
[CI Badge]: https://github.com/lancedb/lance/actions/workflows/rust.yml/badge.svg
[Docs]: https://lancedb.github.io/lance/
[Docs Badge]: https://img.shields.io/badge/docs-passing-brightgreen
[crates.io]: https://crates.io/crates/lance
[crates.io badge]: https://img.shields.io/crates/v/lance.svg
[Python versions]: https://pypi.org/project/pylance/
[Python versions badge]: https://img.shields.io/pypi/pyversions/pylance

[![CI Badge]][CI]
[![Docs Badge]][Docs]
[![crates.io badge]][crates.io]
[![Python versions badge]][Python versions]

&lt;/p&gt;
&lt;/div&gt;

&lt;hr /&gt;

Lance is a modern columnar data format that is optimized for ML workflows and datasets. Lance is perfect for:

1. Building search engines and feature stores.
2. Large-scale ML training requiring high performance IO and shuffles.
3. Storing, querying, and inspecting deeply nested data for robotics or large blobs like images, point clouds, and more.

The key features of Lance include:

* **High-performance random access:** 100x faster than Parquet without sacrificing scan performance.

* **Vector search:** find nearest neighbors in milliseconds and combine OLAP-queries with vector search.

* **Zero-copy, automatic versioning:** manage versions of your data without needing extra infrastructure.

* **Ecosystem integrations:** Apache Arrow, Pandas, Polars, DuckDB, Ray, Spark and more on the way.

&gt; [!TIP]
&gt; Lance is in active development and we welcome contributions. Please see our [contributing guide](docs/contributing.rst) for more information.

## Quick Start

**Installation**

```shell
pip install pylance
```

To install a preview release:

```shell
pip install --pre --extra-index-url https://pypi.fury.io/lancedb/ pylance
```

&gt; [!TIP]
&gt; Preview releases are released more often than full releases and contain the
&gt; latest features and bug fixes. They receive the same level of testing as full releases.
&gt; We guarantee they will remain published and available for download for at
&gt; least 6 months. When you want to pin to a specific version, prefer a stable release.

**Converting to Lance**

```python
import lance

import pandas as pd
import pyarrow as pa
import pyarrow.dataset

df = pd.DataFrame({&quot;a&quot;: [5], &quot;b&quot;: [10]})
uri = &quot;/tmp/test.parquet&quot;
tbl = pa.Table.from_pandas(df)
pa.dataset.write_dataset(tbl, uri, format=&#039;parquet&#039;)

parquet = pa.dataset.dataset(uri, format=&#039;parquet&#039;)
lance.write_dataset(parquet, &quot;/tmp/test.lance&quot;)
```

**Reading Lance data**
```python
dataset = lance.dataset(&quot;/tmp/test.lance&quot;)
assert isinstance(dataset, pa.dataset.Dataset)
```

**Pandas**
```python
df = dataset.to_table().to_pandas()
df
```

**DuckDB**
```python
import duckdb

# If this segfaults, make sure you have duckdb v0.7+ installed
duckdb.query(&quot;SELECT * FROM dataset LIMIT 10&quot;).to_df()
```

**Vector search**

Download the sift1m subset

```shell
wget ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz
tar -xzf sift.tar.gz
```

Convert it to Lance

```python
import lance
from lance.vector import vec_to_table
import numpy as np
import struct

nvecs = 1000000
ndims = 128
with open(&quot;sift/sift_base.fvecs&quot;, mode=&quot;rb&quot;) as fobj:
    buf = fobj.read()
    data = np.array(struct.unpack(&quot;&lt;128000000f&quot;, buf[4 : 4 + 4 * nvecs * ndims])).reshape((nvecs, ndims))
    dd = dict(zip(range(nvecs), data))

table = vec_to_table(dd)
uri = &quot;vec_data.lance&quot;
sift1m = lance.write_dataset(table, uri, max_rows_per_group=8192, max_rows_per_file=1024*1024)
```

Build the index

```python
sift1m.create_index(&quot;vector&quot;,
                    index_type=&quot;IVF_PQ&quot;,
                    num_partitions=256,  # IVF
                    num_sub_vectors=16)  # PQ
```

Search the dataset

```python
# Get top 10 similar vectors
import duckdb

dataset = lance.dataset(uri)

# Sample 100 query vectors. If this segfaults, make sure you have duckdb v0.7+ installed
sample = duckdb.query(&quot;SELECT vector FROM dataset USING SAMPLE 100&quot;).to_df()
query_vectors = np.array([np.array(x) for x in sample.vector])

# Get nearest neighbors for all of them
rs = [dataset.to_table(nearest={&quot;column&quot;: &quot;vector&quot;, &quot;k&quot;: 10, &quot;q&quot;: q})
      for q in query_vectors]
```

## Directory structure

| Directory          | Description              |
|--------------------|--------------------------|
| [rust](./rust)     | Core Rust implementation |
| [python](./python) | Python bindings (PyO3)   |
| [java](./java)     | Java bindings (JNI)      |
| [docs](./docs)     | Documentation source     |

## What makes Lance different

Here we will highlight a few aspects of Lance‚Äôs design. For more details, see the full [Lance design document](https://lancedb.github.io/lance/format.html).

**Vector index**: Vector index for similarity search over embedding space.
Support both CPUs (``x86_64`` and ``arm``) and GPU (``Nvidia (cuda)`` and ``Apple Silicon (mps)``).

**Encodings**: To achieve both fast columnar scan and sub-linear point queries, Lance uses custom encodings and layouts.

**Nested fields**: Lance stores each subfield as a separate column to support efficient filters like ‚Äúfind images where detected objects include cats‚Äù.

**Versioning**: A Manifest can be used to record snapshots. Currently we support creating new versions automatically via appends, overwrites, and index creation.

**Fast updates** (ROADMAP): Updates will be supported via write-ahead logs.

**Rich secondary indices**: Support `BTree`, `Bitmap`, `Full text search`, `Label list`,
`NGrams`, and more.

## Benchmarks

### Vector search

We used the SIFT dataset to benchmark our results with 1M vectors of 128D

1. For 100 randomly sampled query vectors, we get &lt;1ms average response time (on a 2023 m2 MacBook Air)

![avg_latency.png](docs/avg_latency.png)

2. ANNs are always a trade-off between recall and performance

![avg_latency.png](docs/recall_vs_latency.png)

### Vs. parquet

We create a Lance dataset using the Oxford Pet dataset to do some preliminary performance testing of Lance as compared to Parquet and raw image/XMLs. For analytics queries, Lance is 50-100x better than reading the raw metadata. For batched random access, Lance is 100x better than both parquet and raw files.

![](docs/lance_perf.png)

## Why are you building yet another data format?!

The machine learning development cycle involves the steps:

```mermaid
graph LR
    A[Collection] --&gt; B[Exploration];
    B --&gt; C[Analytics];
    C --&gt; D[Feature Engineer];
    D --&gt; E[Training];
    E --&gt; F[Evaluation];
    F --&gt; C;
    E --&gt; G[Deployment];
    G --&gt; H[Monitoring];
    H --&gt; A;
```

People use different data representations to varying stages for the performance or limited by the tooling available.
Academia mainly uses XML / JSON for annotations and zipped images/sensors data for deep learning, which
is difficult to integrate into data infrastructure and slow to train over cloud storage.
While industry uses data lakes (Parquet-based techniques, i.e., Delta Lake, Iceberg) or data warehouses (AWS Redshift
or Google BigQuery) to collect and analyze data, they have to convert the data into training-friendly formats, such
as [Rikai](https://github.com/eto-ai/rikai)/[Petastorm](https://github.com/uber/petastorm)
or [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord).
Multiple single-purpose data transforms, as well as syncing copies between cloud storage to local training
instances have become a common practice.

While each of the existing data formats excels at the workload it was originally designed for, we need a new data format
tailored for multistage ML development cycles to reduce and data silos.

A comparison of different data formats in each stage of ML development cycle.

|                     | Lance | Parquet &amp; ORC | JSON &amp; XML | TFRecord | Database | Warehouse |
|---------------------|-------|---------------|------------|----------|----------|-----------|
| Analytics           | Fast  | Fast          | Slow       | Slow     | Decent   | Fast      |
| Feature Engineering | Fast  | Fast          | Decent     | Slow     | Decent   | Good      |
| Training            | Fast  | Decent        | Slow       | Fast     | N/A      | N/A       |
| Exploration         | Fast  | Slow          | Fast       | Slow     | Fast     | Decent    |
| Infra Support       | Rich  | Rich          | Decent     | Limited  | Rich     | Rich      |

## Community Highlights

Lance is currently used in production by:
* [LanceDB](https://github.com/lancedb/lancedb), a serverless, low-latency vector database for ML applications
* [LanceDB Enterprise](https://docs.lancedb.com/enterprise/introduction), hyperscale LanceDB with enterprise SLA.
* Leading multimodal Gen AI companies for training over petabyte-scale multimodal data.
* Self-driving car company for large-scale storage, retrieval and processing of multi-modal data.
* E-commerce company for billion-scale+ vector personalized search.
* and more.

## Presentations, Blogs and Talks

* [Designing a Table Format for ML Workloads](https://blog.lancedb.com/designing-a-table-format-for-ml-workloads/), Feb 2025.
* [Transforming Multimodal Data Management with LanceDB, Ray Summit](https://www.youtube.com/watch?v=xmTFEzAh8ho), Oct 2024.
* [Lance v2: A columnar container format for modern data](https://blog.lancedb.com/lance-v2/), Apr 2024.
* [Lance Deep Dive](https://drive.google.com/file/d/1Orh9rK0Mpj9zN_gnQF1eJJFpAc6lStGm/view?usp=drive_link). July 2023.
* [Lance: A New Columnar Data Format](https://docs.google.com/presentation/d/1a4nAiQAkPDBtOfXFpPg7lbeDAxcNDVKgoUkw3cUs2rE/edit#slide=id.p), [Scipy 2022, Austin, TX](https://www.scipy2022.scipy.org/posters). July, 2022.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[biomejs/biome]]></title>
            <link>https://github.com/biomejs/biome</link>
            <guid>https://github.com/biomejs/biome</guid>
            <pubDate>Fri, 04 Jul 2025 00:05:06 GMT</pubDate>
            <description><![CDATA[A toolchain for web projects, aimed to provide functionalities to maintain them. Biome offers formatter and linter, usable via CLI and LSP.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/biomejs/biome">biomejs/biome</a></h1>
            <p>A toolchain for web projects, aimed to provide functionalities to maintain them. Biome offers formatter and linter, usable via CLI and LSP.</p>
            <p>Language: Rust</p>
            <p>Stars: 19,867</p>
            <p>Forks: 630</p>
            <p>Stars today: 18 stars today</p>
            <h2>README</h2><pre>./packages/@biomejs/biome/README.md</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[helix-editor/helix]]></title>
            <link>https://github.com/helix-editor/helix</link>
            <guid>https://github.com/helix-editor/helix</guid>
            <pubDate>Fri, 04 Jul 2025 00:05:05 GMT</pubDate>
            <description><![CDATA[A post-modern modal text editor.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/helix-editor/helix">helix-editor/helix</a></h1>
            <p>A post-modern modal text editor.</p>
            <p>Language: Rust</p>
            <p>Stars: 38,338</p>
            <p>Forks: 2,905</p>
            <p>Stars today: 32 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;h1&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;logo_dark.svg&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;logo_light.svg&quot;&gt;
  &lt;img alt=&quot;Helix&quot; height=&quot;128&quot; src=&quot;logo_light.svg&quot;&gt;
&lt;/picture&gt;
&lt;/h1&gt;

[![Build status](https://github.com/helix-editor/helix/actions/workflows/build.yml/badge.svg)](https://github.com/helix-editor/helix/actions)
[![GitHub Release](https://img.shields.io/github/v/release/helix-editor/helix)](https://github.com/helix-editor/helix/releases/latest)
[![Documentation](https://shields.io/badge/-documentation-452859)](https://docs.helix-editor.com/)
[![GitHub contributors](https://img.shields.io/github/contributors/helix-editor/helix)](https://github.com/helix-editor/helix/graphs/contributors)
[![Matrix Space](https://img.shields.io/matrix/helix-community:matrix.org)](https://matrix.to/#/#helix-community:matrix.org)

&lt;/div&gt;

![Screenshot](./screenshot.png)

A [Kakoune](https://github.com/mawww/kakoune) / [Neovim](https://github.com/neovim/neovim) inspired editor, written in Rust.

The editing model is very heavily based on Kakoune; during development I found
myself agreeing with most of Kakoune&#039;s design decisions.

For more information, see the [website](https://helix-editor.com) or
[documentation](https://docs.helix-editor.com/).

All shortcuts/keymaps can be found [in the documentation on the website](https://docs.helix-editor.com/keymap.html).

[Troubleshooting](https://github.com/helix-editor/helix/wiki/Troubleshooting)

# Features

- Vim-like modal editing
- Multiple selections
- Built-in language server support
- Smart, incremental syntax highlighting and code editing via tree-sitter

Although it&#039;s primarily a terminal-based editor, I am interested in exploring
a custom renderer (similar to Emacs) using wgpu or skulpin.

Note: Only certain languages have indentation definitions at the moment. Check
`runtime/queries/&lt;lang&gt;/` for `indents.scm`.

# Installation

[Installation documentation](https://docs.helix-editor.com/install.html).

[![Packaging status](https://repology.org/badge/vertical-allrepos/helix-editor.svg?exclude_unsupported=1)](https://repology.org/project/helix-editor/versions)

# Contributing

Contributing guidelines can be found [here](./docs/CONTRIBUTING.md).

# Getting help

Your question might already be answered on the [FAQ](https://github.com/helix-editor/helix/wiki/FAQ).

Discuss the project on the community [Matrix Space](https://matrix.to/#/#helix-community:matrix.org) (make sure to join `#helix-editor:matrix.org` if you&#039;re on a client that doesn&#039;t support Matrix Spaces yet).

# Credits

Thanks to [@jakenvac](https://github.com/jakenvac) for designing the logo!
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[tursodatabase/turso]]></title>
            <link>https://github.com/tursodatabase/turso</link>
            <guid>https://github.com/tursodatabase/turso</guid>
            <pubDate>Fri, 04 Jul 2025 00:05:04 GMT</pubDate>
            <description><![CDATA[Turso Database is a project to build the next evolution of SQLite.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/tursodatabase/turso">tursodatabase/turso</a></h1>
            <p>Turso Database is a project to build the next evolution of SQLite.</p>
            <p>Language: Rust</p>
            <p>Stars: 11,522</p>
            <p>Forks: 441</p>
            <p>Stars today: 114 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;turso.png&quot; alt=&quot;Turso Database&quot; width=&quot;800&quot;/&gt;
  &lt;h1 align=&quot;center&quot;&gt;Turso Database&lt;/h1&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;i&gt;Turso Database&lt;/i&gt; is an in-process SQL database, compatible with SQLite.
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a title=&quot;Build Status&quot; target=&quot;_blank&quot; href=&quot;https://github.com/tursodatabase/turso/actions/workflows/rust.yml&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/tursodatabase/turso/rust.yml?style=flat-square&quot;&gt;&lt;/a&gt;
  &lt;a title=&quot;Releases&quot; target=&quot;_blank&quot; href=&quot;https://github.com/tursodatabase/turso/releases&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/release/tursodatabase/turso?style=flat-square&amp;color=9CF&quot;&gt;&lt;/a&gt;
  &lt;a title=&quot;Rust&quot; target=&quot;_blank&quot; href=&quot;https://crates.io/crates/turso&quot;&gt;&lt;img alt=&quot;PyPI&quot; src=&quot;https://img.shields.io/crates/v/turso&quot;&gt;&lt;/a&gt;
  &lt;a title=&quot;JavaScript&quot; target=&quot;_blank&quot; href=&quot;https://www.npmjs.com/package/@tursodatabase/turso&quot;&gt;&lt;img alt=&quot;PyPI&quot; src=&quot;https://img.shields.io/npm/v/@tursodatabase/turso&quot;&gt;&lt;/a&gt;
  &lt;a title=&quot;Python&quot; target=&quot;_blank&quot; href=&quot;https://pypi.org/project/pyturso/&quot;&gt;&lt;img alt=&quot;PyPI&quot; src=&quot;https://img.shields.io/pypi/v/pyturso&quot;&gt;&lt;/a&gt;
  &lt;a title=&quot;MIT&quot; target=&quot;_blank&quot; href=&quot;https://github.com/tursodatabase/turso/blob/main/LICENSE.md&quot;&gt;&lt;img src=&quot;http://img.shields.io/badge/license-MIT-orange.svg?style=flat-square&quot;&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;a title=&quot;GitHub Pull Requests&quot; target=&quot;_blank&quot; href=&quot;https://github.com/tursodatabase/turso/pulls&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-pr-closed/tursodatabase/turso.svg?style=flat-square&amp;color=FF9966&quot;&gt;&lt;/a&gt;
  &lt;a title=&quot;GitHub Commits&quot; target=&quot;_blank&quot; href=&quot;https://github.com/tursodatabase/turso/commits/main&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/commit-activity/m/tursodatabase/turso.svg?style=flat-square&quot;&gt;&lt;/a&gt;
  &lt;a title=&quot;Last Commit&quot; target=&quot;_blank&quot; href=&quot;https://github.com/tursodatabase/turso/commits/main&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/last-commit/tursodatabase/turso.svg?style=flat-square&amp;color=FF9900&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a title=&quot;Developer&#039;s Discord&quot; target=&quot;_blank&quot; href=&quot;https://discord.gg/jgjmyYgHwB&quot;&gt;&lt;img alt=&quot;Chat with the Core Developers on Discord&quot; src=&quot;https://img.shields.io/discord/1258658826257961020?label=Discord&amp;logo=Discord&amp;style=social&amp;label=Core%20Developers&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a title=&quot;Users&#039;s Discord&quot; target=&quot;_blank&quot; href=&quot;https://tur.so/discord&quot;&gt;&lt;img alt=&quot;Chat with other users of Turso (and Turso Cloud) on Discord&quot; src=&quot;https://img.shields.io/discord/933071162680958986?label=Discord&amp;logo=Discord&amp;style=social&amp;label=Users&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

---

## Features and Roadmap

Turso Database is a _work-in-progress_, in-process OLTP database engine library written in Rust that has:

* **SQLite compatibility** [[doc](COMPAT.md)] for SQL dialect, file formats, and the C API
* **Language bindings** for JavaScript/WebAssembly, Rust, Go, Python, and [Java](bindings/java)
* **Asynchronous I/O** support on Linux with `io_uring`
* **OS support** for Linux, macOS, and Windows

In the future, we will be also working on:

* **`BEGIN CONCURRENT`** for improved write throughput.
* **Indexing for vector search**.
* **Improved schema management** including better `ALTER` support and strict column types by default.

## Getting Started

Please see the [Turso Database Manual](docs/manual.md) for more information.

&lt;details&gt;
&lt;summary&gt;üíª Command Line&lt;/summary&gt;
&lt;br&gt;
You can install the latest `turso` release with:

```shell
curl --proto &#039;=https&#039; --tlsv1.2 -LsSf \
  https://github.com/tursodatabase/turso/releases/latest/download/turso_cli-installer.sh | sh
```

Then launch the shell to execute SQL statements:

```console
Turso
Enter &quot;.help&quot; for usage hints.
Connected to a transient in-memory database.
Use &quot;.open FILENAME&quot; to reopen on a persistent database
turso&gt; CREATE TABLE users (id INT PRIMARY KEY, username TEXT);
turso&gt; INSERT INTO users VALUES (1, &#039;alice&#039;);
turso&gt; INSERT INTO users VALUES (2, &#039;bob&#039;);
turso&gt; SELECT * FROM users;
1|alice
2|bob
```

You can also build and run the latest development version with:

```shell
cargo run
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;ü¶Ä Rust&lt;/summary&gt;
&lt;br&gt;

```console
cargo add turso
```

Example usage:

```rust
let db = Builder::new_local(&quot;sqlite.db&quot;).build().await?;
let conn = db.connect()?;

let res = conn.query(&quot;SELECT * FROM users&quot;, ()).await?;
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;‚ú® JavaScript&lt;/summary&gt;
&lt;br&gt;

```console
npm i @tursodatabase/turso
```

Example usage:

```js
import { Database } from &#039;@tursodatabase/turso&#039;;

const db = new Database(&#039;sqlite.db&#039;);
const stmt = db.prepare(&#039;SELECT * FROM users&#039;);
const users = stmt.all();
console.log(users);
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üêç Python&lt;/summary&gt;
&lt;br&gt;

```console
pip install pyturso
```

Example usage:

```python
import turso

con = turso.connect(&quot;sqlite.db&quot;)
cur = con.cursor()
res = cur.execute(&quot;SELECT * FROM users&quot;)
print(res.fetchone())
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üêπ Go&lt;/summary&gt;
&lt;br&gt;

1. Clone the repository
2. Build the library and set your LD_LIBRARY_PATH to include turso&#039;s target directory
```console
cargo build --package limbo-go
export LD_LIBRARY_PATH=/path/to/limbo/target/debug:$LD_LIBRARY_PATH
```
3. Use the driver

```console
go get github.com/tursodatabase/turso
go install github.com/tursodatabase/turso
```

Example usage:
```go
import (
    &quot;database/sql&quot;
    _ &quot;github.com/tursodatabase/turso&quot;
)

conn, _ = sql.Open(&quot;sqlite3&quot;, &quot;sqlite.db&quot;)
defer conn.Close()

stmt, _ := conn.Prepare(&quot;select * from users&quot;)
defer stmt.Close()

rows, _ = stmt.Query()
for rows.Next() {
    var id int
    var username string
    _ := rows.Scan(&amp;id, &amp;username)
    fmt.Printf(&quot;User: ID: %d, Username: %s\n&quot;, id, username)
}
```
&lt;/details&gt;

&lt;details&gt;

&lt;summary&gt;‚òïÔ∏è Java&lt;/summary&gt;
&lt;br&gt;

We integrated Turso Database into JDBC. For detailed instructions on how to use Turso Database with java, please refer to
the [README.md under bindings/java](bindings/java/README.md).
&lt;/details&gt;

## Contributing

We&#039;d love to have you contribute to Turso Database! Please check out the [contribution guide] to get started.

## FAQ

### Is Turso Database ready for production use?

Turso Database is currently under heavy development and is **not** ready for production use.

### How is Turso Database different from Turso&#039;s libSQL?

Turso Database is a project to build the next evolution of SQLite in Rust, with a strong open contribution focus and features like native async support, vector search, and more. The libSQL project is also an attempt to evolve SQLite in a similar direction, but through a fork rather than a rewrite.

Rewriting SQLite in Rust started as an unassuming experiment, and due to its incredible success, replaces libSQL as our intended direction. At this point, libSQL is production ready, Turso Database is not - although it is evolving rapidly. More details [here](https://turso.tech/blog/we-will-rewrite-sqlite-and-we-are-going-all-in).

## Publications

* Pekka Enberg, Sasu Tarkoma, Jon Crowcroft Ashwin Rao (2024). Serverless Runtime / Database Co-Design With Asynchronous I/O. In _EdgeSys ‚Äò24_. [[PDF]](https://penberg.org/papers/penberg-edgesys24.pdf)
* Pekka Enberg, Sasu Tarkoma, and Ashwin Rao (2023). Towards Database and Serverless Runtime Co-Design. In _CoNEXT-SW ‚Äô23_. [[PDF](https://penberg.org/papers/penberg-conext-sw-23.pdf)] [[Slides](https://penberg.org/papers/penberg-conext-sw-23-slides.pdf)]

## License

This project is licensed under the [MIT license].

### Contribution

Unless you explicitly state otherwise, any contribution intentionally submitted
for inclusion in Turso Database by you, shall be licensed as MIT, without any additional
terms or conditions.

[contribution guide]: https://github.com/tursodatabase/turso/blob/main/CONTRIBUTING.md
[MIT license]: https://github.com/tursodatabase/turso/blob/main/LICENSE.md

## Partners

Thanks to all the partners of Turso!

&lt;a href=&quot;https://blacksmith.sh&quot;&gt;&lt;img src=&quot;assets/blacksmith.svg&quot; width=&quot;400&quot;&gt;&lt;/a&gt;

&lt;a href=&quot;https://antithesis.com/&quot;&gt;&lt;img src=&quot;assets/antithesis.jpg&quot; width=&quot;400&quot;&gt;&lt;/a&gt;

## Contributors

Thanks to all the contributors to Turso Database!

&lt;a href=&quot;https://github.com/tursodatabase/turso/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=tursodatabase/turso&quot; /&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[mmastrac/stylus]]></title>
            <link>https://github.com/mmastrac/stylus</link>
            <guid>https://github.com/mmastrac/stylus</guid>
            <pubDate>Fri, 04 Jul 2025 00:05:03 GMT</pubDate>
            <description><![CDATA[Lightweight status page for home infrastructure]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mmastrac/stylus">mmastrac/stylus</a></h1>
            <p>Lightweight status page for home infrastructure</p>
            <p>Language: Rust</p>
            <p>Stars: 157</p>
            <p>Forks: 4</p>
            <p>Stars today: 4 stars today</p>
            <h2>README</h2><pre># stylus ![CI](https://github.com/mmastrac/stylus/workflows/CI/badge.svg?branch=master) [![crates.io](https://img.shields.io/crates/v/stylus.svg)](https://crates.io/crates/stylus)

**Stylus** (_style + status_) is a lightweight status page for home infrastructure. Configure a set of bash scripts that test
the various parts of your infrastructure, set up HTML/SVG with a diagram of your network, and stylus will
generate you a dynamic stylesheet to give you a visual overview of the current state.

![Screenshot](docs/screenshot-2.png)

[diagrams.net]: https://app.diagrams.net/?splash=0&amp;p=svgdata

## Running

[See the wiki page on running Stylus here](https://github.com/mmastrac/stylus/wiki/Running-Stylus).

## Theory of operation

**Stylus** acts as a webserver with special endpoints and a status monitoring tool.

The status monitoring portion is based around scripts, written in any shell scripting language you like. Each
script is run on an interval, and if the script returns `0` that is considered &quot;up&quot; for a given service. If the
service times out, or returns a non-zero error this is considered a soft (&quot;yellow&quot;) or hard (&quot;red&quot;) failure.

The special endpoints available on the webserver are:

  * `/style.css`: A dynamically generated CSS file based on the current
  * `/status.json`: A JSON representation of the current state

The `style.css` endpoint may be linked by a HTML or SVG file served from the `static` directory that is configured. If
desired, the HTML page can dynamically refresh the CSS periodically using Javascript. See the included example for a
sample of how this might work.

## Getting started

The first step to get started is copying the example to a location you&#039;d like to use to run your status monitoring
scripts. For each of the servers you&#039;d like to monitor, create a new subdirectory under `monitor.d` with the name of
that server.

Using a tool like [diagrams.net], create an SVG diagram of your network. Attach an SVG DOM attribute to the elements
you&#039;d like to style with status changes. If you&#039;re using [diagrams.net], this can be done using the `svgdata` plugin.
Alternatively, you can use the automatic identifiers generated by your SVG editor as your monitoring identifiers.

From the SVG you&#039;ve generated, create CSS selectors and rules that will apply styles to the appropriate elements as
statuses change. The SVG `fill` attribute is a good candidate to change, but ensure that you&#039;re using `!important` on
all your rules to override the fill colors created by your SVG editor.

## Configuration

Example `config.yaml` for a **Stylus** install. This configuration attaches metadata to the various states and has
selectors that apply to both and HTML (for a status table) and CSS (for a status SVG image).

```yaml
version: 1
server:
  port: 8000
  static: static/

monitor:
  dir: monitor.d/

css:
  # Arbitrary metadata can be associated with the four states
  metadata:
    blank:
      color: &quot;white&quot;
    red:
      color: &quot;#fa897b&quot;
    yellow:
      color: &quot;#ffdd94&quot;
    green:
      color: &quot;#d0e6a5&quot;
  rules:
    # Multiple CSS rules with handlebars replacements are supported
    - selectors: &quot;#{{monitor.id}}&quot;
      declarations: &quot;
        background-color: {{monitor.status.css.metadata.color}} !important;
      &quot;
```

The monitors are configured by creating a subdirectory in the monitor directory (default `monitor.d/`) and
placing a `config.yaml` in that monitor subdirectory.

```yaml
# ID is optional and will be inferred from the directory
id: router-1
test:
  interval: 60s
  timeout: 30s
  command: test.sh
```

## Test scripts

The test scripts are usually pretty simple. Note that the docker container ships with a number of useful utilities,
but you can consider manually installing additional packages (either creating an additional docker container or manually
running alpine&#039;s `apk` tool inside the container) to handle your specific cases.

### Ping

Unless you have a particularly lossy connection, a single ping should be enough to test whether a host is up:

```bash
#!/bin/bash
set -xeuf -o pipefail
ping -c 1 8.8.8.8
```

### cURL

For hosts with services that may be up or down, you may want to use cURL to test whether the service itself
is reachable.

```bash
#!/bin/bash
set -xeuf -o pipefail
curl --retry 2 --max-time 5 --connect-timeout 5 http://192.168.1.1:9000
```

### Advanced techniques

Tools such as `jq`, `sed`, or `awk` can be used for more advanced tests (ie: APIs). If needed, ssh can be used to
connect to hosts and remote tests can be executed. `snmpwalk` and `snmpget` can also be used to construct tests for
devices that speak SNMP. 

## Performance

**Stylus** is very lightweight, both from a processing and memory perspective.

On a Raspberry Pi 1B, **Stylus** uses less than 1% of CPU while refreshing CSS at a rate of 1/s. On a 2015 MacBook Pro, 
Stylus uses approximately 0.1% of a single core while actively refreshing.

**Stylus** uses approxmately 2MB to monitor 15 services on a Raspberry Pi (according to [ps_mem](https://raw.githubusercontent.com/pixelb/ps_mem/master/ps_mem.py)).

When not actively monitored, **Stylus** uses a nearly unmeasurable amount of CPU and is pretty much limited by how
heavyweight your test scripts are.

## Screenshots

### Included example

![Screenshot](docs/screenshot-1.png)

### My personal network

![Screenshot](docs/screenshot-2.png)

[diagrams.net]: https://app.diagrams.net/?splash=0&amp;p=svgdata

## Historical Note

Note that this project was originally written using deno, but was rewritten in Rust to support Raspberry Pis. The
original deno source is available in the `deno` branch.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[lldap/lldap]]></title>
            <link>https://github.com/lldap/lldap</link>
            <guid>https://github.com/lldap/lldap</guid>
            <pubDate>Fri, 04 Jul 2025 00:05:02 GMT</pubDate>
            <description><![CDATA[Light LDAP implementation]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lldap/lldap">lldap/lldap</a></h1>
            <p>Light LDAP implementation</p>
            <p>Language: Rust</p>
            <p>Stars: 5,185</p>
            <p>Forks: 266</p>
            <p>Stars today: 24 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;lldap - Light LDAP implementation for authentication&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;i style=&quot;font-size:24px&quot;&gt;LDAP made easy.&lt;/i&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/lldap/lldap/actions/workflows/rust.yml?query=branch%3Amain&quot;&gt;
    &lt;img
      src=&quot;https://github.com/lldap/lldap/actions/workflows/rust.yml/badge.svg&quot;
      alt=&quot;Build&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/h5PEdRMNyP&quot;&gt;
    &lt;img alt=&quot;Discord&quot; src=&quot;https://img.shields.io/discord/898492935446876200?label=discord&amp;logo=discord&quot; /&gt;
  &lt;/a&gt;

  &lt;a href=&quot;https://twitter.com/nitnelave1?ref_src=twsrc%5Etfw&quot;&gt;
    &lt;img
      src=&quot;https://img.shields.io/twitter/follow/nitnelave1?style=social&quot;
      alt=&quot;Twitter Follow&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/rust-secure-code/safety-dance/&quot;&gt;
    &lt;img
      src=&quot;https://img.shields.io/badge/unsafe-forbidden-success.svg&quot;
      alt=&quot;Unsafe forbidden&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://app.codecov.io/gh/lldap/lldap&quot;&gt;
    &lt;img alt=&quot;Codecov&quot; src=&quot;https://img.shields.io/codecov/c/github/lldap/lldap&quot; /&gt;
  &lt;/a&gt;
  &lt;br/&gt;
  &lt;a href=&quot;https://www.buymeacoffee.com/nitnelave&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png&quot; alt=&quot;Buy Me A Coffee&quot; style=&quot;height: 41px !important;width: 174px !important;box-shadow: 0px 3px 2px 0px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 3px 2px 0px rgba(190, 190, 190, 0.5) !important;&quot; &gt;
  &lt;/a&gt;
&lt;/p&gt;

- [About](#about)
- [Installation](#installation)
  - [With Docker](#with-docker)
  - [With Podman](#with-podman)
  - [With Kubernetes](#with-kubernetes)
  - [From a package repository](#from-a-package-repository)
  - [With FreeBSD](#with-freebsd)
  - [From source](#from-source)
    - [Backend](#backend)
    - [Frontend](#frontend)
  - [Cross-compilation](#cross-compilation)
- [Usage](#usage)
  - [Recommended architecture](#recommended-architecture)
- [Client configuration](#client-configuration)
  - [Compatible services](#compatible-services)
  - [General configuration guide](#general-configuration-guide)
  - [Integration with OS&#039;s](#integration-with-oss)
  - [Sample client configurations](#sample-client-configurations)
  - [Incompatible services](#incompatible-services)
- [Migrating from SQLite](#migrating-from-sqlite)
- [Comparisons with other services](#comparisons-with-other-services)
  - [vs OpenLDAP](#vs-openldap)
  - [vs FreeIPA](#vs-freeipa)
  - [vs Kanidm](#vs-kanidm)
- [I can&#039;t log in!](#i-cant-log-in)
- [Discord Integration](#discord-integration)
- [Contributions](#contributions)

## About

This project is a lightweight authentication server that provides an
opinionated, simplified LDAP interface for authentication. It integrates with
many backends, from KeyCloak to Authelia to Nextcloud and
[more](#compatible-services)!

&lt;img
  src=&quot;https://raw.githubusercontent.com/lldap/lldap/master/screenshot.png&quot;
  alt=&quot;Screenshot of the user list page&quot;
  width=&quot;50%&quot;
  align=&quot;right&quot;
/&gt;

It comes with a frontend that makes user management easy, and allows users to
edit their own details or reset their password by email.

The goal is _not_ to provide a full LDAP server; if you&#039;re interested in that,
check out OpenLDAP. This server is a user management system that is:

- simple to setup (no messing around with `slapd`),
- simple to manage (friendly web UI),
- low resources,
- opinionated with basic defaults so you don&#039;t have to understand the
  subtleties of LDAP.

It mostly targets self-hosting servers, with open-source components like
Nextcloud, Airsonic and so on that only support LDAP as a source of external
authentication.

For more features (OAuth/OpenID support, reverse proxy, ...) you can install
other components (KeyCloak, Authelia, ...) using this server as the source of
truth for users, via LDAP.

By default, the data is stored in SQLite, but you can swap the backend with
MySQL/MariaDB or PostgreSQL.

## Installation

### With Docker

The image is available at `lldap/lldap`. You should persist the `/data`
folder, which contains your configuration and the SQLite database (you can
remove this step if you use a different DB and configure with environment
variables only).

Configure the server by copying the `lldap_config.docker_template.toml` to
`/data/lldap_config.toml` and updating the configuration values (especially the
`jwt_secret` and `ldap_user_pass`, unless you override them with env variables).
Environment variables should be prefixed with `LLDAP_` to override the
configuration.

If the `lldap_config.toml` doesn&#039;t exist when starting up, LLDAP will use
default one. The default admin password is `password`, you can change the
password later using the web interface.

Secrets can also be set through a file. The filename should be specified by the
variables `LLDAP_JWT_SECRET_FILE` or `LLDAP_KEY_SEED_FILE`, and the file
contents are loaded into the respective configuration parameters. Note that
`_FILE` variables take precedence.

Example for docker compose:

- You can use either the `:latest` tag image or `:stable` as used in this example.
- `:latest` tag image contains recently pushed code or feature tests, in which some instability can be expected.
- If `UID` and `GID` no defined LLDAP will use default `UID` and `GID` number `1000`.
- If no `TZ` is set, default `UTC` timezone will be used.
- You can generate the secrets by running `./generate_secrets.sh`

```yaml
version: &quot;3&quot;

volumes:
  lldap_data:
    driver: local

services:
  lldap:
    image: lldap/lldap:stable
    ports:
      # For LDAP, not recommended to expose, see Usage section.
      #- &quot;3890:3890&quot;
      # For LDAPS (LDAP Over SSL), enable port if LLDAP_LDAPS_OPTIONS__ENABLED set true, look env below
      #- &quot;6360:6360&quot;
      # For the web front-end
      - &quot;17170:17170&quot;
    volumes:
      - &quot;lldap_data:/data&quot;
      # Alternatively, you can mount a local folder
      # - &quot;./lldap_data:/data&quot;
    environment:
      - UID=####
      - GID=####
      - TZ=####/####
      - LLDAP_JWT_SECRET=REPLACE_WITH_RANDOM
      - LLDAP_KEY_SEED=REPLACE_WITH_RANDOM
      - LLDAP_LDAP_BASE_DN=dc=example,dc=com
      - LLDAP_LDAP_USER_PASS=adminPas$word
      # If using LDAPS, set enabled true and configure cert and key path
      # - LLDAP_LDAPS_OPTIONS__ENABLED=true
      # - LLDAP_LDAPS_OPTIONS__CERT_FILE=/path/to/certfile.crt
      # - LLDAP_LDAPS_OPTIONS__KEY_FILE=/path/to/keyfile.key
      # You can also set a different database:
      # - LLDAP_DATABASE_URL=mysql://mysql-user:password@mysql-server/my-database
      # - LLDAP_DATABASE_URL=postgres://postgres-user:password@postgres-server/my-database
      # If using SMTP, set the following variables
      # - LLDAP_SMTP_OPTIONS__ENABLE_PASSWORD_RESET=true
      # - LLDAP_SMTP_OPTIONS__SERVER=smtp.example.com
      # - LLDAP_SMTP_OPTIONS__PORT=465 # Check your smtp providor&#039;s documentation for this setting
      # - LLDAP_SMTP_OPTIONS__SMTP_ENCRYPTION=TLS # How the connection is encrypted, either &quot;NONE&quot; (no encryption, port 25), &quot;TLS&quot; (sometimes called SSL, port 465) or &quot;STARTTLS&quot; (sometimes called TLS, port 587).
      # - LLDAP_SMTP_OPTIONS__USER=no-reply@example.com # The SMTP user, usually your email address
      # - LLDAP_SMTP_OPTIONS__PASSWORD=PasswordGoesHere # The SMTP password
      # - LLDAP_SMTP_OPTIONS__FROM=no-reply &lt;no-reply@example.com&gt; # The header field, optional: how the sender appears in the email. The first is a free-form name, followed by an email between &lt;&gt;.
      # - LLDAP_SMTP_OPTIONS__TO=admin &lt;admin@example.com&gt; # Same for reply-to, optional.
```

Then the service will listen on two ports, one for LDAP and one for the web
front-end.

### With Podman

LLDAP works well with rootless Podman either through command line deployment
or using [quadlets](example_configs/podman-quadlets/). The example quadlets
include configuration with postgresql and file based secrets, but have comments
for several other deployment strategies.

### With Kubernetes

See https://github.com/Evantage-WS/lldap-kubernetes for a LLDAP deployment for Kubernetes

You can bootstrap your lldap instance (users, groups)
using [bootstrap.sh](example_configs/bootstrap/bootstrap.md#kubernetes-job).
It can be run by Argo CD for managing users in git-opt way, or as a one-shot job.

### From a package repository

**Do not open issues in this repository for problems with third-party
pre-built packages. Report issues downstream.**

Depending on the distribution you use, it might be possible to install lldap
from a package repository, officially supported by the distribution or
community contributed.

Each package offers a [systemd service](https://wiki.archlinux.org/title/systemd#Using_units) `lldap.service` or [rc.d_lldap](example_configs/freebsd/rc.d_lldap) `rc.d/lldap` to (auto-)start and stop lldap.&lt;br&gt;
When using the distributed packages, the default login is `admin/password`. You can change that from the web UI after starting the service.

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Arch Linux&lt;/b&gt;&lt;/summary&gt;
&lt;br&gt;
  Arch Linux offers unofficial support through the &lt;a href=&quot;https://wiki.archlinux.org/title/Arch_User_Repository&quot;&gt;Arch User Repository (AUR)&lt;/a&gt;.&lt;br&gt;
  The package descriptions can be used &lt;a href=&quot;https://wiki.archlinux.org/title/Arch_User_Repository#Getting_started&quot;&gt;to create and install packages&lt;/a&gt;.&lt;br&gt;&lt;br&gt;
  Support: &lt;a href=&quot;https://github.com/lldap/lldap/discussions/1044&quot;&gt;Discussions&lt;/a&gt;&lt;br&gt;
  Package repository: &lt;a href=&quot;https://aur.archlinux.org/packages&quot;&gt;Arch User Repository&lt;/a&gt;&lt;br&gt;&lt;br&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;Package name&lt;/td&gt;
    &lt;td&gt;Maintainer&lt;/td&gt;
    &lt;td&gt;Description&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://aur.archlinux.org/packages/lldap&quot;&gt;lldap&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://github.com/Zepmann&quot;&gt;@Zepmann&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Builds the latest stable version.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://aur.archlinux.org/packages/lldap-bin&quot;&gt;lldap-bin&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://github.com/Zepmann&quot;&gt;@Zepmann&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Uses the latest pre-compiled binaries from the &lt;a href=&quot;https://github.com/lldap/lldap/releases&quot;&gt;releases in this repository&lt;/a&gt;.&lt;br&gt;
        This package is recommended if you want to run LLDAP on a system with limited resources.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://aur.archlinux.org/packages/lldap-git&quot;&gt;lldap-git&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;Builds the latest main branch code.&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
LLDAP configuration file: /etc/lldap.toml&lt;br&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Debian&lt;/b&gt;&lt;/summary&gt;
&lt;br&gt;
  Unofficial Debian support is offered through the &lt;a href=&quot;https://build.opensuse.org/&quot;&gt;openSUSE Build Service&lt;/a&gt;.&lt;br&gt;&lt;br&gt;
  Maintainer: &lt;a href=&quot;https://github.com/Masgalor&quot;&gt;@Masgalor&lt;/a&gt;&lt;br&gt;
  Support: &lt;a href=&quot;https://codeberg.org/Masgalor/LLDAP-Packaging/issues&quot;&gt;Codeberg&lt;/a&gt;, &lt;a href=&quot;https://github.com/lldap/lldap/discussions&quot;&gt;Discussions&lt;/a&gt;&lt;br&gt;
  Package repository: &lt;a href=&quot;https://software.opensuse.org//download.html?project=home%3AMasgalor%3ALLDAP&amp;package=lldap&quot;&gt;SUSE openBuildService&lt;/a&gt;&lt;br&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;Available packages:&lt;/td&gt;
    &lt;td&gt;lldap&lt;/td&gt;
    &lt;td&gt;Light LDAP server for authentication.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;lldap-extras&lt;/td&gt;
    &lt;td&gt;Meta-Package for LLDAP and its tools and extensions.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;lldap-migration-tool&lt;/td&gt;
    &lt;td&gt;CLI migration tool to go from OpenLDAP to LLDAP.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;lldap-set-password&lt;/td&gt;
    &lt;td&gt;CLI tool to set a user password in LLDAP.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;lldap-cli&lt;/td&gt;
    &lt;td&gt;LLDAP-CLI is an unofficial command line interface for LLDAP.&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
LLDAP configuration file: /etc/lldap/lldap_config.toml&lt;br&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;&lt;b&gt;CentOS&lt;/b&gt;&lt;/summary&gt;
&lt;br&gt;
  Unofficial CentOS support is offered through the &lt;a href=&quot;https://build.opensuse.org/&quot;&gt;openSUSE Build Service&lt;/a&gt;.&lt;br&gt;&lt;br&gt;
  Maintainer: &lt;a href=&quot;https://github.com/Masgalor&quot;&gt;@Masgalor&lt;/a&gt;&lt;br&gt;
  Support: &lt;a href=&quot;https://codeberg.org/Masgalor/LLDAP-Packaging/issues&quot;&gt;Codeberg&lt;/a&gt;, &lt;a href=&quot;https://github.com/lldap/lldap/discussions&quot;&gt;Discussions&lt;/a&gt;&lt;br&gt;
  Package repository: &lt;a href=&quot;https://software.opensuse.org//download.html?project=home%3AMasgalor%3ALLDAP&amp;package=lldap&quot;&gt;SUSE openBuildService&lt;/a&gt;&lt;br&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;Available packages:&lt;/td&gt;
    &lt;td&gt;lldap&lt;/td&gt;
    &lt;td&gt;Light LDAP server for authentication.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;lldap-extras&lt;/td&gt;
    &lt;td&gt;Meta-Package for LLDAP and its tools and extensions.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;lldap-migration-tool&lt;/td&gt;
    &lt;td&gt;CLI migration tool to go from OpenLDAP to LLDAP.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;lldap-set-password&lt;/td&gt;
    &lt;td&gt;CLI tool to set a user password in LLDAP.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;lldap-cli&lt;/td&gt;
    &lt;td&gt;LLDAP-CLI is an unofficial command line interface for LLDAP.&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
LLDAP configuration file: /etc/lldap/lldap_config.toml&lt;br&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Fedora&lt;/b&gt;&lt;/summary&gt;
&lt;br&gt;
  Unofficial Fedora support is offered through the &lt;a href=&quot;https://build.opensuse.org/&quot;&gt;openSUSE Build Service&lt;/a&gt;.&lt;br&gt;&lt;br&gt;
  Maintainer: &lt;a href=&quot;https://github.com/Masgalor&quot;&gt;@Masgalor&lt;/a&gt;&lt;br&gt;
  Support: &lt;a href=&quot;https://codeberg.org/Masgalor/LLDAP-Packaging/issues&quot;&gt;Codeberg&lt;/a&gt;, &lt;a href=&quot;https://github.com/lldap/lldap/discussions&quot;&gt;Discussions&lt;/a&gt;&lt;br&gt;
  Package repository: &lt;a href=&quot;https://software.opensuse.org//download.html?project=home%3AMasgalor%3ALLDAP&amp;package=lldap&quot;&gt;SUSE openBuildService&lt;/a&gt;&lt;br&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;Available packages:&lt;/td&gt;
    &lt;td&gt;lldap&lt;/td&gt;
    &lt;td&gt;Light LDAP server for authentication.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;lldap-extras&lt;/td&gt;
    &lt;td&gt;Meta-Package for LLDAP and its tools and extensions.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;lldap-migration-tool&lt;/td&gt;
    &lt;td&gt;CLI migration tool to go from OpenLDAP to LLDAP.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;lldap-set-password&lt;/td&gt;
    &lt;td&gt;CLI tool to set a user password in LLDAP.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;lldap-cli&lt;/td&gt;
    &lt;td&gt;LLDAP-CLI is an unofficial command line interface for LLDAP.&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
LLDAP configuration file: /etc/lldap/lldap_config.toml&lt;br&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;&lt;b&gt;OpenSUSE&lt;/b&gt;&lt;/summary&gt;
&lt;br&gt;
  Unofficial OpenSUSE support is offered through the &lt;a href=&quot;https://build.opensuse.org/&quot;&gt;openSUSE Build Service&lt;/a&gt;.&lt;br&gt;&lt;br&gt;
  Maintainer: &lt;a href=&quot;https://github.com/Masgalor&quot;&gt;@Masgalor&lt;/a&gt;&lt;br&gt;
  Support: &lt;a href=&quot;https://codeberg.org/Masgalor/LLDAP-Packaging/issues&quot;&gt;Codeberg&lt;/a&gt;, &lt;a href=&quot;https://github.com/lldap/lldap/discussions&quot;&gt;Discussions&lt;/a&gt;&lt;br&gt;
  Package repository: &lt;a href=&quot;https://software.opensuse.org//download.html?project=home%3AMasgalor%3ALLDAP&amp;package=lldap&quot;&gt;SUSE openBuildService&lt;/a&gt;&lt;br&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;Available packages:&lt;/td&gt;
    &lt;td&gt;lldap&lt;/td&gt;
    &lt;td&gt;Light LDAP server for authentication.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;lldap-extras&lt;/td&gt;
    &lt;td&gt;Meta-Package for LLDAP and its tools and extensions.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;lldap-migration-tool&lt;/td&gt;
    &lt;td&gt;CLI migration tool to go from OpenLDAP to LLDAP.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;lldap-set-password&lt;/td&gt;
    &lt;td&gt;CLI tool to set a user password in LLDAP.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;lldap-cli&lt;/td&gt;
    &lt;td&gt;LLDAP-CLI is an unofficial command line interface for LLDAP.&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
LLDAP configuration file: /etc/lldap/lldap_config.toml&lt;br&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Ubuntu&lt;/b&gt;&lt;/summary&gt;
&lt;br&gt;
  Unofficial Ubuntu support is offered through the &lt;a href=&quot;https://build.opensuse.org/&quot;&gt;openSUSE Build Service&lt;/a&gt;.&lt;br&gt;&lt;br&gt;
  Maintainer: &lt;a href=&quot;https://github.com/Masgalor&quot;&gt;@Masgalor&lt;/a&gt;&lt;br&gt;
  Support: &lt;a href=&quot;https://codeberg.org/Masgalor/LLDAP-Packaging/issues&quot;&gt;Codeberg&lt;/a&gt;, &lt;a href=&quot;https://github.com/lldap/lldap/discussions&quot;&gt;Discussions&lt;/a&gt;&lt;br&gt;
  Package repository: &lt;a href=&quot;https://software.opensuse.org//download.html?project=home%3AMasgalor%3ALLDAP&amp;package=lldap&quot;&gt;SUSE openBuildService&lt;/a&gt;&lt;br&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;Available packages:&lt;/td&gt;
    &lt;td&gt;lldap&lt;/td&gt;
    &lt;td&gt;Light LDAP server for authentication.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;lldap-extras&lt;/td&gt;
    &lt;td&gt;Meta-Package for LLDAP and its tools and extensions.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;lldap-migration-tool&lt;/td&gt;
    &lt;td&gt;CLI migration tool to go from OpenLDAP to LLDAP.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;lldap-set-password&lt;/td&gt;
    &lt;td&gt;CLI tool to set a user password in LLDAP.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;lldap-cli&lt;/td&gt;
    &lt;td&gt;LLDAP-CLI is an unofficial command line interface for LLDAP.&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
LLDAP configuration file: /etc/lldap/lldap_config.toml&lt;br&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;&lt;b&gt;FreeBSD&lt;/b&gt;&lt;/summary&gt;
&lt;br&gt;
  Official FreeBSD support is offered through the &lt;a href=&quot;https://www.freshports.org/&quot;&gt;FreeBSD Freshport Build Service&lt;/a&gt;.&lt;br&gt;&lt;br&gt;
  Maintainer: &lt;a href=&quot;https://github.com/aokblast&quot;&gt;@aokblast&lt;/a&gt;&lt;br&gt;
  Support: &lt;a href=&quot;https://bugs.freebsd.org/bugzilla/&quot;&gt;Bugzilla&lt;/a&gt;, &lt;a href=&quot;https://github.com/lldap/lldap/discussions&quot;&gt;Discussions&lt;/a&gt;&lt;br&gt;
  Package repository: &lt;a href=&quot;https://www.freshports.org/net/lldap/&quot;&gt;FreeBSD Freshport Build&lt;/a&gt;&lt;br&gt;
  FreeBSD Setup and Migration Manual: &lt;a href=&quot;https://github.com/lldap/lldap/blob/main/example_configs/freebsd/freebsd-install.md&quot;&gt; Using FreeBSD &lt;/a&gt;&lt;br&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;Available packages:&lt;/td&gt;
    &lt;td&gt;lldap&lt;/td&gt;
    &lt;td&gt;Light LDAP server for authentication.&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
LLDAP configuration file: /usr/local/lldap_server/lldap_config.toml&lt;br&gt;
&lt;/details&gt;

### From source

#### Backend

To compile the project, you&#039;ll need:

- curl and gzip: `sudo apt install curl gzip`
- Rust/Cargo: [rustup.rs](https://rustup.rs/)

Then you can compile the server (and the migration tool if you want):

```shell
cargo build --release -p lldap -p lldap_migration_tool
```

The resulting binaries will be in `./target/release/`. Alternatively, you can
just run `cargo run -- run` to run the server.

#### Frontend

To bring up the server, you&#039;ll need to compile the frontend. In addition to
`cargo`, you&#039;ll need WASM-pack, which can be installed by running `cargo install wasm-pack`.

Then you can build the frontend files with

```shell
./app/build.sh
```

(you&#039;ll need to run this after every front-end change to update the WASM
package served).

The default config is in `src/infra/configuration.rs`, but you can override it
by creating an `lldap_config.toml`, setting environment variables or passing
arguments to `cargo run`. Have a look at the docker template:
`lldap_config.docker_template.toml`.

You can also install it as a systemd service, see
[lldap.service](example_configs/lldap.service).

### Cross-compilation

Docker images are provided for AMD64, ARM64 and ARM/V7.

If you want to cross-compile yourself, you can do so by installing
[`cross`](https://github.com/rust-embedded/cross):

```sh
cargo install cross
cross build --target=armv7-unknown-linux-musleabihf -p lldap --release
./app/build.sh
```

(Replace `armv7-unknown-linux-musleabihf` with the correct Rust target for your
device.)

You can then get the compiled server binary in
`target/armv7-unknown-linux-musleabihf/release/lldap` and the various needed files
(`index.html`, `main.js`, `pkg` folder) in the `app` folder. Copy them to the
Raspberry Pi (or other target), with the folder structure maintained (`app`
files in an `app` folder next to the binary).

## Usage

The simplest way to use LLDAP is through the web front-end. There you can
create users, set passwords, add them to groups and so on. Users can also
connect to the web UI and change their information, or request a password reset
link (if you configured the SMTP client).

You can create and manage custom attributes through the Web UI, or through the
community-contributed CLI frontend (
[Zepmann/lldap-cli](https://github.com/Zepmann/lldap-cli)). This is necessary
for some service integrations.

The [bootstrap.sh](scripts/bootstrap.sh) script can enforce a list of
users/groups/attributes from a given 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[cocoindex-io/cocoindex]]></title>
            <link>https://github.com/cocoindex-io/cocoindex</link>
            <guid>https://github.com/cocoindex-io/cocoindex</guid>
            <pubDate>Fri, 04 Jul 2025 00:05:01 GMT</pubDate>
            <description><![CDATA[Data transformation framework for AI. Ultra performant, with incremental processing.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/cocoindex-io/cocoindex">cocoindex-io/cocoindex</a></h1>
            <p>Data transformation framework for AI. Ultra performant, with incremental processing.</p>
            <p>Language: Rust</p>
            <p>Stars: 2,071</p>
            <p>Forks: 140</p>
            <p>Stars today: 22 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://cocoindex.io/images/github.svg&quot; alt=&quot;CocoIndex&quot;&gt;
&lt;/p&gt;

&lt;h2 align=&quot;center&quot;&gt;Extract, Transform, Index Data. Easy and Fresh. üå¥&lt;/h2&gt;

&lt;div align=&quot;center&quot;&gt;

[![GitHub](https://img.shields.io/github/stars/cocoindex-io/cocoindex?color=5B5BD6)](https://github.com/cocoindex-io/cocoindex)
[![Documentation](https://img.shields.io/badge/Documentation-394e79?logo=readthedocs&amp;logoColor=00B9FF)](https://cocoindex.io/docs/getting_started/quickstart)
[![License](https://img.shields.io/badge/license-Apache%202.0-5B5BD6?logoColor=white)](https://opensource.org/licenses/Apache-2.0)
[![PyPI version](https://img.shields.io/pypi/v/cocoindex?color=5B5BD6)](https://pypi.org/project/cocoindex/)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/cocoindex)](https://pypistats.org/packages/cocoindex)

[![CI](https://github.com/cocoindex-io/cocoindex/actions/workflows/CI.yml/badge.svg?event=push&amp;color=5B5BD6)](https://github.com/cocoindex-io/cocoindex/actions/workflows/CI.yml)
[![release](https://github.com/cocoindex-io/cocoindex/actions/workflows/release.yml/badge.svg?event=push&amp;color=5B5BD6)](https://github.com/cocoindex-io/cocoindex/actions/workflows/release.yml)
[![Discord](https://img.shields.io/discord/1314801574169673738?logo=discord&amp;color=5B5BD6&amp;logoColor=white)](https://discord.com/invite/zpA9S2DR7s)
&lt;/div&gt;

**CocoIndex** is an ultra performant data transformation framework, with its core engine written in Rust. The problem it tries to solve is to make it easy to prepare fresh data for AI - either creating embedding, building knowledge graphs, or performing other data transformations - and take real-time data pipelines beyond traditional SQL.

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://cocoindex.io/images/cocoindex-features.png&quot; alt=&quot;CocoIndex Features&quot; width=&quot;500&quot;&gt;
&lt;/p&gt;

The philosophy is to have the framework handle the source updates, and having developers only worry about defining a series of data transformation, inspired by spreadsheet.

## Dataflow programming
Unlike a workflow orchestration framework where data is usually opaque, in CocoIndex, data and data operations are first class citizens. CocoIndex follows the idea of [Dataflow](https://en.wikipedia.org/wiki/Dataflow_programming) programming model. Each transformation creates a new field solely based on input fields, without hidden states and value mutation. All data before/after each transformation is observable, with lineage out of the box.

**Particularly**, users don&#039;t explicitly mutate data by creating, updating and deleting. Rather, they define something like - for a set of source data, this is the transformation or formula. The framework takes care of the data operations such as when to create, update, or delete.

```python
# import
data[&#039;content&#039;] = flow_builder.add_source(...)

# transform
data[&#039;out&#039;] = data[&#039;content&#039;]
    .transform(...)
    .transform(...)

# collect data
collector.collect(...)

# export to db, vector db, graph db ...
collector.export(...)
```

## Data Freshness
As a data framework, CocoIndex takes it to the next level on data freshness. **Incremental processing** is one of the core values provided by CocoIndex.

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/f4eb29b3-84ee-4fa0-a1e2-80eedeeabde6&quot; alt=&quot;Incremental Processing&quot; width=&quot;700&quot;&gt;
&lt;/p&gt;

The frameworks takes care of
- Change data capture.
- Figure out what exactly needs to be updated, and only updating that without having to recompute everything.

This makes it fast to reflect any source updates to the target store. If you have concerns with surfacing stale data to AI agents and are spending lots of efforts working on infra piece to optimize the latency, the framework actually handles it for you.


## Quick Start:
If you&#039;re new to CocoIndex, we recommend checking out
- üìñ [Documentation](https://cocoindex.io/docs)
- ‚ö°  [Quick Start Guide](https://cocoindex.io/docs/getting_started/quickstart)
- üé¨ [Quick Start Video Tutorial](https://youtu.be/gv5R8nOXsWU?si=9ioeKYkMEnYevTXT)

### Setup

1. Install CocoIndex Python library

```bash
pip install -U cocoindex
```

2. [Install Postgres](https://cocoindex.io/docs/getting_started/installation#-install-postgres) if you don&#039;t have one. CocoIndex uses it for incremental processing.


### Define data flow

Follow [Quick Start Guide](https://cocoindex.io/docs/getting_started/quickstart) to define your first indexing flow. An example flow looks like:

```python
@cocoindex.flow_def(name=&quot;TextEmbedding&quot;)
def text_embedding_flow(flow_builder: cocoindex.FlowBuilder, data_scope: cocoindex.DataScope):
    # Add a data source to read files from a directory
    data_scope[&quot;documents&quot;] = flow_builder.add_source(cocoindex.sources.LocalFile(path=&quot;markdown_files&quot;))

    # Add a collector for data to be exported to the vector index
    doc_embeddings = data_scope.add_collector()

    # Transform data of each document
    with data_scope[&quot;documents&quot;].row() as doc:
        # Split the document into chunks, put into `chunks` field
        doc[&quot;chunks&quot;] = doc[&quot;content&quot;].transform(
            cocoindex.functions.SplitRecursively(),
            language=&quot;markdown&quot;, chunk_size=2000, chunk_overlap=500)

        # Transform data of each chunk
        with doc[&quot;chunks&quot;].row() as chunk:
            # Embed the chunk, put into `embedding` field
            chunk[&quot;embedding&quot;] = chunk[&quot;text&quot;].transform(
                cocoindex.functions.SentenceTransformerEmbed(
                    model=&quot;sentence-transformers/all-MiniLM-L6-v2&quot;))

            # Collect the chunk into the collector.
            doc_embeddings.collect(filename=doc[&quot;filename&quot;], location=chunk[&quot;location&quot;],
                                   text=chunk[&quot;text&quot;], embedding=chunk[&quot;embedding&quot;])

    # Export collected data to a vector index.
    doc_embeddings.export(
        &quot;doc_embeddings&quot;,
        cocoindex.targets.Postgres(),
        primary_key_fields=[&quot;filename&quot;, &quot;location&quot;],
        vector_indexes=[
            cocoindex.VectorIndexDef(
                field_name=&quot;embedding&quot;,
                metric=cocoindex.VectorSimilarityMetric.COSINE_SIMILARITY)])
```

It defines an index flow like this:

&lt;img width=&quot;363&quot; alt=&quot;Data Flow&quot; src=&quot;https://github.com/user-attachments/assets/2ea7be6d-3d94-42b1-b2bd-22515577e463&quot; /&gt;


## üöÄ Examples and demo

| Example | Description |
|---------|-------------|
| [Text Embedding](examples/text_embedding) | Index text documents with embeddings for semantic search |
| [Code Embedding](examples/code_embedding) | Index code embeddings for semantic search |
| [PDF Embedding](examples/pdf_embedding) | Parse PDF and index text embeddings for semantic search |
| [Manuals LLM Extraction](examples/manuals_llm_extraction) | Extract structured information from a manual using LLM |
| [Amazon S3 Embedding](examples/amazon_s3_embedding) | Index text documents from Amazon S3 |
| [Google Drive Text Embedding](examples/gdrive_text_embedding) | Index text documents from Google Drive |
| [Docs to Knowledge Graph](examples/docs_to_knowledge_graph) | Extract relationships from Markdown documents and build a knowledge graph |
| [Embeddings to Qdrant](examples/text_embedding_qdrant) | Index documents in a Qdrant collection for semantic search |
| [FastAPI Server with Docker](examples/fastapi_server_docker) | Run the semantic search server in a Dockerized FastAPI setup |
| [Product Recommendation](examples/product_recommendation) | Build real-time product recommendations with LLM and graph database|
| [Image Search with Vision API](examples/image_search) | Generates detailed captions for images using a vision model, embeds them, enables live-updating semantic search via FastAPI and served on a React frontend|

More coming and stay tuned üëÄ!

## üìñ Documentation
For detailed documentation, visit [CocoIndex Documentation](https://cocoindex.io/docs), including a [Quickstart guide](https://cocoindex.io/docs/getting_started/quickstart).

## ü§ù Contributing
We love contributions from our community ‚ù§Ô∏è. For details on contributing or running the project for development, check out our [contributing guide](https://cocoindex.io/docs/about/contributing).

## üë• Community
Welcome with a huge coconut hug ü••‚ãÜÔΩ°Àöü§ó. We are super excited for community contributions of all kinds - whether it&#039;s code improvements, documentation updates, issue reports, feature requests, and discussions in our Discord.

Join our community here:

- üåü [Star us on GitHub](https://github.com/cocoindex-io/cocoindex)
- üëã [Join our Discord community](https://discord.com/invite/zpA9S2DR7s)
- ‚ñ∂Ô∏è [Subscribe to our YouTube channel](https://www.youtube.com/@cocoindex-io)
- üìú [Read our blog posts](https://cocoindex.io/blogs/)

## Support us:
We are constantly improving, and more features and examples are coming soon. If you love this project, please drop us a star ‚≠ê at GitHub repo [![GitHub](https://img.shields.io/github/stars/cocoindex-io/cocoindex?color=5B5BD6)](https://github.com/cocoindex-io/cocoindex) to stay tuned and help us grow.

## License
CocoIndex is Apache 2.0 licensed.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[paradedb/paradedb]]></title>
            <link>https://github.com/paradedb/paradedb</link>
            <guid>https://github.com/paradedb/paradedb</guid>
            <pubDate>Fri, 04 Jul 2025 00:05:00 GMT</pubDate>
            <description><![CDATA[ParadeDB is a modern Elasticsearch alternative built on Postgres. Built for real-time, update-heavy workloads.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/paradedb/paradedb">paradedb/paradedb</a></h1>
            <p>ParadeDB is a modern Elasticsearch alternative built on Postgres. Built for real-time, update-heavy workloads.</p>
            <p>Language: Rust</p>
            <p>Stars: 7,252</p>
            <p>Forks: 248</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://paradedb.com&quot;&gt;&lt;img src=&quot;docs/logo/readme.svg&quot; alt=&quot;ParadeDB&quot;&gt;&lt;/a&gt;
&lt;br&gt;
&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;b&gt;Postgres for Search and Analytics&lt;/b&gt; &lt;br /&gt;
&lt;/p&gt;

&lt;h3 align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://paradedb.com&quot;&gt;Website&lt;/a&gt; &amp;bull;
  &lt;a href=&quot;https://docs.paradedb.com&quot;&gt;Docs&lt;/a&gt; &amp;bull;
  &lt;a href=&quot;https://join.slack.com/t/paradedbcommunity/shared_invite/zt-32abtyjg4-yoYoi~RPh9MSW8tDbl0BQw&quot;&gt;Community&lt;/a&gt; &amp;bull;
  &lt;a href=&quot;https://paradedb.com/blog/&quot;&gt;Blog&lt;/a&gt; &amp;bull;
  &lt;a href=&quot;https://docs.paradedb.com/changelog/&quot;&gt;Changelog&lt;/a&gt;
&lt;/h3&gt;

---

[![Artifact Hub](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/paradedb)](https://artifacthub.io/packages/search?repo=paradedb)
[![Docker Pulls](https://img.shields.io/docker/pulls/paradedb/paradedb)](https://hub.docker.com/r/paradedb/paradedb)
[![License](https://img.shields.io/github/license/paradedb/paradedb?color=blue)](https://github.com/paradedb/paradedb?tab=AGPL-3.0-1-ov-file#readme)
[![Slack URL](https://img.shields.io/badge/Join%20Slack-purple?logo=slack&amp;link=https%3A%2F%2Fjoin.slack.com%2Ft%2Fparadedbcommunity%2Fshared_invite%2Fzt-32abtyjg4-yoYoi~RPh9MSW8tDbl0BQw)](https://join.slack.com/t/paradedbcommunity/shared_invite/zt-32abtyjg4-yoYoi~RPh9MSW8tDbl0BQw)
[![X URL](https://img.shields.io/twitter/url?url=https%3A%2F%2Ftwitter.com%2Fparadedb&amp;label=Follow%20%40paradedb)](https://x.com/paradedb)

[ParadeDB](https://paradedb.com) is a modern Elasticsearch alternative built on Postgres. Built for real-time, update-heavy workloads.

## Roadmap

- [x] Search
  - [x] Full-text search with BM25 with [pg_search](https://github.com/paradedb/paradedb/tree/dev/pg_search#overview)
  - [x] Dense and sparse vector search with [pgvector](https://github.com/pgvector/pgvector#pgvector)
  - [ ] Distributed search
- [ ] Analytics
  - [x] Faceted search over Postgres data with [pg_search](https://github.com/paradedb/paradedb/tree/dev/pg_search#overview)
  - [ ] Column-oriented Postgres table access method

For a detailed roadmap, see the [ParadeDB Roadmap for 2025](https://github.com/orgs/paradedb/discussions/2041).

## Get Started

To get started, please visit our [documentation](https://docs.paradedb.com).

## Deploying ParadeDB

ParadeDB and its extensions can be deployed in one of two ways:

- Docker image based on [Postgres](https://hub.docker.com/_/postgres) ([see deployment instructions](https://docs.paradedb.com/deploy/aws))
- Kubernetes Helm chart based on [CloudNativePG](https://artifacthub.io/packages/helm/cloudnative-pg/cloudnative-pg) ([see deployment instructions](https://docs.paradedb.com/deploy/helm))

For more information, including enterprise features and support, please [contact us by email](mailto:sales@paradedb.com).

### Extensions

You can find prebuilt binaries for the ParadeDB Postgres extensions on Debian 11, 12, Ubuntu 22.04 and 24.04, Red Hat Enterprise Linux 8 and 9, and macOS 14 (Sonoma) and 15 (Sequoia) for Postgres 14+ in the [GitHub Releases](https://github.com/paradedb/paradedb/releases/latest).

ParadeDB supports all versions supported by the PostgreSQL Global Development Group, which includes PostgreSQL 14+, and you can compile the extensions for other versions of Postgres by following the instructions in the respective extension&#039;s README.

### Docker Image

To quickly get a ParadeDB instance up and running, simply pull and run the latest Docker image:

```bash
docker run --name paradedb -e POSTGRES_PASSWORD=password paradedb/paradedb
```

This will start a ParadeDB instance with default user `postgres` and password `password`. You can then connect to the database using `psql`:

```bash
docker exec -it paradedb psql -U postgres
```

To install ParadeDB locally or on-premise, we recommend using our `docker-compose.yml` file. Alternatively, you can pass the appropriate environment variables to the `docker run` command, replacing the &lt;&gt; with your desired values:

```bash
docker run \
  --name paradedb \
  -e POSTGRES_USER=&lt;user&gt; \
  -e POSTGRES_PASSWORD=&lt;password&gt; \
  -e POSTGRES_DB=&lt;dbname&gt; \
  -v paradedb_data:/var/lib/postgresql/data/ \
  -p 5432:5432 \
  -d \
  paradedb/paradedb:latest
```

This will start a ParadeDB instance with non-root user `&lt;user&gt;` and password `&lt;password&gt;`. The `-v` flag enables your ParadeDB data to persist across restarts in a Docker volume named `paradedb_data`.

You can then connect to the database using `psql`:

```bash
docker exec -it paradedb psql -U &lt;user&gt; -d &lt;dbname&gt; -p 5432 -W
```

### Helm Chart

ParadeDB is also available for Kubernetes via our Helm chart. You can find our Helm chart in the [ParadeDB Helm Chart GitHub repository](https://github.com/paradedb/charts) or download it directly from [Artifact Hub](https://artifacthub.io/packages/helm/paradedb/paradedb).

### ParadeDB Cloud

At the moment, ParadeDB is not available as a managed cloud service. If you are interested in a ParadeDB Cloud service, please let us know by joining our [waitlist](https://form.typeform.com/to/jHkLmIzx).

## Support

If you&#039;re missing a feature or have found a bug, please open a
[GitHub Issue](https://github.com/paradedb/paradedb/issues/new/choose).

To get community support, you can:

- Post a question in the [ParadeDB Slack Community](https://join.slack.com/t/paradedbcommunity/shared_invite/zt-32abtyjg4-yoYoi~RPh9MSW8tDbl0BQw)
- Ask for help on our [GitHub Discussions](https://github.com/paradedb/paradedb/discussions)

If you need commercial support, please [contact the ParadeDB team](mailto:sales@paradedb.com).

## Contributing

We welcome community contributions, big or small, and are here to guide you along
the way. To get started contributing, check our [first timer issues](https://github.com/paradedb/paradedb/labels/good%20first%20issue)
or message us in the [ParadeDB Community Slack](https://join.slack.com/t/paradedbcommunity/shared_invite/zt-32abtyjg4-yoYoi~RPh9MSW8tDbl0BQw). Once you contribute, ping us in Slack and we&#039;ll send you some ParadeDB swag!

For more information on how to contribute, please see our
[Contributing Guide](/CONTRIBUTING.md).

This project is released with a [Contributor Code of Conduct](/CODE_OF_CONDUCT.md).
By participating in this project, you agree to follow its terms.

Thank you for helping us make ParadeDB better for everyone :heart:.

## License

ParadeDB is licensed under the [GNU Affero General Public License v3.0](LICENSE) and as commercial software. For commercial licensing, please contact us at [sales@paradedb.com](mailto:sales@paradedb.com).
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[apache/arrow-rs]]></title>
            <link>https://github.com/apache/arrow-rs</link>
            <guid>https://github.com/apache/arrow-rs</guid>
            <pubDate>Fri, 04 Jul 2025 00:04:59 GMT</pubDate>
            <description><![CDATA[Official Rust implementation of Apache Arrow]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/apache/arrow-rs">apache/arrow-rs</a></h1>
            <p>Official Rust implementation of Apache Arrow</p>
            <p>Language: Rust</p>
            <p>Stars: 3,006</p>
            <p>Forks: 965</p>
            <p>Stars today: 2 stars today</p>
            <h2>README</h2><pre>&lt;!---
  Licensed to the Apache Software Foundation (ASF) under one
  or more contributor license agreements.  See the NOTICE file
  distributed with this work for additional information
  regarding copyright ownership.  The ASF licenses this file
  to you under the Apache License, Version 2.0 (the
  &quot;License&quot;); you may not use this file except in compliance
  with the License.  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing,
  software distributed under the License is distributed on an
  &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
  KIND, either express or implied.  See the License for the
  specific language governing permissions and limitations
  under the License.
--&gt;

# Native Rust implementation of Apache Arrow and Apache Parquet

Welcome to the [Rust][rust] implementation of [Apache Arrow], the popular in-memory columnar format.

This repository contains the following crates:

| Crate              | Description                                                                  | Latest API Docs                                  | README                            |
| ------------------ | ---------------------------------------------------------------------------- | ------------------------------------------------ | --------------------------------- |
| [`arrow`]          | Core functionality (memory layout, arrays, low level computations)           | [docs.rs](https://docs.rs/arrow/latest)          | [(README)][arrow-readme]          |
| [`arrow-flight`]   | Support for Arrow-Flight IPC protocol                                        | [docs.rs](https://docs.rs/arrow-flight/latest)   | [(README)][flight-readme]         |
| [`parquet`]        | Support for Parquet columnar file format                                     | [docs.rs](https://docs.rs/parquet/latest)        | [(README)][parquet-readme]        |
| [`parquet_derive`] | A crate for deriving RecordWriter/RecordReader for arbitrary, simple structs | [docs.rs](https://docs.rs/parquet-derive/latest) | [(README)][parquet-derive-readme] |

The current development version the API documentation in this repo can be found [here](https://arrow.apache.org/rust).

Note: previously the [`object_store`] crate was also part of this repository,
but it has been moved to the [arrow-rs-object-store repository]

[apache arrow]: https://arrow.apache.org/
[`arrow`]: https://crates.io/crates/arrow
[`parquet`]: https://crates.io/crates/parquet
[`parquet_derive`]: https://crates.io/crates/parquet-derive
[`arrow-flight`]: https://crates.io/crates/arrow-flight
[arrow-rs-object-store repository]: https://github.com/apache/arrow-rs-object-store

## Release Versioning and Schedule

The Arrow Rust project releases approximately monthly and follows [Semantic
Versioning].

Due to available maintainer and testing bandwidth, [`arrow`] crates ([`arrow`],
[`arrow-flight`], etc.) are released on the same schedule with the same versions
as the [`parquet`] and [`parquet-derive`] crates.

This crate releases every month. We release new major versions (with potentially
breaking API changes) at most once a quarter, and release incremental minor
versions in the intervening months. See [ticket #5368] for more details.

To keep our maintenance burden down, we do regularly scheduled releases (major
and minor) from the `main` branch. How we handle PRs with breaking API changes
is described in the [contributing] guide.

[contributing]: CONTRIBUTING.md#breaking-changes

Planned Release Schedule

| Approximate Date | Version    | Notes                                   |
| ---------------- | ---------- | --------------------------------------- |
| July 2025        | [`56.0.0`] | Major, potentially breaking API changes |
| August 2025      | [`56.1.0`] | Minor, NO breaking API changes          |
| September 2025   | [`56.2.0`] | Minor, NO breaking API changes          |
| October 2025     | [`57.0.0`] | Major, potentially breaking API changes |

[`56.0.0`]: https://github.com/apache/arrow-rs/issues/7395
[`56.1.0`]: https://github.com/apache/arrow-rs/issues/7837
[`56.2.0`]: https://github.com/apache/arrow-rs/issues/7836
[`57.0.0`]: https://github.com/apache/arrow-rs/issues/7835
[ticket #5368]: https://github.com/apache/arrow-rs/issues/5368
[semantic versioning]: https://semver.org/

### Rust Version Compatibility Policy

arrow-rs, parquet and object_store are built and tested with stable Rust, and will keep a rolling MSRV (minimum supported Rust version) that can only be updated in major releases on a need by basis (e.g. project dependencies bump their MSRV or a particular Rust feature is useful for us etc.). The new MSRV if selected will be at least 6 months old. The minor releases are guaranteed to have the same MSRV.

Note: If a Rust hotfix is released for the current MSRV, the MSRV will be updated to the specific minor version that includes all applicable hotfixes preceding other policies.

E.g.

in Apr 2025 we will release version 55.0.0 which might have a version bump. But the Rust version selected in this case will be at most version 1.81.

### Guidelines for `panic` vs `Result`

In general, use panics for bad states that are unreachable, unrecoverable or harmful.
For those caused by invalid user input, however, we prefer to report that invalidity
gracefully as an error result instead of panicking. In general, invalid input should result
in an `Error` as soon as possible. It _is_ ok for code paths after validation to assume
validation has already occurred and panic if not. See [ticket #6737] for more nuances.

[ticket #6737]: https://github.com/apache/arrow-rs/issues/6737

### Deprecation Guidelines

Minor releases may deprecate, but not remove APIs. Deprecating APIs allows
downstream Rust programs to still compile, but generate compiler warnings. This
gives downstream crates time to migrate prior to API removal.

To deprecate an API:

- Mark the API as deprecated using `#[deprecated]` and specify the exact arrow-rs version in which it was deprecated
- Concisely describe the preferred API to help the user transition

The deprecated version is the next version which will be released (please
consult the list above). To mark the API as deprecated, use the
`#[deprecated(since = &quot;...&quot;, note = &quot;...&quot;)]` attribute.

Foe example

```rust
#[deprecated(since = &quot;51.0.0&quot;, note = &quot;Use `date_part` instead&quot;)]
```

In general, deprecated APIs will remain in the codebase for at least two major releases after
they were deprecated (typically between 6 - 9 months later). For example, an API
deprecated in `51.3.0` can be removed in `54.0.0` (or later). Deprecated APIs
may be removed earlier or later than these guidelines at the discretion of the
maintainers.

## Related Projects

There are several related crates in different repositories

| Crate               | Description                                                  | Documentation                      |
| ------------------- | ------------------------------------------------------------ | ---------------------------------- |
| [`object_store`]    | Object Storage (aws, azure, gcp, local, in-memory) interface | [(README)](object_store-readme)    |
| [`datafusion`]      | In-memory query engine with SQL support                      | [(README)][datafusion-readme]      |
| [`ballista`]        | Distributed query execution                                  | [(README)][ballista-readme]        |
| [`parquet_opendal`] | Use [`opendal`] for [`parquet`] Arrow IO                     | [(README)][parquet_opendal-readme] |

[`datafusion`]: https://crates.io/crates/datafusion
[`ballista`]: https://crates.io/crates/ballista
[`parquet_opendal`]: https://crates.io/crates/parquet_opendal
[parquet_opendal-readme]: https://github.com/apache/opendal/blob/main/integrations/parquet/README.md
[object_store-readme]: https://github.com/apache/arrow-rs-object-store/blob/main/README.md

Collectively, these crates support a wider array of functionality for analytic computations in Rust.

For example, you can write SQL queries or a `DataFrame` (using the
[`datafusion`] crate) to read a parquet file (using the [`parquet`] crate),
evaluate it in-memory using Arrow&#039;s columnar format (using the [`arrow`] crate),
and send to another process (using the [`arrow-flight`] crate).

Generally speaking, the [`arrow`] crate offers functionality for using Arrow
arrays, and [`datafusion`] offers most operations typically found in SQL,
including `join`s and window functions.

You can find more details about each crate in their respective READMEs.

## Arrow Rust Community

The `dev@arrow.apache.org` mailing list serves as the core communication channel for the Arrow community. Instructions for signing up and links to the archives can be found on the [Arrow Community](https://arrow.apache.org/community/) page. All major announcements and communications happen there.

The Rust Arrow community also uses the official [ASF Slack](https://s.apache.org/slack-invite) for informal discussions and coordination. This is
a great place to meet other contributors and get guidance on where to contribute. Join us in the `#arrow-rust` channel and feel free to ask for an invite via:

1. the `dev@arrow.apache.org` mailing list
2. the [GitHub Discussions][discussions]
3. the [Discord channel](https://discord.gg/YAb2TdazKQ)

The Rust implementation uses [GitHub issues][issues] as the system of record for new features and bug fixes and
this plays a critical role in the release process.

For design discussions we generally use GitHub issues.

There is more information in the [contributing] guide.

[rust]: https://www.rust-lang.org/
[`object_store`]: https://crates.io/crates/object-store
[arrow-readme]: arrow/README.md
[contributing]: CONTRIBUTING.md
[parquet-readme]: parquet/README.md
[flight-readme]: arrow-flight/README.md
[datafusion-readme]: https://github.com/apache/datafusion/blob/main/README.md
[ballista-readme]: https://github.com/apache/datafusion-ballista/blob/main/README.md
[parquet-derive-readme]: parquet_derive/README.md
[issues]: https://github.com/apache/arrow-rs/issues
[discussions]: https://github.com/apache/arrow-rs/discussions
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[nushell/nushell]]></title>
            <link>https://github.com/nushell/nushell</link>
            <guid>https://github.com/nushell/nushell</guid>
            <pubDate>Fri, 04 Jul 2025 00:04:58 GMT</pubDate>
            <description><![CDATA[A new type of shell]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/nushell/nushell">nushell/nushell</a></h1>
            <p>A new type of shell</p>
            <p>Language: Rust</p>
            <p>Stars: 35,642</p>
            <p>Forks: 1,870</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre># Nushell &lt;!-- omit in toc --&gt;
[![Crates.io](https://img.shields.io/crates/v/nu.svg)](https://crates.io/crates/nu)
[![Build Status](https://img.shields.io/github/actions/workflow/status/nushell/nushell/ci.yml?branch=main)](https://github.com/nushell/nushell/actions)
[![Nightly Build](https://github.com/nushell/nushell/actions/workflows/nightly-build.yml/badge.svg)](https://github.com/nushell/nushell/actions/workflows/nightly-build.yml)
[![Discord](https://img.shields.io/discord/601130461678272522.svg?logo=discord)](https://discord.gg/NtAbbGn)
[![The Changelog #363](https://img.shields.io/badge/The%20Changelog-%23363-61c192.svg)](https://changelog.com/podcast/363)
[![GitHub commit activity](https://img.shields.io/github/commit-activity/m/nushell/nushell)](https://github.com/nushell/nushell/graphs/commit-activity)
[![GitHub contributors](https://img.shields.io/github/contributors/nushell/nushell)](https://github.com/nushell/nushell/graphs/contributors)

A new type of shell.

![Example of nushell](assets/nushell-autocomplete6.gif &quot;Example of nushell&quot;)

## Table of Contents &lt;!-- omit in toc --&gt;

- [Status](#status)
- [Learning About Nu](#learning-about-nu)
- [Installation](#installation)
- [Configuration](#configuration)
- [Philosophy](#philosophy)
  - [Pipelines](#pipelines)
  - [Opening files](#opening-files)
  - [Plugins](#plugins)
- [Goals](#goals)
- [Officially Supported By](#officially-supported-by)
- [Contributing](#contributing)
- [License](#license)

## Status

This project has reached a minimum-viable-product level of quality. Many people use it as their daily driver, but it may be unstable for some commands. Nu&#039;s design is subject to change as it matures.

## Learning About Nu

The [Nushell book](https://www.nushell.sh/book/) is the primary source of Nushell documentation. You can find [a full list of Nu commands in the book](https://www.nushell.sh/commands/), and we have many examples of using Nu in our [cookbook](https://www.nushell.sh/cookbook/).

We&#039;re also active on [Discord](https://discord.gg/NtAbbGn); come and chat with us!

## Installation

To quickly install Nu:

```bash
# Linux and macOS
brew install nushell
# Windows
winget install nushell
```

To use `Nu` in GitHub Action, check [setup-nu](https://github.com/marketplace/actions/setup-nu) for more detail.

Detailed installation instructions can be found in the [installation chapter of the book](https://www.nushell.sh/book/installation.html). Nu is available via many package managers:

[![Packaging status](https://repology.org/badge/vertical-allrepos/nushell.svg?columns=3)](https://repology.org/project/nushell/versions)

For details about which platforms the Nushell team actively supports, see [our platform support policy](devdocs/PLATFORM_SUPPORT.md).

## Configuration

The default configurations can be found at [sample_config](crates/nu-utils/src/default_files)
which are the configuration files one gets when they startup Nushell for the first time.

It sets all of the default configuration to run Nushell.  From here one can
then customize this file for their specific needs.

To see where *config.nu* is located on your system simply type this command.

```rust
$nu.config-path
```

Please see our [book](https://www.nushell.sh) for all of the Nushell documentation.


## Philosophy

Nu draws inspiration from projects like PowerShell, functional programming languages, and modern CLI tools.
Rather than thinking of files and data as raw streams of text, Nu looks at each input as something with structure.
For example, when you list the contents of a directory what you get back is a table of rows, where each row represents an item in that directory.
These values can be piped through a series of steps, in a series of commands called a &#039;pipeline&#039;.

### Pipelines

In Unix, it&#039;s common to pipe between commands to split up a sophisticated command over multiple steps.
Nu takes this a step further and builds heavily on the idea of _pipelines_.
As in the Unix philosophy, Nu allows commands to output to stdout and read from stdin.
Additionally, commands can output structured data (you can think of this as a third kind of stream).
Commands that work in the pipeline fit into one of three categories:

-   Commands that produce a stream (e.g., `ls`)
-   Commands that filter a stream (e.g., `where type == &quot;dir&quot;`)
-   Commands that consume the output of the pipeline (e.g., `table`)

Commands are separated by the pipe symbol (`|`) to denote a pipeline flowing left to right.

```shell
ls | where type == &quot;dir&quot; | table
# =&gt; ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
# =&gt; ‚îÇ #  ‚îÇ   name   ‚îÇ type ‚îÇ  size   ‚îÇ   modified    ‚îÇ
# =&gt; ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
# =&gt; ‚îÇ  0 ‚îÇ .cargo   ‚îÇ dir  ‚îÇ     0 B ‚îÇ 9 minutes ago ‚îÇ
# =&gt; ‚îÇ  1 ‚îÇ assets   ‚îÇ dir  ‚îÇ     0 B ‚îÇ 2 weeks ago   ‚îÇ
# =&gt; ‚îÇ  2 ‚îÇ crates   ‚îÇ dir  ‚îÇ 4.0 KiB ‚îÇ 2 weeks ago   ‚îÇ
# =&gt; ‚îÇ  3 ‚îÇ docker   ‚îÇ dir  ‚îÇ     0 B ‚îÇ 2 weeks ago   ‚îÇ
# =&gt; ‚îÇ  4 ‚îÇ docs     ‚îÇ dir  ‚îÇ     0 B ‚îÇ 2 weeks ago   ‚îÇ
# =&gt; ‚îÇ  5 ‚îÇ images   ‚îÇ dir  ‚îÇ     0 B ‚îÇ 2 weeks ago   ‚îÇ
# =&gt; ‚îÇ  6 ‚îÇ pkg_mgrs ‚îÇ dir  ‚îÇ     0 B ‚îÇ 2 weeks ago   ‚îÇ
# =&gt; ‚îÇ  7 ‚îÇ samples  ‚îÇ dir  ‚îÇ     0 B ‚îÇ 2 weeks ago   ‚îÇ
# =&gt; ‚îÇ  8 ‚îÇ src      ‚îÇ dir  ‚îÇ 4.0 KiB ‚îÇ 2 weeks ago   ‚îÇ
# =&gt; ‚îÇ  9 ‚îÇ target   ‚îÇ dir  ‚îÇ     0 B ‚îÇ a day ago     ‚îÇ
# =&gt; ‚îÇ 10 ‚îÇ tests    ‚îÇ dir  ‚îÇ 4.0 KiB ‚îÇ 2 weeks ago   ‚îÇ
# =&gt; ‚îÇ 11 ‚îÇ wix      ‚îÇ dir  ‚îÇ     0 B ‚îÇ 2 weeks ago   ‚îÇ
# =&gt; ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ
```

Because most of the time you&#039;ll want to see the output of a pipeline, `table` is assumed.
We could have also written the above:

```shell
ls | where type == &quot;dir&quot;
```

Being able to use the same commands and compose them differently is an important philosophy in Nu.
For example, we could use the built-in `ps` command to get a list of the running processes, using the same `where` as above.

```shell
ps | where cpu &gt; 0
# =&gt; ‚ï≠‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
# =&gt; ‚îÇ # ‚îÇ  pid  ‚îÇ   name    ‚îÇ  cpu  ‚îÇ    mem    ‚îÇ  virtual  ‚îÇ
# =&gt; ‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
# =&gt; ‚îÇ 0 ‚îÇ  2240 ‚îÇ Slack.exe ‚îÇ 16.40 ‚îÇ 178.3 MiB ‚îÇ 232.6 MiB ‚îÇ
# =&gt; ‚îÇ 1 ‚îÇ 16948 ‚îÇ Slack.exe ‚îÇ 16.32 ‚îÇ 205.0 MiB ‚îÇ 197.9 MiB ‚îÇ
# =&gt; ‚îÇ 2 ‚îÇ 17700 ‚îÇ nu.exe    ‚îÇ  3.77 ‚îÇ  26.1 MiB ‚îÇ   8.8 MiB ‚îÇ
# =&gt; ‚ï∞‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ
```

### Opening files

Nu can load file and URL contents as raw text or structured data (if it recognizes the format).
For example, you can load a .toml file as structured data and explore it:

```shell
open Cargo.toml
# =&gt; ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
# =&gt; ‚îÇ bin              ‚îÇ [table 1 row]      ‚îÇ
# =&gt; ‚îÇ dependencies     ‚îÇ {record 25 fields} ‚îÇ
# =&gt; ‚îÇ dev-dependencies ‚îÇ {record 8 fields}  ‚îÇ
# =&gt; ‚îÇ features         ‚îÇ {record 10 fields} ‚îÇ
# =&gt; ‚îÇ package          ‚îÇ {record 13 fields} ‚îÇ
# =&gt; ‚îÇ patch            ‚îÇ {record 1 field}   ‚îÇ
# =&gt; ‚îÇ profile          ‚îÇ {record 3 fields}  ‚îÇ
# =&gt; ‚îÇ target           ‚îÇ {record 3 fields}  ‚îÇ
# =&gt; ‚îÇ workspace        ‚îÇ {record 1 field}   ‚îÇ
# =&gt; ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ
```

We can pipe this into a command that gets the contents of one of the columns:

```shell
open Cargo.toml | get package
# =&gt; ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
# =&gt; ‚îÇ authors       ‚îÇ [list 1 item]                      ‚îÇ
# =&gt; ‚îÇ default-run   ‚îÇ nu                                 ‚îÇ
# =&gt; ‚îÇ description   ‚îÇ A new type of shell                ‚îÇ
# =&gt; ‚îÇ documentation ‚îÇ https://www.nushell.sh/book/       ‚îÇ
# =&gt; ‚îÇ edition       ‚îÇ 2018                               ‚îÇ
# =&gt; ‚îÇ exclude       ‚îÇ [list 1 item]                      ‚îÇ
# =&gt; ‚îÇ homepage      ‚îÇ https://www.nushell.sh             ‚îÇ
# =&gt; ‚îÇ license       ‚îÇ MIT                                ‚îÇ
# =&gt; ‚îÇ metadata      ‚îÇ {record 1 field}                   ‚îÇ
# =&gt; ‚îÇ name          ‚îÇ nu                                 ‚îÇ
# =&gt; ‚îÇ repository    ‚îÇ https://github.com/nushell/nushell ‚îÇ
# =&gt; ‚îÇ rust-version  ‚îÇ 1.60                               ‚îÇ
# =&gt; ‚îÇ version       ‚îÇ 0.72.0                             ‚îÇ
# =&gt; ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ
```

And if needed we can drill down further:

```shell
open Cargo.toml | get package.version
# =&gt; 0.72.0
```

### Plugins

Nu supports plugins that offer additional functionality to the shell and follow the same structured data model that built-in commands use. There are a few examples in the `crates/nu_plugins_*` directories.

Plugins are binaries that are available in your path and follow a `nu_plugin_*` naming convention.
These binaries interact with nu via a simple JSON-RPC protocol where the command identifies itself and passes along its configuration, making it available for use.
If the plugin is a filter, data streams to it one element at a time, and it can stream data back in return via stdin/stdout.
If the plugin is a sink, it is given the full vector of final data and is given free reign over stdin/stdout to use as it pleases.

The [awesome-nu repo](https://github.com/nushell/awesome-nu#plugins) lists a variety of nu-plugins while the [showcase repo](https://github.com/nushell/showcase) *shows* off informative blog posts that have been written about Nushell along with videos that highlight technical
topics that have been presented.

## Goals

Nu adheres closely to a set of goals that make up its design philosophy. As features are added, they are checked against these goals.

-   First and foremost, Nu is cross-platform. Commands and techniques should work across platforms and Nu has [first-class support for Windows, macOS, and Linux](devdocs/PLATFORM_SUPPORT.md).

-   Nu ensures compatibility with existing platform-specific executables.

-   Nu&#039;s workflow and tools should have the usability expected of modern software in 2022 (and beyond).

-   Nu views data as either structured or unstructured. It is a structured shell like PowerShell.

-   Finally, Nu views data functionally. Rather than using mutation, pipelines act as a means to load, change, and save data without mutable state.

## Officially Supported By

Please submit an issue or PR to be added to this list.

-   [zoxide](https://github.com/ajeetdsouza/zoxide)
-   [starship](https://github.com/starship/starship)
-   [oh-my-posh](https://ohmyposh.dev)
-   [Couchbase Shell](https://couchbase.sh)
-   [virtualenv](https://github.com/pypa/virtualenv)
-   [atuin](https://github.com/ellie/atuin)
-   [clap](https://github.com/clap-rs/clap/tree/master/clap_complete_nushell)
-   [Dorothy](http://github.com/bevry/dorothy)
-   [Direnv](https://github.com/direnv/direnv/blob/master/docs/hook.md#nushell)
-   [x-cmd](https://x-cmd.com/mod/nu)
-   [vfox](https://github.com/version-fox/vfox)

## Contributing

See [Contributing](CONTRIBUTING.md) for details. Thanks to all the people who already contributed!

&lt;a href=&quot;https://github.com/nushell/nushell/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contributors-img.web.app/image?repo=nushell/nushell&amp;max=750&amp;columns=20&quot; /&gt;
&lt;/a&gt;

## License

The project is made available under the MIT license. See the `LICENSE` file for more information.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
    </channel>
</rss>