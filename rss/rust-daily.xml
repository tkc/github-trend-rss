<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for rust - Rust Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for rust.</description>
        <lastBuildDate>Thu, 16 Oct 2025 00:05:49 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[MystenLabs/sui]]></title>
            <link>https://github.com/MystenLabs/sui</link>
            <guid>https://github.com/MystenLabs/sui</guid>
            <pubDate>Thu, 16 Oct 2025 00:05:49 GMT</pubDate>
            <description><![CDATA[Sui, a next-generation smart contract platform with high throughput, low latency, and an asset-oriented programming model powered by the Move programming language]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/MystenLabs/sui">MystenLabs/sui</a></h1>
            <p>Sui, a next-generation smart contract platform with high throughput, low latency, and an asset-oriented programming model powered by the Move programming language</p>
            <p>Language: Rust</p>
            <p>Stars: 7,399</p>
            <p>Forks: 11,645</p>
            <p>Stars today: 4 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/MystenLabs/sui/refs/heads/main/docs/site/static/img/logo.svg&quot; alt=&quot;Logo&quot; width=&quot;100&quot; height=&quot;100&quot;&gt;
&lt;/p&gt;

# Welcome to Sui

[![Github release](https://img.shields.io/github/v/release/MystenLabs/sui.svg?sort=semver)](https://github.com/MystenLabs/sui/releases/latest)
[![License](https://img.shields.io/github/license/MystenLabs/sui)](https://github.com/MystenLabs/sui/blob/main/LICENSE)

[Sui](https://sui.io) is a next-generation smart contract platform with high throughput, low latency, and an asset-oriented programming model powered by the [Move programming language](https://github.com/MystenLabs/awesome-move).

## Sui Highlights

Sui offers the following benefits and capabilities:

 * Unmatched scalability, instant settlement
 * A safe smart contract language accessible to mainstream developers
 * Ability to define rich and composable on-chain assets
 * Better user experience for web3 apps

Sui is the only blockchain today that can scale with the growth of web3 while achieving industry-leading performance, cost, programmability, and usability. Sui demonstrates capacity beyond the transaction processing capabilities of established systems ‚Äì traditional and blockchain alike. Sui is the first internet-scale programmable blockchain platform, a foundational layer for web3.

## Sui Architecture

```mermaid
flowchart LR
    CC(CLI Client) --&gt; ClientService
    RC(Rest Client) --&gt; ClientService
    RPCC(RPC Client) --&gt; ClientService
    ClientService --&gt; AuthorityAggregator
    AuthorityAggregator --&gt; AC1[AuthorityClient] &amp; AC2[AuthorityClient]
    subgraph Authority1
      AS[AuthorityState]
    end
    subgraph Authority2
      AS2[AuthorityState]
    end
    AC1 &lt;==&gt;|Network TCP| Authority1
    AC2 &lt;==&gt;|Network TCP| Authority2
```

## Sui Overview

Sui is a smart contract platform maintained by a permissionless set of authorities that play a role similar to validators or miners in other blockchain systems.

Sui offers scalability and unprecedented low-latency for common use cases. Sui makes the vast majority of transactions processable in parallel, which makes better use of processing resources, and offers the option to increase throughput with more resources. Sui forgoes consensus to instead use simpler and lower-latency primitives for common use cases, such as payment transactions and asset transfers. This is unprecedented in the blockchain world and enables a number of new latency-sensitive distributed applications, ranging from gaming to retail payment at physical points of sale.

Sui is written in [Rust](https://www.rust-lang.org) and supports smart contracts written in the [Move programming language](https://github.com/move-language/move) to define assets that may have an owner. Move programs define operations on these assets including custom rules for their creation, the transfer of these assets to new owners, and operations that mutate assets.

Sui has a native token called SUI, with a fixed supply. The SUI token is used to pay for gas, and is also used as [delegated stake on authorities](https://learn.bybit.com/blockchain/delegated-proof-of-stake-dpos/) within an epoch. The voting power of authorities within this epoch is a function of this delegated stake. Authorities are periodically reconfigured according to the stake delegated to them. In any epoch, the set of authorities is [Byzantine fault tolerant](https://pmg.csail.mit.edu/papers/osdi99.pdf). At the end of the epoch, fees collected through all transactions processed are distributed to authorities according to their contribution to the operation of the system. Authorities can in turn share some of the fees as rewards to users that delegated stakes to them.

Sui is supported by several cutting-edge [peer-reviewed studies](https://github.com/MystenLabs/sui/blob/main/docs/content/concepts/research-papers.mdx) and extensive years of open-source development.

## More About Sui

Use the following links to learn more about Sui and the Sui ecosystem:

 * Learn more about working with Sui in the [Sui Documentation](https://docs.sui.io/).
 * Join the Sui community on [Sui Discord](https://discord.gg/sui).
 * Find out more about the Sui ecosystem on the [Sui Resources](https://sui.io/resources/) page.
 * Review information about Sui governance, [decentralization](https://suifoundation.org/decentralization), and [Developer Grants Program](https://sui.io/grants-hub) on the [Sui Foundation](https://sui.io/about) site.


 ## How to Contribute

 See the [Contributing Guide](CONTRIBUTING.md) for details on how to contribute to Sui.

 ## Code of Conduct

 See the [Code of Conduct](CODE_OF_CONDUCT.MD) for details on our code of conduct.

 ## License

 See the [LICENSE](LICENSE) file for more details.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[vectordotdev/vector]]></title>
            <link>https://github.com/vectordotdev/vector</link>
            <guid>https://github.com/vectordotdev/vector</guid>
            <pubDate>Thu, 16 Oct 2025 00:05:48 GMT</pubDate>
            <description><![CDATA[A high-performance observability data pipeline.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/vectordotdev/vector">vectordotdev/vector</a></h1>
            <p>A high-performance observability data pipeline.</p>
            <p>Language: Rust</p>
            <p>Stars: 20,502</p>
            <p>Forks: 1,877</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre>[![Nightly](https://github.com/vectordotdev/vector/actions/workflows/nightly.yml/badge.svg)](https://github.com/vectordotdev/vector/actions/workflows/nightly.yml)
[![E2E Test Suite](https://github.com/vectordotdev/vector/actions/workflows/e2e.yml/badge.svg)](https://github.com/vectordotdev/vector/actions/workflows/e2e.yml)
[![Component Features](https://github.com/vectordotdev/vector/actions/workflows/component_features.yml/badge.svg)](https://github.com/vectordotdev/vector/actions/workflows/component_features.yml)

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;website/static/img/diagram.svg&quot; alt=&quot;Vector&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;
    &lt;a href=&quot;https://vector.dev/docs/setup/quickstart/&quot;&gt;Quickstart&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://vector.dev/docs/&quot;&gt;Docs&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://vector.dev/guides/&quot;&gt;Guides&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://vector.dev/components/&quot;&gt;Integrations&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://chat.vector.dev&quot;&gt;Chat&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://vector.dev/releases/latest/download/&quot;&gt;Download&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://rust-doc.vector.dev/&quot;&gt;Rust Crate Docs&lt;/a&gt;
  &lt;/strong&gt;
&lt;/p&gt;

## What is Vector?

Vector is a high-performance, end-to-end (agent &amp; aggregator) observability data
pipeline that puts you in control of your observability data.
[Collect][docs.sources], [transform][docs.transforms], and [route][docs.sinks]
all your logs and metrics to any vendors you want today and any other
vendors you may want tomorrow. Vector enables dramatic cost reduction, novel
data enrichment, and data security where you need it, not where it is most
convenient for your vendors. Additionally, it is open source and up to 10x
faster than every alternative in the space.

To get started, follow our [**quickstart guide**][docs.quickstart] or [**install
Vector**][docs.installation].

Vector is maintained by Datadog&#039;s [Community Open Source Engineering team](https://opensource.datadoghq.com/about/#the-community-open-source-engineering-team).

### Principles

* **Reliable** - Built in [Rust][urls.rust], Vector&#039;s primary design goal is reliability.
* **End-to-end** - Deploys as an [agent][docs.roles#agent] or [aggregator][docs.roles#aggregator]. Vector is a complete platform.
* **Unified** - [Logs][docs.data-model.log], [metrics][docs.data-model.metric] (beta), and traces (coming soon). One tool for all of your data.

### Use cases

* Reduce total observability costs.
* Transition vendors without disrupting workflows.
* Enhance data quality and improve insights.
* Consolidate agents and eliminate agent fatigue.
* Improve overall observability performance and reliability.

### Community

* Vector is relied on by startups and enterprises like **Atlassian**, **T-Mobile**,
  **Comcast**, **Zendesk**, **Discord**, **Fastly**, **CVS**, **Trivago**,
  **Tuple**, **Douban**, **Visa**, **Mambu**, **Blockfi**, **Claranet**,
  **Instacart**, **Forcepoint**, and [many more][urls.production_users].
* Vector is **downloaded over 100,000 times per day**.
* Vector&#039;s largest user **processes over 500TB daily**.
* Vector has **over 500 contributors** and growing.

## Documentation

All user documentation is available at **[vector.dev/docs](https://vector.dev/docs)**.

Other Resources:

* [**Vector Calendar**][urls.vector_calendar]
* **Policies**:
  * [**Code of Conduct**][urls.vector_code_of_conduct]
  * [**Contributing**][urls.vector_contributing_policy]
  * [**Privacy**][urls.vector_privacy_policy]
  * [**Releases**][urls.vector_releases_policy]
  * [**Versioning**][urls.vector_versioning_policy]
  * [**Security**][urls.vector_security_policy]

## Comparisons

### Performance

The following performance tests demonstrate baseline performance between
common protocols with the exception of the Regex Parsing test.

| Test                                                                                                                   | Vector          | Filebeat | FluentBit       | FluentD   | Logstash  | SplunkUF        | SplunkHF |
| ---------------------------------------------------------------------------------------------------------------------- | --------------- | -------- | --------------- | --------- | --------- | --------------- | -------- |
| [TCP to Blackhole](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/tcp_to_blackhole_performance) | _**86mib/s**_   | n/a      | 64.4mib/s       | 27.7mib/s | 40.6mib/s | n/a             | n/a      |
| [File to TCP](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_to_tcp_performance)           | _**76.7mib/s**_ | 7.8mib/s | 35mib/s         | 26.1mib/s | 3.1mib/s  | 40.1mib/s       | 39mib/s  |
| [Regex Parsing](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/regex_parsing_performance)       | 13.2mib/s       | n/a      | _**20.5mib/s**_ | 2.6mib/s  | 4.6mib/s  | n/a             | 7.8mib/s |
| [TCP to HTTP](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/tcp_to_http_performance)           | _**26.7mib/s**_ | n/a      | 19.6mib/s       | &lt;1mib/s   | 2.7mib/s  | n/a             | n/a      |
| [TCP to TCP](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/tcp_to_tcp_performance)             | 69.9mib/s       | 5mib/s   | 67.1mib/s       | 3.9mib/s  | 10mib/s   | _**70.4mib/s**_ | 7.6mib/s |

To learn more about our performance tests, please see the [Vector test harness][urls.vector_test_harness].

### Correctness

The following correctness tests are not exhaustive, but they demonstrate
fundamental differences in quality and attention to detail:

| Test                                                                                                                                 | Vector | Filebeat | FluentBit | FluentD | Logstash | Splunk UF | Splunk HF |
| ------------------------------------------------------------------------------------------------------------------------------------ | ------ | -------- | --------- | ------- | -------- | --------- | --------- |
| [Disk Buffer Persistence](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/disk_buffer_persistence_correctness) | **‚úì**  | ‚úì        |           |         | ‚ö†        | ‚úì         | ‚úì         |
| [File Rotate (create)](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_rotate_create_correctness)         | **‚úì**  | ‚úì        | ‚úì         | ‚úì       | ‚úì        | ‚úì         | ‚úì         |
| [File Rotate (copytruncate)](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_rotate_truncate_correctness) | **‚úì**  |          |           |         |          | ‚úì         | ‚úì         |
| [File Truncation](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_truncate_correctness)                   | **‚úì**  | ‚úì        | ‚úì         | ‚úì       | ‚úì        | ‚úì         | ‚úì         |
| [Process (SIGHUP)](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/sighup_correctness)                         | **‚úì**  |          |           |         | ‚ö†        | ‚úì         | ‚úì         |
| [JSON (wrapped)](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/wrapped_json_correctness)                     | **‚úì**  | ‚úì        | ‚úì         | ‚úì       | ‚úì        | ‚úì         | ‚úì         |

To learn more about our correctness tests, please see the [Vector test harness][urls.vector_test_harness].

### Features

Vector is an end-to-end, unified, open data platform.

|                     | **Vector** | Beats | Fluentbit | Fluentd | Logstash | Splunk UF | Splunk HF | Telegraf |
| ------------------- | ---------- | ----- | --------- | ------- | -------- | --------- | --------- | -------- |
| **End-to-end**      | **‚úì**      |       |           |         |          |           |           | ‚úì        |
| Agent               | **‚úì**      | ‚úì     | ‚úì         |         |          | ‚úì         |           | ‚úì        |
| Aggregator          | **‚úì**      |       |           | ‚úì       | ‚úì        |           | ‚úì         | ‚úì        |
| **Unified**         | **‚úì**      |       |           |         |          |           |           | ‚úì        |
| Logs                | **‚úì**      | ‚úì     | ‚úì         | ‚úì       | ‚úì        | ‚úì         | ‚úì         | ‚úì        |
| Metrics             | **‚úì**      | ‚ö†     | ‚ö†         | ‚ö†       | ‚ö†        | ‚ö†         | ‚ö†         | ‚úì        |
| Traces              | üöß         |       |           |         |          |           |           |          |
| **Open**            | **‚úì**      |       | ‚úì         | ‚úì       |          |           |           | ‚úì        |
| Open-source         | **‚úì**      | ‚úì     | ‚úì         | ‚úì       | ‚úì        |           |           | ‚úì        |
| Vendor-neutral      | **‚úì**      |       | ‚úì         | ‚úì       |          |           |           | ‚úì        |
| **Reliability**     | **‚úì**      |       |           |         |          |           |           |          |
| Memory-safe         | **‚úì**      |       |           |         |          |           |           | ‚úì        |
| Delivery guarantees | **‚úì**      |       |           |         |          | ‚úì         | ‚úì         |          |
| Multi-core          | **‚úì**      | ‚úì     | ‚úì         | ‚úì       | ‚úì        | ‚úì         | ‚úì         | ‚úì        |


‚ö† = Not interoperable, metrics are represented as structured logs

---

&lt;p align=&quot;center&quot;&gt;
  Developed with ‚ù§Ô∏è by &lt;strong&gt;&lt;a href=&quot;https://datadoghq.com&quot;&gt;Datadog&lt;/a&gt;&lt;/strong&gt; - &lt;a href=&quot;https://github.com/vectordotdev/vector/security/policy&quot;&gt;Security Policy&lt;/a&gt; - &lt;a href=&quot;https://github.com/vectordotdev/vector/blob/master/PRIVACY.md&quot;&gt;Privacy Policy&lt;/a&gt;
&lt;/p&gt;

[docs.about.concepts]: https://vector.dev/docs/introduction/concepts/
[docs.about.introduction]: https://vector.dev/docs/introduction/
[docs.administration.monitoring]: https://vector.dev/docs/administration/monitoring/
[docs.administration.management]: https://vector.dev/docs/administration/management/
[docs.administration.upgrading]: https://vector.dev/docs/administration/upgrading/
[docs.administration.validating]: https://vector.dev/docs/administration/validating/
[docs.architecture.concurrency-model]: https://vector.dev/docs/architecture/concurrency-model/
[docs.architecture.data-model]: https://vector.dev/docs/architecture/data-model/
[docs.architecture.pipeline-model]: https://vector.dev/docs/architecture/pipeline-model/
[docs.architecture.runtime-model]: https://vector.dev/docs/architecture/runtime-model/
[docs.configuration.sinks]: https://vector.dev/docs/reference/configuration/sinks/
[docs.configuration.sources]: https://vector.dev/docs/reference/configuration/sources/
[docs.configuration.tests]: https://vector.dev/docs/reference/configuration/tests/
[docs.configuration.transforms]: https://vector.dev/docs/reference/configuration/transforms/
[docs.configuration.enrichment_tables]: https://vector.dev/docs/reference/configuration/global-options/#enrichment_tables
[docs.data-model.log]: https://vector.dev/docs/architecture/data-model/log/
[docs.data-model.metric]: https://vector.dev/docs/architecture/data-model/metric/
[docs.deployment.roles]: https://vector.dev/docs/setup/deployment/roles/
[docs.deployment.topologies]: https://vector.dev/docs/setup/deployment/topologies/
[docs.deployment]: https://vector.dev/docs/setup/deployment/
[docs.installation.manual]: https://vector.dev/docs/setup/installation/manual/
[docs.installation.operating_systems]: https://vector.dev/docs/setup/installation/operating-systems/
[docs.installation.package_managers]: https://vector.dev/docs/setup/installation/package-managers/
[docs.installation.platforms]: https://vector.dev/docs/setup/installation/platforms/
[docs.installation]: https://vector.dev/docs/setup/installation/
[docs.architecture.adaptive-request-concurrency]: https://vector.dev/docs/architecture/arc/
[docs.platforms.kubernetes]: https://vector.dev/docs/setup/installation/platforms/kubernetes/
[docs.quickstart]: https://vector.dev/docs/setup/quickstart/
[docs.reference.api]: https://vector.dev/docs/reference/api/
[docs.reference.cli]: https://vector.dev/docs/reference/cli/
[docs.reference.vrl]: https://vector.dev/docs/reference/vrl/
[docs.roles#agent]: https://vector.dev/docs/setup/deployment/roles/#agent
[docs.roles#aggregator]: https://vector.dev/docs/setup/deployment/roles/#aggregator
[docs.setup.installation]: https://vector.dev/docs/setup/installation/
[docs.setup.quickstart]: https://vector.dev/docs/setup/quickstart/
[docs.sinks.aws_cloudwatch_logs]: https://vector.dev/docs/reference/configuration/sinks/aws_cloudwatch_logs/
[docs.sinks.aws_s3]: https://vector.dev/docs/reference/configuration/sinks/aws_s3/
[docs.sinks.clickhouse]: https://vector.dev/docs/reference/configuration/sinks/clickhouse/
[docs.sinks.elasticsearch]: https://vector.dev/docs/reference/configuration/sinks/elasticsearch/
[docs.sinks.gcp_cloud_storage]: https://vector.dev/docs/reference/configuration/sinks/gcp_cloud_storage/
[docs.sinks]: https://vector.dev/docs/reference/configuration/sinks/
[docs.sources.docker_logs]: https://vector.dev/docs/reference/configuration/sources/docker_logs/
[docs.sources.file]: https://vector.dev/docs/reference/configuration/sources/file/
[docs.sources.http]: https://vector.dev/docs/reference/configuration/sources/http/
[docs.sources.journald]: https://vector.dev/docs/reference/configuration/sources/journald/
[docs.sources.kafka]: https://vector.dev/docs/reference/configuration/sources/kafka/
[docs.sources.socket]: https://vector.dev/docs/reference/configuration/sources/socket/
[docs.sources]: https://vector.dev/docs/reference/configuration/sources/
[docs.transforms.dedupe]: https://vector.dev/docs/reference/configuration/transforms/dedupe/
[docs.transforms.filter]: https://vector.dev/docs/reference/configuration/transforms/filter/
[docs.transforms.log_to_metric]: https://vector.dev/docs/reference/configuration/transforms/log_to_metric/
[docs.transforms.lua]: https://vector.dev/docs/reference/configuration/transforms/lua/
[docs.transforms.remap]: https://vector.dev/docs/reference/configuration/transforms/remap/
[docs.transforms]: https://vector.dev/docs/reference/configuration/transforms/
[docs.introduction.architecture]: https://vector.dev/docs/architecture/
[docs.introduction.guarantees]: https://vector.dev/docs/introduction/guarantees/
[docs.introduction.architecture]: https://vector.dev/docs/architecture/
[urls.production_users]: https://github.com/vectordotdev/vector/issues/790
[urls.rust]: https://www.rust-lang.org/
[urls.vector_calendar]: https://calendar.vector.dev
[urls.vector_chat]: https://chat.vector.dev
[urls.vector_code_of_conduct]: https://github.com/vectordotdev/vector/blob/master/CODE_OF_CONDUCT.md
[urls.vector_contributing_policy]: https://github.com/vectordotdev/vector/blob/master/CONTRIBUTING.md
[urls.vector_community]: https://vector.dev/community/
[urls.vector_privacy_policy]: https://github.com/vectordotdev/vector/blob/master/PRIVACY.md
[urls.vector_release_policy]: https://github.com/vectordotdev/vector/blob/master/RELEASING.md
[urls.vector_releases]: https://vector.dev/releases/
[urls.vector_releases_policy]: https://github.com/vectordotdev/vector/blob/master/RELEASES.md
[urls.vector_security_policy]: https://github.com/vectordotdev/vector/security/policy
[urls.vector_test_harness]: https://github.com/vectordotdev/vector-test-harness/
[urls.vector_twitter]: https://twitter.com/vectordotdev
[urls.vector_versioning_policy]: https://github.com/vectordotdev/vector/blob/master/VERSIONING.md
[urls.vote_feature]: https://github.com/vectordotdev/vector/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc+label%3A%22type%3A+feature%22
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[lancedb/lance]]></title>
            <link>https://github.com/lancedb/lance</link>
            <guid>https://github.com/lancedb/lance</guid>
            <pubDate>Thu, 16 Oct 2025 00:05:47 GMT</pubDate>
            <description><![CDATA[Modern columnar data format for ML and LLMs implemented in Rust. Convert from parquet in 2 lines of code for 100x faster random access, vector index, and data versioning. Compatible with Pandas, DuckDB, Polars, Pyarrow, and PyTorch with more integrations coming..]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lancedb/lance">lancedb/lance</a></h1>
            <p>Modern columnar data format for ML and LLMs implemented in Rust. Convert from parquet in 2 lines of code for 100x faster random access, vector index, and data versioning. Compatible with Pandas, DuckDB, Polars, Pyarrow, and PyTorch with more integrations coming..</p>
            <p>Language: Rust</p>
            <p>Stars: 5,504</p>
            <p>Forks: 449</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;p align=&quot;center&quot;&gt;

&lt;img width=&quot;257&quot; alt=&quot;Lance Logo&quot; src=&quot;https://user-images.githubusercontent.com/917119/199353423-d3e202f7-0269-411d-8ff2-e747e419e492.png&quot;&gt;

**Modern columnar data format for ML. Convert from Parquet in 2-lines of code for 100x faster random access, zero-cost schema evolution, rich secondary indices, versioning, and more.&lt;br/&gt;**
**Compatible with Pandas, DuckDB, Polars, Pyarrow, and Ray with more integrations on the way.**

&lt;a href=&quot;https://lancedb.github.io/lance/&quot;&gt;Documentation&lt;/a&gt; ‚Ä¢
&lt;a href=&quot;https://blog.lancedb.com/&quot;&gt;Blog&lt;/a&gt; ‚Ä¢
&lt;a href=&quot;https://discord.gg/zMM32dvNtd&quot;&gt;Discord&lt;/a&gt; ‚Ä¢
&lt;a href=&quot;https://x.com/lancedb&quot;&gt;X&lt;/a&gt;

[CI]: https://github.com/lancedb/lance/actions/workflows/rust.yml
[CI Badge]: https://github.com/lancedb/lance/actions/workflows/rust.yml/badge.svg
[Docs]: https://lancedb.github.io/lance/
[Docs Badge]: https://img.shields.io/badge/docs-passing-brightgreen
[crates.io]: https://crates.io/crates/lance
[crates.io badge]: https://img.shields.io/crates/v/lance.svg
[Python versions]: https://pypi.org/project/pylance/
[Python versions badge]: https://img.shields.io/pypi/pyversions/pylance

[![CI Badge]][CI]
[![Docs Badge]][Docs]
[![crates.io badge]][crates.io]
[![Python versions badge]][Python versions]

&lt;/p&gt;
&lt;/div&gt;

&lt;hr /&gt;

Lance is a modern columnar data format that is optimized for ML workflows and datasets. Lance is perfect for:

1. Building search engines and feature stores.
2. Large-scale ML training requiring high performance IO and shuffles.
3. Storing, querying, and inspecting deeply nested data for robotics or large blobs like images, point clouds, and more.

The key features of Lance include:

* **High-performance random access:** 100x faster than Parquet without sacrificing scan performance.

* **Vector search:** find nearest neighbors in milliseconds and combine OLAP-queries with vector search.

* **Zero-copy, automatic versioning:** manage versions of your data without needing extra infrastructure.

* **Ecosystem integrations:** Apache Arrow, Pandas, Polars, DuckDB, Ray, Spark and more on the way.

&gt; [!TIP]
&gt; Lance is in active development and we welcome contributions. Please see our [contributing guide](https://lancedb.github.io/lance/community/contributing) for more information.

## Quick Start

**Installation**

```shell
pip install pylance
```

To install a preview release:

```shell
pip install --pre --extra-index-url https://pypi.fury.io/lancedb/ pylance
```

&gt; [!TIP]
&gt; Preview releases are released more often than full releases and contain the
&gt; latest features and bug fixes. They receive the same level of testing as full releases.
&gt; We guarantee they will remain published and available for download for at
&gt; least 6 months. When you want to pin to a specific version, prefer a stable release.

**Converting to Lance**

```python
import lance

import pandas as pd
import pyarrow as pa
import pyarrow.dataset

df = pd.DataFrame({&quot;a&quot;: [5], &quot;b&quot;: [10]})
uri = &quot;/tmp/test.parquet&quot;
tbl = pa.Table.from_pandas(df)
pa.dataset.write_dataset(tbl, uri, format=&#039;parquet&#039;)

parquet = pa.dataset.dataset(uri, format=&#039;parquet&#039;)
lance.write_dataset(parquet, &quot;/tmp/test.lance&quot;)
```

**Reading Lance data**
```python
dataset = lance.dataset(&quot;/tmp/test.lance&quot;)
assert isinstance(dataset, pa.dataset.Dataset)
```

**Pandas**
```python
df = dataset.to_table().to_pandas()
df
```

**DuckDB**
```python
import duckdb

# If this segfaults, make sure you have duckdb v0.7+ installed
duckdb.query(&quot;SELECT * FROM dataset LIMIT 10&quot;).to_df()
```

**Vector search**

Download the sift1m subset

```shell
wget ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz
tar -xzf sift.tar.gz
```

Convert it to Lance

```python
import lance
from lance.vector import vec_to_table
import numpy as np
import struct

nvecs = 1000000
ndims = 128
with open(&quot;sift/sift_base.fvecs&quot;, mode=&quot;rb&quot;) as fobj:
    buf = fobj.read()
    data = np.array(struct.unpack(&quot;&lt;128000000f&quot;, buf[4 : 4 + 4 * nvecs * ndims])).reshape((nvecs, ndims))
    dd = dict(zip(range(nvecs), data))

table = vec_to_table(dd)
uri = &quot;vec_data.lance&quot;
sift1m = lance.write_dataset(table, uri, max_rows_per_group=8192, max_rows_per_file=1024*1024)
```

Build the index

```python
sift1m.create_index(&quot;vector&quot;,
                    index_type=&quot;IVF_PQ&quot;,
                    num_partitions=256,  # IVF
                    num_sub_vectors=16)  # PQ
```

Search the dataset

```python
# Get top 10 similar vectors
import duckdb

dataset = lance.dataset(uri)

# Sample 100 query vectors. If this segfaults, make sure you have duckdb v0.7+ installed
sample = duckdb.query(&quot;SELECT vector FROM dataset USING SAMPLE 100&quot;).to_df()
query_vectors = np.array([np.array(x) for x in sample.vector])

# Get nearest neighbors for all of them
rs = [dataset.to_table(nearest={&quot;column&quot;: &quot;vector&quot;, &quot;k&quot;: 10, &quot;q&quot;: q})
      for q in query_vectors]
```

## Directory structure

| Directory          | Description              |
|--------------------|--------------------------|
| [rust](./rust)     | Core Rust implementation |
| [python](./python) | Python bindings (PyO3)   |
| [java](./java)     | Java bindings (JNI)      |
| [docs](./docs)     | Documentation source     |

## What makes Lance different

Here we will highlight a few aspects of Lance‚Äôs design. For more details, see the full [Lance design document](https://lancedb.github.io/lance/format).

**Vector index**: Vector index for similarity search over embedding space.
Support both CPUs (``x86_64`` and ``arm``) and GPU (``Nvidia (cuda)`` and ``Apple Silicon (mps)``).

**Encodings**: To achieve both fast columnar scan and sub-linear point queries, Lance uses custom encodings and layouts.

**Nested fields**: Lance stores each subfield as a separate column to support efficient filters like ‚Äúfind images where detected objects include cats‚Äù.

**Versioning**: A Manifest can be used to record snapshots. Currently we support creating new versions automatically via appends, overwrites, and index creation.

**Fast updates** (ROADMAP): Updates will be supported via write-ahead logs.

**Rich secondary indices**: Support `BTree`, `Bitmap`, `Full text search`, `Label list`,
`NGrams`, and more.

## Benchmarks

### Vector search

We used the SIFT dataset to benchmark our results with 1M vectors of 128D

1. For 100 randomly sampled query vectors, we get &lt;1ms average response time (on a 2023 m2 MacBook Air)

![avg_latency.png](docs/src/images/avg_latency.png)

2. ANNs are always a trade-off between recall and performance

![avg_latency.png](docs/src/images/recall_vs_latency.png)

### Vs. parquet

We create a Lance dataset using the Oxford Pet dataset to do some preliminary performance testing of Lance as compared to Parquet and raw image/XMLs. For analytics queries, Lance is 50-100x better than reading the raw metadata. For batched random access, Lance is 100x better than both parquet and raw files.

![](docs/src/images/lance_perf.png)

## Why are you building yet another data format?!

The machine learning development cycle involves the steps:

```mermaid
graph LR
    A[Collection] --&gt; B[Exploration];
    B --&gt; C[Analytics];
    C --&gt; D[Feature Engineer];
    D --&gt; E[Training];
    E --&gt; F[Evaluation];
    F --&gt; C;
    E --&gt; G[Deployment];
    G --&gt; H[Monitoring];
    H --&gt; A;
```

People use different data representations to varying stages for the performance or limited by the tooling available.
Academia mainly uses XML / JSON for annotations and zipped images/sensors data for deep learning, which
is difficult to integrate into data infrastructure and slow to train over cloud storage.
While industry uses data lakes (Parquet-based techniques, i.e., Delta Lake, Iceberg) or data warehouses (AWS Redshift
or Google BigQuery) to collect and analyze data, they have to convert the data into training-friendly formats, such
as [Rikai](https://github.com/eto-ai/rikai)/[Petastorm](https://github.com/uber/petastorm)
or [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord).
Multiple single-purpose data transforms, as well as syncing copies between cloud storage to local training
instances have become a common practice.

While each of the existing data formats excels at the workload it was originally designed for, we need a new data format
tailored for multistage ML development cycles to reduce and data silos.

A comparison of different data formats in each stage of ML development cycle.

|                     | Lance | Parquet &amp; ORC | JSON &amp; XML | TFRecord | Database | Warehouse |
|---------------------|-------|---------------|------------|----------|----------|-----------|
| Analytics           | Fast  | Fast          | Slow       | Slow     | Decent   | Fast      |
| Feature Engineering | Fast  | Fast          | Decent     | Slow     | Decent   | Good      |
| Training            | Fast  | Decent        | Slow       | Fast     | N/A      | N/A       |
| Exploration         | Fast  | Slow          | Fast       | Slow     | Fast     | Decent    |
| Infra Support       | Rich  | Rich          | Decent     | Limited  | Rich     | Rich      |

## Community Highlights

Lance is currently used in production by:
* [LanceDB](https://github.com/lancedb/lancedb), a serverless, low-latency vector database for ML applications
* [LanceDB Enterprise](https://docs.lancedb.com/enterprise/introduction), hyperscale LanceDB with enterprise SLA.
* Leading multimodal Gen AI companies for training over petabyte-scale multimodal data.
* Self-driving car company for large-scale storage, retrieval and processing of multi-modal data.
* E-commerce company for billion-scale+ vector personalized search.
* and more.

## Presentations, Blogs and Talks

* [Designing a Table Format for ML Workloads](https://blog.lancedb.com/designing-a-table-format-for-ml-workloads/), Feb 2025.
* [Transforming Multimodal Data Management with LanceDB, Ray Summit](https://www.youtube.com/watch?v=xmTFEzAh8ho), Oct 2024.
* [Lance v2: A columnar container format for modern data](https://blog.lancedb.com/lance-v2/), Apr 2024.
* [Lance Deep Dive](https://drive.google.com/file/d/1Orh9rK0Mpj9zN_gnQF1eJJFpAc6lStGm/view?usp=drive_link). July 2023.
* [Lance: A New Columnar Data Format](https://docs.google.com/presentation/d/1a4nAiQAkPDBtOfXFpPg7lbeDAxcNDVKgoUkw3cUs2rE/edit#slide=id.p), [Scipy 2022, Austin, TX](https://www.scipy2022.scipy.org/posters). July, 2022.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[rgwood/systemctl-tui]]></title>
            <link>https://github.com/rgwood/systemctl-tui</link>
            <guid>https://github.com/rgwood/systemctl-tui</guid>
            <pubDate>Thu, 16 Oct 2025 00:05:46 GMT</pubDate>
            <description><![CDATA[A fast, simple TUI for interacting with systemd services and their logs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/rgwood/systemctl-tui">rgwood/systemctl-tui</a></h1>
            <p>A fast, simple TUI for interacting with systemd services and their logs</p>
            <p>Language: Rust</p>
            <p>Stars: 1,508</p>
            <p>Forks: 35</p>
            <p>Stars today: 62 stars today</p>
            <h2>README</h2><pre># systemctl-tui

[![Crates.io](https://img.shields.io/crates/v/systemctl-tui.svg)](https://crates.io/crates/systemctl-tui)

A fast, simple TUI for interacting with [systemd](https://en.wikipedia.org/wiki/Systemd) services and their logs.
![Screenshot from 2025-01-23 21-44-31](https://github.com/user-attachments/assets/caac6034-d4e3-4c54-8163-24a8a6d39cb4)

`systemctl-tui` can quickly browse service status and logs, start/stop/restart/reload services, and view/edit unit files. It aims to do a small number of things well.

## Install

Note: this project only works on Linux (WSL works _if_ you [have systemd enabled](https://devblogs.microsoft.com/commandline/systemd-support-is-now-available-in-wsl/)). Binaries are published for x64 and ARM64 in the [GitHub releases](https://github.com/rgwood/systemctl-tui/releases), and [distro packages](#distro-packages) are available.

### Binary Release

Automated install/update (don&#039;t forget to always verify what you&#039;re piping into bash):

```sh
curl https://raw.githubusercontent.com/rgwood/systemctl-tui/master/install.sh | bash
```
The script installs downloaded binary to `$HOME/.local/bin` directory by default, but it can be changed by setting `DIR` environment variable.

### Rust

If you&#039;d rather build from scratch you will need [Rust installed](https://rustup.rs/). Then either:

1. Run `cargo install systemctl-tui --locked`
2. Clone the repo and run `cargo build --release` to get a release binary at `target/release/systemctl-tui`

### Distro Packages

&lt;details&gt;
  &lt;summary&gt;Packaging status&lt;/summary&gt;

[![Packaging status](https://repology.org/badge/vertical-allrepos/systemctl-tui.svg)](https://repology.org/project/systemctl-tui/versions)

&lt;/details&gt;

#### Arch Linux

`systemctl-tui` can be installed from the [official repositories](https://archlinux.org/packages/extra/x86_64/systemctl-tui/):

```sh
pacman -S systemctl-tui
```

#### Nix

[A Nix package](https://search.nixos.org/packages?query=systemctl-tui) is available and can be installed as follows:

```sh
nix-shell -p systemctl-tui
```

#### Optional:

1. Alias `systemctl-tui` to `st` for quick access
2. Create a symlink so `systemctl-tui` can be used with sudo:

```sh
sudo ln -s ~/.cargo/bin/systemctl-tui /usr/bin/systemctl-tui
```

## Help
![image](https://github.com/rgwood/systemctl-tui/assets/26268125/b1b49850-61c4-4667-9110-20a34f917055)

## Credits

- Inspired by the truly wonderful [Lazygit](https://github.com/jesseduffield/lazygit)
- [`sysz`](https://github.com/joehillen/sysz) is so cool
- Used [`ratatui-template`](https://github.com/kdheepak/ratatui-template/) to get started
- systemd code partially taken from [`servicer`](https://github.com/servicer-labs/servicer)
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[openobserve/openobserve]]></title>
            <link>https://github.com/openobserve/openobserve</link>
            <guid>https://github.com/openobserve/openobserve</guid>
            <pubDate>Thu, 16 Oct 2025 00:05:45 GMT</pubDate>
            <description><![CDATA[üöÄ 10x easier, üöÄ 140x lower storage cost, üöÄ high performance, üöÄ petabyte scale - Elasticsearch/Splunk/Datadog alternative for üöÄ (logs, metrics, traces, RUM, Error tracking, Session replay).]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/openobserve/openobserve">openobserve/openobserve</a></h1>
            <p>üöÄ 10x easier, üöÄ 140x lower storage cost, üöÄ high performance, üöÄ petabyte scale - Elasticsearch/Splunk/Datadog alternative for üöÄ (logs, metrics, traces, RUM, Error tracking, Session replay).</p>
            <p>Language: Rust</p>
            <p>Stars: 16,846</p>
            <p>Forks: 672</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://openobserve.ai&quot;&gt;&lt;img src=&quot;https://openobserve.ai/img/logo/o2-logo-readme.svg&quot; alt=&quot;OpenObserve&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;em&gt;üöÄ 10x easier, üöÄ 140x lower storage cost, üöÄ high performance, üöÄ petabyte scale - Elasticsearch/Splunk/Datadog alternative for üöÄ (logs, metrics, traces).&lt;/em&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/openobserve/openobserve&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/last-commit/openobserve/openobserve&quot; alt=&quot;Last Commit&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/openobserve/openobserve/stargazers&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/openobserve/openobserve&quot; alt=&quot;GitHub Stars&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/openobserve/openobserve/issues&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/issues/openobserve/openobserve&quot; alt=&quot;GitHub Issues&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/openobserve/openobserve/graphs/contributors&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/contributors/openobserve/openobserve&quot; alt=&quot;Contributors&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/openobserve/openobserve/releases&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/v/release/openobserve/openobserve&quot; alt=&quot;GitHub Release&quot;&gt;
&lt;/a&gt;
&lt;/p&gt;

OpenObserve (O2 for short) is a cloud-native observability platform built specifically for logs, metrics, traces, analytics, RUM (Real User Monitoring - Performance, Errors, Session Replay) designed to work at petabyte scale.

It is straightforward and easy to operate, in contrast to Elasticsearch, which requires understanding and tuning numerous settings. Get OpenObserve up and running in under 2 minutes.

OpenObserve serves as a seamless replacement for Elasticsearch for users who ingest data using APIs and perform searches. OpenObserve comes with its own user interface, eliminating the need for separate installation.

You can reduce your log storage costs by ~140x compared to Elasticsearch by using OpenObserve. Below, we present the results from pushing logs from our production Kubernetes cluster to both Elasticsearch and OpenObserve using Fluent Bit.

![OpenObserve Vs Elasticsearch](./screenshots/zo_vs_es.png)

## üé• Introduction Video

[![OpenObserve Introduction](./screenshots/o2_intro.webp)](https://www.youtube.com/watch?v=4VwuC1tpRP4)

## üåü Features:

- **Logs, Metrics, Traces**: Comprehensive support for various data types.
- **OpenTelemetry Support**: Full compatibility with OTLP for logs, metrics, and traces.
- **Real User Monitoring (RUM)**: Includes performance tracking, error logging, and session replay.
- **Dashboards, Reports, Alerts**: Features over 18 different chart types for comprehensive data visualization for on-the-fly analysis and reporting along with alerting.
- **Pipelines**: Enrich, redact, reduce, normalize data on the fly. Stream processing for logs to metrics and more.
- **Advanced Embedded GUI**: Intuitive and user-friendly interface.
- **SQL and PromQL Support**: Query logs and traces with SQL, and metrics with SQL and PromQL.
- **Single Binary or HA Installation**: Install using a single binary for small deployments or in HA mode for large deployments.
- **Versatile Storage Options**: Supports local disk, S3, MinIO, GCS, Azure Blob Storage.
- **High Availability and Clustering**: Ensures reliable and scalable performance.
- **Dynamic Schema**: Adapts to your data structure seamlessly.
- **Built-in Authentication**: Secure and ready to use.
- **Ease of Operation**: Designed for simplicity and efficiency.
- **Seamless Upgrades**: Hassle-free updates.
- **Multilingual UI**: Supports 11 languages, including English, Spanish, German, French, Chinese, and more.

For a full list of features, check the [documentation](https://openobserve.ai/docs/#project-status-features-and-roadmap).

## ‚ö°Ô∏è Quick start

### üê≥ Docker:
```bash
docker run -d \
      --name openobserve \
      -v $PWD/data:/data \
      -p 5080:5080 \
      -e ZO_ROOT_USER_EMAIL=&quot;root@example.com&quot; \
      -e ZO_ROOT_USER_PASSWORD=&quot;Complexpass#123&quot; \
      public.ecr.aws/zinclabs/openobserve:latest
```

### üêô Docker Compose:
```yaml
services:
  openobserve:
    image: public.ecr.aws/zinclabs/openobserve:latest
    restart: unless-stopped
    environment:
      ZO_ROOT_USER_EMAIL: &quot;root@example.com&quot;
      ZO_ROOT_USER_PASSWORD: &quot;Complexpass#123&quot;
    ports:
      - &quot;5080:5080&quot;
    volumes:
      - data:/data
volumes:
  data:
```

For other ways to quickly install OpenObserve or use OpenObserve cloud, check [quickstart documentation](https://openobserve.ai/docs/quickstart).

For installing OpenObserve in HA mode, check [HA deployment documentation](https://openobserve.ai/docs/ha_deployment/).

&lt;!-- ## Enterprise Vs Open source Vs Cloud edition

OpenObserve is available in three different editions:


| Feature | Open Source (Self hosted) | Enterprise (Self hosted) | Cloud |
| --- | --- | --- | --- | 
| Logs | ‚úÖ | ‚úÖ | ‚úÖ |
| Metrics | ‚úÖ | ‚úÖ | ‚úÖ |
| Traces | ‚úÖ | ‚úÖ | ‚úÖ |
| RUM | ‚úÖ | ‚úÖ | ‚úÖ |
| Alerts | ‚úÖ | ‚úÖ | ‚úÖ |
| Dashboards | ‚úÖ | ‚úÖ | ‚úÖ |
| Reports | ‚úÖ | ‚úÖ | ‚úÖ |
| VRL functions | ‚úÖ | ‚úÖ | ‚úÖ |
| Pipelines | ‚úÖ | ‚úÖ | ‚úÖ |
| High Availability | ‚úÖ | ‚úÖ | ‚úÖ |
| Multitenancy (Organizations) | ‚úÖ | ‚úÖ | ‚úÖ |
| Dynamic schema and schema evolution | ‚úÖ | ‚úÖ | ‚úÖ |
| Advanced multilingual GUI | ‚úÖ | ‚úÖ | ‚úÖ |
| Single Sign On | ‚ùå | ‚úÖ | ‚úÖ |
| Role Based Access Control (RBAC) | ‚ùå | ‚úÖ | ‚úÖ |
| Federated search / Super cluster | ‚ùå | ‚úÖ | ‚ùå |
| Query management | ‚ùå | ‚úÖ | ‚ùå |
| Workload management (QoS) | ‚ùå | ‚úÖ | ‚ùå |
| Audit trail | ‚ùå | ‚úÖ | ‚ùå |
| Ability to influence roadmap | ‚ùå | ‚úÖ | ‚úÖ on enterprise plan |
| License | AGPL | Enterprise | Cloud |
| Support | Community | Enterprise | Cloud |
| Cost | Free | If self hosted, free for up to 200 GB/Day data ingested &lt;br&gt; Paid thereafter  | Free 200 GB/Month data ingested &lt;br&gt; Paid thereafter | --&gt;


## üì∑ Screenshots

### Home

![Home](./screenshots/zo_home.png)

### Logs

![Logs](./screenshots/logs.png)

### Traces (OpenTelemetry)

Trace details page
![Traces using OpenTelemetry](./screenshots/traces.png)

Golden metrics based on traces
![Traces golden metrics](./screenshots/traces-overall.png)

### Visualizations and Dashboards

![Dashboard](./screenshots/dashboard.png)
![Dashboard](./screenshots/dashboard2.png)
![Create panel](./screenshots/create-panel.png)
![Map](./screenshots/map.png)

### Front end monitoring

Performance analytics
![Performance](./screenshots/performance.png)

Session replay
![Session replay](./screenshots/session-replay.png)

Error tracking
![Error tracking](./screenshots/error-tracking.png)


### Alerts

![Alerts](./screenshots/alerts.png)


### Streams

![Streams](./screenshots/streams.png)

### Ingestion

![Ingestion](./screenshots/ingestion1.png)

### Pipeline

Pipeline
![Pipeline](./screenshots/pipeline.png)

Function
![Function](./screenshots/function.png)


### IAM

SSO (Single Sign On)
![SSO](./screenshots/sso.png)

RBAC (Role Based Access Control)
![RBAC](./screenshots/iam_rbac.png)


### SBOM

Software Bill of Materials for OpenObserve

#### Rust

SBOM can be found [here](./openobserve.cdx.xml). You can analyze it using [dependency track](https://dependencytrack.org/).

In order to generate the SBOM, you can use the following commands:

Install cargo-cyclonedx:

````bash
cargo install cargo-cyclonedx
````

Generate the SBOM:
```bash
cargo-cyclonedx cyclonedx
```

#### JavaScript

SBOM can be found [here](./web/sbom.json ). You can analyze it using [dependency track](https://dependencytrack.org/).

In order to generate the SBOM, you can use the following commands:

Install cyclonedx-npm:

````bash
npm install --global @cyclonedx/cyclonedx-npm
````

Generate the SBOM:
```bash
cd web
cyclonedx-npm &gt; sbom.json         
```


## ‚öñÔ∏è License

OpenObserve is licensed under the AGPL-3.0 license. For more details, see the [LICENSE](https://github.com/openobserve/openobserve/blob/main/LICENSE).

## üåç Community

### üîó Join OpenObserve community on Slack

[![Slack](./screenshots/slack.png)](https://short.openobserve.ai/community)

Easiest way to get support is to join the [Slack channel](https://short.openobserve.ai/community).

### üì± Join OpenObserve community on WeChat

&lt;img src=&quot;./screenshots/wechat_qr.jpg&quot; width=&quot;300&quot;&gt;
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[InfinitiBit/graphbit]]></title>
            <link>https://github.com/InfinitiBit/graphbit</link>
            <guid>https://github.com/InfinitiBit/graphbit</guid>
            <pubDate>Thu, 16 Oct 2025 00:05:44 GMT</pubDate>
            <description><![CDATA[GraphBit is the world‚Äôs first enterprise-grade Agentic AI framework, built on a Rust core with a Python wrapper for unmatched speed, security, and scalability. It enables reliable multi-agent workflows with minimal CPU and memory usage, making it production-ready for real-world enterprise environments.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/InfinitiBit/graphbit">InfinitiBit/graphbit</a></h1>
            <p>GraphBit is the world‚Äôs first enterprise-grade Agentic AI framework, built on a Rust core with a Python wrapper for unmatched speed, security, and scalability. It enables reliable multi-agent workflows with minimal CPU and memory usage, making it production-ready for real-world enterprise environments.</p>
            <p>Language: Rust</p>
            <p>Stars: 392</p>
            <p>Forks: 77</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# GraphBit - High Performance Agentic Framework

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;assets/logo(circle).png&quot; width=&quot;160px&quot; alt=&quot;Logo&quot; /&gt;
&lt;/p&gt;

&lt;!-- Added placeholders for links, fill it up when the corresponding links are available. --&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://graphbit.ai/&quot;&gt;Website&lt;/a&gt; | 
    &lt;a href=&quot;https://docs.graphbit.ai/&quot;&gt;Docs&lt;/a&gt; |
    &lt;a href=&quot;https://discord.com/invite/huVJwkyu&quot;&gt;Discord&lt;/a&gt;
    &lt;br /&gt;&lt;br /&gt;
&lt;/p&gt;

[![Build Status](https://img.shields.io/github/actions/workflow/status/InfinitiBit/graphbit/update-docs.yml?branch=main)](https://github.com/InfinitiBit/graphbit/actions/workflows/update-docs.yml)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/InfinitiBit/graphbit/blob/main/CONTRIBUTING.md)
[![Rust Version](https://img.shields.io/badge/rust-1.70+-blue.svg)](https://www.rust-lang.org)
[![Python Version](https://img.shields.io/badge/python-3.10--3.13-blue.svg)](https://www.python.org)

**Type-Safe AI Agent Workflows with Rust Performance**

&lt;/div&gt;

Graphbit is an **industry-grade agentic AI framework** built for developers and AI teams that demand stability, scalability, and low resource usage. 

Written in **Rust** for maximum performance and safety, it delivers up to **68√ó lower CPU usage** and **140√ó lower memory** footprint than certain leading alternatives while consistently using far fewer resources than the rest, all while maintaining comparable throughput and execution speed. See [benchmarks](benchmarks/report/framework-benchmark-report.md) for more details.

Designed to run **multi-agent workflows in parallel**, Graphbit persists memory across steps, recovers from failures, and ensures **100% task success** under load. Its lightweight, resource-efficient architecture enables deployment in both **high-scale enterprise environments** and **low-resource edge scenarios**. With built-in observability and concurrency support, Graphbit eliminates the bottlenecks that slow decision-making and erode ROI. 

##  Key Features

- **Tool Selection** - LLMs intelligently select tools based on descriptions
- **Type Safety** - Strong typing throughout the execution pipeline
- **Reliability** - Circuit breakers, retry policies, and error handling
- **Multi-LLM Support** - OpenAI, Azure OpenAI, Anthropic, OpenRouter, DeepSeek, Replicate, Ollama, TogetherAI
- **Resource Management** - Concurrency controls and memory optimization
- **Observability** - Built-in metrics and execution tracing

##  Quick Start

### Installation 

Recommended to use virtual environment.

```bash
pip install graphbit
```

### Environment Setup
Set up API keys you want to use in your project:
```bash
# OpenAI (optional ‚Äì required if using OpenAI models)
export OPENAI_API_KEY=your_openai_api_key_here

# Anthropic (optional ‚Äì required if using Anthropic models)
export ANTHROPIC_API_KEY=your_anthropic_api_key_here
```

&gt; **Security Note**: Never commit API keys to version control. Always use environment variables or secure secret management.

### Basic Usage
```python
import os

from graphbit import LlmConfig, Executor, Workflow, Node, tool

# Initialize and configure
config = LlmConfig.openai(os.getenv(&quot;OPENAI_API_KEY&quot;), &quot;gpt-4o-mini&quot;)

# Create executor
executor = Executor(config)

# Create tools with clear descriptions for LLM selection
@tool(_description=&quot;Get current weather information for any city&quot;)
def get_weather(location: str) -&gt; dict:
    return {&quot;location&quot;: location, &quot;temperature&quot;: 22, &quot;condition&quot;: &quot;sunny&quot;}

@tool(_description=&quot;Perform mathematical calculations and return results&quot;)
def calculate(expression: str) -&gt; str:
    return f&quot;Result: {eval(expression)}&quot;

# Build workflow
workflow = Workflow(&quot;Analysis Pipeline&quot;)

# Create agent nodes
smart_agent = Node.agent(
    name=&quot;Smart Agent&quot;,
    prompt=&quot;What&#039;s the weather in Paris and calculate 15 + 27?&quot;,
    system_prompt=&quot;You are an assistant skilled in weather lookup and math calculations. Use tools to answer queries accurately.&quot;,
    tools=[get_weather, calculate]
)

processor = Node.agent(
    name=&quot;Data Processor&quot;,
    prompt=&quot;Process the results obtained from Smart Agent.&quot;,
    system_prompt=&quot;&quot;&quot;You process and organize results from other agents.

    - Summarize and clarify key points
    - Structure your output for easy reading
    - Focus on actionable insights
    &quot;&quot;&quot;
)

# Connect and execute
id1 = workflow.add_node(smart_agent)
id2 = workflow.add_node(processor)
workflow.connect(id1, id2)

result = executor.execute(workflow)
print(f&quot;Workflow completed: {result.is_success()}&quot;)
print(&quot;\nSmart Agent Output: \n&quot;, result.get_node_output(&quot;Smart Agent&quot;))
print(&quot;\nData Processor Output: \n&quot;, result.get_node_output(&quot;Data Processor&quot;))
```

## High-Level Architecture

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/architecture.svg&quot; height=&quot;250&quot; alt=&quot;GraphBit Architecture&quot;&gt;
&lt;/p&gt;

Three-tier design for reliability and performance:
- **Rust Core** - Workflow engine, agents, and LLM providers
- **Orchestration Layer** - Project management and execution
- **Python API** - PyO3 bindings with async support

## Python API Integrations

GraphBit provides a rich Python API for building and integrating agentic workflows, including executors, nodes, LLM clients, and embeddings. For the complete list of classes, methods, and usage examples, see the [Python API Reference](docs/api-reference/python-api.md).

## Contributing to GraphBit

We welcome contributions. To get started, please see the [Contributing](CONTRIBUTING.md) file for development setup and guidelines.

GraphBit is built by a wonderful community of researchers and engineers.

&lt;a href=&quot;https://github.com/Infinitibit/graphbit/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=Infinitibit/graphbit&quot; /&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[lancedb/lancedb]]></title>
            <link>https://github.com/lancedb/lancedb</link>
            <guid>https://github.com/lancedb/lancedb</guid>
            <pubDate>Thu, 16 Oct 2025 00:05:43 GMT</pubDate>
            <description><![CDATA[Developer-friendly, embedded retrieval engine for multimodal AI. Search More; Manage Less.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lancedb/lancedb">lancedb/lancedb</a></h1>
            <p>Developer-friendly, embedded retrieval engine for multimodal AI. Search More; Manage Less.</p>
            <p>Language: Rust</p>
            <p>Stars: 7,749</p>
            <p>Forks: 616</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre>&lt;a href=&quot;https://cloud.lancedb.com&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/92dad0a2-2a37-4ce1-b783-0d1b4f30a00c&quot; alt=&quot;LanceDB Cloud Public Beta&quot; width=&quot;100%&quot; style=&quot;max-width: 100%;&quot;&gt;
&lt;/a&gt;
&lt;div align=&quot;center&quot;&gt;

[![LanceDB](docs/src/assets/hero-header.png)](https://lancedb.com)
[![Website](https://img.shields.io/badge/-Website-100000?style=for-the-badge&amp;labelColor=645cfb&amp;color=645cfb)](https://lancedb.com/)
[![Blog](https://img.shields.io/badge/Blog-100000?style=for-the-badge&amp;labelColor=645cfb&amp;color=645cfb)](https://blog.lancedb.com/)
[![Discord](https://img.shields.io/badge/-Discord-100000?style=for-the-badge&amp;logo=discord&amp;logoColor=white&amp;labelColor=645cfb&amp;color=645cfb)](https://discord.gg/zMM32dvNtd)
[![Twitter](https://img.shields.io/badge/-Twitter-100000?style=for-the-badge&amp;logo=x&amp;logoColor=white&amp;labelColor=645cfb&amp;color=645cfb)](https://twitter.com/lancedb)
[![LinkedIn](https://img.shields.io/badge/-LinkedIn-100000?style=for-the-badge&amp;logo=linkedin&amp;logoColor=white&amp;labelColor=645cfb&amp;color=645cfb)](https://www.linkedin.com/company/lancedb/)


&lt;img src=&quot;docs/src/assets/lancedb.png&quot; alt=&quot;LanceDB&quot; width=&quot;50%&quot;&gt;

# **The Multimodal AI Lakehouse**

[**How to Install** ](#how-to-install) ‚ú¶ [**Detailed Documentation**](https://lancedb.github.io/lancedb/) ‚ú¶ [**Tutorials and Recipes**](https://github.com/lancedb/vectordb-recipes/tree/main) ‚ú¶  [**Contributors**](#contributors) 

**The ultimate multimodal data platform for AI/ML applications.** 

LanceDB is designed for fast, scalable, and production-ready vector search. It is built on top of the Lance columnar format. You can store, index, and search over petabytes of multimodal data and vectors with ease. 
LanceDB is a central location where developers can build, train and analyze their AI workloads.

&lt;/div&gt;

&lt;br&gt;

## **Demo: Multimodal Search by Keyword, Vector or with SQL**
&lt;img max-width=&quot;750px&quot; alt=&quot;LanceDB Multimodal Search&quot; src=&quot;https://github.com/lancedb/lancedb/assets/917119/09c5afc5-7816-4687-bae4-f2ca194426ec&quot;&gt;

## **Star LanceDB to get updates!**

&lt;details&gt;
&lt;summary&gt;‚≠ê Click here ‚≠ê  to see how fast we&#039;re growing!&lt;/summary&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=lancedb/lancedb&amp;theme=dark&amp;type=Date&quot;&gt;
  &lt;img width=&quot;100%&quot; src=&quot;https://api.star-history.com/svg?repos=lancedb/lancedb&amp;theme=dark&amp;type=Date&quot;&gt;
&lt;/picture&gt;
&lt;/details&gt;

## **Key Features**:

- **Fast Vector Search**: Search billions of vectors in milliseconds with state-of-the-art indexing.
- **Comprehensive Search**: Support for vector similarity search, full-text search and SQL.
- **Multimodal Support**: Store, query and filter vectors, metadata and multimodal data (text, images, videos, point clouds, and more).
- **Advanced Features**: Zero-copy, automatic versioning, manage versions of your data without needing extra infrastructure. GPU support in building vector index.

### **Products**:
- **Open Source &amp; Local**: 100% open source, runs locally or in your cloud. No vendor lock-in.
- **Cloud and Enterprise**: Production-scale vector search with no servers to manage. Complete data sovereignty and security.

### **Ecosystem**:
- **Columnar Storage**: Built on the Lance columnar format for efficient storage and analytics.
- **Seamless Integration**: Python, Node.js, Rust, and REST APIs for easy integration. Native Python and Javascript/Typescript support.
- **Rich Ecosystem**: Integrations with [**LangChain** ü¶úÔ∏èüîó](https://python.langchain.com/docs/integrations/vectorstores/lancedb/), [**LlamaIndex** ü¶ô](https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/LanceDBIndexDemo.html), Apache-Arrow, Pandas, Polars, DuckDB and more on the way.

## **How to Install**:

Follow the [Quickstart](https://lancedb.github.io/lancedb/basic/) doc to set up LanceDB locally. 

**API &amp; SDK:** We also support Python, Typescript and Rust SDKs

| Interface | Documentation |
|-----------|---------------|
| Python SDK | https://lancedb.github.io/lancedb/python/python/ |
| Typescript SDK | https://lancedb.github.io/lancedb/js/globals/ |
| Rust SDK | https://docs.rs/lancedb/latest/lancedb/index.html |
| REST API | https://docs.lancedb.com/api-reference/introduction |

## **Join Us and Contribute**

We welcome contributions from everyone! Whether you&#039;re a developer, researcher, or just someone who wants to help out. 

If you have any suggestions or feature requests, please feel free to open an issue on GitHub or discuss it on our [**Discord**](https://discord.gg/G5DcmnZWKB) server.

[**Check out the GitHub Issues**](https://github.com/lancedb/lancedb/issues) if you would like to work on the features that are planned for the future. If you have any suggestions or feature requests, please feel free to open an issue on GitHub. 

## **Contributors**

&lt;a href=&quot;https://github.com/lancedb/lancedb/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=lancedb/lancedb&quot; /&gt;
&lt;/a&gt;


## **Stay in Touch With Us**
&lt;div align=&quot;center&quot;&gt;

&lt;/br&gt;

[![Website](https://img.shields.io/badge/-Website-100000?style=for-the-badge&amp;labelColor=645cfb&amp;color=645cfb)](https://lancedb.com/)
[![Blog](https://img.shields.io/badge/Blog-100000?style=for-the-badge&amp;labelColor=645cfb&amp;color=645cfb)](https://blog.lancedb.com/)
[![Discord](https://img.shields.io/badge/-Discord-100000?style=for-the-badge&amp;logo=discord&amp;logoColor=white&amp;labelColor=645cfb&amp;color=645cfb)](https://discord.gg/zMM32dvNtd)
[![Twitter](https://img.shields.io/badge/-Twitter-100000?style=for-the-badge&amp;logo=x&amp;logoColor=white&amp;labelColor=645cfb&amp;color=645cfb)](https://twitter.com/lancedb)
[![LinkedIn](https://img.shields.io/badge/-LinkedIn-100000?style=for-the-badge&amp;logo=linkedin&amp;logoColor=white&amp;labelColor=645cfb&amp;color=645cfb)](https://www.linkedin.com/company/lancedb/)

&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[nautechsystems/nautilus_trader]]></title>
            <link>https://github.com/nautechsystems/nautilus_trader</link>
            <guid>https://github.com/nautechsystems/nautilus_trader</guid>
            <pubDate>Thu, 16 Oct 2025 00:05:42 GMT</pubDate>
            <description><![CDATA[A high-performance algorithmic trading platform and event-driven backtester]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/nautechsystems/nautilus_trader">nautechsystems/nautilus_trader</a></h1>
            <p>A high-performance algorithmic trading platform and event-driven backtester</p>
            <p>Language: Rust</p>
            <p>Stars: 15,764</p>
            <p>Forks: 1,784</p>
            <p>Stars today: 19 stars today</p>
            <h2>README</h2><pre># &lt;img src=&quot;https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-trader-logo.png&quot; width=&quot;500&quot;&gt;

[![codecov](https://codecov.io/gh/nautechsystems/nautilus_trader/branch/master/graph/badge.svg?token=DXO9QQI40H)](https://codecov.io/gh/nautechsystems/nautilus_trader)
[![codspeed](https://img.shields.io/endpoint?url=https://codspeed.io/badge.json)](https://codspeed.io/nautechsystems/nautilus_trader)
![pythons](https://img.shields.io/pypi/pyversions/nautilus_trader)
![pypi-version](https://img.shields.io/pypi/v/nautilus_trader)
![pypi-format](https://img.shields.io/pypi/format/nautilus_trader?color=blue)
[![Downloads](https://pepy.tech/badge/nautilus-trader)](https://pepy.tech/project/nautilus-trader)
[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;logoColor=white)](https://discord.gg/NautilusTrader)

| Branch    | Version                                                                                                                                                                                                                     | Status                                                                                                                                                                                            |
| :-------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `master`  | [![version](https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fmaster%2Fversion.json)](https://packages.nautechsystems.io/simple/nautilus-trader/index.html)  | [![build](https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=nightly)](https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml) |
| `nightly` | [![version](https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fnightly%2Fversion.json)](https://packages.nautechsystems.io/simple/nautilus-trader/index.html) | [![build](https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=nightly)](https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml) |
| `develop` | [![version](https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fdevelop%2Fversion.json)](https://packages.nautechsystems.io/simple/nautilus-trader/index.html) | [![build](https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=develop)](https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml) |

| Platform           | Rust   | Python    |
| :----------------- | :----- | :-------- |
| `Linux (x86_64)`   | 1.89.0 | 3.11-3.13 |
| `Linux (ARM64)`    | 1.89.0 | 3.11-3.13 |
| `macOS (ARM64)`    | 1.89.0 | 3.11-3.13 |
| `Windows (x86_64)` | 1.89.0 | 3.11-3.13 |

- **Docs**: &lt;https://nautilustrader.io/docs/&gt;
- **Website**: &lt;https://nautilustrader.io&gt;
- **Support**: [support@nautilustrader.io](mailto:support@nautilustrader.io)

## Introduction

NautilusTrader is an open-source, high-performance, production-grade algorithmic trading platform,
providing quantitative traders with the ability to backtest portfolios of automated trading strategies
on historical data with an event-driven engine, and also deploy those same strategies live, with no code changes.

The platform is *AI-first*, designed to develop and deploy algorithmic trading strategies within a highly performant
and robust Python-native environment. This helps to address the parity challenge of keeping the Python research/backtest
environment consistent with the production live trading environment.

NautilusTrader&#039;s design, architecture, and implementation philosophy prioritizes software correctness and safety at the
highest level, with the aim of supporting Python-native, mission-critical, trading system backtesting
and live deployment workloads.

The platform is also universal, and asset-class-agnostic ‚Äî  with any REST API or WebSocket feed able to be integrated via modular
adapters. It supports high-frequency trading across a wide range of asset classes and instrument types
including FX, Equities, Futures, Options, Crypto, DeFi, and Betting, enabling seamless operations across multiple venues simultaneously.

![nautilus-trader](https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-trader.png &quot;nautilus-trader&quot;)

## Features

- **Fast**: Core is written in Rust with asynchronous networking using [tokio](https://crates.io/crates/tokio).
- **Reliable**: Rust-powered type- and thread-safety, with optional Redis-backed state persistence.
- **Portable**: OS independent, runs on Linux, macOS, and Windows. Deploy using Docker.
- **Flexible**: Modular adapters mean any REST API or WebSocket feed can be integrated.
- **Advanced**: Time in force `IOC`, `FOK`, `GTC`, `GTD`, `DAY`, `AT_THE_OPEN`, `AT_THE_CLOSE`, advanced order types and conditional triggers. Execution instructions `post-only`, `reduce-only`, and icebergs. Contingency orders including `OCO`, `OUO`, `OTO`.
- **Customizable**: Add user-defined custom components, or assemble entire systems from scratch leveraging the [cache](https://nautilustrader.io/docs/latest/concepts/cache) and [message bus](https://nautilustrader.io/docs/latest/concepts/message_bus).
- **Backtesting**: Run with multiple venues, instruments and strategies simultaneously using historical quote tick, trade tick, bar, order book and custom data with nanosecond resolution.
- **Live**: Use identical strategy implementations between backtesting and live deployments.
- **Multi-venue**: Multiple venue capabilities facilitate market-making and statistical arbitrage strategies.
- **AI Training**: Backtest engine fast enough to be used to train AI trading agents (RL/ES).

![Alt text](https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-art.png &quot;nautilus&quot;)

&gt; *nautilus - from ancient Greek &#039;sailor&#039; and naus &#039;ship&#039;.*
&gt;
&gt; *The nautilus shell consists of modular chambers with a growth factor which approximates a logarithmic spiral.
&gt; The idea is that this can be translated to the aesthetics of design and architecture.*

## Why NautilusTrader?

- **Highly performant event-driven Python**: Native binary core components.
- **Parity between backtesting and live trading**: Identical strategy code.
- **Reduced operational risk**: Enhanced risk management functionality, logical accuracy, and type safety.
- **Highly extendable**: Message bus, custom components and actors, custom data, custom adapters.

Traditionally, trading strategy research and backtesting might be conducted in Python
using vectorized methods, with the strategy then needing to be reimplemented in a more event-driven way
using C++, C#, Java or other statically typed language(s). The reasoning here is that vectorized backtesting code cannot
express the granular time and event dependent complexity of real-time trading, where compiled languages have
proven to be more suitable due to their inherently higher performance, and type safety.

One of the key advantages of NautilusTrader here, is that this reimplementation step is now circumvented - as the critical core components of the platform
have all been written entirely in [Rust](https://www.rust-lang.org/) or [Cython](https://cython.org/).
This means we&#039;re using the right tools for the job, where systems programming languages compile performant binaries,
with CPython C extension modules then able to offer a Python-native environment, suitable for professional quantitative traders and trading firms.

## Why Python?

Python was originally created decades ago as a simple scripting language with a clean straightforward syntax.
It has since evolved into a fully fledged general purpose object-oriented programming language.
Based on the TIOBE index, Python is currently the most popular programming language in the world.
Not only that, Python has become the *de facto lingua franca* of data science, machine learning, and artificial intelligence.

developer/user communities.
However, Python has performance and typing limitations for large-scale, latency-sensitive systems. Cython addresses many of these issues by introducing static typing into Python&#039;s rich ecosystem of libraries and communities.

## Why Rust?

[Rust](https://www.rust-lang.org/) is a multi-paradigm programming language designed for performance and safety, especially safe
concurrency. Rust is &quot;blazingly fast&quot; and memory-efficient (comparable to C and C++) with no garbage collector.
It can power mission-critical systems, run on embedded devices, and easily integrates with other languages.

Rust‚Äôs rich type system and ownership model guarantees memory-safety and thread-safety deterministically ‚Äî
eliminating many classes of bugs at compile-time.

The project increasingly utilizes Rust for core performance-critical components. Python bindings are implemented via Cython and [PyO3](https://pyo3.rs)‚Äîno Rust toolchain is required at install time.

This project makes the [Soundness Pledge](https://raphlinus.github.io/rust/2020/01/18/soundness-pledge.html):

&gt; ‚ÄúThe intent of this project is to be free of soundness bugs.
&gt; The developers will do their best to avoid them, and welcome help in analyzing and fixing them.‚Äù

&gt; [!NOTE]
&gt;
&gt; **MSRV:** NautilusTrader relies heavily on improvements in the Rust language and compiler.
&gt; As a result, the Minimum Supported Rust Version (MSRV) is generally equal to the latest stable release of Rust.

## Integrations

NautilusTrader is modularly designed to work with *adapters*, enabling connectivity to trading venues
and data providers by translating their raw APIs into a unified interface and normalized domain model.

The following integrations are currently supported; see [docs/integrations/](https://nautilustrader.io/docs/latest/integrations/) for details:

| Name                                                                         | ID                    | Type                    | Status                                                  | Docs                                        |
| :--------------------------------------------------------------------------- | :-------------------- | :---------------------- | :------------------------------------------------------ | :------------------------------------------ |
| [Betfair](https://betfair.com)                                               | `BETFAIR`             | Sports Betting Exchange | ![status](https://img.shields.io/badge/stable-green)    | [Guide](docs/integrations/betfair.md)       |
| [Binance](https://binance.com)                                               | `BINANCE`             | Crypto Exchange (CEX)   | ![status](https://img.shields.io/badge/stable-green)    | [Guide](docs/integrations/binance.md)       |
| [Binance US](https://binance.us)                                             | `BINANCE`             | Crypto Exchange (CEX)   | ![status](https://img.shields.io/badge/stable-green)    | [Guide](docs/integrations/binance.md)       |
| [Binance Futures](https://www.binance.com/en/futures)                        | `BINANCE`             | Crypto Exchange (CEX)   | ![status](https://img.shields.io/badge/stable-green)    | [Guide](docs/integrations/binance.md)       |
| [BitMEX](https://www.bitmex.com)                                             | `BITMEX`              | Crypto Exchange (CEX)   | ![status](https://img.shields.io/badge/building-orange) | [Guide](docs/integrations/bitmex.md)        |
| [Bybit](https://www.bybit.com)                                               | `BYBIT`               | Crypto Exchange (CEX)   | ![status](https://img.shields.io/badge/stable-green)    | [Guide](docs/integrations/bybit.md)         |
| [Coinbase International](https://www.coinbase.com/en/international-exchange) | `COINBASE_INTX`       | Crypto Exchange (CEX)   | ![status](https://img.shields.io/badge/stable-green)    | [Guide](docs/integrations/coinbase_intx.md) |
| [Databento](https://databento.com)                                           | `DATABENTO`           | Data Provider           | ![status](https://img.shields.io/badge/stable-green)    | [Guide](docs/integrations/databento.md)     |
| [dYdX](https://dydx.exchange/)                                               | `DYDX`                | Crypto Exchange (DEX)   | ![status](https://img.shields.io/badge/stable-green)    | [Guide](docs/integrations/dydx.md)          |
| [Hyperliquid](https://hyperliquid.xyz)                                       | `HYPERLIQUID`         | Crypto Exchange (DEX)   | ![status](https://img.shields.io/badge/building-orange) | [Guide](docs/integrations/hyperliquid.md)   |
| [Interactive Brokers](https://www.interactivebrokers.com)                    | `INTERACTIVE_BROKERS` | Brokerage (multi-venue) | ![status](https://img.shields.io/badge/stable-green)    | [Guide](docs/integrations/ib.md)            |
| [OKX](https://okx.com)                                                       | `OKX`                 | Crypto Exchange (CEX)   | ![status](https://img.shields.io/badge/beta-yellow)     | [Guide](docs/integrations/okx.md)           |
| [Polymarket](https://polymarket.com)                                         | `POLYMARKET`          | Prediction Market (DEX) | ![status](https://img.shields.io/badge/stable-green)    | [Guide](docs/integrations/polymarket.md)    |
| [Tardis](https://tardis.dev)                                                 | `TARDIS`              | Crypto Data Provider    | ![status](https://img.shields.io/badge/stable-green)    | [Guide](docs/integrations/tardis.md)        |

- **ID**: The default client ID for the integrations adapter clients.
- **Type**: The type of integration (often the venue type).

### Status

- `building`: Under construction and likely not in a usable state.
- `beta`: Completed to a minimally working state and in a beta testing phase.
- `stable`: Stabilized feature set and API, the integration has been tested by both developers and users to a reasonable level (some bugs may still remain).

See the [Integrations](https://nautilustrader.io/docs/latest/integrations/index.html) documentation for further details.

## Versioning and releases

**NautilusTrader is still under active development**. Some features may be incomplete, and while
the API is becoming more stable, breaking changes can occur between releases.
We strive to document these changes in the release notes on a **best-effort basis**.

We aim to follow a **bi-weekly release schedule**, though experimental or larger features may cause delays.

### Branches

We aim to maintain a stable, passing build across all branches.

- `master`: Reflects the source code for the latest released version; recommended for production use.
- `nightly`: Daily snapshots of the `develop` branch for early testing; merged at **14:00 UTC** or on demand.
- `develop`: Active development branch for contributors and feature work.

&gt; [!NOTE]
&gt;
&gt; Our [roadmap](/ROADMAP.md) aims to achieve a **stable API for version 2.x** (likely after the Rust port).
&gt; Once this milestone is reached, we plan to implement a formal deprecation process for any API changes.
&gt; This approach allows us to maintain a rapid development pace for now.

## Precision mode

NautilusTrader supports two precision modes for its core value types (`Price`, `Quantity`, `Money`),
which differ in their internal bit-width and maximum decimal precision.

- **High-precision**: 128-bit integers with up to 16 decimals of precision, and a larger value range.
- **Standard-precision**: 64-bit integers with up to 9 decimals of precision, and a smaller value range.

&gt; [!NOTE]
&gt;
&gt; By default, the official Python wheels **ship** in high-precision (128-bit) mode on Linux and macOS.
&gt; On Windows, only standard-precision (64-bit) is available due to the lack of native 128-bit integer support.
&gt; For the Rust crates, the default is standard-precision unless you explicitly enable the `high-precision` feature flag.

See the [Installation Guide](https://nautilustrader.io/docs/latest/getting_started/installation) for further details.

**Rust feature flag**: To enable high-precision mode in Rust, add the `high-precision` feature to your Cargo.toml:

```toml
[dependencies]
nautilus_model = { version = &quot;*&quot;, features = [&quot;high-precision&quot;] }
```

## Installation

We recommend using the latest supported version of Python and installing [nautilus_trader](https://pypi.org/project/nautilus_trader/) inside a virtual environment to isolate dependencies.

**There are two supported ways to install**:

1. Pre-built binary wheel from PyPI *or* the Nautech Systems package index.
2. Build from source.

&gt; [!TIP]
&gt;
&gt; We highly recommend installing using the [uv](https://docs.astral.sh/uv) package manager with a &quot;vanilla&quot; CPython.
&gt;
&gt; Conda and other Python distributions *may* work but aren‚Äôt officially supported.

### From PyPI

To install the latest binary wheel (or sdist package) from PyPI using Python&#039;s pip package manager:

```bash
pip install -U nautilus_trader
```

### From the Nautech Systems package index

The Nautech Systems package index (`packages.nautechsystems.io`) complies with [PEP-503](https://peps.python.org/pep-0503/) and hosts both stable and development binary wheels for `nautilus_trader`.
This enables users to install either the latest stable release or pre-release versions for testing.

#### Stable wheels

Stable wheels correspond to official releases of `nautilus_trader` on PyPI, and use standard versioning.

To install the latest stable release:

```bash
pip install -U nautilus_trader --index-url=https://packages.nautechsystems.io/simple
```

#### Development wheels

Development wheels are published from both the `nightly` and `develop` branches,
allowing users to test features and fixes ahead of stable releases.

**Note**: Wheels from the `develop` branch are only built for the Linux x86_64 platform to save time
and compute resources, while `nightly` wheels support additional platforms as shown below.

| Platform           | Nightly | Develop |
| :----------------- | :------ | :------ |
| `Linux (x86_64)`   | ‚úì       | ‚úì       |
| `Linux (ARM64)`    | ‚úì       | -       |
| `macOS (ARM64)`    | ‚úì       | -       |
| `Windows (x86_64)` | ‚úì       | -       |

This process also helps preserve compute resources and ensures easy access to the exact binaries tested in CI pipelines,
while adhering to [PEP-440](https://peps.python.org/pep-0440/) versioning standards:

- `develop` wheels use the version format `dev{date}+{build_number}` (e.g., `1.208.0.dev20241212+7001`).
- `nightly` wheels use the version format `a{date}` (alpha) (e.g., `1.208.0a20241212`).

&gt; [!WARNING]
&gt;
&gt; We do not recommend using development wheels in production environments, such as live trading controlling real capital.

#### Installation commands

By default, pip will install the latest stable release. Adding the `--pre` flag ensures that pre-release versions, including development wheels, are considered.

To install the latest available pre-release (including development wheels):

```bash
pip install -U nautilus_trader --pre --index-url=https://packages.nautechsystems.io/simple
```

To install a specific development wheel (e.g., `1.208.0a20241212` for December 12, 2024):

```bash
pip install nautilus_trader==1.208.0a20241212 --index-url=https://packages.nautechsystems.io/simple
```

#### Available versions

You can view all available versions of `nautilus_trader` on the [package index](https://packages.nautechsystems.io

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[eza-community/eza]]></title>
            <link>https://github.com/eza-community/eza</link>
            <guid>https://github.com/eza-community/eza</guid>
            <pubDate>Thu, 16 Oct 2025 00:05:41 GMT</pubDate>
            <description><![CDATA[A modern alternative to ls]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/eza-community/eza">eza-community/eza</a></h1>
            <p>A modern alternative to ls</p>
            <p>Language: Rust</p>
            <p>Stars: 17,933</p>
            <p>Forks: 332</p>
            <p>Stars today: 28 stars today</p>
            <h2>README</h2><pre>&lt;!--
SPDX-FileCopyrightText: 2023-2024 Christina S√∏rensen
SPDX-FileContributor: Christina S√∏rensen

SPDX-License-Identifier: EUPL-1.2
--&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;
   &lt;sup&gt;Special thanks to:&lt;/sup&gt;
   &lt;br&gt;
   &lt;br&gt;
   &lt;a href=&quot;https://www.warp.dev/eza&quot;&gt;
      &lt;img alt=&quot;Warp sponsorship&quot; width=&quot;400&quot; src=&quot;https://github.com/user-attachments/assets/ab8dd143-b0fd-4904-bdc5-dd7ecac94eae&quot;&gt;
   &lt;/a&gt;

### [Warp, the AI terminal for developers](https://www.warp.dev/eza)
[Available for MacOS, Linux, &amp; Windows](https://www.warp.dev/eza)&lt;br&gt;

&lt;/div&gt;
    
# eza

A modern replacement for ls.

&lt;a href=&quot;https://matrix.to/#/#eza-community:gitter.im&quot;&gt;&lt;img alt=&quot;Gitter&quot; src=&quot;https://img.shields.io/gitter/room/eza-community/eza?logo=element&amp;link=https%3A%2F%2Fapp.gitter.im%2F%23%2Froom%2F%23eza%3Agitter.im&amp;link=Gitter%20matrix%20room%20for%20Eza&quot; width=200&gt;&lt;/a&gt;

[![Built with Nix](https://img.shields.io/badge/Built_With-Nix-5277C3.svg?logo=nixos&amp;labelColor=73C3D5)](https://nixos.org)
[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](CODE_OF_CONDUCT.md)

[![Unit tests](https://github.com/eza-community/eza/actions/workflows/unit-tests.yml/badge.svg)](https://github.com/eza-community/eza/actions/workflows/unit-tests.yml)
[![Crates.io](https://img.shields.io/crates/v/eza?link=https%3A%2F%2Fcrates.io%2Fcrates%2Feza)](https://crates.io/crates/eza)
![Crates.io](https://img.shields.io/crates/l/eza?link=https%3A%2F%2Fgithub.com%2Feza-community%2Feza%2Fblob%2Fmain%2FLICENCE)

&lt;/div&gt;

![eza demo gif](docs/images/screenshots.png)

---

**eza** is a modern alternative for the venerable file-listing command-line program `ls` that ships with Unix and Linux operating systems, giving it more features and better defaults.
It uses colours to distinguish file types and metadata.
It knows about symlinks, extended attributes, and Git.
And it‚Äôs **small**, **fast**, and just **one single binary**.

By deliberately making some decisions differently, eza attempts to be a more featureful, more user-friendly version of `ls`.

---

**eza** features not in exa (non-exhaustive):

- Fixes [‚ÄúThe Grid Bug‚Äù](https://github.com/eza-community/eza/issues/66#issuecomment-1656758327) introduced in exa 2021.
- Hyperlink support.
- Mount point details.
- Selinux context output.
- Git repo status output.
- Human readable relative dates.
- Several security fixes.
- Support for `bright` terminal colours.
- Many smaller bug fixes/changes!
- Configuration `theme.yml` file for customization of colors and icons.

...and like, so much more that it became exhausting to update this all the time.
Like seriously, we have a lot of good stuff.

---

&lt;a id=&quot;try-it&quot;&gt;
&lt;h1&gt;Try it!&lt;/h1&gt;
&lt;/a&gt;

### Nix ‚ùÑÔ∏è

If you already have Nix setup with flake support, you can try out eza with the `nix run` command:

    nix run github:eza-community/eza

Nix will build eza and run it.

If you want to pass arguments this way, use e.g. `nix run github:eza-community/eza -- -ol`.

# Installation

eza is available for Windows, macOS and Linux. Platform and distribution
specific installation instructions can be found in [INSTALL.md](INSTALL.md).

[![Packaging status](https://repology.org/badge/vertical-allrepos/eza.svg?columns=3)](https://repology.org/project/eza/versions)

---

&lt;a id=&quot;options&quot;&gt;
&lt;h1&gt;Command-line options&lt;/h1&gt;
&lt;/a&gt;

eza‚Äôs options are almost, but not quite, entirely unlike `ls`‚Äôs. Quick overview:

## Display options

&lt;details&gt;
&lt;summary&gt;Click to expand&lt;/summary&gt;

- **-1**, **--oneline**: display one entry per line
- **-G**, **--grid**: display entries as a grid (default)
- **-l**, **--long**: display extended details and attributes
- **-R**, **--recurse**: recurse into directories
- **-T**, **--tree**: recurse into directories as a tree
- **-x**, **--across**: sort the grid across, rather than downwards
- **-F**, **--classify=(when)**: display type indicator by file names (always, auto, never)
- **--colo[u]r=(when)**: when to use terminal colours (always, auto, never)
- **--colo[u]r-scale=(field)**: highlight levels of `field` distinctly(all, age, size)
- **--color-scale-mode=(mode)**: use gradient or fixed colors in --color-scale. valid options are `fixed` or `gradient`
- **--icons=(when)**: when to display icons (always, auto, never)
- **--hyperlink**: display entries as hyperlinks
- **--absolute=(mode)**: display entries with their absolute path (on, follow, off)
- **-w**, **--width=(columns)**: set screen width in columns

&lt;/details&gt;

## Filtering options

&lt;details&gt;
&lt;summary&gt;Click to expand&lt;/summary&gt;

- **-a**, **--all**: show hidden and &#039;dot&#039; files
- **-d**, **--list-dirs**: list directories like regular files
- **-L**, **--level=(depth)**: limit the depth of recursion
- **-r**, **--reverse**: reverse the sort order
- **-s**, **--sort=(field)**: which field to sort by
- **--group-directories-first**: list directories before other files
- **--group-directories-last**: list directories after other files
- **-D**, **--only-dirs**: list only directories
- **-f**, **--only-files**: list only files
- **--no-symlinks**: don&#039;t show symbolic links
- **--show-symlinks**: explicitly show links (with `--only-dirs`, `--only-files`, to show symlinks that match the filter)
- **--git-ignore**: ignore files mentioned in `.gitignore`
- **-I**, **--ignore-glob=(globs)**: glob patterns (pipe-separated) of files to ignore

Pass the `--all` option twice to also show the `.` and `..` directories.

&lt;/details&gt;

## Long view options

&lt;details&gt;
&lt;summary&gt;Click to expand&lt;/summary&gt;

These options are available when running with `--long` (`-l`):

- **-b**, **--binary**: list file sizes with binary prefixes
- **-B**, **--bytes**: list file sizes in bytes, without any prefixes
- **-g**, **--group**: list each file‚Äôs group
- **--smart-group**: only show group if it has a different name from owner
- **-h**, **--header**: add a header row to each column
- **-H**, **--links**: list each file‚Äôs number of hard links
- **-i**, **--inode**: list each file‚Äôs inode number
- **-m**, **--modified**: use the modified timestamp field
- **-M**, **--mounts**: Show mount details (Linux and MacOS only).
- **-S**, **--blocksize**: show size of allocated file system blocks
- **-t**, **--time=(field)**: which timestamp field to use
- **-u**, **--accessed**: use the accessed timestamp field
- **-U**, **--created**: use the created timestamp field
- **-X**, **--dereference**: dereference symlinks for file information
- **-Z**, **--context**: list each file‚Äôs security context
- **-@**, **--extended**: list each file‚Äôs extended attributes and sizes
- **--changed**: use the changed timestamp field
- **--git**: list each file‚Äôs Git status, if tracked or ignored
- **--git-repos**: list each directory‚Äôs Git status, if tracked
- **--git-repos-no-status**: list whether a directory is a Git repository, but not its status (faster)
- **--no-git**: suppress Git status (always overrides `--git`, `--git-repos`, `--git-repos-no-status`)
- **--time-style**: how to format timestamps. valid timestamp styles are ‚Äò`default`‚Äô, ‚Äò`iso`‚Äô, ‚Äò`long-iso`‚Äô, ‚Äò`full-iso`‚Äô, ‚Äò`relative`‚Äô, or a custom style ‚Äò`+&lt;FORMAT&gt;`‚Äô (E.g., ‚Äò`+%Y-%m-%d %H:%M`‚Äô =&gt; ‚Äò`2023-09-30 13:00`‚Äô. For more specifications on the format string, see the _`eza(1)` manual page_ and [chrono documentation](https://docs.rs/chrono/latest/chrono/format/strftime/index.html).).
- **--total-size**: show recursive directory size
- **--no-permissions**: suppress the permissions field
- **-o**, **--octal-permissions**: list each file&#039;s permission in octal format
- **--no-filesize**: suppress the filesize field
- **--no-user**: suppress the user field
- **--no-time**: suppress the time field
- **--stdin**: read file names from stdin

Some of the options accept parameters:

- Valid **--colo\[u\]r** options are **always**, **automatic** (or **auto** for short), and **never**.
- Valid sort fields are **accessed**, **changed**, **created**, **extension**, **Extension**, **inode**, **modified**, **name**, **Name**, **size**, **type**, and **none**. Fields starting with a capital letter sort uppercase before lowercase. The modified field has the aliases **date**, **time**, and **newest**, while its reverse has the aliases **age** and **oldest**.
- Valid time fields are **modified**, **changed**, **accessed**, and **created**.
- Valid time styles are **default**, **iso**, **long-iso**, **full-iso**, and **relative**.



See the `man` pages for further documentation of usage. They are available
- online [in the repo](https://github.com/eza-community/eza/tree/main/man)
- in your terminal via `man eza`, as of version [`[0.18.13] - 2024-04-25`](https://github.com/eza-community/eza/blob/main/CHANGELOG.md#01813---2024-04-25)
&lt;/details&gt;


## Custom Themes
&lt;details&gt;
&lt;summary&gt;Click to expand&lt;/summary&gt;

**Eza** has recently added support for a `theme.yml` file, where you can specify all of the existing theme-ing options
available for the `LS_COLORS` and `EXA_COLORS` environment variables, as well as the option to specify different icons
for different file types and extensions. Any existing environment variables set will continue to work and will take
precedence for backwards compatibility.

#### **New** Pre-made themes
Check out the themes available in the official [eza-themes](https://github.com/eza-community/eza-themes) repository, or contribute your own.

An example theme file is available in `docs/theme.yml`, and needs to either be placed in a directory specified by the 
environment variable `EZA_CONFIG_DIR`, or will looked for by default in `$XDG_CONFIG_HOME/eza`.

Full details are available on the [man page](https://github.com/eza-community/eza/tree/main/man/eza_colors-explanation.5.md) and an example theme file is included [here](https://github.com/eza-community/eza/tree/main/docs/theme.yml)

&lt;/details&gt;


# Hacking on eza

If you wanna contribute to eza, firstly, you&#039;re expected to follow our 
[code of conduct](https://github.com/eza-community/eza/blob/main/CODE_OF_CONDUCT.md). 
After having understood the code of conduct, you can have a look at our
[CONTRIBUTING.md](https://github.com/eza-community/eza/blob/main/CONTRIBUTING.md) 
for more info about actual hacking.

[![Star History Chart](https://api.star-history.com/svg?repos=eza-community/eza&amp;type=Date)](https://star-history.com/#eza-community/eza&amp;Date)
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[vortex-data/vortex]]></title>
            <link>https://github.com/vortex-data/vortex</link>
            <guid>https://github.com/vortex-data/vortex</guid>
            <pubDate>Thu, 16 Oct 2025 00:05:40 GMT</pubDate>
            <description><![CDATA[An extensible, state of the art columnar file format. Formerly at @spiraldb, now an Incubation Stage project at LFAI&Data, part of the Linux Foundation.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/vortex-data/vortex">vortex-data/vortex</a></h1>
            <p>An extensible, state of the art columnar file format. Formerly at @spiraldb, now an Incubation Stage project at LFAI&Data, part of the Linux Foundation.</p>
            <p>Language: Rust</p>
            <p>Stars: 1,836</p>
            <p>Forks: 75</p>
            <p>Stars today: 15 stars today</p>
            <h2>README</h2><pre># üå™Ô∏è Vortex

[![Build Status](https://github.com/vortex-data/vortex/actions/workflows/ci.yml/badge.svg)](https://github.com/vortex-data/vortex/actions)
[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/10567/badge)](https://www.bestpractices.dev/projects/10567)
[![Documentation](https://docs.rs/vortex/badge.svg)](https://docs.vortex.dev)
[![CodSpeed Badge](https://img.shields.io/endpoint?url=https://codspeed.io/badge.json)](https://codspeed.io/vortex-data/vortex)
[![Crates.io](https://img.shields.io/crates/v/vortex.svg)](https://crates.io/crates/vortex)
[![PyPI - Version](https://img.shields.io/pypi/v/vortex-data)](https://pypi.org/project/vortex-data/)
[![Maven - Version](https://img.shields.io/maven-central/v/dev.vortex/vortex-spark)](https://central.sonatype.com/artifact/dev.vortex/vortex-spark)
[![codecov](https://codecov.io/github/vortex-data/vortex/graph/badge.svg)](https://codecov.io/github/vortex-data/vortex)

üìö [Documentation](https://docs.vortex.dev/) | üìä [Performance Benchmarks](https://bench.vortex.dev)

## Overview

Vortex is a next-generation columnar file format and toolkit designed for high-performance data processing.
It is the fastest and most extensible format for building data systems backed by object storage. It provides:

- **‚ö°Ô∏è Blazing Fast Performance**
  - 100x faster random access reads (vs. modern Apache Parquet)
  - 10-20x faster scans
  - 5x faster writes
  - Similar compression ratios
  - Efficient support for wide tables with zero-copy/zero-parse metadata

- **üîß Extensible Architecture**
  - Modeled after Apache DataFusion&#039;s extensible approach
  - Pluggable encoding system, type system, compression strategy, &amp; layout strategy
  - Zero-copy compatibility with Apache Arrow

- **üó≥Ô∏è Open Source, Neutral Governance**
  - A Linux Foundation (LF AI &amp; Data) Project
  - Apache-2.0 Licensed

- **‚ÜîÔ∏è Integrations**
  - Arrow, DataFusion, DuckDB, Spark, Pandas, Polars, &amp; more
  - Apache Iceberg (coming soon)

&gt; üü¢ **Development Status**: Library APIs may change from version to version, but we now consider
&gt; the file format &lt;ins&gt;*stable*&lt;/ins&gt;. From release 0.36.0, all future releases of Vortex should
&gt; maintain backwards compatibility of the file format (i.e., be able to read files written by
&gt; any earlier version &gt;= 0.36.0).

## Key Features

### Core Capabilities

- ‚ú® **Logical Types** - Clean separation between logical schema and physical layout
- üîÑ **Zero-Copy Arrow Integration** - Seamless conversion to/from Apache Arrow arrays
- üß© **Extensible Encodings** - Pluggable physical layouts with built-in optimizations
- üì¶ **Cascading Compression** - Support for nested encoding schemes
- üöÄ **High-Performance Computing** - Optimized compute kernels for encoded data
- üìä **Rich Statistics** - Lazy-loaded summary statistics for optimization

### Technical Architecture

#### Logical vs Physical Design

Vortex strictly separates logical and physical concerns:

- **Logical Layer**: Defines data types and schema
- **Physical Layer**: Handles encoding and storage implementation
- **Built-in Encodings**: Compatible with Apache Arrow&#039;s memory format
- **Extension Encodings**: Optimized compression schemes (RLE, dictionary, etc.)

## Quick Start

### Installation

#### Rust Crate

All features are exported through the main `vortex` crate.

```bash
cargo add vortex
```

#### Python Package

```bash
uv add vortex-data
```

#### Command Line UI (vx)

For browsing the structure of Vortex files, you can use the `vx` command-line tool.

```bash
# Install latest release
cargo install vortex-tui --locked

# Or build from source
cargo install --path vortex-tui --locked

# Usage
vx browse &lt;file&gt;
```

### Development Setup

#### Prerequisites (macOS)

```bash
# Optional but recommended dependencies
brew install flatbuffers protobuf  # For .fbs and .proto files
brew install duckdb               # For benchmarks

# Install Rust toolchain
curl --proto &#039;=https&#039; --tlsv1.2 -sSf https://sh.rustup.rs | sh
# or
brew install rustup

# Initialize submodules
git submodule update --init --recursive

# Setup dependencies with uv
uv sync --all-packages
```

### Performance Optimization

For optimal performance, we suggest using [MiMalloc](https://github.com/microsoft/mimalloc):

```rust,ignore
#[global_allocator]
static GLOBAL_ALLOC: MiMalloc = MiMalloc;
```

## Project Information

### License

Licensed under the Apache License, Version 2.0.

### Governance

Vortex is an independent open-source project and not controlled by any single company. The Vortex Project is a
sub-project of the Linux Foundation Projects. The governance model is documented in
[CONTRIBUTING.md](CONTRIBUTING.md) and is subject to the terms of
the [Technical Charter](https://vortex.dev/charter.pdf).

### Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Reporting Vulnerabilities

If you discovery a security vulnerability, please email &lt;vuln-report@vortex.dev&gt;.

### Trademarks

Copyright ¬© Vortex a Series of LF Projects, LLC.
For terms of use, trademark policy, and other project policies please see &lt;https://lfprojects.org&gt;

## Acknowledgments üèÜ

The Vortex project benefits enormously from groundbreaking work from the academic &amp; open-source communities.

### Research in Vortex

- [BtrBlocks](https://www.cs.cit.tum.de/fileadmin/w00cfj/dis/papers/btrblocks.pdf) - Efficient columnar compression
- [FastLanes](https://www.vldb.org/pvldb/vol16/p2132-afroozeh.pdf) &amp; [FastLanes on GPU](https://dbdbd2023.ugent.be/abstracts/felius_fastlanes.pdf) - High-performance integer compression
- [FSST](https://www.vldb.org/pvldb/vol13/p2649-boncz.pdf) - Fast random access string compression
- [ALP](https://ir.cwi.nl/pub/33334/33334.pdf) &amp; [G-ALP](https://dl.acm.org/doi/pdf/10.1145/3736227.3736242) - Adaptive lossless floating-point compression
- [Procella](https://dl.acm.org/citation.cfm?id=3360438) - YouTube&#039;s unified data system
- [Anyblob](https://www.durner.dev/app/media/papers/anyblob-vldb23.pdf) - High-performance access to object storage
- [ClickHouse](https://www.vldb.org/pvldb/vol17/p3731-schulze.pdf) - Fast analytics for everyone
- [MonetDB/X100](https://www.cidrdb.org/cidr2005/papers/P19.pdf) - Hyper-Pipelining Query Execution
- [Morsel-Driven Parallelism](https://db.in.tum.de/~leis/papers/morsels.pdf): A NUMA-Aware Query Evaluation Format for the Many-Core Age
- [The FastLanes File Format](https://github.com/cwida/FastLanes/blob/dev/docs/specification.pdf) - Expression Operators

### Vortex in Research

- [Anyblox](https://gienieczko.com/anyblox-paper) - A Framework for Self-Decoding Datasets
- [F3](https://dl.acm.org/doi/pdf/10.1145/3749163) - Open-Source Data File Format for the Future

### Open Source Inspiration

- [Apache Arrow](https://arrow.apache.org)
- [Apache DataFusion](https://github.com/apache/datafusion)
- [parquet2](https://github.com/jorgecarleitao/parquet2) by Jorge Leitao
- [DuckDB](https://github.com/duckdb/duckdb)
- [Velox](https://github.com/facebookincubator/velox) &amp; [Nimble](https://github.com/facebookincubator/nimble)

#### Thanks to all contributors who have shared their knowledge and code with the community! üöÄ
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[openai/codex]]></title>
            <link>https://github.com/openai/codex</link>
            <guid>https://github.com/openai/codex</guid>
            <pubDate>Thu, 16 Oct 2025 00:05:39 GMT</pubDate>
            <description><![CDATA[Lightweight coding agent that runs in your terminal]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/openai/codex">openai/codex</a></h1>
            <p>Lightweight coding agent that runs in your terminal</p>
            <p>Language: Rust</p>
            <p>Stars: 47,660</p>
            <p>Forks: 5,754</p>
            <p>Stars today: 207 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;code&gt;npm i -g @openai/codex&lt;/code&gt;&lt;br /&gt;or &lt;code&gt;brew install codex&lt;/code&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;Codex CLI&lt;/strong&gt; is a coding agent from OpenAI that runs locally on your computer.
&lt;/br&gt;
&lt;/br&gt;If you want Codex in your code editor (VS Code, Cursor, Windsurf), &lt;a href=&quot;https://developers.openai.com/codex/ide&quot;&gt;install in your IDE&lt;/a&gt;
&lt;/br&gt;If you are looking for the &lt;em&gt;cloud-based agent&lt;/em&gt; from OpenAI, &lt;strong&gt;Codex Web&lt;/strong&gt;, go to &lt;a href=&quot;https://chatgpt.com/codex&quot;&gt;chatgpt.com/codex&lt;/a&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;./.github/codex-cli-splash.png&quot; alt=&quot;Codex CLI splash&quot; width=&quot;80%&quot; /&gt;
  &lt;/p&gt;

---

## Quickstart

### Installing and running Codex CLI

Install globally with your preferred package manager. If you use npm:

```shell
npm install -g @openai/codex
```

Alternatively, if you use Homebrew:

```shell
brew install codex
```

Then simply run `codex` to get started:

```shell
codex
```

&lt;details&gt;
&lt;summary&gt;You can also go to the &lt;a href=&quot;https://github.com/openai/codex/releases/latest&quot;&gt;latest GitHub Release&lt;/a&gt; and download the appropriate binary for your platform.&lt;/summary&gt;

Each GitHub Release contains many executables, but in practice, you likely want one of these:

- macOS
  - Apple Silicon/arm64: `codex-aarch64-apple-darwin.tar.gz`
  - x86_64 (older Mac hardware): `codex-x86_64-apple-darwin.tar.gz`
- Linux
  - x86_64: `codex-x86_64-unknown-linux-musl.tar.gz`
  - arm64: `codex-aarch64-unknown-linux-musl.tar.gz`

Each archive contains a single entry with the platform baked into the name (e.g., `codex-x86_64-unknown-linux-musl`), so you likely want to rename it to `codex` after extracting it.

&lt;/details&gt;

### Using Codex with your ChatGPT plan

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;./.github/codex-cli-login.png&quot; alt=&quot;Codex CLI login&quot; width=&quot;80%&quot; /&gt;
  &lt;/p&gt;

Run `codex` and select **Sign in with ChatGPT**. We recommend signing into your ChatGPT account to use Codex as part of your Plus, Pro, Team, Edu, or Enterprise plan. [Learn more about what&#039;s included in your ChatGPT plan](https://help.openai.com/en/articles/11369540-codex-in-chatgpt).

You can also use Codex with an API key, but this requires [additional setup](./docs/authentication.md#usage-based-billing-alternative-use-an-openai-api-key). If you previously used an API key for usage-based billing, see the [migration steps](./docs/authentication.md#migrating-from-usage-based-billing-api-key). If you&#039;re having trouble with login, please comment on [this issue](https://github.com/openai/codex/issues/1243).

### Model Context Protocol (MCP)

Codex can access MCP servers. To configure them, refer to the [config docs](./docs/config.md#mcp_servers).

### Configuration

Codex CLI supports a rich set of configuration options, with preferences stored in `~/.codex/config.toml`. For full configuration options, see [Configuration](./docs/config.md).

---

### Docs &amp; FAQ

- [**Getting started**](./docs/getting-started.md)
  - [CLI usage](./docs/getting-started.md#cli-usage)
  - [Running with a prompt as input](./docs/getting-started.md#running-with-a-prompt-as-input)
  - [Example prompts](./docs/getting-started.md#example-prompts)
  - [Memory with AGENTS.md](./docs/getting-started.md#memory-with-agentsmd)
  - [Configuration](./docs/config.md)
- [**Sandbox &amp; approvals**](./docs/sandbox.md)
- [**Authentication**](./docs/authentication.md)
  - [Auth methods](./docs/authentication.md#forcing-a-specific-auth-method-advanced)
  - [Login on a &quot;Headless&quot; machine](./docs/authentication.md#connecting-on-a-headless-machine)
- **Automating Codex**
  - [GitHub Action](https://github.com/openai/codex-action)
  - [TypeScript SDK](./sdk/typescript/README.md)
  - [Non-interactive mode (`codex exec`)](./docs/exec.md)
- [**Advanced**](./docs/advanced.md)
  - [Tracing / verbose logging](./docs/advanced.md#tracing--verbose-logging)
  - [Model Context Protocol (MCP)](./docs/advanced.md#model-context-protocol-mcp)
- [**Zero data retention (ZDR)**](./docs/zdr.md)
- [**Contributing**](./docs/contributing.md)
- [**Install &amp; build**](./docs/install.md)
  - [System Requirements](./docs/install.md#system-requirements)
  - [DotSlash](./docs/install.md#dotslash)
  - [Build from source](./docs/install.md#build-from-source)
- [**FAQ**](./docs/faq.md)
- [**Open source fund**](./docs/open-source-fund.md)

---

## License

This repository is licensed under the [Apache-2.0 License](LICENSE).
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[facebook/pyrefly]]></title>
            <link>https://github.com/facebook/pyrefly</link>
            <guid>https://github.com/facebook/pyrefly</guid>
            <pubDate>Thu, 16 Oct 2025 00:05:38 GMT</pubDate>
            <description><![CDATA[A fast type checker and language server for Python]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/facebook/pyrefly">facebook/pyrefly</a></h1>
            <p>A fast type checker and language server for Python</p>
            <p>Language: Rust</p>
            <p>Stars: 3,970</p>
            <p>Forks: 160</p>
            <p>Stars today: 64 stars today</p>
            <h2>README</h2><pre># Pyrefly: A fast type checker and language server for Python with powerful IDE features

[![pyrefly](https://img.shields.io/endpoint?url=https://pyrefly.org/badge.json)](https://github.com/facebook/pyrefly)
[![PyPI](https://img.shields.io/pypi/v/pyrefly.svg?color=blue)](https://pypi.python.org/pypi/pyrefly)
[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;logoColor=white)](https://discord.gg/Cf7mFQtW7W)
[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)

Pyrefly is a type checker and language server for Python, which provides
lightning-fast type checking along with IDE features such as code navigation,
semantic highlighting, and code completion. It is available as a
[command-line tool](https://pyrefly.org/en/docs/installation/) and a
[VSCode extension](https://marketplace.visualstudio.com/items?itemName=meta.pyrefly).

See the [Pyrefly website](https://pyrefly.org) for full documentation and how to
add Pyrefly to your editor of choice.

Currently under active development with known issues. Please open an issue if
you find bugs.

### Getting Started

- Try out pyrefly in your browser: [Sandbox](https://pyrefly.org/sandbox/)
- Get the command-line tool: `pip install pyrefly`
- Get the VSCode extension:
  [Link](https://marketplace.visualstudio.com/items?itemName=meta.pyrefly)

### Key Features:

- Type Inference: Pyrefly infers types in most locations, apart from function
  parameters. It can infer types of variables and return types.
- Flow Types: Pyrefly can understand your program&#039;s control flow to refine
  static types.
- Incrementality: Pyrefly aims for large-scale incrementality at the module
  level, with optimized checking and parallelism.

## Getting Involved

If you have questions or would like to report a bug, please
[create an issue](https://github.com/facebook/pyrefly/issues).

See our
[contributing guide](https://github.com/facebook/pyrefly/blob/main/CONTRIBUTING.md)
for information on how to contribute to Pyrefly.

Join our [Discord](https://discord.com/invite/Cf7mFQtW7W) to chat about Pyrefly
and types. This is also where we hold biweekly office hours.

## Choices

There are a number of choices when writing a Python type checker. We are taking
inspiration from [Pyre1](https://pyre-check.org/),
[Pyright](https://github.com/microsoft/pyright) and
[MyPy](https://mypy.readthedocs.io/en/stable/). Some notable choices:

- We infer types in most locations, apart from parameters to functions. We do
  infer types of variables and return types. As an example,
  `def foo(x): return True` would result in something equivalent to had you
  written `def foo(x: Any) -&gt; bool: ...`.
- We attempt to infer the type of `[]` to however it is used first, then fix it
  after. For example `xs = []; xs.append(1); xs.append(&quot;&quot;)` will infer that
  `xs: List[int]` and then error on the final statement.
- We use flow types which refine static types, e.g. `x: int = 4` will both know
  that `x` has type `int`, but also that the immediately next usage of `x` will
  be aware the type is `Literal[4]`.
- We aim for large-scale incrementality (at the module level) and optimized
  checking with parallelism, aiming to use the advantages of Rust to keep the
  code a bit simpler.
- We expect large strongly connected components of modules, and do not attempt
  to take advantage of a DAG-shape in the source code.

## Code layout

Pyrefly is split into a number of crates (mostly under `crates/`):

- `pyrefly_util` are general purpose utilities, which have nothing to do with
  Python or type checking. Examples include IO wrappers, locking, command line
  helpers etc.
- `pyrefly_derive` are proc-macros for deriving traits such as `TypeEq` and
  `Visit`.
- `pyrefly_python` are Python utilities with no type-checking aspects, such as
  modelling modules or `sys.info`.
- `pyrefly_bundled` are the third-party
  [typeshed stubs](https://github.com/python/typeshed).
- `pyrefly_config` defines the Pyrefly configuration, along with support for
  reading Mypy/Pyright configuration.
- `pyrefly_types` defines the Pyrefly type along with operations on it.
- `pyrefly_wasm` defines the sandbox code that compiles to WASM.
- `pyrefly` itself is the type checker and everything else.

## Design

There are many nuances of design that change on a regular basis. But the basic
substrate on which the checker is built involves three steps:

1. Figure out what each module exports. That requires solving all `import *`
   statements transitively.
2. For each module in isolation, convert it to bindings, dealing with all
   statements and scope information (both static and flow).
3. Solve those bindings, which may require the solutions of bindings in other
   modules.

If we encounter unknowable information (e.g. recursion) we use `Type::Var` to
insert placeholders which are filled in later.

For each module, we solve the steps sequentially and completely. In particular,
we do not try and solve a specific identifier first (like
[Roslyn](https://github.com/dotnet/roslyn) or
[TypeScript](https://www.typescriptlang.org/)), and do not use fine-grained
incrementality (like [Rust Analyzer](https://github.com/rust-lang/rust-analyzer)
using [Salsa](https://github.com/salsa-rs/salsa)). Instead, we aim for raw
performance and a simpler module-centric design - there&#039;s no need to solve a
single binding in isolation if solving all bindings in a module is fast enough.

### Example of bindings

Given the program:

```python
1: x: int = 4
2: print(x)
```

We might produce the bindings:

- `define int@0` = `from builtins import int`
- `define x@1` = `4: int@0`
- `use x@2` = `x@1`
- `anon @2` = `print(x@2)`
- `export x` = `x@2`

Of note:

- The keys are things like `define` (the definition of something), `use` (a
  usage of a thing) and `anon` (a statement we need to type check, but don&#039;t
  care about the result of).
- In many cases the value of a key refers to other keys.
- Some keys are imported from other modules, via `export` keys and `import`
  values.
- In order to disambiguate identifiers we use the textual position at which they
  occur (in the example we&#039;ve used `@line`, but in reality it&#039;s the byte offset
  in the file).

### Example of `Var`

Given the program:

```python
1: x = 1
2: while test():
3:     x = x
4: print(x)
```

We end up with the bindings:

- `x@1` = `1`
- `x@3` = `phi(x@1, x@3)`
- `x@4` = `phi(x@1, x@3)`

The expression `phi` is the join point of the two values, e.g. `phi(int, str)`
would be `int | str`. We skip the distinction between `define` and `use`, since
it is not necessary for this example.

When solving `x@3` we encounter recursion. Operationally:

- We start solving `x@3`.
- That requires us to solve `x@1`.
- We solve `x@1` to be `Literal[1]`
- We start solving `x@3`. But we are currently solving `x@3`, so we invent a
  fresh `Var` (let&#039;s call it `?1`) and return that.
- We conclude that `x@3` must be `Literal[1] | ?1`.
- Since `?1` was introduced by `x@3` we record that `?1 = Literal[1] | ?1`. We
  can take the upper reachable bound of that and conclude that
  `?1 = Literal[1]`.
- We simplify `x@3` to just `Literal[1]`.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[jj-vcs/jj]]></title>
            <link>https://github.com/jj-vcs/jj</link>
            <guid>https://github.com/jj-vcs/jj</guid>
            <pubDate>Thu, 16 Oct 2025 00:05:37 GMT</pubDate>
            <description><![CDATA[A Git-compatible VCS that is both simple and powerful]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jj-vcs/jj">jj-vcs/jj</a></h1>
            <p>A Git-compatible VCS that is both simple and powerful</p>
            <p>Language: Rust</p>
            <p>Stars: 21,060</p>
            <p>Forks: 737</p>
            <p>Stars today: 38 stars today</p>
            <h2>README</h2><pre>&lt;div class=&quot;title-block&quot; style=&quot;text-align: center;&quot; align=&quot;center&quot;&gt;

# Jujutsu‚Äîa version control system

&lt;p&gt;&lt;img title=&quot;jj logo&quot; src=&quot;docs/images/jj-logo.svg&quot; width=&quot;320&quot; height=&quot;320&quot;&gt;&lt;/p&gt;

[![Release](https://img.shields.io/github/v/release/martinvonz/jj)](https://github.com/jj-vcs/jj/releases)
[![Release date](https://img.shields.io/github/release-date/martinvonz/jj)](https://github.com/jj-vcs/jj/releases)
&lt;br/&gt;
[![License](https://img.shields.io/github/license/martinvonz/jj)](https://github.com/jj-vcs/jj/blob/main/LICENSE)
[![Discord](https://img.shields.io/discord/968932220549103686.svg?label=&amp;logo=discord&amp;logoColor=ffffff&amp;color=7389D8&amp;labelColor=6A7EC2)](https://discord.gg/dkmfj3aGQN)
[![IRC](https://img.shields.io/badge/irc-%23jujutsu-blue.svg)](https://web.libera.chat/?channel=#jujutsu)

**[Homepage] &amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;**
**[Installation] &amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;**
**[Getting Started] &amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;**
**[Development Roadmap] &amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;**
**[Contributing](#contributing)**

[Homepage]: https://jj-vcs.github.io/jj
[Installation]: https://jj-vcs.github.io/jj/latest/install-and-setup
[Getting Started]: https://jj-vcs.github.io/jj/latest/tutorial
[Development Roadmap]: https://jj-vcs.github.io/jj/latest/roadmap

&lt;/div&gt;

## Introduction

Jujutsu is a powerful [version control system](https://en.wikipedia.org/wiki/Version_control)
for software projects. You use it to get a copy of your code, track changes
to the code, and finally publish those changes for others to see and use.
It is designed from the ground up to be easy to use‚Äîwhether you&#039;re new or
experienced, working on brand new projects alone, or large scale software
projects with large histories and teams.

Jujutsu is unlike most other systems, because internally it abstracts the user
interface and version control algorithms from the *storage systems* used to
serve your content. This allows it to serve as a VCS with many possible physical
backends, that may have their own data or networking models‚Äîlike [Mercurial] or
[Breezy], or hybrid systems like Google&#039;s cloud-based design, [Piper/CitC].

[Mercurial]: https://www.mercurial-scm.org/
[Breezy]: https://www.breezy-vcs.org/
[Piper/CitC]: https://youtu.be/W71BTkUbdqE?t=645

Today, we use Git repositories as a storage layer to serve and track content,
making it **compatible with many of your favorite Git-based tools, right now!**
All core developers use Jujutsu to develop Jujutsu, right here on GitHub. But it
should hopefully work with your favorite Git forges, too.

We combine many distinct design choices and concepts from other version control
systems into a single tool. Some of those sources of inspiration include:

- **Git**: We make an effort to [be fast][perf]‚Äîwith a snappy UX, efficient
  algorithms, correct data structures, and good-old-fashioned attention to
  detail. The default storage backend uses Git repositories for &quot;physical
  storage&quot;, for wide interoperability and ease of onboarding.

- **Mercurial &amp; Sapling**: There are many Mercurial-inspired features, such as
  the [revset] language to select commits. There is [no explicit index][no-index]
  or staging area. Branches are &quot;anonymous&quot; like Mercurial, so you don&#039;t need
  to make up a name for each small change. Primitives for rewriting history are
  powerful and simple. Formatting output is done with a robust template language
  that can be configured by the user.

- **Darcs**: Jujutsu keeps track of conflicts as [first-class
  objects][conflicts] in its model; they are first-class in the same way commits
  are, while alternatives like Git simply think of conflicts as textual diffs.
  While not as rigorous as systems like Darcs (which is based on a formalized
  theory of patches, as opposed to snapshots), the effect is that many forms of
  conflict resolution can be performed and propagated automatically.

[perf]: https://github.com/jj-vcs/jj/discussions/49
[revset]: https://jj-vcs.github.io/jj/latest/revsets/
[no-index]: https://jj-vcs.github.io/jj/latest/git-comparison/#the-index
[conflicts]: https://jj-vcs.github.io/jj/latest/conflicts/

And it adds several innovative, useful features of its own:

- **Working-copy-as-a-commit**: Changes to files are [recorded automatically][wcc]
  as normal commits, and amended on every subsequent change. This &quot;snapshot&quot;
  design simplifies the user-facing data model (commits are the only visible
  object), simplifies internal algorithms, and completely subsumes features like
  Git&#039;s stashes or the index/staging-area.

- **Operation log &amp; undo**: Jujutsu records every operation that is performed on the
  repository, from commits, to pulls, to pushes. This makes debugging problems like
  &quot;what just happened?&quot; or &quot;how did I end up here?&quot; easier, *especially* when
  you&#039;re helping your coworker answer those questions about their repository!
  And because everything is recorded, you can undo that mistake you just made
  with ease. Version control has finally entered [the 1960s][undo-history]!

- **Automatic rebase and conflict resolution**: When you modify a commit, every
  descendent is automatically rebased on top of the freshly-modified one. This
  makes &quot;patch-based&quot; workflows a breeze. If you resolve a conflict in a commit,
  the _resolution_ of that conflict is also propagated through descendants as
  well. In effect, this is a completely transparent version of `git rebase
  --update-refs` combined with `git rerere`, supported by design.

&gt; [!WARNING]
&gt; The following features are available for use, but experimental; they may have
&gt; bugs, backwards incompatible storage changes, and user-interface changes!

- **Safe, concurrent replication**: Have you ever wanted to store your version
  controlled repositories inside a Dropbox folder? Or continuously backup
  repositories to S3? No? Well, now you can!

  The fundamental problem with using filesystems like Dropbox and backup tools
  like `rsync` on your typical Git/Mercurial repositories is that they rely
  on *local filesystem operations* being atomic, serialized, and non-concurrent
  with respect to other reads and writes‚Äîwhich is _not_ true when operating on
  distributed file systems, or when operations like concurrent file copies (for
  backup) happen while lock files are being held.

  Jujutsu is instead designed to be [safe under concurrent scenarios][conc-safety];
  simply using rsync or Dropbox and then using that resulting repository
  should never result in a repository in a *corrupt state*. The worst that
  _should_ happen is that it will expose conflicts between the local and remote
  state, leaving you to resolve them.

[wcc]: https://jj-vcs.github.io/jj/latest/working-copy/
[undo-history]: https://en.wikipedia.org/wiki/Undo#History
[conc-safety]: https://jj-vcs.github.io/jj/latest/technical/concurrency/

The command-line tool is called `jj` for now because it&#039;s easy to type and easy
to replace (rare in English). The project is called &quot;Jujutsu&quot; because it matches
&quot;jj&quot;.

Jujutsu is relatively young, with lots of work to still be done. If you have any
questions, or want to talk about future plans, please join us on Discord
[![Discord](https://img.shields.io/discord/968932220549103686.svg?label=&amp;logo=discord&amp;logoColor=ffffff&amp;color=7389D8&amp;labelColor=6A7EC2)](https://discord.gg/dkmfj3aGQN),
start a [GitHub Discussion](https://github.com/jj-vcs/jj/discussions), or
send an IRC message to [`#jujutsu` on Libera
Chat](https://web.libera.chat/?channel=#jujutsu). The developers monitor all of
these channels[^bridge].

[^bridge]: To be more precise, the `#jujutsu` Libera IRC channel is bridged to
one of the channels on jj&#039;s Discord. Some of the developers stay on Discord and
use the bridge to follow IRC.

### News and Updates üì£

- **December 2024**: The `jj` Repository has moved to the `jj-vcs` GitHub
  organization.
- **November 2024**: Version 0.24 is released which adds `jj file annotate`,
  which is equivalent to `git blame` or `hg annotate`.
- **September 2024**: Martin gave a [presentation about Jujutsu][merge-vid-2024] at
  Git Merge 2024.
- **Feb 2024**: Version 0.14 is released, which deprecates [&quot;jj checkout&quot; and &quot;jj merge&quot;](CHANGELOG.md#0140---2024-02-07),
  as well as `jj init --git`, which is now just called `jj git init`.
- **Oct 2023**: Version 0.10.0 is released! Now includes a bundled merge and
  diff editor for all platforms, &quot;immutable revsets&quot; to avoid accidentally
  `edit`-ing the wrong revisions, and lots of polish.
- **Jan 2023**: Martin gave a presentation about Google&#039;s plans for Jujutsu at
  Git Merge 2022!
  See the [slides][merge-slides] or the [recording][merge-talk].

### Related Media

- **Mar 2024**: Chris Krycho started [a YouTube series about Jujutsu][krycho-yt].
- **Feb 2024**: Chris Krycho published an article about Jujutsu called [jj init][krycho]
  and Steve Klabnik followed up with the [Jujutsu Tutorial][klabnik].
- **Jan 2024**: Jujutsu was featured in an LWN.net article called
  [Jujutsu: a new, Git-compatible version control system][lwn].
- **Jan 2023**: Martin&#039;s Talk about Jujutsu at Git Merge 2022, [video][merge-talk]
  and the associated [slides][merge-slides].

The wiki also contains a more extensive list of [media references][wiki-media].

[krycho-yt]: https://www.youtube.com/playlist?list=PLelyiwKWHHAq01Pvmpf6x7J0y-yQpmtxp
[krycho]: https://v5.chriskrycho.com/essays/jj-init/
[klabnik]: https://steveklabnik.github.io/jujutsu-tutorial/
[lwn]: https://lwn.net/Articles/958468/
[merge-talk]: https://www.youtube.com/watch?v=bx_LGilOuE4
[merge-slides]: https://docs.google.com/presentation/d/1F8j9_UOOSGUN9MvHxPZX_L4bQ9NMcYOp1isn17kTC_M/view
[merge-vid-2024]: https://www.youtube.com/watch?v=LV0JzI8IcCY
[wiki-media]: https://github.com/jj-vcs/jj/wiki/Media

## Getting started

&gt; [!IMPORTANT]
&gt; Jujutsu is an **experimental version control system**. While Git compatibility
&gt; is stable, and most developers use it daily for all their needs, there may
&gt; still be work-in-progress features, suboptimal UX, and workflow gaps that make
&gt; it unusable for your particular use.

Follow the [installation
instructions](https://jj-vcs.github.io/jj/latest/install-and-setup) to
obtain and configure `jj`.

The best way to get started is probably to go through [the
tutorial](https://jj-vcs.github.io/jj/latest/tutorial). Also see the [Git
comparison](https://jj-vcs.github.io/jj/latest/git-comparison), which
includes a table of `jj` vs. `git` commands.

As you become more familiar with Jujutsu, the following resources may be helpful:

- The [FAQ](https://jj-vcs.github.io/jj/latest/FAQ).
- The [Glossary](https://jj-vcs.github.io/jj/latest/glossary).
- The `jj help` command (e.g. `jj help rebase`).
- The `jj help -k &lt;keyword&gt;` command (e.g. `jj help -k config`). Use `jj help --help`
  to see what keywords are available.

If you are using a **prerelease** version of `jj`, you would want to consult
[the docs for the prerelease (main branch)
version](https://jj-vcs.github.io/jj/prerelease/). You can also get there
from the docs for the latest release by using the website&#039;s version switcher. The version switcher is visible in
the header of the website when you scroll to the top of any page.

## Features

### Compatible with Git

Jujutsu is designed so that the underlying data and storage model is abstract.
Today, only the Git backend is production-ready. The Git backend uses the
[gitoxide](https://github.com/Byron/gitoxide) Rust library.

[backends]: https://jj-vcs.github.io/jj/latest/glossary#backend

The Git backend is fully featured and maintained, and allows you to use Jujutsu
with any Git remote. The commits you create will look like regular Git commits.
You can fetch branches from a regular Git remote and push branches to the
remote. You can always switch back to Git.

Here is how you can explore a GitHub repository with `jj`.

&lt;img src=&quot;demos/git_compat.png&quot; /&gt;

You can even have a [colocated local
repository](https://jj-vcs.github.io/jj/latest/git-compatibility#colocated-jujutsugit-repos)
where you can use both `jj` and `git` commands interchangeably.

### The working copy is automatically committed

Jujutsu uses a real commit to represent the working copy. Checking out a commit
results a new working-copy commit on top of the target commit. Almost all
commands automatically amend the working-copy commit.

The working-copy being a commit means that commands never fail because the
working copy is dirty (no &quot;error: Your local changes to the following
files...&quot;), and there is no need for `git stash`. Also, because the working copy
is a commit, commands work the same way on the working-copy commit as on any
other commit, so you can set the commit message before you&#039;re done with the
changes.

&lt;img src=&quot;demos/working_copy.png&quot; /&gt;

### The repo is the source of truth

With Jujutsu, the working copy plays a smaller role than with Git. Commands
snapshot the working copy before they start, then they update the repo, and then
the working copy is updated (if the working-copy commit was modified). Almost
all commands (even checkout!) operate on the commits in the repo, leaving the
common functionality of snapshotting and updating of the working copy to
centralized code. For example, `jj restore` (similar to `git restore`) can
restore from any commit and into any commit, and `jj describe` can set the
commit message of any commit (defaults to the working-copy commit).

### Entire repo is under version control

All operations you perform in the repo are recorded, along with a snapshot of
the repo state after the operation. This means that you can easily restore to
an earlier repo state, simply undo your operations one-by-one or even _revert_ a
particular operation which does not have to be the most recent one.

&lt;img src=&quot;demos/operation_log.png&quot; /&gt;

### Conflicts can be recorded in commits

If an operation results in
[conflicts](https://jj-vcs.github.io/jj/latest/glossary#conflict),
information about those conflicts will be recorded in the commit(s). The
operation will succeed. You can then resolve the conflicts later. One
consequence of this design is that there&#039;s no need to continue interrupted
operations. Instead, you get a single workflow for resolving conflicts,
regardless of which command caused them. This design also lets Jujutsu rebase
merge commits correctly (unlike both Git and Mercurial).

Basic conflict resolution:

&lt;img src=&quot;demos/resolve_conflicts.png&quot; /&gt;

Juggling conflicts:

&lt;img src=&quot;demos/juggle_conflicts.png&quot; /&gt;

### Automatic rebase

Whenever you modify a commit, any descendants of the old commit will be rebased
onto the new commit. Thanks to the conflict design described above, that can be
done even if there are conflicts. Bookmarks pointing to rebased commits will be
updated. So will the working copy if it points to a rebased commit.

### Comprehensive support for rewriting history

Besides the usual rebase command, there&#039;s `jj describe` for editing the
description (commit message) of an arbitrary commit. There&#039;s also `jj diffedit`,
which lets you edit the changes in a commit without checking it out. To split
a commit into two, use `jj split`. You can even move part of the changes in a
commit to any other commit using `jj squash -i --from X --into Y`.

## Status

The tool is fairly feature-complete, but some important features like support
for Git submodules are not yet completed. There
are also several performance bugs. It&#039;s likely that workflows and setups
different from what the core developers use are not well supported, e.g. there
is no native support for email-based workflows.

Today, all core developers use `jj` to work on `jj`. I (Martin von Zweigbergk)
have almost exclusively used `jj` to develop the project itself since early
January 2021. I haven&#039;t had to re-clone from source (I don&#039;t think I&#039;ve even had
to restore from backup).

There *will* be changes to workflows and backward-incompatible changes to the
on-disk formats before version 1.0.0. For any format changes, we&#039;ll try to
implement transparent upgrades (as we&#039;ve done with recent changes), or provide
upgrade commands or scripts if requested.

## Related work

There are several tools trying to solve similar problems as Jujutsu. See
[related work](https://jj-vcs.github.io/jj/latest/related-work) for details.

## Contributing

We welcome outside contributions, and there&#039;s plenty of things to do, so
don&#039;t be shy. Please ask if you want a pointer on something you can help with,
and hopefully we can all figure something out.

We do have [a few policies and
suggestions](https://jj-vcs.github.io/jj/prerelease/contributing/)
for contributors. The broad TL;DR:

- Bug reports are very welcome!
- Every commit that lands in the `main` branch is code reviewed.
- Please behave yourself, and obey the Community Guidelines.
- There **is** a mandatory CLA you must agree to. Importantly, it **does not**
  transfer copyright ownership to Google or anyone else; it simply gives us the
  right to safely redistribute and use your changes.

### Mandatory Google Disclaimer

I (Martin von Zweigbergk, &lt;martinvonz@google.com&gt;) started Jujutsu as a hobby
project in late 2019, and it has evolved into my full-time project at Google,
with several other Googlers (now) assisting development in various capacities.
That said, **this is not a Google product**.

## License

Jujutsu is available as Open Source Software, under the Apache 2.0 license. See
[`LICENSE`](./LICENSE) for details about copyright and redistribution.

The `jj` logo was contributed by J. Jennings and is licensed under a Creative
Commons License, see [`docs/images/LICENSE`](docs/images/LICENSE).
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[tauri-apps/tauri]]></title>
            <link>https://github.com/tauri-apps/tauri</link>
            <guid>https://github.com/tauri-apps/tauri</guid>
            <pubDate>Thu, 16 Oct 2025 00:05:36 GMT</pubDate>
            <description><![CDATA[Build smaller, faster, and more secure desktop and mobile applications with a web frontend.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/tauri-apps/tauri">tauri-apps/tauri</a></h1>
            <p>Build smaller, faster, and more secure desktop and mobile applications with a web frontend.</p>
            <p>Language: Rust</p>
            <p>Stars: 97,462</p>
            <p>Forks: 3,104</p>
            <p>Stars today: 57 stars today</p>
            <h2>README</h2><pre>&lt;img src=&quot;.github/splash.png&quot; alt=&quot;Tauri&quot; /&gt;

[![status](https://img.shields.io/badge/status-stable-blue.svg)](https://github.com/tauri-apps/tauri/tree/dev)
[![License](https://img.shields.io/badge/License-MIT%20or%20Apache%202-green.svg)](https://opencollective.com/tauri)
[![test core](https://img.shields.io/github/actions/workflow/status/tauri-apps/tauri/test-core.yml?label=test%20core&amp;logo=github)](https://github.com/tauri-apps/tauri/actions/workflows/test-core.yml)
[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Ftauri-apps%2Ftauri.svg?type=shield)](https://app.fossa.com/projects/git%2Bgithub.com%2Ftauri-apps%2Ftauri?ref=badge_shield)
[![Chat Server](https://img.shields.io/badge/chat-discord-7289da.svg)](https://discord.com/invite/tauri)
[![website](https://img.shields.io/badge/website-tauri.app-purple.svg)](https://tauri.app)
[![https://good-labs.github.io/greater-good-affirmation/assets/images/badge.svg](https://good-labs.github.io/greater-good-affirmation/assets/images/badge.svg)](https://good-labs.github.io/greater-good-affirmation)
[![support](https://img.shields.io/badge/sponsor-Open%20Collective-blue.svg)](https://opencollective.com/tauri)

## Introduction

Tauri is a framework for building tiny, blazingly fast binaries for all major desktop platforms. Developers can integrate any front-end framework that compiles to HTML, JS and CSS for building their user interface. The backend of the application is a rust-sourced binary with an API that the front-end can interact with.

The user interface in Tauri apps currently leverages [`tao`](https://docs.rs/tao) as a window handling library on macOS, Windows, Linux, Android and iOS. To render your application, Tauri uses [WRY](https://github.com/tauri-apps/wry), a library which provides a unified interface to the system webview, leveraging WKWebView on macOS &amp; iOS, WebView2 on Windows, WebKitGTK on Linux and Android System WebView on Android.

To learn more about the details of how all of these pieces fit together, please consult this [ARCHITECTURE.md](https://github.com/tauri-apps/tauri/blob/dev/ARCHITECTURE.md) document.

## Getting Started

If you are interested in making a tauri app, please visit the [documentation website](https://tauri.app).

The quickest way to get started is to install the [prerequisites](https://v2.tauri.app/start/prerequisites/) for your system and create a new project with [`create-tauri-app`](https://github.com/tauri-apps/create-tauri-app/#usage). For example with `npm`:

```sh
npm create tauri-app@latest
```

## Features

The list of Tauri&#039;s features includes, but is not limited to:

- Built-in app bundler to create app bundles in formats like `.app`, `.dmg`, `.deb`, `.rpm`, `.AppImage` and Windows installers like `.exe` (via NSIS) and `.msi` (via WiX).
- Built-in self updater (desktop only)
- System tray icons
- Native notifications
- Native WebView Protocol (tauri doesn&#039;t create a localhost http(s) server to serve the WebView contents)
- GitHub action for streamlined CI
- VS Code extension

### Platforms

Tauri currently supports development and distribution on the following platforms:

| Platform   | Versions                                                                                                        |
| :--------- | :-------------------------------------------------------------------------------------------------------------- |
| Windows    | 7 and above                                                                                                     |
| macOS      | 10.15 and above                                                                                                 |
| Linux      | webkit2gtk 4.0 for Tauri v1 (for example Ubuntu 18.04). webkit2gtk 4.1 for Tauri v2 (for example Ubuntu 22.04). |
| iOS/iPadOS | 9 and above                                                                                                     |
| Android    | 7 and above (currently 8 and above)                                                                             |

## Contributing

Before you start working on something, it&#039;s best to check if there is an existing issue first. It&#039;s also a good idea to stop by the Discord server and confirm with the team if it makes sense or if someone else is already working on it.

Please make sure to read the [Contributing Guide](./.github/CONTRIBUTING.md) before making a pull request.

Thank you to everyone contributing to Tauri!

### Documentation

Documentation in a polyglot system is a tricky proposition. To this end, we prefer to use inline documentation in the Rust &amp; JS source code as much as possible. Check out the hosting repository for the documentation site for further information: &lt;https://github.com/tauri-apps/tauri-docs&gt;

## Partners

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;middle&quot;&gt;
        &lt;a href=&quot;https://crabnebula.dev&quot; target=&quot;_blank&quot;&gt;
          &lt;img src=&quot;.github/sponsors/crabnebula.svg&quot; alt=&quot;CrabNebula&quot; width=&quot;283&quot;&gt;
        &lt;/a&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

For the complete list of sponsors please visit our [website](https://tauri.app#sponsors) and [Open Collective](https://opencollective.com/tauri).

## Organization

Tauri aims to be a sustainable collective based on principles that guide sustainable free and open software communities. To this end it has become a Programme within the [Commons Conservancy](https://commonsconservancy.org/), and you can contribute financially via [Open Collective](https://opencollective.com/tauri).

## Licenses

Code: (c) 2015 - Present - The Tauri Programme within The Commons Conservancy.

MIT or MIT/Apache 2.0 where applicable.

Logo: CC-BY-NC-ND

- Original Tauri Logo Designs by [Alve Larsson](https://alve.io/), [Daniel Thompson-Yvetot](https://github.com/nothingismagick) and [Guillaume Chau](https://github.com/akryum)

[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Ftauri-apps%2Ftauri.svg?type=large)](https://app.fossa.com/projects/git%2Bgithub.com%2Ftauri-apps%2Ftauri?ref=badge_large)
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[apache/opendal]]></title>
            <link>https://github.com/apache/opendal</link>
            <guid>https://github.com/apache/opendal</guid>
            <pubDate>Thu, 16 Oct 2025 00:05:35 GMT</pubDate>
            <description><![CDATA[Apache OpenDAL: One Layer, All Storage.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/apache/opendal">apache/opendal</a></h1>
            <p>Apache OpenDAL: One Layer, All Storage.</p>
            <p>Language: Rust</p>
            <p>Stars: 4,486</p>
            <p>Forks: 646</p>
            <p>Stars today: 5 stars today</p>
            <h2>README</h2><pre># Apache OpenDAL‚Ñ¢: *One Layer, All Storage.*

[![](https://img.shields.io/github/discussions/apache/opendal)](https://github.com/apache/opendal/discussions)
[![](https://img.shields.io/discord/1081052318650339399?logo=discord&amp;label=discord)](https://opendal.apache.org/discord)
[![](https://deepwiki.com/badge.svg)](https://deepwiki.com/apache/opendal)

OpenDAL (`/Àào ä.p…ôn.d√¶l/`, pronounced &quot;OH-puhn-dal&quot;) is an Open Data Access Layer that enables seamless interaction with diverse storage services.

OpenDAL&#039;s development is guided by its vision of **One Layer, All Storage** and its core principles: **Open Community**, **Solid Foundation**, **Fast Access**, **Object Storage First**, and **Extensible Architecture**. Read the explained vision at [OpenDAL Vision](https://opendal.apache.org/vision).

&lt;img src=&quot;https://opendal.apache.org/img/architectural.png&quot; alt=&quot;OpenDAL Architectural&quot; width=&quot;61.8%&quot; /&gt;

## For *ANY* languages

| Name              | Release                                          | Docs                                                                              | Used By |
| ----------------- | ------------------------------------------------ | --------------------------------------------------------------------------------- | ----------------------------------- |
| [Rust Core]       | [![Rust Core Image]][Rust Core Link]             | [![Docs Release]][Rust Core Release Docs] [![Docs Dev]][Rust Core Dev Docs]       | [![Rust Core Users Image]][Rust Core Users]            |
| [C Binding]       | -                                                | [![Docs Dev]][C Binding Dev Docs]                                                 | [![C Binding Users Image]][C Binding Users]            |
| [Cpp Binding]     | -                                                | [![Docs Dev]][Cpp Binding Dev Docs]                                               | - |
| [D Binding]       | -                                                | -                                                                                 | - |
| [Dart Binding]    | -                                                | -                                                                                 | - |
| [Dotnet Binding]  | -                                                | -                                                                                 | - |
| [Go Binding]      | [![Go Binding Image]][Go Binding Link]           | [![Docs Release]][Go Release Docs]                                                | - |
| [Haskell Binding] | -                                                | -                                                                                 | - |
| [Java Binding]    | [![Java Binding Image]][Java Binding Link]       | [![Docs Release]][Java Binding Release Docs] [![Docs Dev]][Java Binding Dev Docs] | [![Java Binding Users Image]][Java Binding Users]   |
| [Lua Binding]     | -                                                | -                                                                                 | - |
| [Node.js Binding] | [![Node.js Binding Image]][Node.js Binding Link] | [![Docs Dev]][Node.js Binding Dev Docs]                                           | - |
| [OCaml Binding]   | -                                                | -                                                                                 | - |
| [PHP Binding]     | -                                                | -                                                                                 | - |
| [Python Binding]  | [![Python Binding Image]][Python Binding Link]   | [![Docs Dev]][Python Binding Dev Docs]                                            | [![Python Binding Users Image]][Python Binding Users] |
| [Ruby Binding]    | -                                                | -                                                                                 | - |
| [Swift Binding]   | -                                                | -                                                                                 | - |
| [Zig Binding]     | -                                                | -                                                                                 | - |

[Docs Release]: https://img.shields.io/badge/docs-release-blue
[Docs Dev]: https://img.shields.io/badge/docs-dev-blue
[Rust Core]: core/README.md
[Rust Core Image]: https://img.shields.io/crates/v/opendal.svg
[Rust Core Link]: https://crates.io/crates/opendal
[Rust Core Release Docs]: https://docs.rs/opendal
[Rust Core Dev Docs]: https://opendal.apache.org/docs/rust/opendal/
[Rust Core Users Image]: https://github.com/user-attachments/assets/2726c336-8509-491d-92d8-1be2040d5136
[Rust Core Users]: core/users.md

[C Binding]: bindings/c/README.md
[C Binding Dev Docs]: https://opendal.apache.org/docs/c/
[C Binding Users Image]: https://github.com/user-attachments/assets/b1cf4d79-8478-4eac-ae04-0bbe0d6a993d
[C Binding Users]: bindings/c/users.md
[Cpp Binding]: bindings/cpp/README.md
[Cpp Binding Dev Docs]: https://opendal.apache.org/docs/cpp/
[D Binding]: bindings/d/README.md
[Dart Binding]: bindings/dart/README.md
[Dotnet Binding]: bindings/dotnet/README.md
[Go Binding]: bindings/go/README.md
[Go Binding Image]: https://badge.fury.io/go/github.com%2Fapache%2Fopendal%2Fbindings%2Fgo.svg
[Go Binding Link]: https://pkg.go.dev/github.com/apache/opendal/bindings/go
[Go Release Docs]: https://pkg.go.dev/github.com/apache/opendal/bindings/go
[Haskell Binding]: bindings/haskell/README.md
[Java Binding]: bindings/java/README.md
[Java Binding Image]: https://img.shields.io/maven-central/v/org.apache.opendal/opendal-java
[Java Binding Link]: https://central.sonatype.com/artifact/org.apache.opendal/opendal-java
[Java Binding Release Docs]: https://javadoc.io/doc/org.apache.opendal/opendal-java
[Java Binding Dev Docs]: https://opendal.apache.org/docs/java/
[Java Binding Users Image]: https://github.com/user-attachments/assets/f20a59a9-8f23-4919-a165-980ed4e6e0d0
[Java Binding Users]: bindings/java/users.md
[Lua Binding]: bindings/lua/README.md
[Node.js Binding]: bindings/nodejs/README.md
[Node.js Binding Image]: https://img.shields.io/npm/v/opendal
[Node.js Binding Link]: https://www.npmjs.com/package/opendal
[Node.js Binding Dev Docs]: https://opendal.apache.org/docs/nodejs/
[OCaml Binding]: bindings/ocaml/README.md
[PHP Binding]: bindings/php/README.md
[Python Binding]: bindings/python/README.md
[Python Binding Image]: https://img.shields.io/pypi/v/opendal
[Python Binding Link]: https://pypi.org/project/opendal/
[Python Binding Dev Docs]: https://opendal.apache.org/docs/python/
[Python Binding Users Image]: https://github.com/user-attachments/assets/6bba7e5b-cada-4cf2-81e3-09d4e4535dcb 
[Python Binding Users]: bindings/python/users.md
[Ruby Binding]: bindings/ruby/README.md
[Swift Binding]: bindings/swift/README.md
[Zig Binding]: bindings/zig/README.md

## For *ANY* methods

| Name  | Description                                                        | Release                   |
| ----- | ------------------------------------------------------------------ | ------------------------- |
| [oay] | Access data via API Gateway                                        | [![oay image]][oay crate] |
| [oli] | Access data via Command Line (alternative to s3cmd, s3cli, azcopy) | [![oli image]][oli crate] |
| [ofs] | Access data via POSIX file system API (alternative to s3fs)        | [![ofs image]][ofs crate] |

[oay]: bin/oay/README.md
[oay image]: https://img.shields.io/crates/v/oay.svg
[oay crate]: https://crates.io/crates/oay
[oli]: bin/oli/README.md
[oli image]: https://img.shields.io/crates/v/oli.svg
[oli crate]: https://crates.io/crates/oli
[ofs]: bin/ofs/README.md
[ofs image]: https://img.shields.io/crates/v/ofs.svg
[ofs crate]: https://crates.io/crates/ofs

## For *ANY* integrations

| Name                   | Description                                                                   | Release                                     | Docs                                                                              |
| ---------------------- | ----------------------------------------------------------------------------- | ------------------------------------------- | --------------------------------------------------------------------------------- |
| [dav-server-opendalfs] | a [dav-server-rs] implementation using opendal.                               | [![dav-server image]][dav-server crate]     | [![Docs Release]][dav-server release docs] [![Docs Dev]][dav-server dev docs]     |
| [object_store_opendal] | an [object_store] implementation using opendal.                               | [![object_store image]][object_store crate] | [![Docs Release]][object_store release docs] [![Docs Dev]][object_store dev docs] |
| [fuse3_opendal]        | Access data via integrations to [fuse3]                                       | [![fuse3 image]][fuse3 crate]               | [![Docs Release]][fuse3 release docs] [![Docs Dev]][fuse3 dev docs]               |
| [virtiofs_opendal]     | Access data via integrations to [vhost-user-backend]                          | [![virtiofs image]][virtiofs crate]         | [![Docs Release]][virtiofs release docs] [![Docs Dev]][virtiofs dev docs]         |
| [unftp-sbe-opendal]    | an [unftp] storage backend implementation using opendal.                      | [![unftp-sbe image]][unftp-sbe crate]       | [![Docs Release]][unftp-sbe release docs] [![Docs Dev]][unftp-sbe dev docs]       |
| [parquet_opendal]      | Provides [`parquet`](https://crates.io/crates/parquet) efficient IO utilities | [![parquet image]][parquet crate]           | [![Docs Release]][parquet release docs] [![Docs Dev]][parquet dev docs]           |

[dav-server-opendalfs]: integrations/dav-server/README.md
[dav-server-rs]: https://github.com/messense/dav-server-rs
[dav-server image]: https://img.shields.io/crates/v/dav-server-opendalfs.svg
[dav-server crate]: https://crates.io/crates/dav-server-opendalfs
[dav-server release docs]: https://docs.rs/dav-server-opendalfs/
[dav-server dev docs]: https://opendal.apache.org/docs/dav-server-opendalfs/dav_server_opendalfs/

[object_store_opendal]: integrations/object_store/README.md
[object_store]: https://docs.rs/object_store
[object_store image]: https://img.shields.io/crates/v/object_store_opendal.svg
[object_store crate]: https://crates.io/crates/object_store_opendal
[object_store release docs]: https://docs.rs/object_store_opendal/
[object_store dev docs]: https://opendal.apache.org/docs/object-store-opendal/object_store_opendal/

[fuse3_opendal]: integrations/fuse3/README.md
[fuse3]: https://docs.rs/fuse3
[fuse3 image]: https://img.shields.io/crates/v/fuse3_opendal.svg
[fuse3 crate]: https://crates.io/crates/fuse3_opendal
[fuse3 release docs]: https://docs.rs/fuse3_opendal/
[fuse3 dev docs]: https://opendal.apache.org/docs/fuse3-opendal/fuse3_opendal/

[virtiofs_opendal]: integrations/virtiofs/README.md
[vhost-user-backend]: https://docs.rs/vhost-user-backend
[virtiofs image]: https://img.shields.io/crates/v/virtiofs_opendal.svg
[virtiofs crate]: https://crates.io/crates/virtiofs_opendal
[virtiofs release docs]: https://docs.rs/virtiofs_opendal/
[virtiofs dev docs]: https://opendal.apache.org/docs/virtiofs-opendal/virtiofs_opendal/

[unftp-sbe-opendal]: integrations/unftp-sbe/README.md
[unftp]: https://crates.io/crates/unftp
[unftp-sbe image]: https://img.shields.io/crates/v/unftp-sbe-opendal.svg
[unftp-sbe crate]: https://crates.io/crates/unftp-sbe-opendal
[unftp-sbe release docs]: https://docs.rs/unftp-sbe-opendal/
[unftp-sbe dev docs]: https://opendal.apache.org/docs/unftp-sbe-opendal/unftp_sbe_opendal/

[parquet_opendal]: integrations/parquet/README.md
[parquet image]: https://img.shields.io/crates/v/parquet-opendal.svg
[parquet crate]: https://crates.io/crates/parquet-opendal
[parquet release docs]: https://docs.rs/parquet-opendal/
[parquet dev docs]: https://opendal.apache.org/docs/parquet-opendal/parquet_opendal/

## For *ANY* services

| Type                           | Services                                                                                                                                 |
| ------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------- |
| Standard Storage Protocols     | ftp http [sftp] [webdav]                                                                                                                 |
| Object Storage Services        | [azblob] [cos] [gcs] [obs] [oss] [s3] &lt;br&gt; [b2] [openstack_swift] [upyun] [vercel_blob]                                                  |
| File Storage Services          | fs [alluxio] [azdls] [azfile] [compfs] &lt;br&gt; [dbfs] [gridfs] [hdfs] [hdfs_native] [ipfs] [webhdfs]                                        |
| Consumer Cloud Storage Service | [aliyun_drive] [gdrive] [onedrive] [dropbox] [icloud] [koofr] &lt;br&gt; [pcloud] [seafile] [yandex_disk]                                      |
| Key-Value Storage Services     | [cacache] [cloudflare_kv] [dashmap] memory [etcd] &lt;br&gt; [foundationdb] [persy] [redis] [rocksdb] [sled] &lt;br&gt; [redb] [tikv] [atomicserver] |
| Database Storage Services      | [d1] [mongodb] [mysql] [postgresql] [sqlite] [surrealdb]                                                                                 |
| Cache Storage Services         | [ghac] [memcached] [mini_moka] [moka] [vercel_artifacts]                                                                                 |
| Git Based Storage Services     | [huggingface]                                                                                                                            |

[sftp]: https://datatracker.ietf.org/doc/html/draft-ietf-secsh-filexfer-02
[webdav]: https://datatracker.ietf.org/doc/html/rfc4918

[azblob]: https://azure.microsoft.com/en-us/services/storage/blobs/
[cos]: https://www.tencentcloud.com/products/cos
[gcs]: https://cloud.google.com/storage
[obs]: https://www.huaweicloud.com/intl/en-us/product/obs.html
[oss]: https://www.aliyun.com/product/oss
[s3]: https://aws.amazon.com/s3/
[b2]: https://www.backblaze.com/
[openstack_swift]: https://docs.openstack.org/swift/latest/
[upyun]: https://www.upyun.com/
[vercel_blob]: https://vercel.com/docs/storage/vercel-blob

[alluxio]: https://docs.alluxio.io/os/user/stable/en/api/REST-API.html
[azdls]: https://azure.microsoft.com/en-us/products/storage/data-lake-storage/
[azfile]: https://learn.microsoft.com/en-us/rest/api/storageservices/file-service-rest-api
[compfs]: https://github.com/compio-rs/compio/
[dbfs]: https://docs.databricks.com/en/dbfs/index.html
[gridfs]: https://www.mongodb.com/docs/manual/core/gridfs/
[hdfs]: https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html
[hdfs_native]: https://github.com/Kimahriman/hdfs-native
[ipfs]: https://ipfs.tech/
[webhdfs]: https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/WebHDFS.html

[aliyun_drive]: https://www.aliyundrive.com/
[gdrive]: https://www.google.com/drive/
[onedrive]: https://www.microsoft.com/en-us/microsoft-365/onedrive/online-cloud-storage
[dropbox]: https://www.dropbox.com/
[icloud]: https://www.icloud.com/iclouddrive
[koofr]: https://koofr.eu/
[pcloud]: https://www.pcloud.com/
[seafile]: https://www.seafile.com/
[yandex_disk]: https://360.yandex.com/disk/

[cacache]: https://crates.io/crates/cacache
[cloudflare_kv]: https://developers.cloudflare.com/kv/
[dashmap]: https://github.com/xacrimon/dashmap
[etcd]: https://etcd.io/
[foundationdb]: https://www.foundationdb.org/
[persy]: https://crates.io/crates/persy
[redis]: https://redis.io/
[rocksdb]: http://rocksdb.org/
[sled]: https://crates.io/crates/sled
[redb]: https://crates.io/crates/redb
[tikv]: https://tikv.org/
[atomicserver]: https://github.com/atomicdata-dev/atomic-server

[d1]: https://developers.cloudflare.com/d1/
[mongodb]: https://www.mongodb.com/
[mysql]: https://www.mysql.com/
[postgresql]: https://www.postgresql.org/
[sqlite]: https://www.sqlite.org/
[surrealdb]: https://surrealdb.com/

[ghac]: https://docs.github.com/en/actions/using-workflows/caching-dependencies-to-speed-up-workflows
[memcached]: https://memcached.org/
[mini_moka]: https://github.com/moka-rs/mini-moka
[moka]: https://github.com/moka-rs/moka
[vercel_artifacts]: https://vercel.com/docs/concepts/monorepos/remote-caching

[huggingface]: https://huggingface.co/

## Examples

The examples are available at [here](./examples/).

## Documentation

The documentation is available at &lt;https://opendal.apache.org&gt;.

## Contribute

OpenDAL is an active open-source project. We are always open to people who want to use it or contribute to it. Here are some ways to go.

- Start with [Contributing Guide](CONTRIBUTING.md).
- Submit [Issues](https://github.com/apache/opendal/issues/new) for bug report or feature requests.
- Start [Discussions](https://github.com/apache/opendal/discussions/new?category=q-a) for questions or ideas.
- Talk to community directly at [Discord](https://opendal.apache.org/discord).
- Report security vulnerabilities to [private mailing list](mailto:private@opendal.apache.org)

## Branding

The first and most prominent mentions must use the full form: **Apache OpenDAL‚Ñ¢** of the name for any individual usage (webpage, handout, slides, etc.) Depending on the context and writing style, you should use the full form of the name sufficiently often to ensure that readers clearly understand the association of both the OpenDAL project and the OpenDAL software product to the ASF as the parent organization.

For more details, see the [Apache Product Name Usage Guide](https://www.apache.org/foundation/marks/guide).

## License and Trademarks

Licensed under the Apache License, Version 2.0: &lt;http://www.apache.org/licenses/LICENSE-2.0&gt;

Apache OpenDAL, OpenDAL, and Apache are either registered trademarks or trademarks of the Apache Software Foundation.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[chroma-core/chroma]]></title>
            <link>https://github.com/chroma-core/chroma</link>
            <guid>https://github.com/chroma-core/chroma</guid>
            <pubDate>Thu, 16 Oct 2025 00:05:34 GMT</pubDate>
            <description><![CDATA[Open-source search and retrieval database for AI applications.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/chroma-core/chroma">chroma-core/chroma</a></h1>
            <p>Open-source search and retrieval database for AI applications.</p>
            <p>Language: Rust</p>
            <p>Stars: 23,897</p>
            <p>Forks: 1,870</p>
            <p>Stars today: 24 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trychroma.com&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/891664/227103090-6624bf7d-9524-4e05-9d2c-c28d5d451481.png&quot; alt=&quot;Chroma logo&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;b&gt;Chroma - the open-source embedding database&lt;/b&gt;. &lt;br /&gt;
    The fastest way to build Python or JavaScript LLM apps with memory!
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://discord.gg/MMeYNTmh3x&quot; target=&quot;_blank&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/discord/1073293645303795742?cacheSeconds=3600&quot; alt=&quot;Discord&quot;&gt;
  &lt;/a&gt; |
  &lt;a href=&quot;https://github.com/chroma-core/chroma/blob/master/LICENSE&quot; target=&quot;_blank&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/badge/License-Apache_2.0-blue.svg&quot; alt=&quot;License&quot;&gt;
  &lt;/a&gt; |
  &lt;a href=&quot;https://docs.trychroma.com/&quot; target=&quot;_blank&quot;&gt;
      Docs
  &lt;/a&gt; |
  &lt;a href=&quot;https://www.trychroma.com/&quot; target=&quot;_blank&quot;&gt;
      Homepage
  &lt;/a&gt;
&lt;/p&gt;

```bash
pip install chromadb # python client
# for javascript, npm install chromadb!
# for client-server mode, chroma run --path /chroma_db_path
```

## Chroma Cloud

Our hosted service, Chroma Cloud, powers serverless vector and full-text search. It&#039;s extremely fast, cost-effective, scalable and painless. Create a DB and try it out in under 30 seconds with $5 of free credits.

[Get started with Chroma Cloud](https://trychroma.com/signup)

## API

The core API is only 4 functions (run our [üí° Google Colab](https://colab.research.google.com/drive/1QEzFyqnoFxq7LUGyP1vzR4iLt9PpCDXv?usp=sharing)):

```python
import chromadb
# setup Chroma in-memory, for easy prototyping. Can add persistence easily!
client = chromadb.Client()

# Create collection. get_collection, get_or_create_collection, delete_collection also available!
collection = client.create_collection(&quot;all-my-documents&quot;)

# Add docs to the collection. Can also update and delete. Row-based API coming soon!
collection.add(
    documents=[&quot;This is document1&quot;, &quot;This is document2&quot;], # we handle tokenization, embedding, and indexing automatically. You can skip that and add your own embeddings as well
    metadatas=[{&quot;source&quot;: &quot;notion&quot;}, {&quot;source&quot;: &quot;google-docs&quot;}], # filter on these!
    ids=[&quot;doc1&quot;, &quot;doc2&quot;], # unique for each doc
)

# Query/search 2 most similar results. You can also .get by id
results = collection.query(
    query_texts=[&quot;This is a query document&quot;],
    n_results=2,
    # where={&quot;metadata_field&quot;: &quot;is_equal_to_this&quot;}, # optional filter
    # where_document={&quot;$contains&quot;:&quot;search_string&quot;}  # optional filter
)
```

Learn about all features on our [Docs](https://docs.trychroma.com)

## Features
- __Simple__: Fully-typed, fully-tested, fully-documented == happiness
- __Integrations__: [`ü¶úÔ∏èüîó LangChain`](https://blog.langchain.dev/langchain-chroma/) (python and js), [`ü¶ô LlamaIndex`](https://twitter.com/atroyn/status/1628557389762007040) and more soon
- __Dev, Test, Prod__: the same API that runs in your python notebook, scales to your cluster
- __Feature-rich__: Queries, filtering, regex and more
- __Free &amp; Open Source__: Apache 2.0 Licensed

## Use case: ChatGPT for ______

For example, the `&quot;Chat your data&quot;` use case:
1. Add documents to your database. You can pass in your own embeddings, embedding function, or let Chroma embed them for you.
2. Query relevant documents with natural language.
3. Compose documents into the context window of an LLM like `GPT4` for additional summarization or analysis.

## Embeddings?

What are embeddings?

- [Read the guide from OpenAI](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings)
- __Literal__: Embedding something turns it from image/text/audio into a list of numbers. üñºÔ∏è or üìÑ =&gt; `[1.2, 2.1, ....]`. This process makes documents &quot;understandable&quot; to a machine learning model.
- __By analogy__: An embedding represents the essence of a document. This enables documents and queries with the same essence to be &quot;near&quot; each other and therefore easy to find.
- __Technical__: An embedding is the latent-space position of a document at a layer of a deep neural network. For models trained specifically to embed data, this is the last layer.
- __A small example__: If you search your photos for &quot;famous bridge in San Francisco&quot;. By embedding this query and comparing it to the embeddings of your photos and their metadata - it should return photos of the Golden Gate Bridge.

Embeddings databases (also known as **vector databases**) store embeddings and allow you to search by nearest neighbors rather than by substrings like a traditional database. By default, Chroma uses [Sentence Transformers](https://docs.trychroma.com/guides/embeddings#default:-all-minilm-l6-v2) to embed for you but you can also use OpenAI embeddings, Cohere (multilingual) embeddings, or your own.

## Get involved

Chroma is a rapidly developing project. We welcome PR contributors and ideas for how to improve the project.
- [Join the conversation on Discord](https://discord.gg/MMeYNTmh3x) - `#contributing` channel
- [Review the üõ£Ô∏è Roadmap and contribute your ideas](https://docs.trychroma.com/roadmap)
- [Grab an issue and open a PR](https://github.com/chroma-core/chroma/issues) - [`Good first issue tag`](https://github.com/chroma-core/chroma/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22)
- [Read our contributing guide](https://docs.trychroma.com/contributing)

**Release Cadence**
We currently release new tagged versions of the `pypi` and `npm` packages on Mondays. Hotfixes go out at any time during the week.

## License

[Apache 2.0](./LICENSE)
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[oxc-project/oxc]]></title>
            <link>https://github.com/oxc-project/oxc</link>
            <guid>https://github.com/oxc-project/oxc</guid>
            <pubDate>Thu, 16 Oct 2025 00:05:33 GMT</pubDate>
            <description><![CDATA[‚öì A collection of JavaScript tools written in Rust.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/oxc-project/oxc">oxc-project/oxc</a></h1>
            <p>‚öì A collection of JavaScript tools written in Rust.</p>
            <p>Language: Rust</p>
            <p>Stars: 16,736</p>
            <p>Forks: 684</p>
            <p>Stars today: 37 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img alt=&quot;OXC Logo&quot; src=&quot;https://cdn.jsdelivr.net/gh/oxc-project/oxc-assets/preview-universal.png&quot; width=&quot;700&quot;&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;

[![MIT licensed][license-badge]][license-url]
[![Build Status][ci-badge]][ci-url]
[![Code Coverage][code-coverage-badge]][code-coverage-url]
[![CodSpeed Badge](https://img.shields.io/endpoint?url=https://codspeed.io/badge.json)](https://codspeed.io/oxc-project/oxc)
[![Sponsors][sponsors-badge]][sponsors-url]

[![Discord chat][discord-badge]][discord-url]
[![Playground][playground-badge]][playground-url]
[![Website][website-badge]][website-url]

&lt;/div&gt;

## ‚öì Oxc

The Oxidation Compiler is a collection of high-performance tools for JavaScript and TypeScript written in Rust.

Our goal is to enable a new generation of faster, more reliable development tools by providing:

- **Performance**: 2-100x faster than existing JavaScript tools
- **Reliability**: 100% compatibility with JavaScript and TypeScript standards
- **Modularity**: Use individual tools or compose them into complete toolchains
- **Developer Experience**: Clear error messages and seamless editor integration

We are building a parser, linter, formatter, transformer, minifier, resolver ... all written in Rust.

For more information, check out our documentation at [oxc.rs](https://oxc.rs) and architecture guide in [ARCHITECTURE.md](./ARCHITECTURE.md).

## VoidZero Inc.

Oxc is a project of [VoidZero](https://voidzero.dev/), see our announcement [Announcing VoidZero - Next Generation Toolchain for JavaScript](https://voidzero.dev/blog).

If you have requirements for JavaScript tools at scale, please [get in touch](https://forms.gle/WQgjyzYJpwurpxWKA)!

## üôãWho&#039;s using Oxc?

- [Rolldown] uses the [oxc][docs-oxc-url] crate for parsing and transformation.
- [Nova engine](https://trynova.dev) uses the [oxc][docs-oxc-url] crate for parsing.
- [Rolldown][rolldown], [swc-node](https://github.com/swc-project/swc-node) and [knip](https://github.com/webpro-nl/knip) use the [oxc_resolver][docs-resolver-url] crate for module resolution.
- Projects and companies like [Preact](https://github.com/preactjs/preact/blob/4c20c23c16dd60f380ce9fe98afc93041a7e1562/oxlint.json), [Shopify](https://oxc.rs/blog/2023-12-12-announcing-oxlint.html#_50-100-times-faster-than-eslint), ByteDance and Shopee uses oxlint for linting.
- ...[and many more](https://oxc.rs/docs/guide/projects.html)

## ‚úçÔ∏è Contribute

See [CONTRIBUTING.md](./CONTRIBUTING.md) for guidance.

Check out some of the [good first issues](https://github.com/oxc-project/oxc/contribute) or ask us on [Discord][discord-url].

If you are unable to contribute by code, you can still participate by:

- Add a [GitHub Star](https://github.com/oxc-project/oxc/stargazers) to the project.
- Join us on [Discord][discord-url].
- [Follow me on X](https://x.com/boshen_c) and post about this project.

## ‚ö°Ô∏è Linter Quick Start

The linter is ready to catch mistakes for you. It comes with 93 rules turned on by default (out of 430+ in total) and no configuration is required.

To get started, run [oxlint][npm-oxlint] or via `npx`:

```bash
npx oxlint@latest
```

To give you an idea of its capabilities, here is an example from the [vscode] repository, which finishes linting 4800+ files in 0.7 seconds.

&lt;p float=&quot;left&quot; align=&quot;left&quot;&gt;
  &lt;img src=&quot;https://cdn.jsdelivr.net/gh/oxc-project/oxc-assets/linter-screenshot.png&quot; width=&quot;60%&quot;&gt;
&lt;/p&gt;

## ‚ö°Ô∏è Performance

- The parser aims to be the fastest Rust-based ready-for-production parser.
- The linter is more than 50 times faster than [ESLint], and scales with the number of CPU cores.

&lt;p float=&quot;left&quot; align=&quot;middle&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/Boshen/bench-javascript-parser-written-in-rust/main/bar-graph.svg&quot; width=&quot;49%&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/Boshen/bench-javascript-linter/main/bar-graph.svg&quot; width=&quot;49%&quot;&gt;
&lt;/p&gt;

## ‚å®Ô∏è Rust, Node.js and Wasm Usage

### Rust

Individual crates are published, you may use them to build your own JavaScript tools.

- The umbrella crate [oxc][docs-oxc-url] exports all public crates from this repository.
- The AST and parser crates [oxc_ast][docs-ast-url] and [oxc_parser][docs-parser-url] are production ready.
- The resolver crate [oxc_resolver][docs-resolver-url] for module resolution is also production ready.
- Example usages of these crates can be found in their respective `crates/*/examples` directory.

We have optimized Rust compilation speed to ensure developing your own Oxc-based tools remains efficient.
Our [CI runs](https://github.com/oxc-project/oxc/actions/workflows/ci.yml?query=branch%3Amain) complete in approximately 3 minutes.

### Node.js

- via napi: [oxc-parser][npm-napi-parser], [oxc-transform][npm-napi-transform]

### Wasm

- [@oxc-parser/wasm](https://www.npmjs.com/package/@oxc-parser/wasm)

---

## üéØ Tools

- [AST and Parser](#-ast-and-parser)
- [Linter](#-linter)
- [Resolver](#-resolver)
- [Minifier](#-minifier)
- [Formatter](#-formatter)
- [Transformer](#-transformer)

### üî∏ AST and Parser

Oxc maintains its own AST and parser, which is by far the fastest and most conformant JavaScript and TypeScript (including JSX and TSX) parser written in Rust.

As the parser often represents a key performance bottleneck in JavaScript tooling, any minor improvements can have a cascading effect on our downstream tools.

#### üèÜ Parser Performance

Our [benchmark][parser-benchmark] reveals that the Oxc parser surpasses the speed of the [swc] parser by approximately 3 times and the [Biome][biome] parser by 5 times.

### üî∏ Linter

The linter embraces convention over configuration, eliminating the need for extensive configuration and plugin setup.
Unlike other linters like [ESLint], which often require intricate configurations and plugin installations (e.g. [@typescript-eslint]),
our linter only requires a single command that you can immediately run on your codebase:

```bash
npx oxlint@latest
```

#### üèÜ Linter Performance

The linter is 50 - 100 times faster than [ESLint] depending on the number of rules and number of CPU cores used.
It completes in less than a second for most codebases with a few hundred files and completes in a few seconds for
larger monorepos. See [bench-javascript-linter](https://github.com/Boshen/bench-javascript-linter) for details.

As an upside, the binary is approximately 5MB, whereas [ESLint] and its associated plugin dependencies can easily exceed 100.

You may also download the linter binary from the [latest release tag](https://github.com/oxc-project/oxc/releases/latest) as a standalone binary,
this lets you run the linter without a Node.js installation in your CI.

### üî∏ Resolver

Module resolution plays a crucial role in JavaScript tooling, especially for tasks like multi-file analysis or bundling. However, it can often become a performance bottleneck.
To address this, we developed [oxc_resolver][docs-resolver-url].

The resolver is production-ready and is currently being used in [Rolldown][rolldown]. Usage and examples can be found in its own [repository](https://github.com/oxc-project/oxc_resolver).

### üî∏ Transformer

A transformer is responsible for turning higher versions of ECMAScript to a lower version that can be used in older browsers.

TypeScript, React, ES6 transforms are complete.

[oxc-transform][npm-napi-transform] can be used for experimentation.

### üî∏ Isolated Declarations

[TypeScript Isolated Declarations Emit](https://devblogs.microsoft.com/typescript/announcing-typescript-5-5/#isolated-declarations) without using the TypeScript compiler.

Our [benchmark](https://github.com/oxc-project/bench-transformer) indicates that our implementation is at least 20 times faster than the TypeScript compiler.

The [npm package](https://www.npmjs.com/package/oxc-transform) or [crate](https://crates.io/crates/oxc_isolated_declarations) can be used for this task.

### üî∏ Minifier

JavaScript minification plays a crucial role in optimizing website performance as it reduces the amount of data sent to users,
resulting in faster page loads.
This holds tremendous economic value, particularly for e-commerce websites, where every second can equate to millions of dollars.

However, existing minifiers typically require a trade-off between compression quality and speed.
You have to choose between the slowest for the best compression or the fastest for less compression.
But what if we could develop a faster minifier without compromising on compression?

We are actively working on a prototype that aims to achieve this goal,
by porting all test cases from well-known minifiers such as [google-closure-compiler], [terser], [esbuild], and [tdewolff-minify].

Preliminary results indicate that we are on track to achieve our objectives.
With the Oxc minifier, you can expect faster minification times without sacrificing compression quality.

See [minification benchmarks](https://github.com/privatenumber/minification-benchmarks) for comparisons.

### üî∏ Formatter

While [prettier] has established itself as the de facto code formatter for JavaScript, there is a significant demand in the developer community for a less opinionated alternative. Recognizing this need, our ambition is to undertake research and development to create a new JavaScript formatter that offers increased flexibility and customization options.

The [prototype](https://github.com/oxc-project/oxc/tree/main/crates/oxc_formatter) is currently work in progress.

---

## üß™Test Infrastructure

In Oxc, correctness and reliability are taken extremely seriously.

We spend half of our time on strengthening the test infrastructure to prevent problems from propagating to downstream tools.

[Test Infrastructure](https://oxc.rs/docs/learn/architecture/test.html) documents our test procedures:

- Conformance suite on Test262, Babel, TypeScript
- Lots of fuzzing
- Linter snapshot diagnostics
- oxlint ecosystem ci
- Idempotency testing
- Code coverage
- End to end 3000 top npm packages

---

## üìö Learning Resources

- My small tutorial on [how to write a JavaScript Parser in Rust](https://oxc.rs/docs/learn/parser_in_rust/intro.html)
- My small article [Pursuit of Performance on Building a JavaScript Compiler](https://oxc.rs/docs/learn/performance.html)
- [And more](https://oxc.rs/docs/learn/references.html)

## ü§ù Credits

This project was incubated with the assistance of these exceptional mentors and their projects:

- [Biome][biome] - [@ematipico](https://github.com/ematipico)
- [Ruff][ruff] - [@charliermarsh](https://github.com/charliermarsh), [@MichaReiser](https://github.com/MichaReiser)
- [quick-lint-js](https://github.com/quick-lint/quick-lint-js) - [@strager](https://github.com/strager)
- [elm-review](https://package.elm-lang.org/packages/jfmengels/elm-review/latest) - [@jfmengels](https://github.com/jfmengels)

Special thanks go to

- [@domonji](https://github.com/domonji) for bootstrapping this project together, and also completing the TypeScript parser.
- [@tongtong-lu](https://github.com/tongtong-lu) and [@guan-wy](https://github.com/guan-wy) for designing the [project logo](https://github.com/oxc-project/oxc-assets).

## ‚ù§ Who&#039;s [Sponsoring Oxc](https://github.com/sponsors/Boshen)?

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/sponsors/Boshen&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/Boshen/sponsors/main/sponsors.svg&quot; alt=&quot;My sponsors&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

## üìñ License

Oxc is free and open-source software licensed under the [MIT License](./LICENSE).

Oxc ports or copies code from other open source projects, their licenses are listed in [**Third-party library licenses**](./THIRD-PARTY-LICENSE).

[discord-badge]: https://img.shields.io/discord/1079625926024900739?logo=discord&amp;label=Discord
[discord-url]: https://discord.gg/9uXCAwqQZW
[license-badge]: https://img.shields.io/badge/license-MIT-blue.svg
[license-url]: https://github.com/oxc-project/oxc/blob/main/LICENSE
[ci-badge]: https://github.com/oxc-project/oxc/actions/workflows/ci.yml/badge.svg?event=push&amp;branch=main
[ci-url]: https://github.com/oxc-project/oxc/actions/workflows/ci.yml?query=event%3Apush+branch%3Amain
[npm-badge]: https://img.shields.io/npm/v/oxlint/latest?color=brightgreen
[npm-url]: https://www.npmjs.com/package/oxlint/v/latest
[code-size-badge]: https://img.shields.io/github/languages/code-size/oxc-project/oxc
[code-size-url]: https://github.com/oxc-project/oxc
[code-coverage-badge]: https://codecov.io/gh/oxc-project/oxc/graph/badge.svg?token=FVHEH0BQLJ
[code-coverage-url]: https://codecov.io/gh/oxc-project/oxc
[sponsors-badge]: https://img.shields.io/github/sponsors/Boshen
[sponsors-url]: https://github.com/sponsors/Boshen
[playground-badge]: https://img.shields.io/badge/Playground-blue?color=9BE4E0
[playground-url]: https://playground.oxc.rs/
[website-badge]: https://img.shields.io/badge/Website-blue
[website-url]: https://oxc.rs
[crate-oxc-url]: https://crates.io/crates/oxc
[crate-ast-url]: https://crates.io/crates/oxc_ast
[crate-parser-url]: https://crates.io/crates/oxc_parser
[docs-oxc-url]: https://docs.rs/oxc
[docs-ast-url]: https://docs.rs/oxc_ast
[docs-parser-url]: https://docs.rs/oxc_parser
[docs-resolver-url]: https://docs.rs/oxc_resolver
[Boshen]: https://github.com/boshen
[CompactString]: https://github.com/ParkMyCar/compact_str
[ESLint]: https://eslint.org/
[acorn]: https://github.com/acornjs/acorn
[babel]: https://babel.dev
[bumpalo]: https://docs.rs/bumpalo
[contributors]: https://github.com/oxc-project/oxc/graphs/contributors
[enhanced-resolve]: https://github.com/webpack/enhanced-resolve
[esbuild]: https://esbuild.github.io/
[eslint-plugin-import]: https://www.npmjs.com/package/eslint-plugin-import
[eslint-plugin-jest]: https://www.npmjs.com/package/eslint-plugin-jest
[estree]: https://github.com/estree/estree
[google-closure-compiler]: https://github.com/google/closure-compiler
[minification-benchmarks]: https://github.com/privatenumber/minification-benchmarks
[npm-napi-parser]: https://www.npmjs.com/package/oxc-parser
[npm-napi-transform]: https://www.npmjs.com/package/oxc-transform
[npm-oxlint]: https://www.npmjs.com/package/oxlint
[parser-benchmark]: https://github.com/Boshen/bench-javascript-parser-written-in-rust
[prettier]: https://prettier.io
[biome]: https://biomejs.dev/
[ruff]: https://beta.ruff.rs
[swc]: https://swc.rs
[tdewolff-minify]: https://github.com/tdewolff/minify
[terser]: https://terser.org
[vscode]: https://github.com/microsoft/vscode
[@typescript-eslint]: https://typescript-eslint.io
[rolldown]: https://rolldown.rs
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[j178/prek]]></title>
            <link>https://github.com/j178/prek</link>
            <guid>https://github.com/j178/prek</guid>
            <pubDate>Thu, 16 Oct 2025 00:05:32 GMT</pubDate>
            <description><![CDATA[‚ö° Better `pre-commit`, re-engineered in Rust]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/j178/prek">j178/prek</a></h1>
            <p>‚ö° Better `pre-commit`, re-engineered in Rust</p>
            <p>Language: Rust</p>
            <p>Stars: 1,921</p>
            <p>Forks: 62</p>
            <p>Stars today: 75 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# prek

&lt;img width=&quot;220&quot; alt=&quot;prek&quot; src=&quot;./docs/assets/logo_512.webp&quot; /&gt;

[![CI](https://github.com/j178/prek/actions/workflows/ci.yml/badge.svg)](https://github.com/j178/prek/actions/workflows/ci.yml)
[![codecov](https://codecov.io/github/j178/prek/graph/badge.svg?token=MP6TY24F43)](https://codecov.io/github/j178/prek)
[![GitHub Downloads](https://img.shields.io/github/downloads/j178/prek/total?logo=github)](https://github.com/j178/prek/releases)
[![PyPI Downloads](https://img.shields.io/pypi/dm/prek?logo=python)](https://pepy.tech/projects/prek)
[![Discord](https://img.shields.io/discord/1403581202102878289?logo=discord)](https://discord.gg/3NRJUqJz86)

&lt;/div&gt;

&lt;!-- description:start --&gt;
[pre-commit](https://pre-commit.com/) is a framework to run hooks written in many languages, and it manages the
language toolchain and dependencies for running the hooks.

*prek* is a reimagined version of pre-commit, built in Rust.
It is designed to be a faster, dependency-free and drop-in alternative for it,
while also providing some additional long-requested features.
&lt;!-- description:end --&gt;

&gt; [!WARNING]
&gt; prek is not production-ready yet. Some subcommands and languages are not implemented. See the current gaps for drop-in parity: [TODO](https://prek.j178.dev/todo/).
&gt;
&gt; It&#039;s already being adopted by [some projects](#who-is-using-prek), please give it a try - we&#039;d love your feedback!

&lt;!-- features:start --&gt;
## Features

- üöÄ A single binary with no dependencies, does not require Python or any other runtime.
- ‚ö° About [10x faster](https://prek.j178.dev/benchmark/) than `pre-commit` and uses only half the disk space.
- üîÑ Fully compatible with the original pre-commit configurations and hooks.
- üèóÔ∏è Built-in support for monorepos (i.e. [workspace mode](https://prek.j178.dev/workspace/)).
- üêç Integration with [`uv`](https://github.com/astral-sh/uv) for managing Python virtual environments and dependencies.
- üõ†Ô∏è Improved toolchain installations for Python, Node.js, Go, Rust and Ruby, shared between hooks.
- üì¶ [Built-in](https://prek.j178.dev/builtin/) Rust-native implementation of some common hooks.
&lt;!-- features:end --&gt;

## How to migrate

prek is designed as a drop-in replacement:

- [Install prek](#installation)
- Replace `pre-commit` with `prek` in your commands
- Your existing `.pre-commit-config.yaml` works unchanged

```console
$ prek run
trim trailing whitespace.................................................Passed
fix end of files.........................................................Passed
typos....................................................................Passed
cargo fmt................................................................Passed
cargo clippy.............................................................Passed
```

For configuring `.pre-commit-config.yaml` and writing hooks, you can refer to the [pre-commit documentation](https://pre-commit.com/) as prek is fully compatible with it.

&lt;!-- why:start --&gt;
## Why prek?

### prek is way faster

- It is about [10x faster](https://prek.j178.dev/benchmark/) than `pre-commit` and uses only half the disk space.
- It redesigned how hook environments and toolchains are managed, they are all shared between hooks, which reduces the disk space usage and speeds up the installation process.
- Repositories are cloned in parallel, and hooks are installed in parallel if their dependencies are disjoint.
- It uses [`uv`](https://github.com/astral-sh/uv) for creating Python virtualenvs and installing dependencies, which is known for its speed and efficiency.
- It implements some common hooks in Rust, [built in prek](https://prek.j178.dev/builtin/), which are faster than their Python counterparts.

### prek provides a better user experience

- No need to install Python or any other runtime, just download a single binary.
- No hassle with your Python version or virtual environments, prek automatically installs the required Python version and creates a virtual environment for you.
- Built-in support for workspaces (or monorepos), each subproject can have its own `.pre-commit-config.yaml` file.
- `prek run` has some nifty improvements over `pre-commit run`, such as:
  - `prek run --directory &lt;dir&gt;` runs hooks for files in the specified directory, no need to use `git ls-files -- &lt;dir&gt; | xargs pre-commit run --files` anymore.
  - `prek run --last-commit` runs hooks for files changed in the last commit.
  - `prek run [HOOK] [HOOK]` selects and runs multiple hooks.
- `prek list` command lists all available hooks, their ids, and descriptions, providing a better overview of the configured hooks.
- prek provides shell completions for `prek run &lt;hook_id&gt;` command, making it easier to run specific hooks without remembering their ids.

For more detailed improvements prek offers, take a look at [Difference from pre-commit](https://prek.j178.dev/diff/).

## Who is using prek?

prek is pretty new, but it is already being used or recommend by some projects and organizations:

- [Airflow](https://github.com/apache/airflow/issues/44995)
- [PDM](https://github.com/pdm-project/pdm/pull/3593)
- [basedpyright](https://github.com/DetachHead/basedpyright/pull/1413)
- [OpenLineage](https://github.com/OpenLineage/OpenLineage/pull/3965)
- [Authlib](https://github.com/authlib/authlib/pull/804)
- [pre-commit-crocodile](https://radiandevcore.gitlab.io/tools/pre-commit-crocodile/)
- [PaperQA2](https://github.com/Future-House/paper-qa/pull/1098)
- [requests-cache](https://github.com/requests-cache/requests-cache/pull/1116)

&lt;!-- why:end --&gt;

## Installation

&lt;details&gt;
&lt;summary&gt;Standalone installer&lt;/summary&gt;

prek provides a standalone installer script to download and install the tool,

On Linux and macOS:

&lt;!-- linux-standalone-install:start --&gt;
```bash
curl --proto &#039;=https&#039; --tlsv1.2 -LsSf https://github.com/j178/prek/releases/download/v0.2.8/prek-installer.sh | sh
```
&lt;!-- linux-standalone-install:end --&gt;

On Windows:

&lt;!-- windows-standalone-install:start --&gt;
```powershell
powershell -ExecutionPolicy ByPass -c &quot;irm https://github.com/j178/prek/releases/download/v0.2.8/prek-installer.ps1 | iex&quot;
```
&lt;!-- windows-standalone-install:end --&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;PyPI&lt;/summary&gt;

&lt;!-- pypi-install:start --&gt;
prek is published as Python binary wheel to PyPI, you can install it using `pip`, `uv` (recommended), or `pipx`:

```bash
# Using uv (recommended)
uv tool install prek

# Using pip
pip install prek

# Using pipx
pipx install prek
```
&lt;!-- pypi-install:end --&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Homebrew&lt;/summary&gt;

&lt;!-- homebrew-install:start --&gt;
```bash
brew install prek
```
&lt;!-- homebrew-install:end --&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;mise&lt;/summary&gt;

&lt;!-- mise-install:start --&gt;
To use prek with [mise](https://mise.jdx.dev):

```bash
mise use prek
```
&lt;!-- mise-install:end --&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Cargo binstall&lt;/summary&gt;

&lt;!-- cargo-binstall:start --&gt;
Install pre-compiled binaries from GitHub using Cargo binstall (Rust 1.89+ is required):

```bash
cargo binstall prek --git https://github.com/j178/prek
```
&lt;!-- cargo-binstall:end --&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Cargo&lt;/summary&gt;

&lt;!-- cargo-install:start --&gt;
Build from source using Cargo (Rust 1.89+ is required):

```bash
cargo install --locked --git https://github.com/j178/prek
```
&lt;!-- cargo-install:end --&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;npmjs&lt;/summary&gt;

&lt;!-- npmjs-install:start --&gt;
prek is published as a Node.js package, you can install it using `npm`, `pnpm`, or `npx`:

```bash
# Using npm
npm add -D @j178/prek

# Using pnpm
pnpm add -D @j178/prek

# Using npx
npx @j178/prek --version

# or install globally
npm install -g @j178/prek

# then use `prek` command
prek --version
```
&lt;!-- npmjs-install:end --&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Nix&lt;/summary&gt;

&lt;!-- nix-install:start --&gt;
prek is [available in Nix as `prek`](https://search.nixos.org/packages?channel=unstable&amp;show=prek&amp;query=prek).

```shell
# Choose what&#039;s appropriate for your use case.
# One-off in a shell:
nix-shell -p prek
# NixOS or non-NixOS without flakes:
nix-env -iA nixos.prek
# Non-NixOS with flakes:
nix profile install nixpkgs#prek
```
&lt;!-- nix-install:end --&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Conda&lt;/summary&gt;

&lt;!-- conda-forge-install:start --&gt;
prek is [available as `prek` via conda-forge](https://anaconda.org/conda-forge/prek).

```shell
conda install conda-forge::prek
```
&lt;!-- conda-forge-install:end --&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;GitHub Releases&lt;/summary&gt;

&lt;!-- pre-built-binaries:start --&gt;
Pre-built binaries are available for download from the [GitHub releases](https://github.com/j178/prek/releases) page.
&lt;!-- pre-built-binaries:end --&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;GitHub Actions&lt;/summary&gt;

&lt;!-- github-actions:start --&gt;
prek can be used in GitHub Actions via the [j178/prek-action](https://github.com/j178/prek-action) repository.

Example workflow:

```yaml
name: Prek checks
on: [push, pull_request]

jobs:
  prek:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v5
      - uses: j178/prek-action@v1
```

This action installs prek and runs `prek run --all-files` on your repository.
&lt;!-- github-actions:end --&gt;
&lt;/details&gt;

&lt;!-- self-update:start --&gt;
If installed via the standalone installer, prek can update itself to the latest version:

```bash
prek self update
```
&lt;!-- self-update:end --&gt;

## Acknowledgements

This project is heavily inspired by the original [pre-commit](https://pre-commit.com/) tool, and it wouldn&#039;t be possible without the hard work
of the maintainers and contributors of that project.

And a special thanks to the [Astral](https://github.com/astral-sh) team for their remarkable projects, particularly [uv](https://github.com/astral-sh/uv),
from which I&#039;ve learned a lot on how to write efficient and idiomatic Rust code.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[Jondolf/avian]]></title>
            <link>https://github.com/Jondolf/avian</link>
            <guid>https://github.com/Jondolf/avian</guid>
            <pubDate>Thu, 16 Oct 2025 00:05:31 GMT</pubDate>
            <description><![CDATA[ECS-driven 2D and 3D physics engine for the Bevy game engine.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Jondolf/avian">Jondolf/avian</a></h1>
            <p>ECS-driven 2D and 3D physics engine for the Bevy game engine.</p>
            <p>Language: Rust</p>
            <p>Stars: 2,498</p>
            <p>Forks: 185</p>
            <p>Stars today: 17 stars today</p>
            <h2>README</h2><pre># ![Avian Physics](https://raw.githubusercontent.com/Jondolf/avian/avian/assets/branding/logo.svg)

[![MIT/Apache 2.0](https://img.shields.io/badge/license-MIT%2FApache-blue.svg)](https://github.com/Jondolf/avian#license)
[![ci](https://github.com/Jondolf/avian/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/Jondolf/avian/actions/workflows/ci.yml)
[![2D crates.io](https://img.shields.io/crates/v/avian2d?label=2D%20crates.io)](https://crates.io/crates/avian2d)
[![2D docs.rs](https://img.shields.io/docsrs/avian2d?label=2D%20docs.rs)](https://docs.rs/avian2d)
[![3D crates.io](https://img.shields.io/crates/v/avian3d?label=3D%20crates.io)](https://crates.io/crates/avian3d)
[![3D docs.rs](https://img.shields.io/docsrs/avian3d?label=3D%20docs.rs)](https://docs.rs/avian3d)

**Avian** is an ECS-driven 2D and 3D physics engine for the [Bevy game engine](https://bevyengine.org/).

---

## Design

Below are some of the core design principles used in Avian.

- **Made with Bevy, for Bevy.** No wrappers around existing engines.
- **Provide an ergonomic and familiar API.** Ergonomics is key for a good experience.
- **Utilize the ECS as much as possible.** The engine should feel like a part of Bevy, and it shouldn&#039;t
    need to maintain a separate physics world.
- **Use a highly modular plugin architecture.** Users should be able to replace parts of the engine
    with their own implementations.
- **Have good documentation.** A physics engine is pointless if you don&#039;t know how to use it.

## Features

Below are some of the current features of Avian.

- Dynamic, kinematic and static rigid bodies
  - Linear and angular velocity
  - External forces, torque and impulses
  - Gravity and gravity scale
  - Linear and angular damping
  - Locking translational and rotational axes
  - Rigid body dominance
  - Continuous Collision Detection (CCD)
  - Automatic deactivation with sleeping
- Collision detection powered by [Parry](https://parry.rs)
  - Colliders with configurable collision layers, density, material properties and more
  - Collider generation for meshes and entire scenes
  - Collision events
  - Access to colliding entities
  - Filtering and modifying collisions with collision hooks
  - Manual contact queries and intersection tests
- Constraints and joints
  - Several built-in joint types: fixed, distance, prismatic, revolute, spherical
  - Support for custom joints and other constraints using XPBD
- Spatial queries
  - Raycasting, shapecasting, point projection and intersection tests
  - Ergonomic component-based API for raycasts and shapecasts
  - Flexible `SpatialQuery` system parameter
  - Spatial query filters
- `Transform` interpolation and extrapolation for fixed timesteps
- Debug rendering for colliders, AABBs, contacts, joints, spatial queries, and more
- Configurable scheduling and high customizability
- Highly modular plugin architecture, freely extend and replace parts of the engine
- Support for custom collision backends
- `f32`/`f64` precision (`f32` by default)

You can find a more complete list along with documentation in the
[Table of Contents](https://docs.rs/avian3d/latest/avian3d/#table-of-contents)
on docs.rs.

## Documentation

- [2D documentation](https://docs.rs/avian2d)
- [3D documentation](https://docs.rs/avian3d)

## Usage Example

First, add `avian2d` or `avian3d` to your dependencies in `Cargo.toml`:

```toml
# For 2D applications:
[dependencies]
avian2d = &quot;0.4&quot;

# For 3D applications:
[dependencies]
avian3d = &quot;0.4&quot;

# If you want to use the most up-to-date version, you can follow the main branch:
[dependencies]
avian3d = { git = &quot;https://github.com/Jondolf/avian&quot;, branch = &quot;main&quot; }
```

Below is a very simple example where a cube with initial angular velocity falls onto a circular platform.
This is a modified version of Bevy&#039;s [`3d_scene`](https://bevyengine.org/examples/3d-rendering/3d-scene/) example.

```rust
use avian3d::prelude::*;
use bevy::prelude::*;

fn main() {
    App::new()
        // Enable physics
        .add_plugins((DefaultPlugins, PhysicsPlugins::default()))
        .add_systems(Startup, setup)
        .run();
}

fn setup(
    mut commands: Commands,
    mut meshes: ResMut&lt;Assets&lt;Mesh&gt;&gt;,
    mut materials: ResMut&lt;Assets&lt;StandardMaterial&gt;&gt;,
) {
    // Static physics object with a collision shape
    commands.spawn((
        RigidBody::Static,
        Collider::cylinder(4.0, 0.1),
        Mesh3d(meshes.add(Cylinder::new(4.0, 0.1))),
        MeshMaterial3d(materials.add(Color::WHITE)),
    ));

    // Dynamic physics object with a collision shape and initial angular velocity
    commands.spawn((
        RigidBody::Dynamic,
        Collider::cuboid(1.0, 1.0, 1.0),
        AngularVelocity(Vec3::new(2.5, 3.5, 1.5)),
        Mesh3d(meshes.add(Cuboid::from_length(1.0))),
        MeshMaterial3d(materials.add(Color::srgb_u8(124, 144, 255))),
        Transform::from_xyz(0.0, 4.0, 0.0),
    ));

    // Light
    commands.spawn((
        PointLight {
            shadows_enabled: true,
            ..default()
        },
        Transform::from_xyz(4.0, 8.0, 4.0),
    ));

    // Camera
    commands.spawn((
        Camera3d::default(),
        Transform::from_xyz(-2.5, 4.5, 9.0).looking_at(Vec3::ZERO, Dir3::Y),
    ));
}
```

![A spinning cube falling onto a circular platform](https://github.com/user-attachments/assets/14d25e7e-9d46-467c-9fe6-dc408cd23398)

## More Examples

You can find lots of 2D and 3D examples in [/crates/avian2d/examples](/crates/avian2d/examples) and [/crates/avian3d/examples](/crates/avian3d/examples) respectively.

The examples support both `f32` and `f64` precisions, so the code contains some feature-dependent types like `Scalar` and `Vector`.
In actual usage these are not needed, so you can just use `f32` or `f64` types depending on the features you have chosen.

By default the examples use `f32`. To run the `f64` versions, you need to disable default features and manually choose the dimension
and precision:

```shell
# Manually specify dimension and precision. `parry-f64` enables collision detection using Parry.
cargo run --example cubes --no-default-features --features &quot;3d f64 parry-f64&quot;
```

## Supported Bevy Versions

| Bevy    | Avian  |
| ------- | ------ |
| 0.17    | 0.4    |
| 0.16    | 0.3    |
| 0.15    | 0.2    |
| 0.14    | 0.1    |

&lt;details&gt;
  &lt;summary&gt;Bevy XPBD versions (the predecessor of Avian)&lt;/summary&gt;

  | Bevy | Bevy XPBD |
  | ---- | --------- |
  | 0.14 | 0.5       |
  | 0.13 | 0.4       |
  | 0.12 | 0.3       |
  | 0.11 | 0.2       |
  | 0.10 | 0.1       |

&lt;/details&gt;

## Contributing

If you encounter any problems, feel free to open issues or create pull requests.
For larger changes and additions, it&#039;s better to open an issue or ask me for input
before making a pull request.

You can also ask for help or ask questions on the [Bevy Discord](https://discord.com/invite/gMUk5Ph)
server&#039;s Avian Physics topic in `#ecosystem-crates`. My username on the Discord is `Jondolf` (`@jondolfdev`).

## Acknowledgements

Huge thanks to the entire Bevy community for the incredible support!
All of your contributions, insight and requests are a massive help
in driving the state of physics in Bevy forward, and it&#039;s what
keeps me motivated to build the best engine I can.

I would also like to give a special thanks to [Johan Helsing][johan-helsing]
for inspiring this project and helping me significantly in the early stages.
His original [tutorial series][johan-xpbd-tutorial] is the reason `avian`
exists in the first place, and without his support and contributions,
the project wouldn&#039;t be anywhere near where it is today.

[johan-helsing]: https://github.com/johanhelsing
[johan-xpbd-tutorial]: https://johanhelsing.studio/posts/bevy-xpbd

## License

Avian is free and open source. All code in this repository is dual-licensed under either:

- MIT License ([LICENSE-MIT](/LICENSE-MIT) or &lt;http://opensource.org/licenses/MIT&gt;)
- Apache License, Version 2.0 ([LICENSE-APACHE](/LICENSE-APACHE) or &lt;http://www.apache.org/licenses/LICENSE-2.0&gt;)

at your option.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[dbt-labs/dbt-fusion]]></title>
            <link>https://github.com/dbt-labs/dbt-fusion</link>
            <guid>https://github.com/dbt-labs/dbt-fusion</guid>
            <pubDate>Thu, 16 Oct 2025 00:05:30 GMT</pubDate>
            <description><![CDATA[The next-generation engine for dbt]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/dbt-labs/dbt-fusion">dbt-labs/dbt-fusion</a></h1>
            <p>The next-generation engine for dbt</p>
            <p>Language: Rust</p>
            <p>Stars: 516</p>
            <p>Forks: 67</p>
            <p>Stars today: 8 stars today</p>
            <h2>README</h2><pre>&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;img src=&quot;assets/dbt-fusion-engine.png&quot; alt=&quot;dbt Fusion Engine&quot; width=&quot;400&quot; style=&quot;border-radius: 6px;&quot; /&gt;
&lt;/div&gt;

---

# dbt F‚ú¶SION engine (BETA)

This repo hosts components of the dbt Fusion engine, the foundation for future innovation in `dbt`. The dbt Fusion engine is written in Rust and is designed for speed, correctness, and has a native understanding of SQL across multiple data warehouse SQL dialects.

&gt; [!IMPORTANT]  
&gt; **Note: the dbt Fusion Engine is in Beta!**

Bugs and missing functionality compared to dbt Core will be resolved continuously in the leadup to a final release (more info: [the dbt Fusion Engine: the Path to GA](https://docs.getdbt.com/blog/dbt-fusion-engine-path-to-ga)). 

The dbt Fusion engine is a ground up, first principles rewrite of the dbt Core execution engine, built to be interoperable with the standard dbt authoring layer. Fusion enforces some ambiguous areas of the authoring spec more strictly than dbt Core to ensure correctness (for example, dbt Core does not proactively validate most YAML configurations). Many of these discrepancies can be fixed automatically with the [dbt Autofix](https://github.com/dbt-labs/dbt-autofix) tool.

Beyond conformance with dbt Core, Fusion also contains new SQL Comprehension capabilities, a language server, modern ADBC drivers for warehouse connections, and more. While dbt Core was written in Python, the dbt Fusion engine is written in Rust, and compiled to a single application binary.

You can install dbt-fusion onto your local machine, a docker container, or a machine in the cloud. It is designed for flexible installation, with no dependencies on other libraries. The only libraries that dbt Fusion will load are it&#039;s corresponding database drivers.

The dbt Fusion engine is being released to this repository incrementally, so, until this note is removed this repository contains only a subset of the crates that make the core of the engine work. These crates are published incrementally starting on May 28.

## Getting Started with the dbt Fusion engine

&gt; [!TIP]  
&gt; You don&#039;t have to build this project from source to use the new dbt! We recommend using the precompiled binary with additional capabilities: 

There are several ways to get started with Fusion (for more, see the [Quickstart for the dbt Fusion engine](https://docs.getdbt.com/guides/fusion?step=1))
1. **Download dbt the vs-code extension** - For most people the best experience. This will install the dbt fusion CLI and Language Server on your system - see the docs page: [Install the dbt VS Code extension](https://docs.getdbt.com/docs/install-dbt-extension).
2. **Install Fusion Directly** Install just the fusion CLI with the command below or see dbt&#039;s documentation: [About Fusion installation](https://docs.getdbt.com/docs/fusion/install-fusion)

``` bash
curl -fsSL https://public.cdn.getdbt.com/fs/install/install.sh | sh -s -- --update
```

3. **Build Fusion from Source** - See the below section: [Compiling from Source](#compiling-from-source)


### Supported Operating Systems and CPU Microarchitectures
Fusion &amp; associated drivers are compiled for each CPU microarchitecture and operating system independently. This allows for hardware level optimization.

Legend:
* üü¢ - Supported today
* üü° - Unsupported today

| Operating System    | X86-64 | ARM  |
|-------------------|----------|------|
| MacOS             |   üü¢     |  üü¢  |
| Linux             |   üü¢     |  üü¢  |
| Windows           |   üü¢     |  üü°  |


## Timeline

| Target Date | Milestone                   | Description                                  |
|-------------|-----------------------------|----------------------------------------------|
| 2025-05-28  | Initial release of Fusion   | Published source code of parser, schemas, dbt-jinja, and Snowflake ADBC driver. |
| 2025-06-09  | Databricks Adapter release  | Databricks ADBC driver, and adapter for Fusion |
| 2025-06-30  | BigQuery Adapter release    | BigQuery ADBC driver, and adapter for Fusion |
| 2025-07-31  | Redshift Adapter release    | Redshift ADBC driver, and adapter for Fusion |
| 2025-08-30  | ANTLR Grammars release + SQL Parser  | The SQL grammar used by the ANTLR parser generator.  |

### Top Level Components Released to Date
Releases of various Fusion components will be iterative as each component reaches maturity &amp; readiness for contribution.

- [x] `dbt-jinja` - All Rust extension of mini-jinja to support dbt&#039;s jinja functions &amp; other capabilities
- [x] `dbt-parser` - Rust parser for dbt projects
- [x] `dbt-snowflake` - database driver
- [x] `dbt-schemas` - complete, correct, machine generated json schemas for dbt&#039;s authoring surface
- [ ] `dbt-sql` - ANTLR grammars and generated parsers
  - [ ] snowflake.g4 
  - [ ] bigquery.g4
  - [ ] redshift.g4
  - [ ] databricks.g4
- [ ] Fusion: the comprehensive dbt fusion engine release.

## FAQ

&lt;details&gt;
  &lt;summary&gt;&lt;i&gt;Can I contribute to the dbt Fusion engine?&lt;/i&gt;&lt;/summary&gt;

  Yes absolutely!. Please see [`CONTRIBUTING.md`](CONTRIBUTING.md) for contribution guidelines
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;i&gt;How is dbt Fusion different from dbt Core?&lt;/i&gt;&lt;/summary&gt;
  The dbt Fusion engine is a ground-up rewrite of dbt Core, with many additional capabilities.
  *Things that are the same:*
  * The YML authoring format including profiles, configuration, seeds, data tests, and unit tests
  * The materialization libraries
  * dbt&#039;s library managemenet system (although `dbt deps` are installed automatically)

  *Additional capabilities provided by Fusion:*
  * All new Arrow Database Connector (ADBC) drivers for faster data transfers and unified connection handling
  * A language server and corresponding VS-Code extension (compatible with Cursor) for ease of development
  * Multi-dialect SQL compilation, validation, &amp; static analysis
  * Standalone distribution. No JVM, or python required. 
  * Automatic installation of dependencies, whether that&#039;s a dbt package, or database driver
  * dbt code-signed &amp; secure distributions
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;i&gt;This repo doesn&#039;t have all of dbt&#039;s functionality, when will the rest come?&lt;/i&gt;&lt;/summary&gt;
  dbt Fusion&#039;s source code is being published as components are finalized. Please see the above section: [Timeline](#timeline).
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;i&gt;Can I use dbt Fusion today?&lt;/i&gt;&lt;/summary&gt;

  | State        | Description                                                                                                                                                         | Workaround                                                                 | Resolvable by |
  |--------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------|---------------|
  | Unblocked    | You can adopt the dbt Fusion engine with no changes to your project                                                                                                 | ---                                                                        | ---           |
  | Soft blocked | Your project contains functionality (for more info: [How to get ready for the new dbt engine](https://www.getdbt.com/blog/how-to-get-ready-for-the-new-dbt-engine). | Resolve deprecations with the dbt-autofix script or workflow in dbt Studio | Users         |
  | Hard blocked | Your project contains Python models or uses a not-yet-supported adapter                                                                                             | Remove unsupported functionality if possible                               | dbt Labs      |&lt;/details&gt;



## Compiling from Source

The primary CLI in this repository is the `dbt-sa-cli`. To compile the CLI, you need the Rust toolchain. 

Let&#039;s start with Rust, run the following command to install Rust on your machine:

Linux:

```shell
sudo ./scripts/setup_dev_env_linux.sh
```

Mac:

```shell
curl --proto &#039;=https&#039; --tlsv1.2 -sSf https://sh.rustup.rs | sh
```

Great! We have Rust installed. To confirm, run the following command:

```shell
cargo -v
```

You should see a printout like:
```
Rust&#039;s package manager

Usage: cargo [+toolchain] [OPTIONS] [COMMAND]
       cargo [+toolchain] [OPTIONS] -Zscript &lt;MANIFEST_RS&gt; [ARGS]...

Options:
  -V, --version                  Print version info and exit
...
```

Cargo is Rust&#039;s build system and package manager. If you&#039;re familiar with Python, pip would be a sufficient comparison. We&#039;ll use cargo to run command to build the local `dbt-sa-cli` binary and run helper scripts via `cargo xtask`. More on that later.

To build the binary locally, `cd` to the this repo&#039;s directory and run:

```shell
cargo build
```

This will compile our Rust code into the `dbt-sa-cli` binary. After this completes, you should see a new executable in `target/debug/dbt-sa-cli`. You can run this executable by passing the path directly into the CLI, so if you&#039;re in the root of this git repo, you can run:

```shell
target/debug/dbt-sa-cli
```

If built correctly, you should see output like:
```shell
&gt; ./target/debug/dbt
Usage: dbt &lt;COMMAND&gt;

Commands:
  parse    Parse models
  ...
```

You might be wondering why it was built into the `debug` directory - this is because our default profile is `debug` when running `cargo build`. Our `debug` profile compiles the code faster, but sacrifices optimizations to do so. Therefore, if you want to benchmark the parser, build with the flag `cargo build --release`. The compile will take longer, but the build will mimic the experience of the end user.

If you expect to use this executable often, we recommend creating an alias for it in your `~/.zshrc`. To do so, start by getting the absolute path to the executable with:

```shell
cd target/debug &amp;&amp; pwd
```

## Running Tests

To run tests, increase the stack size and use nextest.

```
 RUST_MIN_STACK=8388608 cargo nextest run --no-fail-fast
```

# License
The dbt Fusion engine is a monorepo and contains more than one License. Most code is licensed under ELv2. For more, please see [`LICENSES.md`](LICENSES.md).

# Acknowledgments
*To the dbt community:* dbt the tool &amp; dbt Labs the company would not be here without the incredible community of authors, contributors, practitioners, and enthusiasts. dbt Fusion is an evolution of that work &amp; stands on the shoulders of what has come before. 

*To the Arrow Community:* dbt Labs is committing fully to the Arrow ecosystem. Fusion exclusively uses the Arrow type system from drivers through adapters into the internals of the compiler &amp; runtime.

*To the DataFusion Community:* The intermediate representation of the SQL compiler is the DataFusion logical plan which has proven to be pragmatic, extensible, and easy to work with in all the right ways.

Thank you all. dbt, Arrow, and DataFusion have become truly global software projects. dbt Labs is committed to contributing meaningfully to these efforts over the coming months and years.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
    </channel>
</rss>