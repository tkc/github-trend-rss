<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for rust - Rust Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for rust.</description>
        <lastBuildDate>Sat, 12 Jul 2025 00:05:38 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[block/goose]]></title>
            <link>https://github.com/block/goose</link>
            <guid>https://github.com/block/goose</guid>
            <pubDate>Sat, 12 Jul 2025 00:05:38 GMT</pubDate>
            <description><![CDATA[an open source, extensible AI agent that goes beyond code suggestions - install, execute, edit, and test with any LLM]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/block/goose">block/goose</a></h1>
            <p>an open source, extensible AI agent that goes beyond code suggestions - install, execute, edit, and test with any LLM</p>
            <p>Language: Rust</p>
            <p>Stars: 15,943</p>
            <p>Forks: 1,352</p>
            <p>Stars today: 116 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# codename goose

_a local, extensible, open source AI agent that automates engineering tasks_

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://opensource.org/licenses/Apache-2.0&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/License-Apache_2.0-blue.svg&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/7GaTvbDwga&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/discord/1287729918100246654?logo=discord&amp;logoColor=white&amp;label=Join+Us&amp;color=blueviolet&quot; alt=&quot;Discord&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/block/goose/actions/workflows/ci.yml&quot;&gt;
     &lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/block/goose/ci.yml?branch=main&quot; alt=&quot;CI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;

goose is your on-machine AI agent, capable of automating complex development tasks from start to finish. More than just code suggestions, goose can build entire projects from scratch, write and execute code, debug failures, orchestrate workflows, and interact with external APIs - _autonomously_.

Whether you&#039;re prototyping an idea, refining existing code, or managing intricate engineering pipelines, goose adapts to your workflow and executes tasks with precision.

Designed for maximum flexibility, goose works with any LLM and supports multi-model configuration to optimize performance and cost, seamlessly integrates with MCP servers, and is available as both a desktop app as well as CLI - making it the ultimate AI assistant for developers who want to move faster and focus on innovation.

# Quick Links
- [Quickstart](https://block.github.io/goose/docs/quickstart)
- [Installation](https://block.github.io/goose/docs/getting-started/installation)
- [Tutorials](https://block.github.io/goose/docs/category/tutorials)
- [Documentation](https://block.github.io/goose/docs/category/getting-started)


# Goose Around with Us
- [Discord](https://discord.gg/block-opensource)
- [YouTube](https://www.youtube.com/@blockopensource)
- [LinkedIn](https://www.linkedin.com/company/block-opensource)
- [Twitter/X](https://x.com/blockopensource)
- [Bluesky](https://bsky.app/profile/opensource.block.xyz)
- [Nostr](https://njump.me/opensource@block.xyz)
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[pola-rs/polars]]></title>
            <link>https://github.com/pola-rs/polars</link>
            <guid>https://github.com/pola-rs/polars</guid>
            <pubDate>Sat, 12 Jul 2025 00:05:37 GMT</pubDate>
            <description><![CDATA[Dataframes powered by a multithreaded, vectorized query engine, written in Rust]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pola-rs/polars">pola-rs/polars</a></h1>
            <p>Dataframes powered by a multithreaded, vectorized query engine, written in Rust</p>
            <p>Language: Rust</p>
            <p>Stars: 34,383</p>
            <p>Forks: 2,297</p>
            <p>Stars today: 19 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pola.rs&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/pola-rs/polars-static/master/banner/polars_github_banner.svg&quot; alt=&quot;Polars logo&quot;&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://crates.io/crates/polars&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/crates/v/polars.svg&quot; alt=&quot;crates.io Latest Release&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://pypi.org/project/polars/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/polars.svg&quot; alt=&quot;PyPi Latest Release&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.npmjs.com/package/nodejs-polars&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/npm/v/nodejs-polars.svg&quot; alt=&quot;NPM Latest Release&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://rpolars.r-universe.dev&quot;&gt;
    &lt;img src=&quot;https://rpolars.r-universe.dev/badges/polars&quot; alt=&quot;R-universe Latest Release&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://doi.org/10.5281/zenodo.7697217&quot;&gt;
    &lt;img src=&quot;https://zenodo.org/badge/DOI/10.5281/zenodo.7697217.svg&quot; alt=&quot;DOI Latest Release&quot;/&gt;
  &lt;/a&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;b&gt;Documentation&lt;/b&gt;:
  &lt;a href=&quot;https://docs.pola.rs/api/python/stable/reference/index.html&quot;&gt;Python&lt;/a&gt;
  -
  &lt;a href=&quot;https://docs.rs/polars/latest/polars/&quot;&gt;Rust&lt;/a&gt;
  -
  &lt;a href=&quot;https://pola-rs.github.io/nodejs-polars/index.html&quot;&gt;Node.js&lt;/a&gt;
  -
  &lt;a href=&quot;https://pola-rs.github.io/r-polars/index.html&quot;&gt;R&lt;/a&gt;
  |
  &lt;b&gt;StackOverflow&lt;/b&gt;:
  &lt;a href=&quot;https://stackoverflow.com/questions/tagged/python-polars&quot;&gt;Python&lt;/a&gt;
  -
  &lt;a href=&quot;https://stackoverflow.com/questions/tagged/rust-polars&quot;&gt;Rust&lt;/a&gt;
  -
  &lt;a href=&quot;https://stackoverflow.com/questions/tagged/nodejs-polars&quot;&gt;Node.js&lt;/a&gt;
  -
  &lt;a href=&quot;https://stackoverflow.com/questions/tagged/r-polars&quot;&gt;R&lt;/a&gt;
  |
  &lt;a href=&quot;https://docs.pola.rs/&quot;&gt;User guide&lt;/a&gt;
  |
  &lt;a href=&quot;https://discord.gg/4UfP5cfBE7&quot;&gt;Discord&lt;/a&gt;
&lt;/p&gt;

## Polars: Blazingly fast DataFrames in Rust, Python, Node.js, R, and SQL

Polars is a DataFrame interface on top of an OLAP Query Engine implemented in Rust using
[Apache Arrow Columnar Format](https://arrow.apache.org/docs/format/Columnar.html) as the memory
model.

- Lazy | eager execution
- Multi-threaded
- SIMD
- Query optimization
- Powerful expression API
- Hybrid Streaming (larger-than-RAM datasets)
- Rust | Python | NodeJS | R | ...

To learn more, read the [user guide](https://docs.pola.rs/).

## Python

```python
&gt;&gt;&gt; import polars as pl
&gt;&gt;&gt; df = pl.DataFrame(
...     {
...         &quot;A&quot;: [1, 2, 3, 4, 5],
...         &quot;fruits&quot;: [&quot;banana&quot;, &quot;banana&quot;, &quot;apple&quot;, &quot;apple&quot;, &quot;banana&quot;],
...         &quot;B&quot;: [5, 4, 3, 2, 1],
...         &quot;cars&quot;: [&quot;beetle&quot;, &quot;audi&quot;, &quot;beetle&quot;, &quot;beetle&quot;, &quot;beetle&quot;],
...     }
... )

# embarrassingly parallel execution &amp; very expressive query language
&gt;&gt;&gt; df.sort(&quot;fruits&quot;).select(
...     &quot;fruits&quot;,
...     &quot;cars&quot;,
...     pl.lit(&quot;fruits&quot;).alias(&quot;literal_string_fruits&quot;),
...     pl.col(&quot;B&quot;).filter(pl.col(&quot;cars&quot;) == &quot;beetle&quot;).sum(),
...     pl.col(&quot;A&quot;).filter(pl.col(&quot;B&quot;) &gt; 2).sum().over(&quot;cars&quot;).alias(&quot;sum_A_by_cars&quot;),
...     pl.col(&quot;A&quot;).sum().over(&quot;fruits&quot;).alias(&quot;sum_A_by_fruits&quot;),
...     pl.col(&quot;A&quot;).reverse().over(&quot;fruits&quot;).alias(&quot;rev_A_by_fruits&quot;),
...     pl.col(&quot;A&quot;).sort_by(&quot;B&quot;).over(&quot;fruits&quot;).alias(&quot;sort_A_by_B_by_fruits&quot;),
... )
shape: (5, 8)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ fruits   â”† cars     â”† literal_stri â”† B   â”† sum_A_by_ca â”† sum_A_by_fr â”† rev_A_by_fr â”† sort_A_by_B â”‚
â”‚ ---      â”† ---      â”† ng_fruits    â”† --- â”† rs          â”† uits        â”† uits        â”† _by_fruits  â”‚
â”‚ str      â”† str      â”† ---          â”† i64 â”† ---         â”† ---         â”† ---         â”† ---         â”‚
â”‚          â”†          â”† str          â”†     â”† i64         â”† i64         â”† i64         â”† i64         â”‚
â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡
â”‚ &quot;apple&quot;  â”† &quot;beetle&quot; â”† &quot;fruits&quot;     â”† 11  â”† 4           â”† 7           â”† 4           â”† 4           â”‚
â”‚ &quot;apple&quot;  â”† &quot;beetle&quot; â”† &quot;fruits&quot;     â”† 11  â”† 4           â”† 7           â”† 3           â”† 3           â”‚
â”‚ &quot;banana&quot; â”† &quot;beetle&quot; â”† &quot;fruits&quot;     â”† 11  â”† 4           â”† 8           â”† 5           â”† 5           â”‚
â”‚ &quot;banana&quot; â”† &quot;audi&quot;   â”† &quot;fruits&quot;     â”† 11  â”† 2           â”† 8           â”† 2           â”† 2           â”‚
â”‚ &quot;banana&quot; â”† &quot;beetle&quot; â”† &quot;fruits&quot;     â”† 11  â”† 4           â”† 8           â”† 1           â”† 1           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## SQL

```python
&gt;&gt;&gt; df = pl.scan_csv(&quot;docs/assets/data/iris.csv&quot;)
&gt;&gt;&gt; ## OPTION 1
&gt;&gt;&gt; # run SQL queries on frame-level
&gt;&gt;&gt; df.sql(&quot;&quot;&quot;
...	SELECT species,
...	  AVG(sepal_length) AS avg_sepal_length
...	FROM self
...	GROUP BY species
...	&quot;&quot;&quot;).collect()
shape: (3, 2)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ species    â”† avg_sepal_length â”‚
â”‚ ---        â”† ---              â”‚
â”‚ str        â”† f64              â”‚
â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡
â”‚ Virginica  â”† 6.588            â”‚
â”‚ Versicolor â”† 5.936            â”‚
â”‚ Setosa     â”† 5.006            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&gt;&gt;&gt; ## OPTION 2
&gt;&gt;&gt; # use pl.sql() to operate on the global context
&gt;&gt;&gt; df2 = pl.LazyFrame({
...    &quot;species&quot;: [&quot;Setosa&quot;, &quot;Versicolor&quot;, &quot;Virginica&quot;],
...    &quot;blooming_season&quot;: [&quot;Spring&quot;, &quot;Summer&quot;, &quot;Fall&quot;]
...})
&gt;&gt;&gt; pl.sql(&quot;&quot;&quot;
... SELECT df.species,
...     AVG(df.sepal_length) AS avg_sepal_length,
...     df2.blooming_season
... FROM df
... LEFT JOIN df2 ON df.species = df2.species
... GROUP BY df.species, df2.blooming_season
... &quot;&quot;&quot;).collect()
```

SQL commands can also be run directly from your terminal using the Polars CLI:

```bash
# run an inline SQL query
&gt; polars -c &quot;SELECT species, AVG(sepal_length) AS avg_sepal_length, AVG(sepal_width) AS avg_sepal_width FROM read_csv(&#039;docs/assets/data/iris.csv&#039;) GROUP BY species;&quot;

# run interactively
&gt; polars
Polars CLI v0.3.0
Type .help for help.

&gt; SELECT species, AVG(sepal_length) AS avg_sepal_length, AVG(sepal_width) AS avg_sepal_width FROM read_csv(&#039;docs/assets/data/iris.csv&#039;) GROUP BY species;
```

Refer to the [Polars CLI repository](https://github.com/pola-rs/polars-cli) for more information.

## Performance ğŸš€ğŸš€

### Blazingly fast

Polars is very fast. In fact, it is one of the best performing solutions available. See the
[PDS-H benchmarks](https://www.pola.rs/benchmarks.html) results.

### Lightweight

Polars is also very lightweight. It comes with zero required dependencies, and this shows in the
import times:

- polars: 70ms
- numpy: 104ms
- pandas: 520ms

### Handles larger-than-RAM data

If you have data that does not fit into memory, Polars&#039; query engine is able to process your query
(or parts of your query) in a streaming fashion. This drastically reduces memory requirements, so
you might be able to process your 250GB dataset on your laptop. Collect with
`collect(engine=&#039;streaming&#039;)` to run the query streaming. (This might be a little slower, but it is
still very fast!)

## Setup

### Python

Install the latest Polars version with:

```sh
pip install polars
```

We also have a conda package (`conda install -c conda-forge polars`), however pip is the preferred
way to install Polars.

Install Polars with all optional dependencies.

```sh
pip install &#039;polars[all]&#039;
```

You can also install a subset of all optional dependencies.

```sh
pip install &#039;polars[numpy,pandas,pyarrow]&#039;
```

See the [User Guide](https://docs.pola.rs/user-guide/installation/#feature-flags) for more details
on optional dependencies

To see the current Polars version and a full list of its optional dependencies, run:

```python
pl.show_versions()
```

Releases happen quite often (weekly / every few days) at the moment, so updating Polars regularly to
get the latest bugfixes / features might not be a bad idea.

### Rust

You can take latest release from `crates.io`, or if you want to use the latest features /
performance improvements point to the `main` branch of this repo.

```toml
polars = { git = &quot;https://github.com/pola-rs/polars&quot;, rev = &quot;&lt;optional git tag&gt;&quot; }
```

Requires Rust version `&gt;=1.80`.

## Contributing

Want to contribute? Read our [contributing guide](https://docs.pola.rs/development/contributing/).

## Python: compile Polars from source

If you want a bleeding edge release or maximal performance you should compile Polars from source.

This can be done by going through the following steps in sequence:

1. Install the latest [Rust compiler](https://www.rust-lang.org/tools/install)
2. Install [maturin](https://maturin.rs/): `pip install maturin`
3. `cd py-polars` and choose one of the following:
   - `make build`, slow binary with debug assertions and symbols, fast compile times
   - `make build-release`, fast binary without debug assertions, minimal debug symbols, long compile
     times
   - `make build-nodebug-release`, same as build-release but without any debug symbols, slightly
     faster to compile
   - `make build-debug-release`, same as build-release but with full debug symbols, slightly slower
     to compile
   - `make build-dist-release`, fastest binary, extreme compile times

By default the binary is compiled with optimizations turned on for a modern CPU. Specify `LTS_CPU=1`
with the command if your CPU is older and does not support e.g. AVX2.

Note that the Rust crate implementing the Python bindings is called `py-polars` to distinguish from
the wrapped Rust crate `polars` itself. However, both the Python package and the Python module are
named `polars`, so you can `pip install polars` and `import polars`.

## Using custom Rust functions in Python

Extending Polars with UDFs compiled in Rust is easy. We expose PyO3 extensions for `DataFrame` and
`Series` data structures. See more in https://github.com/pola-rs/pyo3-polars.

## Going big...

Do you expect more than 2^32 (~4.2 billion) rows? Compile Polars with the `bigidx` feature flag or,
for Python users, install `pip install polars-u64-idx`.

Don&#039;t use this unless you hit the row boundary as the default build of Polars is faster and consumes
less memory.

## Legacy

Do you want Polars to run on an old CPU (e.g. dating from before 2011), or on an `x86-64` build of
Python on Apple Silicon under Rosetta? Install `pip install polars-lts-cpu`. This version of Polars
is compiled without [AVX](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions) target features.

## Sponsors

[&lt;img src=&quot;https://www.jetbrains.com/company/brand/img/jetbrains_logo.png&quot; height=&quot;50&quot; alt=&quot;JetBrains logo&quot; /&gt;](https://www.jetbrains.com)
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[cloudflare/pingora]]></title>
            <link>https://github.com/cloudflare/pingora</link>
            <guid>https://github.com/cloudflare/pingora</guid>
            <pubDate>Sat, 12 Jul 2025 00:05:36 GMT</pubDate>
            <description><![CDATA[A library for building fast, reliable and evolvable network services.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/cloudflare/pingora">cloudflare/pingora</a></h1>
            <p>A library for building fast, reliable and evolvable network services.</p>
            <p>Language: Rust</p>
            <p>Stars: 24,601</p>
            <p>Forks: 1,432</p>
            <p>Stars today: 25 stars today</p>
            <h2>README</h2><pre># Pingora

![Pingora banner image](./docs/assets/pingora_banner.png)

## What is Pingora
Pingora is a Rust framework to [build fast, reliable and programmable networked systems](https://blog.cloudflare.com/pingora-open-source).

Pingora is battle tested as it has been serving more than 40 million Internet requests per second for [more than a few years](https://blog.cloudflare.com/how-we-built-pingora-the-proxy-that-connects-cloudflare-to-the-internet).

## Feature highlights
* Async Rust: fast and reliable
* HTTP 1/2 end to end proxy
* TLS over OpenSSL, BoringSSL or rustls(experimental).
* gRPC and websocket proxying
* Graceful reload
* Customizable load balancing and failover strategies
* Support for a variety of observability tools

## Reasons to use Pingora
* **Security** is your top priority: Pingora is a more memory safe alternative for services that are written in C/C++
* Your service is **performance-sensitive**: Pingora is fast and efficient
* Your service requires extensive **customization**: The APIs Pingora proxy framework provides are highly programmable

# Getting started

See our [quick starting guide](./docs/quick_start.md) to see how easy it is to build a load balancer.

Our [user guide](./docs/user_guide/index.md) covers more topics such as how to configure and run Pingora servers, as well as how to build custom HTTP servers and proxy logic on top of Pingora&#039;s framework.

API docs are also available for all the crates.

# Notable crates in this workspace
* Pingora: the &quot;public facing&quot; crate to build networked systems and proxies
* Pingora-core: this crate defines the protocols, functionalities and basic traits
* Pingora-proxy: the logic and APIs to build HTTP proxies
* Pingora-error: the common error type used across Pingora crates
* Pingora-http: the HTTP header definitions and APIs
* Pingora-openssl &amp; pingora-boringssl: SSL related extensions and APIs
* Pingora-ketama: the [Ketama](https://github.com/RJ/ketama) consistent algorithm
* Pingora-limits: efficient counting algorithms
* Pingora-load-balancing: load balancing algorithm extensions for pingora-proxy
* Pingora-memory-cache: Async in-memory caching with cache lock to prevent cache stampede
* Pingora-timeout: A more efficient async timer system
* TinyUfo: The caching algorithm behind pingora-memory-cache

Note that Pingora proxy integration with caching should be considered experimental, and as such APIs related to caching are currently highly volatile.

# System requirements

## Systems
Linux is our tier 1 environment and main focus.

We will try our best for most code to compile for Unix environments. This is for developers and users to have an easier time developing with Pingora in Unix-like environments like macOS (though some features might be missing)

Windows support is preliminary by community&#039;s best effort only.

Both x86_64 and aarch64 architectures will be supported.

## Rust version

Pingora keeps a rolling MSRV (minimum supported Rust version) policy of 6 months. This means we will accept PRs that upgrade the MSRV as long as the new Rust version used is at least 6 months old.

Our current MSRV is effectively 1.82.

Previously Pingora advertised an MSRV of 1.72. Older Rust versions may still be able to compile via `cargo update` pinning dependencies such as `backtrace@0.3.74`. The advertised MSRV in config files will be officially bumped to 1.82 in an upcoming release.

Building with the optional feature `boringssl` with Boring &gt;= 4.14 requires Rust 1.80.

## Build Requirements

Some of the crates in this repository have dependencies on additional tools and
libraries that must be satisfied in order to build them:

* Make sure that [Clang] is installed on your system (for boringssl)
* Make sure that [Perl 5] is installed on your system (for openssl)

[Clang]:https://clang.llvm.org/
[Perl 5]:https://www.perl.org/

# Contributing
Please see our [contribution guidelines](./.github/CONTRIBUTING.md).

# License
This project is Licensed under [Apache License, Version 2.0](./LICENSE).
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[emilk/egui]]></title>
            <link>https://github.com/emilk/egui</link>
            <guid>https://github.com/emilk/egui</guid>
            <pubDate>Sat, 12 Jul 2025 00:05:35 GMT</pubDate>
            <description><![CDATA[egui: an easy-to-use immediate mode GUI in Rust that runs on both web and native]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/emilk/egui">emilk/egui</a></h1>
            <p>egui: an easy-to-use immediate mode GUI in Rust that runs on both web and native</p>
            <p>Language: Rust</p>
            <p>Stars: 25,726</p>
            <p>Forks: 1,780</p>
            <p>Stars today: 42 stars today</p>
            <h2>README</h2><pre># ğŸ–Œ egui: an easy-to-use GUI in pure Rust

[&lt;img alt=&quot;github&quot; src=&quot;https://img.shields.io/badge/github-emilk/egui-8da0cb?logo=github&quot; height=&quot;20&quot;&gt;](https://github.com/emilk/egui)
[![Latest version](https://img.shields.io/crates/v/egui.svg)](https://crates.io/crates/egui)
[![Documentation](https://docs.rs/egui/badge.svg)](https://docs.rs/egui)
[![unsafe forbidden](https://img.shields.io/badge/unsafe-forbidden-success.svg)](https://github.com/rust-secure-code/safety-dance/)
[![Build Status](https://github.com/emilk/egui/workflows/Rust/badge.svg)](https://github.com/emilk/egui/actions/workflows/rust.yml)
[![MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/emilk/egui/blob/main/LICENSE-MIT)
[![Apache](https://img.shields.io/badge/license-Apache-blue.svg)](https://github.com/emilk/egui/blob/main/LICENSE-APACHE)
[![Discord](https://img.shields.io/discord/900275882684477440?label=egui%20discord)](https://discord.gg/JFcEma9bJq)


&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://www.rerun.io/&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/78e79463-4357-461b-bbd1-31aa5ef5e1a2&quot; width=&quot;250&quot;&gt;&lt;/a&gt;

egui development is sponsored by [Rerun](https://www.rerun.io/), a startup building&lt;br&gt;
an SDK for visualizing streams of multimodal data.
&lt;/div&gt;

---

ğŸ‘‰ [Click to run the web demo](https://www.egui.rs/#demo) ğŸ‘ˆ

egui (pronounced &quot;e-gooey&quot;) is a simple, fast, and highly portable immediate mode GUI library for Rust. egui runs on the web, natively, and [in your favorite game engine](#integrations).

egui aims to be the easiest-to-use Rust GUI library, and the simplest way to make a web app in Rust.

egui can be used anywhere you can draw textured triangles, which means you can easily integrate it into your game engine of choice.

[`eframe`](https://github.com/emilk/egui/tree/main/crates/eframe) is the official egui framework, which supports writing apps for Web, Linux, Mac, Windows, and Android.


## Example

``` rust
ui.heading(&quot;My egui Application&quot;);
ui.horizontal(|ui| {
    ui.label(&quot;Your name: &quot;);
    ui.text_edit_singleline(&amp;mut name);
});
ui.add(egui::Slider::new(&amp;mut age, 0..=120).text(&quot;age&quot;));
if ui.button(&quot;Increment&quot;).clicked() {
    age += 1;
}
ui.label(format!(&quot;Hello &#039;{name}&#039;, age {age}&quot;));
ui.image(egui::include_image!(&quot;ferris.png&quot;));
```

&lt;img alt=&quot;Dark mode&quot; src=&quot;https://github.com/user-attachments/assets/3b446d29-99d8-4c82-86bb-4d8ef0516017&quot;&gt; &amp;nbsp; &amp;nbsp; &lt;img alt=&quot;Light mode&quot; src=&quot;https://github.com/user-attachments/assets/a5e7da93-89a8-4ba0-86b8-0fa2228a4f62&quot; height=&quot;278&quot;&gt;

## Sections:

* [Example](#example)
* [Quick start](#quick-start)
* [Demo](#demo)
* [Goals](#goals)
* [State / features](#state)
* [Dependencies](#dependencies)
* [Who is egui for?](#who-is-egui-for)
* [Integrations](#integrations)
* [Why immediate mode](#why-immediate-mode)
* [FAQ](#faq)
* [Other](#other)
* [Credits](#credits)

([egui çš„ä¸­æ–‡ç¿»è¯‘æ–‡æ¡£ / chinese translation](https://github.com/Re-Ch-Love/egui-doc-cn/blob/main/README_zh-hans.md))


## Quick start

There are simple examples in [the `examples/` folder](https://github.com/emilk/egui/blob/main/examples/). If you want to write a web app, then go to &lt;https://github.com/emilk/eframe_template/&gt; and follow the instructions. The official docs are at &lt;https://docs.rs/egui&gt;. For inspiration and more examples, check out the [the egui web demo](https://www.egui.rs/#demo) and follow the links in it to its source code.

If you want to integrate egui into an existing engine, go to the [Integrations](#integrations) section.

If you have questions, use [GitHub Discussions](https://github.com/emilk/egui/discussions). There is also [an egui discord server](https://discord.gg/JFcEma9bJq). If you want to contribute to egui, please read the [Contributing Guidelines](https://github.com/emilk/egui/blob/main/CONTRIBUTING.md).

## Demo

[Click to run egui web demo](https://www.egui.rs/#demo) (works in any browser with Wasm and WebGL support). Uses [`eframe`](https://github.com/emilk/egui/tree/main/crates/eframe).

To test the demo app locally, run `cargo run --release -p egui_demo_app`.

The native backend is [`egui_glow`](https://github.com/emilk/egui/tree/main/crates/egui_glow) (using [`glow`](https://crates.io/crates/glow)) and should work out-of-the-box on Mac and Windows, but on Linux you need to first run:

`sudo apt-get install -y libclang-dev libgtk-3-dev libxcb-render0-dev libxcb-shape0-dev libxcb-xfixes0-dev libxkbcommon-dev libssl-dev`

On Fedora Rawhide you need to run:

`dnf install clang clang-devel clang-tools-extra libxkbcommon-devel pkg-config openssl-devel libxcb-devel gtk3-devel atk fontconfig-devel`

**NOTE**: This is just for the demo app - egui itself is completely platform agnostic!

## Goals

* The easiest to use GUI library
* Responsive: target 60 Hz in debug build
* Friendly: difficult to make mistakes, and shouldn&#039;t panic
* Portable: the same code works on the web and as a native app
* Easy to integrate into any environment
* A simple 2D graphics API for custom painting ([`epaint`](https://docs.rs/epaint)).
* Pure immediate mode: no callbacks
* Extensible: [easy to write your own widgets for egui](https://github.com/emilk/egui/blob/main/crates/egui_demo_lib/src/demo/toggle_switch.rs)
* Modular: You should be able to use small parts of egui and combine them in new ways
* Safe: there is no `unsafe` code in egui
* Minimal dependencies

egui is *not* a framework. egui is a library you call into, not an environment you program for.

**NOTE**: egui does not claim to have reached all these goals yet! egui is still work in progress.

### Non-goals

* Become the most powerful GUI library
* Native looking interface

## State

egui is in active development. It works well for what it does, but it lacks many features and the interfaces are still in flux. New releases will have breaking changes.

Still, egui can be used to create professional looking applications, like [the Rerun Viewer](https://app.rerun.io/).

### Features

* Widgets: label, text button, hyperlink, checkbox, radio button, slider, draggable value, text editing, color picker, spinner
* Images
* Layouts: horizontal, vertical, columns, automatic wrapping
* Text editing: multiline, copy/paste, undo, emoji supports
* Windows: move, resize, name, minimize and close. Automatically sized and positioned.
* Regions: resizing, vertical scrolling, collapsing headers (sections), panels
* Rendering: Anti-aliased rendering of lines, circles, text and convex polygons.
* Tooltips on hover
* Accessibility via [AccessKit](https://accesskit.dev/)
* Label text selection
* And more!

Check out the [3rd party egui crates wiki](https://github.com/emilk/egui/wiki/3rd-party-egui-crates) for even more
widgets and features, maintained by the community.

&lt;img src=&quot;https://github.com/user-attachments/assets/13e73b76-e456-42bd-8ec9-220802834268&quot; width=&quot;50%&quot;&gt;

Light Theme:

&lt;img src=&quot;https://github.com/user-attachments/assets/2e38972c-a444-4894-b32f-47a2719cf369&quot; width=&quot;50%&quot;&gt;

## Dependencies
`egui` has a minimal set of default dependencies:

* [`ab_glyph`](https://crates.io/crates/ab_glyph)
* [`ahash`](https://crates.io/crates/ahash)
* [`bitflags`](https://crates.io/crates/bitflags)
* [`nohash-hasher`](https://crates.io/crates/nohash-hasher)
* [`parking_lot`](https://crates.io/crates/parking_lot)

Heavier dependencies are kept out of `egui`, even as opt-in.
All code in `egui` is Wasm-friendly (even outside a browser).

To load images into `egui` you can use the official [`egui_extras`](https://github.com/emilk/egui/tree/main/crates/egui_extras) crate.

[`eframe`](https://github.com/emilk/egui/tree/main/crates/eframe) on the other hand has a lot of dependencies, including [`winit`](https://crates.io/crates/winit), [`image`](https://crates.io/crates/image), graphics crates, clipboard crates, etc,

## Who is egui for?

egui aims to be the best choice when you want a simple way to create a GUI, or you want to add a GUI to a game engine.

If you are not using Rust, egui is not for you. If you want a GUI that looks native, egui is not for you. If you want something that doesn&#039;t break when you upgrade it, egui isn&#039;t for you (yet).

But if you are writing something interactive in Rust that needs a simple GUI, egui may be for you.


## Integrations

egui is built to be easy to integrate into any existing game engine or platform you are working on.
egui itself doesn&#039;t know or care on what OS it is running or how to render things to the screen - that is the job of the egui integration.

An integration needs to do the following each frame:

* **Input**: Gather input (mouse, touches, keyboard, screen size, etc) and give it to egui
* Call into the application GUI code
* **Output**: Handle egui output (cursor changes, paste, texture allocations, â€¦)
* **Painting**: Render the triangle mesh egui produces (see [OpenGL example](https://github.com/emilk/egui/blob/main/crates/egui_glow/src/painter.rs))

### Official integrations

These are the official egui integrations:

* [`eframe`](https://github.com/emilk/egui/tree/main/crates/eframe) for compiling the same app to web/wasm and desktop/native. Uses `egui-winit` and `egui_glow` or `egui-wgpu`
* [`egui_glow`](https://github.com/emilk/egui/tree/main/crates/egui_glow) for rendering egui with [glow](https://github.com/grovesNL/glow) on native and web, and for making native apps
* [`egui-wgpu`](https://github.com/emilk/egui/tree/main/crates/egui-wgpu) for [wgpu](https://crates.io/crates/wgpu) (WebGPU API)
* [`egui-winit`](https://github.com/emilk/egui/tree/main/crates/egui-winit) for integrating with [winit](https://github.com/rust-windowing/winit)

### 3rd party integrations

Check the wiki to find [3rd party integrations](https://github.com/emilk/egui/wiki/3rd-party-integrations)
and [egui crates](https://github.com/emilk/egui/wiki/3rd-party-egui-crates).

### Writing your own egui integration
Missing an integration for the thing you&#039;re working on? Create one, it&#039;s easy!
See &lt;https://docs.rs/egui/latest/egui/#integrating-with-egui&gt;.


## Why immediate mode

`egui` is an [immediate mode GUI library](https://en.wikipedia.org/wiki/Immediate_mode_GUI), as opposed to a *retained mode* GUI library. The difference between retained mode and immediate mode is best illustrated with the example of a button: In a retained GUI you create a button, add it to some UI and install some on-click handler (callback). The button is retained in the UI, and to change the text on it you need to store some sort of reference to it. By contrast, in immediate mode you show the button and interact with it immediately, and you do so every frame (e.g. 60 times per second). This means there is no need for any on-click handler, nor to store any reference to it. In `egui` this looks like this: `if ui.button(&quot;Save file&quot;).clicked() { save(file); }`.

A more detailed description of immediate mode can be found [in the `egui` docs](https://docs.rs/egui/latest/egui/#understanding-immediate-mode).

There are advantages and disadvantages to both systems.

The short of it is this: immediate mode GUI libraries are easier to use, but less powerful.

### Advantages of immediate mode
#### Usability
The main advantage of immediate mode is that the application code becomes vastly simpler:

* You never need to have any on-click handlers and callbacks that disrupts your code flow.
* You don&#039;t have to worry about a lingering callback calling something that is gone.
* Your GUI code can easily live in a simple function (no need for an object just for the UI).
* You don&#039;t have to worry about app state and GUI state being out-of-sync (i.e. the GUI showing something outdated), because the GUI isn&#039;t storing any state - it is showing the latest state *immediately*.

In other words, a whole lot of code, complexity and bugs are gone, and you can focus your time on something more interesting than writing GUI code.

### Disadvantages of immediate mode

#### Layout
The main disadvantage of immediate mode is it makes layout more difficult. Say you want to show a small dialog window in the center of the screen. To position the window correctly the GUI library must first know the size of it. To know the size of the window the GUI library must first layout the contents of the window. In retained mode this is easy: the GUI library does the window layout, positions the window, then checks for interaction (&quot;was the OK button clicked?&quot;).

In immediate mode you run into a paradox: to know the size of the window, we must do the layout, but the layout code also checks for interaction (&quot;was the OK button clicked?&quot;) and so it needs to know the window position *before* showing the window contents. This means we must decide where to show the window *before* we know its size!

This is a fundamental shortcoming of immediate mode GUIs, and any attempt to resolve it comes with its own downsides.

One workaround is to store the size and use it the next frame. This produces a frame-delay for the correct layout, producing occasional flickering the first frame something shows up. `egui` does this for some things such as windows and grid layouts.

The &quot;first-frame jitter&quot; can be covered up with an extra _pass_, which egui supports via `Context::request_discard`.
The downside of this is the added CPU cost of a second pass, so egui only does this in very rare circumstances (the majority of frames are single-pass).

For &quot;atomic&quot; widgets (e.g. a button) `egui` knows the size before showing it, so centering buttons, labels etc is possible in `egui` without any special workarounds.

See [this issue](https://github.com/emilk/egui/issues/4378) for more.

#### CPU usage
Since an immediate mode GUI does a full layout each frame, the layout code needs to be quick. If you have a very complex GUI this can tax the CPU. In particular, having a very large UI in a scroll area (with very long scrollback) can be slow, as the content needs to be laid out each frame.

If you design the GUI with this in mind and refrain from huge scroll areas (or only lay out the part that is in view) then the performance hit is generally pretty small. For most cases you can expect `egui` to take up 1-2 ms per frame, but `egui` still has a lot of room for optimization (it&#039;s not something I&#039;ve focused on yet). `egui` only repaints when there is interaction (e.g. mouse movement) or an animation, so if your app is idle, no CPU is wasted.

If your GUI is highly interactive, then immediate mode may actually be more performant compared to retained mode. Go to any web page and resize the browser window, and you&#039;ll notice that the browser is very slow to do the layout and eats a lot of CPU doing it. Resize a window in `egui` by contrast, and you&#039;ll get smooth 60 FPS at no extra CPU cost.


#### IDs
There are some GUI state that you want the GUI library to retain, even in an immediate mode library such as `egui`. This includes position and sizes of windows and how far the user has scrolled in some UI. In these cases you need to provide `egui` with a seed of a unique identifier (unique within the parent UI). For instance: by default `egui` uses the window titles as unique IDs to store window positions. If you want two windows with the same name (or one window with a dynamic name) you must provide some other ID source to `egui` (some unique integer or string).

`egui` also needs to track which widget is being interacted with (e.g. which slider is being dragged). `egui` uses unique IDs for this as well, but in this case the IDs are automatically generated, so there is no need for the user to worry about it. In particular, having two buttons with the same name is no problem (this is in contrast with [`Dear ImGui`](https://github.com/ocornut/imgui)).

Overall, ID handling is a rare inconvenience, and not a big disadvantage.


## FAQ

Also see [GitHub Discussions](https://github.com/emilk/egui/discussions/categories/q-a).

### Can I use `egui` with non-latin characters?
Yes! But you need to install your own font (`.ttf` or `.otf`) using [`Context::set_fonts`](https://docs.rs/egui/latest/egui/struct.Context.html#method.set_fonts).

### Can I customize the look of egui?
Yes! You can customize the colors, spacing, fonts and sizes of everything using `Context::set_style`.

This is not yet as powerful as say CSS, [but this is going to improve](https://github.com/emilk/egui/issues/3284).

Here is an example (from https://github.com/a-liashenko/TinyPomodoro):

&lt;img src=&quot;https://github.com/user-attachments/assets/e6107237-2547-41d6-996b-9a20ae0345ab&quot; width=&quot;50%&quot;&gt;

### How do I use egui with `async`?
If you call `.await` in your GUI code, the UI will freeze, which is very bad UX. Instead, keep the GUI thread non-blocking and communicate with any concurrent tasks (`async` tasks or other threads) with something like:
* Channels (e.g. [`std::sync::mpsc::channel`](https://doc.rust-lang.org/std/sync/mpsc/fn.channel.html)). Make sure to use [`try_recv`](https://doc.rust-lang.org/std/sync/mpsc/struct.Receiver.html#method.try_recv) so you don&#039;t block the gui thread!
* `Arc&lt;Mutex&lt;Value&gt;&gt;` (background thread sets a value; GUI thread reads it)
* [`poll_promise::Promise`](https://docs.rs/poll-promise)
* [`eventuals::Eventual`](https://docs.rs/eventuals/latest/eventuals/struct.Eventual.html)
* [`tokio::sync::watch::channel`](https://docs.rs/tokio/latest/tokio/sync/watch/fn.channel.html)

### How do I create a file dialog?

The async version of [rfd](https://docs.rs/rfd/latest/rfd/) supports both native and Wasm. See example app here https://github.com/woelper/egui_pick_file which also has a demo available via [gitub pages](https://woelper.github.io/egui_pick_file/).

### What about accessibility, such as screen readers?
egui includes optional support for [AccessKit](https://accesskit.dev/), which currently implements the native accessibility APIs on Windows and macOS. This feature is enabled by default in eframe. For platforms that AccessKit doesn&#039;t yet support, including web, there is an experimental built-in screen reader; in [the web demo](https://www.egui.rs/#demo) you can enable it in the &quot;Backend&quot; tab.

The original discussion of accessibility in egui is at &lt;https://github.com/emilk/egui/issues/167&gt;. Now that AccessKit support is merged, providing a strong foundation for future accessibility work, please open new issues on specific accessibility problems.

### What is the difference between [egui](https://docs.rs/egui) and [eframe](https://github.com/emilk/egui/tree/main/crates/eframe)?

`egui` is a 2D user interface library for laying out and interacting with buttons, sliders, etc.
`egui` has no idea if it is running on the web or natively, and does not know how to collect input or show things on screen.
That is the job of *the integration* or *backend*.

It is common to use `egui` from a game engine (using e.g. [`bevy_egui`](https://docs.rs/bevy_egui)),
but you can also use `egui` stand-alone using `eframe`. `eframe` has integration for web and native, and handles input and rendering.
The _frame_ in `eframe` stands both for the frame in which your egui app resides and also for &quot;framework&quot; (`eframe` is a framework, `egui` is a library).

### How do I render 3D stuff in an egui area?
There are multiple ways to combine egui with 3D. The simplest way is to use a 3D library and have egui sit on top of the 3D view. See for instance [`bevy_egui`](https://github.com/mvlabat/bevy_egui) or [`three-d`](https://github.com/asny/three-d).

If you want to embed 3D into an egui view there are two options:

#### `Shape::Callback`
Example:
* &lt;https://github.com/emilk/egui/blob/main/examples/custom_3d_glow/src/main.rs&gt;

`Shape::Callback` will call your code when egui gets painted, to show anything using whatever the background rendering context is. When using [`eframe`](https://github.com/emilk/egui/tree/main/crates/eframe) this will be [`glow`](https://github.com/grovesNL/glow). Other integrations will give you other rendering contexts, if they support `Shape::Callback` at all.

#### Re

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[anza-xyz/pinocchio]]></title>
            <link>https://github.com/anza-xyz/pinocchio</link>
            <guid>https://github.com/anza-xyz/pinocchio</guid>
            <pubDate>Sat, 12 Jul 2025 00:05:34 GMT</pubDate>
            <description><![CDATA[Create Solana programs with no dependencies attached]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/anza-xyz/pinocchio">anza-xyz/pinocchio</a></h1>
            <p>Create Solana programs with no dependencies attached</p>
            <p>Language: Rust</p>
            <p>Stars: 508</p>
            <p>Forks: 114</p>
            <p>Stars today: 5 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
  &lt;code&gt;pinocchio&lt;/code&gt;
&lt;/h1&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;400&quot; alt=&quot;Limestone&quot; src=&quot;https://github.com/user-attachments/assets/3a1894b4-403f-4c35-90aa-548e7672fe90&quot; /&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  Create Solana programs with no dependencies attached.
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/anza-xyz/pinocchio/actions/workflows/main.yml&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/anza-xyz/pinocchio/main.yml?logo=GitHub&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://crates.io/crates/pinocchio&quot;&gt;&lt;img src=&quot;https://img.shields.io/crates/v/pinocchio?logo=rust&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;right&quot;&gt;
&lt;i&gt;I&#039;ve got no dependencies&lt;/i&gt;&lt;br /&gt;
&lt;i&gt;To hold me down&lt;/i&gt;&lt;br /&gt;
&lt;i&gt;To make me fret&lt;/i&gt;&lt;br /&gt;
&lt;i&gt;Or make me frown&lt;/i&gt;&lt;br /&gt;
&lt;i&gt;I had dependencies&lt;/i&gt;&lt;br /&gt;
&lt;i&gt;But now I&#039;m free&lt;/i&gt;&lt;br /&gt;
&lt;i&gt;There are no dependencies on me&lt;/i&gt;
&lt;/p&gt;

## Overview

Pinocchio is a zero-dependency library to create Solana programs in Rust. It takes advantage of the way SVM loaders serialize the program input parameters into a byte array that is then passed to the program&#039;s entrypoint to define zero-copy types to read the input. Since the communication between a program and SVM loader &amp;mdash; either at the first time the program is called or when one program invokes the instructions of another program &amp;mdash; is done via a byte array, a program can define its own types. This completely eliminates the dependency on the `solana-program` crate, which in turn mitigates dependency issues by having a crate specifically designed to create on-chain programs.

As a result, Pinocchio can be used as a replacement for [`solana-program`](https://crates.io/crates/solana-program) to write on-chain programs, which are optimized in terms of both compute units consumption and binary size.

The library defines:
* program entrypoint
* core data types
* logging macros
* `syscall` functions
* access to system accounts (`sysvars`)
* cross-program invocation

## Features

* Zero dependencies and `no_std` crate
* Efficient `entrypoint!` macro â€“ no copies or allocations
* Improved CU consumption of cross-program invocations

## Getting started

From your project folder:

```bash
cargo add pinocchio
```

This will add `pinocchio` as a dependency to your project.

## Defining the program entrypoint

A Solana program needs to define an entrypoint, which will be called by the runtime to begin the program execution. The `entrypoint!` macro emits the common boilerplate to set up the program entrypoint. The macro will also set up [global allocator](https://doc.rust-lang.org/stable/core/alloc/trait.GlobalAlloc.html) and [custom panic hook](https://github.com/anza-xyz/rust/blob/2830febbc59d44bdd7ad2c3b81731f1d08b96eba/library/std/src/sys/pal/sbf/mod.rs#L49) using the [default_allocator!](https://docs.rs/pinocchio/latest/pinocchio/macro.default_allocator.html) and [default_panic_handler!](https://docs.rs/pinocchio/latest/pinocchio/macro.default_panic_handler.html) macros.

The [`entrypoint!`](https://docs.rs/pinocchio/latest/pinocchio/macro.entrypoint.html) is a convenience macro that invokes three other macros to set all symbols required for a program execution:

* [`program_entrypoint!`](https://docs.rs/pinocchio/latest/pinocchio/macro.program_entrypoint.html): declares the program entrypoint
* [`default_allocator!`](https://docs.rs/pinocchio/latest/pinocchio/macro.default_allocator.html): declares the default (bump) global allocator
* [`default_panic_handler!`](https://docs.rs/pinocchio/latest/pinocchio/macro.default_panic_handler.html): declares the default panic handler

If all dependencies are `no_std`, you should append [`nostd_panic_handler!`](https://docs.rs/pinocchio/latest/pinocchio/macro.nostd_panic_handler.html) to declare a rust runtime panic handler. There&#039;s no need to do this if any dependency is `std` since rust compiler will emit std panic handler.

To use the `entrypoint!` macro, use the following in your entrypoint definition:
```rust
use pinocchio::{
  account_info::AccountInfo,
  entrypoint,
  msg,
  ProgramResult,
  pubkey::Pubkey
};

entrypoint!(process_instruction);

pub fn process_instruction(
  program_id: &amp;Pubkey,
  accounts: &amp;[AccountInfo],
  instruction_data: &amp;[u8],
) -&gt; ProgramResult {
  msg!(&quot;Hello from my program!&quot;);
  Ok(())
}
```

The information from the input is parsed into their own entities:

* `program_id`: the `ID` of the program being called
* `accounts`: the accounts received
* `instruction_data`: data for the instruction

`pinocchio` also offers variations of the program entrypoint (`lazy_program_allocator`) and global allocator (`no_allocator`). In order to use these, the program needs to specify the program entrypoint, global allocator and panic handler individually. The `entrypoint!` macro is equivalent to writing:
```rust
program_entrypoint!(process_instruction);
default_allocator!();
default_panic_handler!();
```
Any of these macros can be replaced by other implementations and `pinocchio` offers a couple of variants for this.

ğŸ“Œ [`lazy_program_entrypoint!`](https://docs.rs/pinocchio/latest/pinocchio/macro.lazy_program_entrypoint.html)

The `entrypoint!` macro looks similar to the &quot;standard&quot; one found in `solana-program`. It parses the whole input and provides the `program_id`, `accounts` and `instruction_data` separately. This consumes compute units before the program begins its execution. In some cases, it is beneficial for a program to have more control when the input parsing is happening, even whether the parsing is needed or not &amp;mdash; this is the purpose of the [`lazy_program_entrypoint!`](https://docs.rs/pinocchio/latest/pinocchio/macro.lazy_program_entrypoint.html) macro. This macro only wraps the program input and provides methods to parse the input on demand.

The `lazy_entrypoint` is suitable for programs that have a single or very few instructions, since it requires the program to handle the parsing, which can become complex as the number of instructions increases. For *larger* programs, the [`program_entrypoint!`](https://docs.rs/pinocchio/latest/pinocchio/macro.program_entrypoint.html) will likely be easier and more efficient to use.

To use the `lazy_program_entrypoint!` macro, use the following in your entrypoint definition:
```rust
use pinocchio::{
  default_allocator,
  default_panic_handler,
  entrypoint::InstructionContext,
  lazy_program_entrypoint,
  msg,
  ProgramResult
};

lazy_program_entrypoint!(process_instruction);
default_allocator!();
default_panic_handler!();

pub fn process_instruction(
  mut context: InstructionContext
) -&gt; ProgramResult {
    msg!(&quot;Hello from my lazy program!&quot;);
    Ok(())
}
```

The `InstructionContext` provides on-demand access to the information of the input:

* `remaining()`: number of remaining accounts to be parsed
* `next_account()`: parsers the next available account (can be used as many times as accounts remaining)
* `instruction_data()`: parsers the instruction data
* `program_id()`: parsers the program id

&gt; âš ï¸ **Note:**
&gt; The `lazy_program_entrypoint!` does not set up a global allocator nor a panic handler. A program should explicitly use one of the provided macros to set them up or include its own implementation.

ğŸ“Œ [`no_allocator!`](https://docs.rs/pinocchio/latest/pinocchio/macro.no_allocator.html)

When writing programs, it can be useful to make sure the program does not attempt to make any allocations. For this cases, `pinocchio` includes a [`no_allocator!`](https://docs.rs/pinocchio/latest/pinocchio/macro.no_allocator.html) macro that sets a global allocator just panics at any attempt to allocate memory.

To use the `no_allocator!` macro, use the following in your entrypoint definition:
```rust
use pinocchio::{
  account_info::AccountInfo,
  default_panic_handler,
  msg,
  no_allocator,
  program_entrypoint,
  ProgramResult,
  pubkey::Pubkey
};

program_entrypoint!(process_instruction);
default_panic_handler!();
no_allocator!();

pub fn process_instruction(
  program_id: &amp;Pubkey,
  accounts: &amp;[AccountInfo],
  instruction_data: &amp;[u8],
) -&gt; ProgramResult {
  msg!(&quot;Hello from `no_std` program!&quot;);
  Ok(())
}
```
&gt; âš ï¸ **Note:**
&gt; The `no_allocator!` macro can also be used in combination with the `lazy_program_entrypoint!`.

## Crate feature: `std`

By default, `pinocchio` is a `no_std` crate. This means that it does not use any code from the standard (`std`) library. While this does not affect how `pinocchio` is used, there is one particular apparent difference. In a `no_std` environment, the `msg!` macro does not provide any formatting options since the `format!` macro requires the `std` library. In order to use `msg!` with formatting, the `std` feature should be enabled when adding `pinocchio` as a dependency:
```
pinocchio = { version = &quot;0.7.0&quot;, features = [&quot;std&quot;] }
```

Instead of enabling the `std` feature to be able to format log messages with `msg!`, it is recommended to use the [`pinocchio-log`](https://crates.io/crates/pinocchio-log) crate. This crate provides a lightweight `log!` macro with better compute units consumption than the standard `format!` macro without requiring the `std` library.

## Advance entrypoint configuration

The symbols emitted by the entrypoint macros &amp;mdash; program entrypoint, global allocator and default panic handler &amp;mdash; can only be defined once globally. If the program crate is also intended to be used as a library, it is common practice to define a Cargo [feature](https://doc.rust-lang.org/cargo/reference/features.html) in your program crate to conditionally enable the module that includes the `entrypoint!` macro invocation. The convention is to name the feature `bpf-entrypoint`.

```rust
#[cfg(feature = &quot;bpf-entrypoint&quot;)]
mod entrypoint {
  use pinocchio::{
    account_info::AccountInfo,
    entrypoint,
    msg,
    ProgramResult,
    pubkey::Pubkey
  };

  entrypoint!(process_instruction);

  pub fn process_instruction(
    program_id: &amp;Pubkey,
    accounts: &amp;[AccountInfo],
    instruction_data: &amp;[u8],
  ) -&gt; ProgramResult {
    msg!(&quot;Hello from my program!&quot;);
    Ok(())
  }
}
```
When building the program binary, you must enable the `bpf-entrypoint` feature:
```bash
cargo build-sbf --features bpf-entrypoint
```

## License

The code is licensed under the [Apache License Version 2.0](LICENSE)

The library in this repository is based/includes code from:
* [`nitrate`](https://github.com/nifty-oss/nitrate)
* [`solana-nostd-entrypoint`](https://github.com/cavemanloverboy/solana-nostd-entrypoint/tree/main)
* [`solana-sdk`](https://github.com/anza-xyz/solana-sdk)
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[nexus-xyz/nexus-cli]]></title>
            <link>https://github.com/nexus-xyz/nexus-cli</link>
            <guid>https://github.com/nexus-xyz/nexus-cli</guid>
            <pubDate>Sat, 12 Jul 2025 00:05:33 GMT</pubDate>
            <description><![CDATA[Command line interface for supplying proofs to the Nexus network.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/nexus-xyz/nexus-cli">nexus-xyz/nexus-cli</a></h1>
            <p>Command line interface for supplying proofs to the Nexus network.</p>
            <p>Language: Rust</p>
            <p>Stars: 859</p>
            <p>Forks: 420</p>
            <p>Stars today: 5 stars today</p>
            <h2>README</h2><pre>[![Release](https://img.shields.io/github/v/release/nexus-xyz/nexus-cli.svg)](https://github.com/nexus-xyz/nexus-cli/releases)
[![CI](https://github.com/nexus-xyz/nexus-cli/actions/workflows/ci.yml/badge.svg)](https://github.com/nexus-xyz/nexus-cli/actions)
[![License](https://img.shields.io/badge/License-Apache_2.0-green.svg)](https://github.com/nexus-xyz/nexus-cli/blob/main/LICENSE-APACHE)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](https://github.com/nexus-xyz/nexus-cli/blob/main/LICENSE-MIT)
[![Twitter](https://img.shields.io/twitter/follow/NexusLabs)](https://x.com/NexusLabs)
[![Discord](https://img.shields.io/badge/Discord-Join-7289da.svg?logo=discord&amp;logoColor=white)](https://discord.com/invite/nexus-xyz)

# Nexus CLI

A high-performance command-line interface for contributing proofs to the Nexus network.

&lt;figure&gt;
    &lt;a href=&quot;https://beta.nexus.xyz/&quot;&gt;
        &lt;img src=&quot;assets/images/nexus-network-image.png&quot; alt=&quot;Nexus Network visualization showing a distributed network of interconnected nodes with a &#039;Launch Network&#039; button in the center&quot;&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
        &lt;strong&gt;Verifiable Computation on a Global Scale&lt;/strong&gt;&lt;br&gt;
        We&#039;re building a global distributed prover network to unite the world&#039;s computers and power a new and better Internet: the Verifiable Internet. Connect to the beta and give it a try today.
    &lt;/figcaption&gt;
&lt;/figure&gt;

## Nexus Network

[Nexus](https://nexus.xyz/) is a global distributed prover network that unites the world&#039;s computers to power a new and
better Internet: the Verifiable Internet.

There have been several testnets so far:

- Testnet 0: [October 8 â€“ 28, 2024](https://blog.nexus.xyz/nexus-launches-worlds-first-open-prover-network/)
- Testnet I: [December 9 â€“ 13, 2024](https://blog.nexus.xyz/the-new-nexus-testnet-is-live/)
- Testnet II: [February 18 â€“ 22, 2025](https://blog.nexus.xyz/testnet-ii-is-open/)
- Devnet: February 22 - June 20 2025
- Testnet III: [Ongoing](https://blog.nexus.xyz/live-everywhere/)

---

## Quick Start

### Installation

#### Precompiled Binary (Recommended)

For the simplest and most reliable installation:

```bash
curl https://cli.nexus.xyz/ | sh
```

This will:
1. Download and install the latest precompiled binary for your platform.
2. Prompt you to accept the Terms of Use.
3. Start the CLI in interactive mode.

The exact installation script is viewable [here](./public/install.sh).

#### Non-Interactive Installation

For automated installations (e.g., in CI):

```bash
curl -sSf https://cli.nexus.xyz/ -o install.sh
chmod +x install.sh
NONINTERACTIVE=1 ./install.sh
```

### Proving

Proving with the CLI is documented [here](https://docs.nexus.xyz/layer-1/testnet/cli-node).

To start with an existing node ID, run:

```bash
nexus-cli start --node-id &lt;your-node-id&gt;
```

Alternatively, you can register your wallet address and create a node ID with the CLI, or at [app.nexus.xyz](https://app.nexus.xyz).

```bash
nexus-cli register-user --wallet-address &lt;your-wallet-address&gt;
nexus-cli register-node
nexus-cli start
```

The `register-user` and `register-node` commands will save your credentials to `~/.nexus/config.json`. To clear credentials, run:

```bash
nexus-cli logout
```

For troubleshooting or to see available command line options, run:

```bash
nexus-cli --help
```

---

## Terms of Use

Use of the CLI is subject to the [Terms of Use](https://nexus.xyz/terms-of-use).
First-time users running interactively will be prompted to accept these terms.

---

## Node ID

During the CLI&#039;s startup, you&#039;ll be asked for your node ID. To skip prompts in a
non-interactive environment, manually create a `~/.nexus/config.json` in the
following format:

```json
{
   &quot;node_id&quot;: &quot;&lt;YOUR NODE ID&gt;&quot;
}
```

---

## Current Limitations

- To submit programs to the network for proving, contact
  [growth@nexus.xyz](mailto:growth@nexus.xyz).

---

## Get Help

- [Network FAQ](https://docs.nexus.xyz/layer-1/network-devnet/faq)
- [Discord Community](https://discord.gg/nexus-xyz)
- Technical issues? [Open an issue](https://github.com/nexus-xyz/nexus-cli/issues)

---

## Contributing

Interested in contributing to the Nexus Network CLI? Check out our
[Contributor Guide](./CONTRIBUTING.md) for:

- Development setup instructions
- How to report issues and submit pull requests
- Our code of conduct and community guidelines
- Tips for working with the codebase

For most users, we recommend using the precompiled binaries as described above.
The contributor guide is intended for those who want to modify or improve the CLI
itself.

### ğŸ›   Developer Guide

The following steps may be required in order to setup a development environment for contributing to the project:

#### Linux

```bash
sudo apt update
sudo apt upgrade
sudo apt install build-essential pkg-config libssl-dev git-all
sudo apt install protobuf-compiler
```

#### macOS

```bash
# Install using Homebrew
brew install protobuf

# Verify installation
protoc --version
```

#### Windows

[Install WSL](https://learn.microsoft.com/en-us/windows/wsl/install),
then see Linux instructions above.

```bash
# Install using Chocolatey
choco install protobuf
```

### Building ProtoBuf files

To build the ProtoBuf files, run the following command in the `clients/cli` directory:

```bash
cargo build --features build_proto
```

### Creating a Release

To create a release, update the package version in `Cargo.toml`, then create and push a new (annotated) tag, e.g.:

```bash
git tag -a v0.1.2 -m &quot;Release v0.1.2&quot;
git push origin v0.1.2
```

This will trigger the GitHub Actions release workflow that compiles binaries and pushes the Docker image, in
addition to creating release.

**WARNING**: Creating a release through the GitHub UI creates a new release but does **NOT** trigger
the workflow. This leads to a release without a Docker image or binaries, which breaks the installation script.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[qdrant/qdrant]]></title>
            <link>https://github.com/qdrant/qdrant</link>
            <guid>https://github.com/qdrant/qdrant</guid>
            <pubDate>Sat, 12 Jul 2025 00:05:32 GMT</pubDate>
            <description><![CDATA[Qdrant - High-performance, massive-scale Vector Database and Vector Search Engine for the next generation of AI. Also available in the cloud https://cloud.qdrant.io/]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/qdrant/qdrant">qdrant/qdrant</a></h1>
            <p>Qdrant - High-performance, massive-scale Vector Database and Vector Search Engine for the next generation of AI. Also available in the cloud https://cloud.qdrant.io/</p>
            <p>Language: Rust</p>
            <p>Stars: 24,619</p>
            <p>Forks: 1,696</p>
            <p>Stars today: 28 stars today</p>
            <h2>README</h2><pre>## How to interview using this template

Open ONLY the `shared/` folder within VSCode and in the Extensions Marketplace find `CodeTogether Live`. Install it.

Inside the extension, you&#039;ll have to click on &#039;Host New Session&#039; and then &#039;Start&#039;.

You now have a link in your clipboard, which you should share with the candidate.

Don&#039;t forget to run `npm run vite.dev` inside your editor terminal, since this is not currently possible from the browser editor (link) with the free version of the extension.

The locally running server will display localhost:5173 as well as a the project within the network, which the user can access remotely to see the SPA in action, something like: `10.5.52.144:5173`.

The candidate should be sharing their screen while they read and later approach the challenge!

## React Templateï¼ˆâš¡ï¸ï¼‰

âš¡ï¸ A minimal React Vite starter template.

### Feature

- âš¡ï¸ Fast - Build tools based on vite.
- ğŸ‘» Small - Based on the smallest runnable build.
- ğŸ’„ Prettier - Integrated Prettier to help you format the code.
- âœ… Safety - Https is enabled by default.
- ğŸ˜ Reliable - Integrated eslint and commitlint.
- ğŸ¤– Intelligent - Integrated renovate to help you maintain the dependent version.

### Preview

[![qekup8.png](https://s1.ax1x.com/2022/03/20/qekup8.png)](https://imgtu.com/i/qekup8)

### Getting Started

```bash
npx degit lzm0x219/template-vite-react myapp

cd myapp

git init
```

#### Prerequisites

- `npm` and/or `pnpm` should be installed.
- `git` should be installed (recommended v2.4.11 or higher)

#### Available scripts

##### `npm run vite.dev`

Runs the app in development mode.
Open https://localhost:5173 to view it in the browser.

The page will automatically reload if you make changes to the code.
You will see the build errors and lint warnings in the console.

##### `npm run vite.build`

Builds the app for production to the `dist` folder.
It correctly bundles React in production mode and optimizes the build for the best performance.

The build is minified and the filenames include the hashes.

Your app is ready to be deployed.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[seanmonstar/reqwest]]></title>
            <link>https://github.com/seanmonstar/reqwest</link>
            <guid>https://github.com/seanmonstar/reqwest</guid>
            <pubDate>Sat, 12 Jul 2025 00:05:31 GMT</pubDate>
            <description><![CDATA[An easy and powerful Rust HTTP Client]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/seanmonstar/reqwest">seanmonstar/reqwest</a></h1>
            <p>An easy and powerful Rust HTTP Client</p>
            <p>Language: Rust</p>
            <p>Stars: 10,811</p>
            <p>Forks: 1,256</p>
            <p>Stars today: 5 stars today</p>
            <h2>README</h2><pre># reqwest

[![crates.io](https://img.shields.io/crates/v/reqwest.svg)](https://crates.io/crates/reqwest)
[![Documentation](https://docs.rs/reqwest/badge.svg)](https://docs.rs/reqwest)
[![MIT/Apache-2 licensed](https://img.shields.io/crates/l/reqwest.svg)](./LICENSE-APACHE)
[![CI](https://github.com/seanmonstar/reqwest/workflows/CI/badge.svg)](https://github.com/seanmonstar/reqwest/actions?query=workflow%3ACI)

An ergonomic, batteries-included HTTP Client for Rust.

- Async and blocking `Client`s
- Plain bodies, JSON, urlencoded, multipart
- Customizable redirect policy
- HTTP Proxies
- HTTPS via system-native TLS (or optionally, rustls)
- Cookie Store
- WASM


## Example

This asynchronous example uses [Tokio](https://tokio.rs) and enables some
optional features, so your `Cargo.toml` could look like this:

```toml
[dependencies]
reqwest = { version = &quot;0.12&quot;, features = [&quot;json&quot;] }
tokio = { version = &quot;1&quot;, features = [&quot;full&quot;] }
```

And then the code:

```rust,no_run
use std::collections::HashMap;

#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    let resp = reqwest::get(&quot;https://httpbin.org/ip&quot;)
        .await?
        .json::&lt;HashMap&lt;String, String&gt;&gt;()
        .await?;
    println!(&quot;{resp:#?}&quot;);
    Ok(())
}
```

## Commercial Support

For private advice, support, reviews, access to the maintainer, and the like, reach out for [commercial support][sponsor].

## Requirements

On Linux:

- OpenSSL with headers. See https://docs.rs/openssl for supported versions
  and more details. Alternatively you can enable the `native-tls-vendored`
  feature to compile a copy of OpenSSL. Or, you can use [rustls](https://github.com/rustls/rustls)
  via `rustls-tls` or other `rustls-tls-*` features.

On Windows and macOS:

- Nothing.

By default, Reqwest uses [rust-native-tls](https://github.com/sfackler/rust-native-tls),
which will use the operating system TLS framework if available, meaning Windows
and macOS. On Linux, it will use the available OpenSSL or fail to build if
not found.


## License

Licensed under either of

- Apache License, Version 2.0 ([LICENSE-APACHE](LICENSE-APACHE) or http://apache.org/licenses/LICENSE-2.0)
- MIT license ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)

### Contribution

Unless you explicitly state otherwise, any contribution intentionally submitted
for inclusion in the work by you, as defined in the Apache-2.0 license, shall
be dual licensed as above, without any additional terms or conditions.

## Sponsors

Support this project by becoming a [sponsor][].

[sponsor]: https://seanmonstar.com/sponsor
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[huggingface/candle]]></title>
            <link>https://github.com/huggingface/candle</link>
            <guid>https://github.com/huggingface/candle</guid>
            <pubDate>Sat, 12 Jul 2025 00:05:30 GMT</pubDate>
            <description><![CDATA[Minimalist ML framework for Rust]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/huggingface/candle">huggingface/candle</a></h1>
            <p>Minimalist ML framework for Rust</p>
            <p>Language: Rust</p>
            <p>Stars: 17,589</p>
            <p>Forks: 1,144</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre># candle
[![discord server](https://dcbadge.vercel.app/api/server/hugging-face-879548962464493619)](https://discord.gg/hugging-face-879548962464493619)
[![Latest version](https://img.shields.io/crates/v/candle-core.svg)](https://crates.io/crates/candle-core)
[![Documentation](https://docs.rs/candle-core/badge.svg)](https://docs.rs/candle-core)
[![License](https://img.shields.io/github/license/base-org/node?color=blue)](https://github.com/huggingface/candle/blob/main/LICENSE-MIT)
[![License](https://img.shields.io/badge/license-Apache%202.0-blue?style=flat-square)](https://github.com/huggingface/candle/blob/main/LICENSE-APACHE)

Candle is a minimalist ML framework for Rust with a focus on performance (including GPU support) 
and ease of use. Try our online demos: 
[whisper](https://huggingface.co/spaces/lmz/candle-whisper),
[LLaMA2](https://huggingface.co/spaces/lmz/candle-llama2),
[T5](https://huggingface.co/spaces/radames/Candle-T5-Generation-Wasm),
[yolo](https://huggingface.co/spaces/lmz/candle-yolo),
[Segment
Anything](https://huggingface.co/spaces/radames/candle-segment-anything-wasm).

## Get started

Make sure that you have [`candle-core`](https://github.com/huggingface/candle/tree/main/candle-core) correctly installed as described in [**Installation**](https://huggingface.github.io/candle/guide/installation.html).

Let&#039;s see how to run a simple matrix multiplication.
Write the following to your `myapp/src/main.rs` file:
```rust
use candle_core::{Device, Tensor};

fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    let device = Device::Cpu;

    let a = Tensor::randn(0f32, 1., (2, 3), &amp;device)?;
    let b = Tensor::randn(0f32, 1., (3, 4), &amp;device)?;

    let c = a.matmul(&amp;b)?;
    println!(&quot;{c}&quot;);
    Ok(())
}
```

`cargo run` should display a tensor of shape `Tensor[[2, 4], f32]`.


Having installed `candle` with Cuda support, simply define the `device` to be on GPU:

```diff
- let device = Device::Cpu;
+ let device = Device::new_cuda(0)?;
```

For more advanced examples, please have a look at the following section.

## Check out our examples

These online demos run entirely in your browser:
- [yolo](https://huggingface.co/spaces/lmz/candle-yolo): pose estimation and
  object recognition.
- [whisper](https://huggingface.co/spaces/lmz/candle-whisper): speech recognition.
- [LLaMA2](https://huggingface.co/spaces/lmz/candle-llama2): text generation.
- [T5](https://huggingface.co/spaces/radames/Candle-T5-Generation-Wasm): text generation.
- [Phi-1.5, and Phi-2](https://huggingface.co/spaces/radames/Candle-Phi-1.5-Wasm): text generation.
- [Segment Anything Model](https://huggingface.co/spaces/radames/candle-segment-anything-wasm): Image segmentation.
- [BLIP](https://huggingface.co/spaces/radames/Candle-BLIP-Image-Captioning): image captioning.

We also provide some command line based examples using state of the art models:

- [LLaMA v1, v2, and v3](./candle-examples/examples/llama/): general LLM, includes
  the SOLAR-10.7B variant.
- [Falcon](./candle-examples/examples/falcon/): general LLM.
- [Codegeex4](./candle-examples/examples/codegeex4-9b/): Code completion, code interpreter, web search, function calling, repository-level
- [GLM4](./candle-examples/examples/glm4/): Open Multilingual Multimodal Chat LMs by THUDM
- [Gemma v1 and v2](./candle-examples/examples/gemma/): 2b and 7b+/9b general LLMs from Google Deepmind.
- [RecurrentGemma](./candle-examples/examples/recurrent-gemma/): 2b and 7b
  Griffin based models from Google that mix attention with a RNN like state.
- [Phi-1, Phi-1.5, Phi-2, and Phi-3](./candle-examples/examples/phi/): 1.3b,
  2.7b, and 3.8b general LLMs with performance on par with 7b models.
- [StableLM-3B-4E1T](./candle-examples/examples/stable-lm/): a 3b general LLM
  pre-trained on 1T tokens of English and code datasets. Also supports
  StableLM-2, a 1.6b LLM trained on 2T tokens, as well as the code variants.
- [Mamba](./candle-examples/examples/mamba/): an inference only
  implementation of the Mamba state space model.
- [Mistral7b-v0.1](./candle-examples/examples/mistral/): a 7b general LLM with
  better performance than all publicly available 13b models as of 2023-09-28.
- [Mixtral8x7b-v0.1](./candle-examples/examples/mixtral/): a sparse mixture of
  experts 8x7b general LLM with better performance than a Llama 2 70B model with
  much faster inference.
- [StarCoder](./candle-examples/examples/bigcode/) and
  [StarCoder2](./candle-examples/examples/starcoder2/): LLM specialized to code generation.
- [Qwen1.5](./candle-examples/examples/qwen/): Bilingual (English/Chinese) LLMs.
- [RWKV v5 and v6](./candle-examples/examples/rwkv/): An RNN with transformer level LLM
  performance.
- [Replit-code-v1.5](./candle-examples/examples/replit-code/): a 3.3b LLM specialized for code completion.
- [Yi-6B / Yi-34B](./candle-examples/examples/yi/): two bilingual
  (English/Chinese) general LLMs with 6b and 34b parameters.
- [Quantized LLaMA](./candle-examples/examples/quantized/): quantized version of
  the LLaMA model using the same quantization techniques as
  [llama.cpp](https://github.com/ggerganov/llama.cpp).

&lt;img src=&quot;https://github.com/huggingface/candle/raw/main/candle-examples/examples/quantized/assets/aoc.gif&quot; width=&quot;600&quot;&gt;
  
- [Stable Diffusion](./candle-examples/examples/stable-diffusion/): text to
  image generative model, support for the 1.5, 2.1, SDXL 1.0 and Turbo versions.

&lt;img src=&quot;https://github.com/huggingface/candle/raw/main/candle-examples/examples/stable-diffusion/assets/stable-diffusion-xl.jpg&quot; width=&quot;200&quot;&gt;

- [Wuerstchen](./candle-examples/examples/wuerstchen/): another text to
  image generative model.

&lt;img src=&quot;https://github.com/huggingface/candle/raw/main/candle-examples/examples/wuerstchen/assets/cat.jpg&quot; width=&quot;200&quot;&gt;

- [yolo-v3](./candle-examples/examples/yolo-v3/) and
  [yolo-v8](./candle-examples/examples/yolo-v8/): object detection and pose
  estimation models.

&lt;img src=&quot;https://github.com/huggingface/candle/raw/main/candle-examples/examples/yolo-v8/assets/bike.od.jpg&quot; width=&quot;200&quot;&gt;&lt;img src=&quot;https://github.com/huggingface/candle/raw/main/candle-examples/examples/yolo-v8/assets/bike.pose.jpg&quot; width=&quot;200&quot;&gt;
- [segment-anything](./candle-examples/examples/segment-anything/): image
  segmentation model with prompt.

&lt;img src=&quot;https://github.com/huggingface/candle/raw/main/candle-examples/examples/segment-anything/assets/sam_merged.jpg&quot; width=&quot;200&quot;&gt;

- [SegFormer](./candle-examples/examples/segformer/): transformer based semantic segmentation model.
- [Whisper](./candle-examples/examples/whisper/): speech recognition model.
- [EnCodec](./candle-examples/examples/encodec/): high-quality audio compression
  model using residual vector quantization.
- [MetaVoice](./candle-examples/examples/metavoice/): foundational model for
  text-to-speech.
- [Parler-TTS](./candle-examples/examples/parler-tts/): large text-to-speech
  model.
- [T5](./candle-examples/examples/t5), [Bert](./candle-examples/examples/bert/),
  [JinaBert](./candle-examples/examples/jina-bert/) : useful for sentence embeddings.
- [DINOv2](./candle-examples/examples/dinov2/): computer vision model trained
  using self-supervision (can be used for imagenet classification, depth
  evaluation, segmentation).
- [VGG](./candle-examples/examples/vgg/),
  [RepVGG](./candle-examples/examples/repvgg): computer vision models.
- [BLIP](./candle-examples/examples/blip/): image to text model, can be used to
  generate captions for an image.
- [CLIP](./candle-examples/examples/clip/): multi-model vision and language
  model.
- [TrOCR](./candle-examples/examples/trocr/): a transformer OCR model, with
  dedicated submodels for hand-writing and printed recognition.
- [Marian-MT](./candle-examples/examples/marian-mt/): neural machine translation
  model, generates the translated text from the input text.
- [Moondream](./candle-examples/examples/moondream/): tiny computer-vision model 
  that can answer real-world questions about images.

Run them using commands like:
```
cargo run --example quantized --release
```

In order to use **CUDA** add `--features cuda` to the example command line. If
you have cuDNN installed, use `--features cudnn` for even more speedups.

There are also some wasm examples for whisper and
[llama2.c](https://github.com/karpathy/llama2.c). You can either build them with
`trunk` or try them online:
[whisper](https://huggingface.co/spaces/lmz/candle-whisper),
[llama2](https://huggingface.co/spaces/lmz/candle-llama2),
[T5](https://huggingface.co/spaces/radames/Candle-T5-Generation-Wasm),
[Phi-1.5, and Phi-2](https://huggingface.co/spaces/radames/Candle-Phi-1.5-Wasm),
[Segment Anything Model](https://huggingface.co/spaces/radames/candle-segment-anything-wasm).

For LLaMA2, run the following command to retrieve the weight files and start a
test server:
```bash
cd candle-wasm-examples/llama2-c
wget https://huggingface.co/spaces/lmz/candle-llama2/resolve/main/model.bin
wget https://huggingface.co/spaces/lmz/candle-llama2/resolve/main/tokenizer.json
trunk serve --release --port 8081
```
And then head over to
[http://localhost:8081/](http://localhost:8081/).

&lt;!--- ANCHOR: useful_libraries ---&gt;

## Useful External Resources
- [`candle-tutorial`](https://github.com/ToluClassics/candle-tutorial): A
  very detailed tutorial showing how to convert a PyTorch model to Candle.
- [`candle-lora`](https://github.com/EricLBuehler/candle-lora): Efficient and
  ergonomic LoRA implementation for Candle. `candle-lora` has      
  out-of-the-box LoRA support for many models from Candle, which can be found
  [here](https://github.com/EricLBuehler/candle-lora/tree/master/candle-lora-transformers/examples).
- [`optimisers`](https://github.com/KGrewal1/optimisers): A collection of optimisers
  including SGD with momentum, AdaGrad, AdaDelta, AdaMax, NAdam, RAdam, and RMSprop.
- [`candle-vllm`](https://github.com/EricLBuehler/candle-vllm): Efficient platform for inference and
  serving local LLMs including an OpenAI compatible API server.
- [`candle-ext`](https://github.com/mokeyish/candle-ext): An extension library to Candle that provides PyTorch functions not currently available in Candle.
- [`candle-coursera-ml`](https://github.com/vishpat/candle-coursera-ml): Implementation of ML algorithms from Coursera&#039;s [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) course.
- [`kalosm`](https://github.com/floneum/floneum/tree/master/interfaces/kalosm): A multi-modal meta-framework in Rust for interfacing with local pre-trained models with support for controlled generation, custom samplers, in-memory vector databases, audio transcription, and more.
- [`candle-sampling`](https://github.com/EricLBuehler/candle-sampling): Sampling techniques for Candle.
- [`gpt-from-scratch-rs`](https://github.com/jeroenvlek/gpt-from-scratch-rs): A port of Andrej Karpathy&#039;s _Let&#039;s build GPT_ tutorial on YouTube showcasing the Candle API on a toy problem.
- [`candle-einops`](https://github.com/tomsanbear/candle-einops): A pure rust implementation of the python [einops](https://github.com/arogozhnikov/einops) library.
- [`atoma-infer`](https://github.com/atoma-network/atoma-infer): A Rust library for fast inference at scale, leveraging FlashAttention2 for efficient attention computation, PagedAttention for efficient KV-cache memory management, and multi-GPU support. It is OpenAI api compatible.
- [`llms-from-scratch-rs`](https://github.com/nerdai/llms-from-scratch-rs): A comprehensive Rust translation of the code from Sebastian Raschka&#039;s Build an LLM from Scratch book.

If you have an addition to this list, please submit a pull request.

&lt;!--- ANCHOR_END: useful_libraries ---&gt;

&lt;!--- ANCHOR: features ---&gt;

## Features

- Simple syntax, looks and feels like PyTorch.
    - Model training.
    - Embed user-defined ops/kernels, such as [flash-attention v2](https://github.com/huggingface/candle/blob/89ba005962495f2bfbda286e185e9c3c7f5300a3/candle-flash-attn/src/lib.rs#L152).
- Backends.
    - Optimized CPU backend with optional MKL support for x86 and Accelerate for macs.
    - CUDA backend for efficiently running on GPUs, multiple GPU distribution via NCCL.
    - WASM support, run your models in a browser.
- Included models.
    - Language Models.
        - LLaMA v1, v2, and v3 with variants such as SOLAR-10.7B.
        - Falcon.
        - StarCoder, StarCoder2.
        - Phi 1, 1.5, 2, and 3.
        - Mamba, Minimal Mamba
        - Gemma v1 2b and 7b+, v2 2b and 9b.
        - Mistral 7b v0.1.
        - Mixtral 8x7b v0.1.
        - StableLM-3B-4E1T, StableLM-2-1.6B, Stable-Code-3B.
        - Replit-code-v1.5-3B.
        - Bert.
        - Yi-6B and Yi-34B.
        - Qwen1.5, Qwen1.5 MoE.
        - RWKV v5 and v6.
    - Quantized LLMs.
        - Llama 7b, 13b, 70b, as well as the chat and code variants.
        - Mistral 7b, and 7b instruct.
        - Mixtral 8x7b.
        - Zephyr 7b a and b (Mistral-7b based).
        - OpenChat 3.5 (Mistral-7b based).
    - Text to text.
        - T5 and its variants: FlanT5, UL2, MADLAD400 (translation), CoEdit (Grammar correction).
        - Marian MT (Machine Translation).
    - Text to image.
        - Stable Diffusion v1.5, v2.1, XL v1.0.
        - Wurstchen v2.
    - Image to text.
        - BLIP.
        - TrOCR.
    - Audio.
        - Whisper, multi-lingual speech-to-text.
        - EnCodec, audio compression model.
        - MetaVoice-1B, text-to-speech model.
        - Parler-TTS, text-to-speech model.
    - Computer Vision Models.
        - DINOv2, ConvMixer, EfficientNet, ResNet, ViT, VGG, RepVGG, ConvNeXT,
          ConvNeXTv2, MobileOne, EfficientVit (MSRA), MobileNetv4, Hiera, FastViT.
        - yolo-v3, yolo-v8.
        - Segment-Anything Model (SAM).
        - SegFormer.
- File formats: load models from safetensors, npz, ggml, or PyTorch files.
- Serverless (on CPU), small and fast deployments.
- Quantization support using the llama.cpp quantized types.

&lt;!--- ANCHOR_END: features ---&gt;

## How to use

&lt;!--- ANCHOR: cheatsheet ---&gt;
Cheatsheet:

|            | Using PyTorch                            | Using Candle                                                     |
|------------|------------------------------------------|------------------------------------------------------------------|
| Creation   | `torch.Tensor([[1, 2], [3, 4]])`         | `Tensor::new(&amp;[[1f32, 2.], [3., 4.]], &amp;Device::Cpu)?`           |
| Creation   | `torch.zeros((2, 2))`                    | `Tensor::zeros((2, 2), DType::F32, &amp;Device::Cpu)?`               |
| Indexing   | `tensor[:, :4]`                          | `tensor.i((.., ..4))?`                                           |
| Operations | `tensor.view((2, 2))`                    | `tensor.reshape((2, 2))?`                                        |
| Operations | `a.matmul(b)`                            | `a.matmul(&amp;b)?`                                                  |
| Arithmetic | `a + b`                                  | `&amp;a + &amp;b`                                                        |
| Device     | `tensor.to(device=&quot;cuda&quot;)`               | `tensor.to_device(&amp;Device::new_cuda(0)?)?`                            |
| Dtype      | `tensor.to(dtype=torch.float16)`         | `tensor.to_dtype(&amp;DType::F16)?`                                  |
| Saving     | `torch.save({&quot;A&quot;: A}, &quot;model.bin&quot;)`      | `candle::safetensors::save(&amp;HashMap::from([(&quot;A&quot;, A)]), &quot;model.safetensors&quot;)?` |
| Loading    | `weights = torch.load(&quot;model.bin&quot;)`      | `candle::safetensors::load(&quot;model.safetensors&quot;, &amp;device)`        |

&lt;!--- ANCHOR_END: cheatsheet ---&gt;


## Structure

- [candle-core](./candle-core): Core ops, devices, and `Tensor` struct definition
- [candle-nn](./candle-nn/): Tools to build real models
- [candle-examples](./candle-examples/): Examples of using the library in realistic settings
- [candle-kernels](./candle-kernels/): CUDA custom kernels
- [candle-datasets](./candle-datasets/): Datasets and data loaders.
- [candle-transformers](./candle-transformers): transformers-related utilities.
- [candle-flash-attn](./candle-flash-attn): Flash attention v2 layer.
- [candle-onnx](./candle-onnx/): ONNX model evaluation.

## FAQ

### Why should I use Candle?

&lt;!--- ANCHOR: goals ---&gt;

Candle&#039;s core goal is to *make serverless inference possible*. Full machine learning frameworks like PyTorch
are very large, which makes creating instances on a cluster slow. Candle allows deployment of lightweight
binaries.

Secondly, Candle lets you *remove Python* from production workloads. Python overhead can seriously hurt performance,
and the [GIL](https://www.backblaze.com/blog/the-python-gil-past-present-and-future/) is a notorious source of headaches.

Finally, Rust is cool! A lot of the HF ecosystem already has Rust crates, like [safetensors](https://github.com/huggingface/safetensors) and [tokenizers](https://github.com/huggingface/tokenizers).

&lt;!--- ANCHOR_END: goals ---&gt;

### Other ML frameworks

- [dfdx](https://github.com/coreylowman/dfdx) is a formidable crate, with shapes being included
  in types. This prevents a lot of headaches by getting the compiler to complain about shape mismatches right off the bat.
  However, we found that some features still require nightly, and writing code can be a bit daunting for non rust experts.

  We&#039;re leveraging and contributing to other core crates for the runtime so hopefully both crates can benefit from each
  other.

- [burn](https://github.com/burn-rs/burn) is a general crate that can leverage multiple backends so you can choose the best
  engine for your workload.

- [tch-rs](https://github.com/LaurentMazare/tch-rs.git) Bindings to the torch library in Rust. Extremely versatile, but they 
  bring in the entire torch library into the runtime. The main contributor of `tch-rs` is also involved in the development
  of `candle`.

### Common Errors

#### Missing symbols when compiling with the mkl feature.

If you get some missing symbols when compiling binaries/tests using the mkl
or accelerate features, e.g. for mkl you get:
```
  = note: /usr/bin/ld: (....o): in function `blas::sgemm&#039;:
          .../blas-0.22.0/src/lib.rs:1944: undefined reference to `sgemm_&#039; collect2: error: ld returned 1 exit status

  = note: some `extern` functions couldn&#039;t be found; some native libraries may need to be installed or have their path specified
  = note: use the `-l` flag to specify native libraries to link
  = note: use the `cargo:rustc-link-lib` directive to specify the native libraries to link with Cargo
```
or for accelerate:
```
Undefined symbols for architecture arm64:
            &quot;_dgemm_&quot;, referenced from:
                candle_core::accelerate::dgemm::h1b71a038552bcabe in libcandle_core...
            &quot;_sgemm_&quot;, referenced from:
                candle_core::accelerate::sgemm::h2cf21c592cba3c47 in libcandle_core...
          ld: symbol(s) not found for architecture arm64
```

This is likely due to a missing linker flag that was needed to enable the mkl library. You
can try adding the following for mkl at the top of your binary:
```rust
extern crate intel_mkl_src;
```
or for accelerate:
```rust
extern crate accelerate_src;
```

#### Cannot run the LLaMA examples: access to source requires login credentials

```
Error: request error: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/tokenizer.json: status code 401
```

This is likely because you&#039;re not permissioned for the LLaMA-v2 model. To fix
this, you have to register on the huggingface-hub, accept the [LLaMA-v2 model
conditions](https://huggingface.co/meta-llama/Llama-2-7b-hf), and set up your
authentication token. See issue
[#350](https://github.com/huggingface/candle/issues/350) for more details.

#### Missing cute/cutlass headers when compiling flash-attn

```
  In file included from kernels/flash_fwd_launch_template.h:11:0,
                   from kernels/flash_fwd_hdim224_fp16_sm80.cu:5:
  kernels/flash_fwd_kernel.h:8:10: fatal error: cute

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[rolldown/rolldown]]></title>
            <link>https://github.com/rolldown/rolldown</link>
            <guid>https://github.com/rolldown/rolldown</guid>
            <pubDate>Sat, 12 Jul 2025 00:05:29 GMT</pubDate>
            <description><![CDATA[Fast Rust bundler for JavaScript/TypeScript with Rollup-compatible API.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/rolldown/rolldown">rolldown/rolldown</a></h1>
            <p>Fast Rust bundler for JavaScript/TypeScript with Rollup-compatible API.</p>
            <p>Language: Rust</p>
            <p>Stars: 11,590</p>
            <p>Forks: 622</p>
            <p>Stars today: 12 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://rolldown.rs&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;
    &lt;img width=&quot;180&quot; src=&quot;https://rolldown.rs/rolldown-round.svg&quot; alt=&quot;Rolldown logo&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;

[![MIT licensed][badge-license]][url-license]
[![NPM version][badge-npm-version]][url-npm]
[![CodSpeed Badge](https://img.shields.io/endpoint?url=https://codspeed.io/badge.json)](https://codspeed.io/rolldown/rolldown)
[![Discord chat][badge-discord]][discord-url]

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

[![NPM Unpacked Size (with version)](https://img.shields.io/npm/unpacked-size/rolldown/latest?label=npm)][url-npm]
[![NPM Unpacked Size darwin-arm64](https://img.shields.io/npm/unpacked-size/%40rolldown%2Fbinding-darwin-arm64/latest?label=darwin-arm64)](https://www.npmjs.com/package/@rolldown/binding-darwin-arm64)
[![NPM Unpacked Size darwin-x64](https://img.shields.io/npm/unpacked-size/%40rolldown%2Fbinding-darwin-x64/latest?label=darwin-x64)](https://www.npmjs.com/package/@rolldown/binding-darwin-x64)
[![NPM Unpacked Size linux-x64-gnu](https://img.shields.io/npm/unpacked-size/%40rolldown%2Fbinding-linux-x64-gnu/latest?label=linux-x64-gnu)](https://www.npmjs.com/package/@rolldown/binding-linux-x64-gnu)
[![NPM Unpacked Size win32-x64](https://img.shields.io/npm/unpacked-size/%40rolldown%2Fbinding-win32-x64-msvc/latest?label=win32-x64)](https://www.npmjs.com/package/@rolldown/binding-win32-x64-msvc)
[![NPM Unpacked Size wasm32-wasi](https://img.shields.io/npm/unpacked-size/%40rolldown%2Fbinding-wasm32-wasi/latest?label=wasm32-wasi)](https://www.npmjs.com/package/@rolldown/binding-wasm32-wasi)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

[![pkg.pr.new](https://pkg.pr.new/badge/pkg.pr.new/pkg.pr.new?style=flat&amp;color=000&amp;logoSize=auto)](https://pkg.pr.new/~/rolldown/rolldown)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

[![rolldown-starter-stackblitz](https://developer.stackblitz.com/img/open_in_stackblitz.svg)](https://stackblitz.com/fork/github/rolldown/rolldown-starter-stackblitz)

&lt;/div&gt;

&gt; ğŸš§ **Beta Software**
&gt;
&gt; Rolldown is currently in beta status. While it can already handle most production use cases, there may still be bugs and rough edges. Most notably, the built-in minification feature is still in alpha status.

# Rolldown

Rolldown is a JavaScript/TypeScript bundler written in Rust intended to serve as the future bundler used in [Vite](https://vitejs.dev/). It provides Rollup-compatible APIs and plugin interface, but will be more similar to esbuild in scope.

For more information, please check out the documentation at [rolldown.rs](https://rolldown.rs/about).

## VoidZero Inc.

Rolldown is a project of [VoidZero](https://voidzero.dev/), see our announcement [Announcing VoidZero - Next Generation Toolchain for JavaScript](https://voidzero.dev/posts/announcing-voidzero-inc).

If you have requirements for JavaScript tools at scale, please [get in touch](https://forms.gle/WQgjyzYJpwurpxWKA)!

## Contributing

We would love to have more contributors involved!

To get started, please read our [Contributing Guide](https://rolldown.rs/contrib-guide/).

## Credits

The Rolldown project is heavily inspired by:

- [Rollup](https://github.com/rollup/rollup), created by [Rich Harris](https://github.com/Rich-Harris) and maintained by [Lukas Taegert-Atkinson](https://github.com/lukastaegert).
- [esbuild](https://github.com/evanw/esbuild), created by [Evan Wallace](https://github.com/evanw).

And supported by:

- [napi-rs](https://github.com/napi-rs/napi-rs) for Node.js add-ons in Rust via Node-API.
- [oxc](https://github.com/oxc-project/oxc) for the underlying parser, resolver, and sourcemap support.

## Licenses

This project is licensed under the [MIT License](LICENSE).

This project also partially contains code derived or copied from the following projects:

- [rollup(MIT)](https://github.com/rollup/rollup/blob/680912e2ceb42c8d5e571e01c6ece0e4889aecbb/LICENSE-CORE.md)
- [esbuild(MIT)](https://github.com/evanw/esbuild/blob/0c8a0a901d9a6c7bbff9b4dd347c8a3f65f6c6dd/LICENSE.md)

Licenses of these projects are listed in [THIRD-PARTY-LICENSE](/THIRD-PARTY-LICENSE)

[badge-discord]: https://img.shields.io/discord/1079625926024900739?logo=discord&amp;label=Discord
[discord-url]: https://chat.rolldown.rs
[badge-license]: https://img.shields.io/badge/license-MIT-blue.svg
[url-license]: https://github.com/rolldown/rolldown/blob/main/LICENSE
[badge-npm-version]: https://img.shields.io/npm/v/rolldown/latest?color=brightgreen
[url-npm]: https://www.npmjs.com/package/rolldown/v/latest
[badge-binary-size-windows]: [https://img.shields.io/npm/unpacked-size/%40rolldown%2Fbinding-win32-x64-msvc/latest]
[badge-binary-size-macos]: [https://img.shields.io/npm/unpacked-size/%40rolldown%2Fbinding-darwin-arm64/latest]
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[EFForg/rayhunter]]></title>
            <link>https://github.com/EFForg/rayhunter</link>
            <guid>https://github.com/EFForg/rayhunter</guid>
            <pubDate>Sat, 12 Jul 2025 00:05:28 GMT</pubDate>
            <description><![CDATA[Rust tool to detect cell site simulators on an orbic mobile hotspot]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/EFForg/rayhunter">EFForg/rayhunter</a></h1>
            <p>Rust tool to detect cell site simulators on an orbic mobile hotspot</p>
            <p>Language: Rust</p>
            <p>Stars: 2,157</p>
            <p>Forks: 167</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre>![Rayhunter Logo - An Orca taking a bite out of a cellular signal bar](https://www.eff.org/files/styles/media_browser_preview/public/banner_library/rayhunter-banner.png)

# Rayhunter

![Tests](https://github.com/EFForg/rayhunter/actions/workflows/main.yml/badge.svg)

Rayhunter is an IMSI Catcher Catcher for the Orbic mobile hotspot. To learn more, check out the [Rayhunter Book](https://efforg.github.io/rayhunter/).
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[bee-san/RustScan]]></title>
            <link>https://github.com/bee-san/RustScan</link>
            <guid>https://github.com/bee-san/RustScan</guid>
            <pubDate>Sat, 12 Jul 2025 00:05:27 GMT</pubDate>
            <description><![CDATA[ğŸ¤– The Modern Port Scanner ğŸ¤–]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/bee-san/RustScan">bee-san/RustScan</a></h1>
            <p>ğŸ¤– The Modern Port Scanner ğŸ¤–</p>
            <p>Language: Rust</p>
            <p>Stars: 16,859</p>
            <p>Forks: 1,141</p>
            <p>Stars today: 28 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;

â¡ï¸ [Discord][discord] | [Installation Guide][toc-install] | [Usage Guide][usage-guide] â¬…ï¸

&lt;img src=&quot;pictures/rustscan.png&quot; height=400px width=400px&gt;

&lt;!--&lt;u&gt;**The Modern Port Scanner.**&lt;/u&gt;--&gt;
**Fast, smart, effective.**

![Arch Linux package][badge-1] ![Built with Rust][badge-2] ![GitHub All Releases][badge-3] ![Crates.io][badge-4] ![Discord][badge-5] ![Actions][badge-6]

&lt;/div&gt;

# ğŸ¤” What is this?

![fast][speed-1]

The Modern Port Scanner. **Find ports quickly (3 seconds at its fastest)**. Run scripts through our scripting engine (Python, Lua, Shell supported).

# ğŸ› ï¸ Installation

You can install RustScan&#039;s binary from our [releases page](https://github.com/RustScan/RustScan/releases).

We would prefer you to install with a package manager so it is tested and works for your system.

RustScan is in many repositories already. Install it with whatever tools you wish:

[![Packaging status](https://repology.org/badge/vertical-allrepos/rustscan.svg)](https://repology.org/project/rustscan/versions)

RustScan only officially supports Cargo installations, if you want to use that please install Rust and then `cargo install rustscan`

Example installations include:

MacOS:

```
  brew install rustscan
```

Arch:

```
  yay rustscan
```

# âœ¨ Features

- Scans all 65k ports in **3 seconds**.
- Full scripting engine support. Automatically pipe results into Nmap, or use our scripts (or write your own) to do whatever you want.
- Adaptive learning. RustScan improves the more you use it. No bloated machine learning here, just basic maths.
- The usuals you would expect. IPv6, CIDR, file input and more.
- Automatically pipes ports into Nmap.

## â€¼ï¸ Important Links

|         &lt;!--Installation Guide--&gt;          |          &lt;!--Documentation--&gt;          |       &lt;!--Discord--&gt;        |
| :----------------------------------------: | :------------------------------------: | :-------------------------: |
| :book: [Installation Guide][links-table-1] | :books: [Documentation][links-table-2] | :parrot: [Discord][discord] |

## ğŸ™‹ Table of Contents

- ğŸ“– [Installation Guide][toc-install]
- ğŸ‹ [Docker Usage][toc-docker-usage]
- ğŸ¦œ [Discord][discord]
- ğŸ¤¸ [Usage][usage-1]

# ğŸ”­ Why RustScan?

RustScan is a modern take on the port scanner. Sleek &amp; fast. All while providing extensive extendability to you.

Not to mention RustScan uses Adaptive Learning to improve itself over time, making it the best port scanner for **you**.

## ğŸ§‹ Speed

![fast][speed-1]

Speed is guaranteed via RustScan. However, if you want to run a slow scan due to stealth, that is possible too.

Firstly, let&#039;s talk code.

We have tests that check to see if RustScan is significantly slower than the previous version. If it is, the continuous integration fails, and we can&#039;t commit code to master unless we make it faster.

[HyperFine][speed-2] is used to monitor RustScan&#039;s performance over time to answer the question, &quot;Are we getting faster? Are we getting slower?&quot;.

Every pull request is reviewed by **one** person, but more often than not, **two** people review it. We test it manually and ensure the code doesn&#039;t negatively affect performance.

[Read more here][speed-3].

## âš™ï¸ Extensible

![scripts][extensible-1]

### _RustScan piping results into the custom Python script_

RustScan has a new scripting engine that allows anyone to write scripts in most languages. Python, Lua, and Shell are all supported.

Want to take your found ports and pipe them into Nmap for further analysis? That&#039;s possible. Want to run `smb-enum` if SMB is found open? Possible.

The possibilities are endless -- and you can write scripts in whatever language you feel comfortable with.

[Read more here][extensible-2].

## ğŸŒŠ Adaptive

![adaptive][adaptive-1]

### _RustScan automatically fine-tunes itself to match the host OS_

RustScan has a cool set of features called &quot;Adaptive Learning&quot;. These features &quot;learn&quot; about the environment you are scanning and how _you_ use RustScan to **improve itself over time**.

We use this umbrella term for any feature that fits this criterion. The list constantly changes, so [check out our wiki for more information][adaptive-learning].

## ğŸ‘©â€ğŸ¦¯ Accessible

![fast][accessible-1]

RustScan is one of the first penetration testing tools that aims to be entirely accessible.

[Most penetration testing tools are not accessible][accessible-2], which negatively affects the whole industry.

RustScan has continuous integration testing that aims to ensure it is accessible, and we are constantly working on ways to improve our accessibility and ensure _everyone_ can use RustScan.

# ğŸ¤¸ Usage

We have 2 usage guides. [Basic Usage][usage-1] and [Things you may want to do][usage-2].

We also have documentation about our config file [here][config-file-here].

# ğŸª Community

[Contributing][community-1] Read this to learn how.

## Contributors âœ¨

&lt;!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section --&gt;

[![All Contributors](https://img.shields.io/badge/all_contributors-26-orange.svg?style=flat-square)](#contributors-)

&lt;!-- ALL-CONTRIBUTORS-BADGE:END --&gt;

Thanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):

&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt;
&lt;!-- prettier-ignore-start --&gt;
&lt;!-- markdownlint-disable --&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://skerritt.blog&quot;&gt;&lt;img src=&quot;https://avatars3.githubusercontent.com/u/10378052?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Bee&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#infra-beeskerritt&quot; title=&quot;Infrastructure (Hosting, Build-Tools, etc)&quot;&gt;ğŸš‡&lt;/a&gt; &lt;a href=&quot;https://github.com/RustScan/RustScan/commits?author=beeskerritt&quot; title=&quot;Tests&quot;&gt;âš ï¸&lt;/a&gt; &lt;a href=&quot;https://github.com/RustScan/RustScan/commits?author=beesan&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt; &lt;a href=&quot;#design-beeskerritt&quot; title=&quot;Design&quot;&gt;ğŸ¨&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://sakiir.ovh&quot;&gt;&lt;img src=&quot;https://avatars1.githubusercontent.com/u/9950578?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;SakiiR&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/RustScan/RustScan/commits?author=SakiiR&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt; &lt;a href=&quot;https://github.com/RustScan/RustScan/issues?q=author%3ASakiiR&quot; title=&quot;Bug reports&quot;&gt;ğŸ›&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/smackhack&quot;&gt;&lt;img src=&quot;https://avatars2.githubusercontent.com/u/48143394?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;smackhack&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#ideas-smackhack&quot; title=&quot;Ideas, Planning, &amp; Feedback&quot;&gt;ğŸ¤”&lt;/a&gt; &lt;a href=&quot;#example-smackhack&quot; title=&quot;Examples&quot;&gt;ğŸ’¡&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;http://bernardoamc.github.io/&quot;&gt;&lt;img src=&quot;https://avatars0.githubusercontent.com/u/428984?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Bernardo Araujo&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/RustScan/RustScan/commits?author=bernardoamc&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt; &lt;a href=&quot;https://github.com/RustScan/RustScan/issues?q=author%3Abernardoamc&quot; title=&quot;Bug reports&quot;&gt;ğŸ›&lt;/a&gt; &lt;a href=&quot;#design-bernardoamc&quot; title=&quot;Design&quot;&gt;ğŸ¨&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/Isona&quot;&gt;&lt;img src=&quot;https://avatars2.githubusercontent.com/u/11759523?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Izzy Whistlecroft&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/RustScan/RustScan/issues?q=author%3AIsona&quot; title=&quot;Bug reports&quot;&gt;ğŸ›&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://imlonghao.com&quot;&gt;&lt;img src=&quot;https://avatars1.githubusercontent.com/u/4951333?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;imlonghao&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/RustScan/RustScan/issues?q=author%3Aimlonghao&quot; title=&quot;Bug reports&quot;&gt;ğŸ›&lt;/a&gt; &lt;a href=&quot;#maintenance-imlonghao&quot; title=&quot;Maintenance&quot;&gt;ğŸš§&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/royharoush&quot;&gt;&lt;img src=&quot;https://avatars3.githubusercontent.com/u/8113056?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;royharoush&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#ideas-royharoush&quot; title=&quot;Ideas, Planning, &amp; Feedback&quot;&gt;ğŸ¤”&lt;/a&gt; &lt;a href=&quot;#design-royharoush&quot; title=&quot;Design&quot;&gt;ğŸ¨&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/Atul9&quot;&gt;&lt;img src=&quot;https://avatars1.githubusercontent.com/u/3390330?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Atul Bhosale&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/RustScan/RustScan/commits?author=Atul9&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://tgotwig.dev&quot;&gt;&lt;img src=&quot;https://avatars0.githubusercontent.com/u/30773779?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Thomas Gotwig&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#platform-TGotwig&quot; title=&quot;Packaging/porting to new platform&quot;&gt;ğŸ“¦&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/remigourdon&quot;&gt;&lt;img src=&quot;https://avatars3.githubusercontent.com/u/2874133?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;RÃ©mi Gourdon&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/RustScan/RustScan/commits?author=remigourdon&quot; title=&quot;Documentation&quot;&gt;ğŸ“–&lt;/a&gt; &lt;a href=&quot;https://github.com/RustScan/RustScan/commits?author=remigourdon&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://cmnatic.co.uk&quot;&gt;&lt;img src=&quot;https://avatars3.githubusercontent.com/u/4163116?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ben (CMNatic)&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/RustScan/RustScan/commits?author=cmnatic&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt; &lt;a href=&quot;https://github.com/RustScan/RustScan/commits?author=cmnatic&quot; title=&quot;Documentation&quot;&gt;ğŸ“–&lt;/a&gt; &lt;a href=&quot;#design-cmnatic&quot; title=&quot;Design&quot;&gt;ğŸ¨&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/Ferryistaken&quot;&gt;&lt;img src=&quot;https://avatars3.githubusercontent.com/u/47927670?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Alessandro Ferrari&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#content-Ferryistaken&quot; title=&quot;Content&quot;&gt;ğŸ–‹&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/Phenomite&quot;&gt;&lt;img src=&quot;https://avatars2.githubusercontent.com/u/8285537?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Phenomite&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#content-Phenomite&quot; title=&quot;Content&quot;&gt;ğŸ–‹&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://supersandro.de/&quot;&gt;&lt;img src=&quot;https://avatars2.githubusercontent.com/u/7258858?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Sandro&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#content-SuperSandro2000&quot; title=&quot;Content&quot;&gt;ğŸ–‹&lt;/a&gt; &lt;a href=&quot;https://github.com/RustScan/RustScan/issues?q=author%3ASuperSandro2000&quot; title=&quot;Bug reports&quot;&gt;ğŸ›&lt;/a&gt; &lt;a href=&quot;https://github.com/RustScan/RustScan/commits?author=SuperSandro2000&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://swag.lgbt&quot;&gt;&lt;img src=&quot;https://avatars2.githubusercontent.com/u/25358963?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Cass&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#platform-caass&quot; title=&quot;Packaging/porting to new platform&quot;&gt;ğŸ“¦&lt;/a&gt; &lt;a href=&quot;https://github.com/RustScan/RustScan/commits?author=caass&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt; &lt;a href=&quot;https://github.com/RustScan/RustScan/issues?q=author%3Acaass&quot; title=&quot;Bug reports&quot;&gt;ğŸ›&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/niklasmohrin&quot;&gt;&lt;img src=&quot;https://avatars0.githubusercontent.com/u/47574893?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Niklas Mohrin&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/RustScan/RustScan/commits?author=niklasmohrin&quot; title=&quot;Documentation&quot;&gt;ğŸ“–&lt;/a&gt; &lt;a href=&quot;https://github.com/RustScan/RustScan/commits?author=niklasmohrin&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt; &lt;a href=&quot;https://github.com/RustScan/RustScan/issues?q=author%3Aniklasmohrin&quot; title=&quot;Bug reports&quot;&gt;ğŸ›&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://liberapay.com/Artem4/&quot;&gt;&lt;img src=&quot;https://avatars0.githubusercontent.com/u/5614476?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Artem Polishchuk&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#platform-tim77&quot; title=&quot;Packaging/porting to new platform&quot;&gt;ğŸ“¦&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/buermarc&quot;&gt;&lt;img src=&quot;https://avatars2.githubusercontent.com/u/44375277?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;buermarc&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/RustScan/RustScan/commits?author=buermarc&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/bergabman&quot;&gt;&lt;img src=&quot;https://avatars1.githubusercontent.com/u/44554109?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;bergabman&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/RustScan/RustScan/commits?author=bergabman&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt; &lt;a href=&quot;https://github.com/RustScan/RustScan/issues?q=author%3Abergabman&quot; title=&quot;Bug reports&quot;&gt;ğŸ›&lt;/a&gt; &lt;a href=&quot;#design-bergabman&quot; title=&quot;Design&quot;&gt;ğŸ¨&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/dmitris&quot;&gt;&lt;img src=&quot;https://avatars0.githubusercontent.com/u/31205?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Dmitry Savintsev&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/RustScan/RustScan/commits?author=dmitris&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/bofh69&quot;&gt;&lt;img src=&quot;https://avatars3.githubusercontent.com/u/1444315?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Sebastian Andersson&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/RustScan/RustScan/commits?author=bofh69&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/mattcorbin&quot;&gt;&lt;img src=&quot;https://avatars3.githubusercontent.com/u/6537765?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Matt Corbin&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/RustScan/RustScan/commits?author=mattcorbin&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;http://rootsploit.com&quot;&gt;&lt;img src=&quot;https://avatars2.githubusercontent.com/u/67270834?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;RootSploit&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#blog-rootsploit&quot; title=&quot;Blogposts&quot;&gt;ğŸ“&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/eiffel-fl&quot;&gt;&lt;img src=&quot;https://avatars2.githubusercontent.com/u/12171754?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;eiffel-fl&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/RustScan/RustScan/commits?author=eiffel-fl&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/u5surf&quot;&gt;&lt;img src=&quot;https://avatars1.githubusercontent.com/u/14180225?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Y.Horie&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/RustScan/RustScan/commits?author=u5surf&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/okrplay&quot;&gt;&lt;img src=&quot;https://avatars3.githubusercontent.com/u/32576280?v=4&quot; width=&quot;100px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Oskar&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/RustScan/RustScan/commits?author=okrplay&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt; &lt;a href=&quot;https://github.com/RustScan/RustScan/commits?author=okrplay&quot; title=&quot;Tests&quot;&gt;âš ï¸&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;!-- markdownlint-enable --&gt;
&lt;!-- prettier-ignore-end --&gt;

&lt;!-- ALL-CONTRIBUTORS-LIST:END --&gt;

This project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind welcome!

&lt;!--Links--&gt;

[Docker]: https://hub.docker.com/r/cmnatic/rustscan &quot;This is the recommended distribution of rustscan&quot;
[kali]: https://github.com/RustScan/RustScan/wiki/Installation-Guide#%EF%B8%8F-debian--kali &quot;Read the install guide&quot;
[Kali/Debian]: https://github.com/RustScan/RustScan/releases &quot;Kali Debian&quot;
[Arch-Linux]: https://archlinux.org/packages/extra/x86_64/rustscan/ &quot;Arch Linux installation of Rustscan&quot;
[Homebrew]: https://formulae.brew.sh/formula/rustscan &quot;Homebrew install of Rustscan&quot;
[usage-1]: https://github.com/RustScan/RustScan/wiki/Usage &quot;Basic Usage of Rustscan&quot;
[usage-0]: https://github.com/RustScan/RustScan/wiki/Installation-Guide#docker-whale &quot;Use Docker Rustscan&quot;
[config-file-here]: https://github.com/RustScan/RustScan/wiki/Config-File &quot;RustScan Configuration File&quot;
[usage-2]: https://github.com/RustScan/RustScan/wiki/Things-you-may-want-to-do-with-RustScan-but-don&#039;t-understand-how &quot;Things you may want to do with rustscan but don&#039;t know how&quot;
[community-1]: https://github.com/RustScan/RustScan/wiki/Contributing &quot;Learn how to contribute&quot;
[distributions-1]: https://software.opensuse.org/package/rustscan?search_term=rustscan &quot;Open Suse rustscan distribution&quot;
[distributions-2]: https://copr.fedorainfracloud.org/coprs/atim/rustscan/ &quot;Rustscan in Fedora&quot;
[repology-1]: https://repology.org/project/rustscan/versions &quot;Packaging Status&quot;
[install-1]: https://github.com/RustScan/RustScan/wiki/Installation-Guide &quot;Installation guide&quot;
[accessible-2]: https://bees.substack.com/p/making-hacking-accessible &quot;Making Hacking Accessible&quot;
[extensible-2]: https://github.com/RustScan/RustScan/wiki/RustScan-Scripting-Engine &quot;Scripting Engine&quot;
[speed-2]: https://github.com/sharkdp/hyperfine &quot;Hyperfine&quot;
[speed-3]: https://github.com/RustScan/RustScan/wiki/Increasing-Speed-&amp;-Accuracy &quot;Increasing Speed &amp; Accuracy&quot;
[toc-community]: https://github.com/RustScan/RustScan#-community &quot;Community&quot;
[links-table-1]: https://github.com/RustScan/RustScan#-full-installation-guide &quot;Full installation guide&quot;
[links-table-2]: https://github.com/bee-san/RustScan/wiki &quot;Rustscan&quot;
[discord]: http://discord.skerritt.blog &quot;Discord blog&quot;
[toc-install]: https://github.com/RustScan/RustScan/wiki/Installation-Guide &quot;Installation Guide Wiki&quot;
[toc-docker-usage]: https://github.com/RustScan/RustScan/wiki/Installation-Guide#docker- &quot;Docker Installation Guide Wiki&quot;
[usage-guide]: https://github.com/RustScan/RustScan#-usage
[adaptive-learning]: https://github.com/RustScan/RustScan/wiki/Adaptive-Learning &quot;Adaptive Learning&quot;

&lt;!--Pictures--&gt;

[DockerPic]: https://github.com/RustScan/RustScan/blob/master/pictures/docker.png?raw=true &quot;Docker install&quot;
[Kali1]: https://github.com/RustScan/RustScan/blob/master/pictures/kali.png?raw=true &quot;Kali Picture&quot;
[Arch]: https://github.com/RustScan/RustScan/blob/master/pictures/arch.png?raw=true &quot;Arch Linux&quot;
[Apple]: https://raw.githubusercontent.com/RustScan/RustScan/master/pictures/apple.png?size &quot;Apple&quot;
[rustscan-svg]: https://repology.org/badge/vertical-allrepos/rustscan.svg &quot;Picture of rustscan repology&quot;
[accessible-1]: pictures/accessible.gif &quot;Fast&quot;
[adaptive-1]: pictures/adaptive.gif &quot;Adaptive&quot;
[extensible-1]: pictures/scripts.gif &quot;Scripts&quot;
[speed-1]: pictures/fast.gif &quot;Speed&quot;
[badge-1]: https://img.shields.io/archlinux/v/extra/x86_64/rustscan?style=plastic&amp;logo=archlinux&amp;link=https%3A%2F%2Farchlinux.org%2Fpackages%2Fextra%2Fx86_64%2Frustscan%2F
[badge-2]: https://img.shields.io/badge/Built%20with-Rust-Purple
[badge-3]: https://img.shields.io/github/downloads/rustscan/rustscan/total?label=GitHub%20Downloads
[badge-4]: https://img.shields.io/crates/d/rustscan?label=Cargo%20Downloads
[badge-5]: https://img.shields.io/discord/754001738184392704
[badge-6]: https://github.com/RustScan/RustScan/actions/workflows/build.yml/badge.svg?branch=master
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[EricLBuehler/mistral.rs]]></title>
            <link>https://github.com/EricLBuehler/mistral.rs</link>
            <guid>https://github.com/EricLBuehler/mistral.rs</guid>
            <pubDate>Sat, 12 Jul 2025 00:05:26 GMT</pubDate>
            <description><![CDATA[Blazingly fast LLM inference.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/EricLBuehler/mistral.rs">EricLBuehler/mistral.rs</a></h1>
            <p>Blazingly fast LLM inference.</p>
            <p>Language: Rust</p>
            <p>Stars: 5,873</p>
            <p>Forks: 430</p>
            <p>Stars today: 18 stars today</p>
            <h2>README</h2><pre>&lt;a name=&quot;top&quot;&gt;&lt;/a&gt;
&lt;h1 align=&quot;center&quot;&gt;
  mistral.rs
&lt;/h1&gt;

&lt;h3 align=&quot;center&quot;&gt;
Blazingly fast LLM inference.
&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
| &lt;a href=&quot;https://ericlbuehler.github.io/mistral.rs/mistralrs/&quot;&gt;&lt;b&gt;Rust Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs-pyo3/API.md&quot;&gt;&lt;b&gt;Python Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://discord.gg/SZrecqK8qw&quot;&gt;&lt;b&gt;Discord&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://matrix.to/#/#mistral.rs:matrix.org&quot;&gt;&lt;b&gt;Matrix&lt;/b&gt;&lt;/a&gt; |
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/EricLBuehler/mistral.rs/stargazers&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/EricLBuehler/mistral.rs?style=social&amp;label=Star&quot; alt=&quot;GitHub stars&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

**Mistral.rs is a cross-platform, highly-multimodal inference engine that brings you:**
- All-in-one multimodal workflow: textâ†”text, text+visionâ†”text, text+vision+audioâ†”text, textâ†’speech, textâ†’image
- APIs: Rust, Python, OpenAI HTTP server, MCP server
- ğŸ”— **MCP Client**: Connect to external tools and services automatically (file systems, web search, databases, APIs)
- Performance: ISQ, PagedAttention, FlashAttention

Please submit requests for new models [here](https://github.com/EricLBuehler/mistral.rs/issues/156).

## Get started fast ğŸš€

1) [Install](#installation-and-build)

2) [Get models](#getting-models)

3) Deploy with our easy to use APIs
    - [Python](examples/python)
    - [Rust](mistralrs/examples)
    - [OpenAI-compatible HTTP server](README.md#openai-http-server)
    - [Interactive mode](README.md#interactive-mode)
    - ğŸ”— [**MCP Client**](examples/MCP_QUICK_START.md) - Connect to external tools automatically

4) Try the **web chat app** for local in-browser conversation (text, vision, and speech support):
    - Quickstart [here](mistralrs-web-chat/README.md)
    - Run the server and visit [http://localhost:8080](http://localhost:8080) by default.

&lt;br&gt;

&lt;!-- Web Chat App --&gt;
&lt;details open&gt;
  &lt;summary&gt;ğŸ–¥ï¸ &lt;strong&gt;Web Chat App&lt;/strong&gt;&lt;/summary&gt;
  &lt;br&gt;
  &lt;img src=&quot;./res/chat.gif&quot; alt=&quot;Web Chat UI Demo&quot; /&gt;
  &lt;br&gt;
  Try our modern in-browser chat with text, vision, and speech support (TTS generation).
&lt;/details&gt;

&lt;!-- Interactive Mode --&gt;
&lt;details&gt;
  &lt;summary&gt;ğŸ’» &lt;strong&gt;Terminal Interactive Mode&lt;/strong&gt;&lt;/summary&gt;
  &lt;br&gt;
  &lt;img src=&quot;./res/demo.gif&quot; alt=&quot;Terminal Interactive Mode&quot; /&gt;
  &lt;br&gt;
  Prefer the terminal? Use interactive mode for a classic CLI experience.
&lt;/details&gt;

&lt;br&gt;

## Quick examples

*After following installation instructions*

- ğŸ’ğŸª†ğŸ’ğŸª†ğŸ’ Run the **Gemma 3n** family (E2B, E4B) with **vision**, **audio**, and **MatFormer** support: [documentation](docs/GEMMA3N.md)  
  &lt;details&gt;
    &lt;summary&gt;Show commands&lt;/summary&gt;

    **Normal use, run the full model (E4B or E2B):**
    ```bash
    ./mistralrs-server -i --isq 8 run -m google/gemma-3n-E4B-it
    ```

    **Use [MatFormer](docs/GEMMA3N.md#using-matformer-with-gemma-3n) to get a balanced smaller model:**
    ```bash
    ./mistralrs-server -i --isq 8 run -m google/gemma-3n-E4B-it \
      --matformer-config-path matformer_configs/gemma3n.csv \
      --matformer-slice-name &quot;Config for E2.49B (block-level)&quot;
    ```
  &lt;/details&gt;
  

- ğŸ¤—ğŸ¤—ğŸ¤— Run the **SmolLM 3** long-context hybrid-reasoning model with full tool-calling support: [documentation](docs/SMOLLM3.md)  
  &lt;details&gt;
    &lt;summary&gt;Show command&lt;/summary&gt;

    **Default, easiest:**
    ```bash
    ./mistralrs-server -i --isq 8 run -m HuggingFaceTB/SmolLM3-3B
    ```

    **UQFF prequantized:**
    ```bash
    ./mistralrs-server -i run -m EricB/SmolLM3-3B-UQFF -f smollm33b-q4k-0.uqff
    ```
  &lt;/details&gt;

- ğŸ”Š Run the **Dia 1.6b** model for highly-realistic dialogue generation: [documentation](docs/DIA.md)  
  &lt;details&gt;
    &lt;summary&gt;Show command&lt;/summary&gt;

    ```bash
    ./mistralrs-server -i speech -m nari-labs/Dia-1.6B -a dia
    ```
  &lt;/details&gt;

- ğŸ¦™ Run the **Llama 3.\* and Llama 4** models with long context &amp; vision support: [docs (llama 3.2)](docs/VLLAMA.md), [docs (llama 4)](docs/LLAMA4.md)  
  &lt;details&gt;
    &lt;summary&gt;Show commands&lt;/summary&gt;

    **Llama 4:**

    ```bash
    ./mistralrs-server -i --isq 4 run -m meta-llama/Llama-4-Scout-17B-16E-Instruct
    ```

    **Llama 3.1/3.2/3.3:**

    ```
    ./mistralrs-server -i --isq 8 run -m meta-llama/Llama-3.2-3B-Instruct
    ```

    **Llama 3.2 vision:**

    ```
    ./mistralrs-server -i --isq 8 run -m meta-llama/Llama-3.2-11B-Vision-Instruct
    ```

  &lt;/details&gt;

- ğŸ’ğŸ’ğŸ’ Run the **Gemma 3** family (1b, 4b, 12b, 27b) with 128k context &amp; vision support: [documentation](docs/GEMMA3.md)  
  &lt;details&gt;
    &lt;summary&gt;Show command&lt;/summary&gt;

    ```bash
    ./mistralrs-server -i --isq 8 run -m google/gemma-3-4b-it
    ```
  &lt;/details&gt;

- ğŸŒ²ğŸ“· Run the **FLUX.1** diffusion model: [documentation](docs/FLUX.md)  
  &lt;details&gt;
    &lt;summary&gt;Show command&lt;/summary&gt;

    ```bash
    ./mistralrs-server -i diffusion -m black-forest-labs/FLUX.1-schnell -a flux
    ```
  &lt;/details&gt;

- ğŸ§  Run the **Qwen 3** hybrid-reasoning model with full tool-calling support: [documentation](docs/QWEN3.md)  
  &lt;details&gt;
    &lt;summary&gt;Show command&lt;/summary&gt;

    ```bash
    ./mistralrs-server -i --isq 8 run -m Qwen/Qwen3-8B
    ```
  &lt;/details&gt;

- ğŸ”— **MCP Client** - Connect to external tools and services automatically: [**Quick Start Guide**](examples/MCP_QUICK_START.md)  
  &lt;details&gt;
    &lt;summary&gt;Show examples&lt;/summary&gt;

    **1. Create config file (`mcp-config.json`):**
    ```json
    {
      &quot;servers&quot;: [{
        &quot;name&quot;: &quot;Filesystem Tools&quot;,
        &quot;source&quot;: {
          &quot;type&quot;: &quot;Process&quot;,
          &quot;command&quot;: &quot;npx&quot;,
          &quot;args&quot;: [&quot;@modelcontextprotocol/server-filesystem&quot;, &quot;/tmp&quot;, &quot;-y&quot;]
        }
      }],
      &quot;auto_register_tools&quot;: true
    }
    ```

    **2. Start server with tools:**
    ```bash
    ./mistralrs-server --mcp-config mcp-config.json --port 1234 run -m Qwen/Qwen3-4B
    ```
    
    **3. Tools work automatically:**
    ```bash
    curl -X POST http://localhost:1234/v1/chat/completions \
      -d &#039;{&quot;model&quot;:&quot;Qwen/Qwen3-4B&quot;,&quot;messages&quot;:[{&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:&quot;List files in /tmp and create hello.txt&quot;}]}&#039;
    ```

    **Python API:**
    ```python
    mcp_config = mistralrs.McpClientConfigPy(
        servers=[mistralrs.McpServerConfigPy(
            name=&quot;Filesystem&quot;,
            source=mistralrs.McpServerSourcePy.Process(
                command=&quot;npx&quot;, 
                args=[&quot;@modelcontextprotocol/server-filesystem&quot;, &quot;/tmp&quot;, &quot;-y&quot;]
            )
        )],
        auto_register_tools=True
    )
    
    runner = mistralrs.Runner(
        which=mistralrs.Which.Plain(model_id=&quot;Qwen/Qwen3-4B&quot;),
        mcp_client_config=mcp_config
    )
    # Tools automatically available!
    ```

    **Rust API:**
    ```rust
    let model = TextModelBuilder::new(&quot;Qwen/Qwen3-4B&quot;)
        .with_mcp_client(mcp_config) // Tools automatically available!
        .build().await?;
    ```
  &lt;/details&gt;

## Description

[mistral.rs](https://github.com/EricLBuehler/mistral.rs) is a blazing-fast, cross-platform LLM inference engine with support for text, vision, image generation, and speech.

**Key Benefits:**

1. **Ease of Use**
   - [OpenAI-compatible HTTP server](docs/HTTP.md)
   - [Rust API](https://ericlbuehler.github.io/mistral.rs/mistralrs/) &amp; [Python API](mistralrs-pyo3/API.md)
   - [Automatic device mapping](docs/DEVICE_MAPPING.md) (multi-GPU, CPU)
   - [Chat templates](docs/CHAT_TOK.md) &amp; tokenizer auto-detection
   - [MCP server](docs/MCP/server.md) for structured, realtime tool calls
   - â­ [MCP client](examples/MCP_QUICK_START.md) to connect to external tools and services automatically

2. **Performance**
   - CPU acceleration (MKL, AVX, NEON, Accelerate)
   - GPU acceleration (CUDA with [FlashAttention](docs/FLASH_ATTENTION.md) &amp; cuDNN, Metal)
   - Automatic [tensor parallelism](docs/DISTRIBUTED/DISTRIBUTED.md) for splitting models across multiple devices
     - CUDA-specialized [NCCL](docs/DISTRIBUTED/NCCL.md)
     - Heterogeneous, flexible [Ring backend](docs/DISTRIBUTED/RING.md)

3. **Quantization**
   - [In-place quantization (ISQ)](docs/ISQ.md) of Hugging Face models
   - [GGML &amp; GGUF support](docs/QUANTS.md): 2â€“8 bit
   - [GPTQ](docs/QUANTS.md), [AWQ](scripts/convert_awq_marlin.py), [AFQ](docs/QUANTS.md), [HQQ](docs/QUANTS.md), [FP8](docs/QUANTS.md), [BNB](https://github.com/TimDettmers/bitsandbytes) (int8/fp4/nf4)
   - â­ Auto-select the fastest quant method
   - [KV cache quantization](docs/PAGED_ATTENTION.md#kv-cache-quantization)

4. **Flexibility**
   - [LoRA](docs/ADAPTER_MODELS.md) &amp; [X-LoRA](docs/ADAPTER_MODELS.md) adapters with weight merging
   - [AnyMoE](docs/ANYMOE.md): create MoE models on any base model
   - [Sampling &amp; penalty options](docs/SAMPLING.md)
   - Prompt chunking for large inputs
   - Integrated [tool calling](docs/TOOL_CALLING.md) with customizable Python/Rust native tool and search callbacks

5. **Advanced Features**
   - High-throughput with [PagedAttention](docs/PAGED_ATTENTION.md) &amp; FlashAttention V2/V3
   - Prefix caching (including multimodal)
   - Customizable quantization with [topology](docs/TOPOLOGY.md) &amp; [UQFF format](docs/UQFF.md)
   - Speculative decoding across models
   - â­ Agentic [web search integration](docs/WEB_SEARCH.md)

## APIs and Integrations

### Rust Crate

Rust multithreaded/async API for easy integration into any application.

- [Docs](https://ericlbuehler.github.io/mistral.rs/mistralrs/)
- [Examples](mistralrs/examples/) including [MCP client integration](mistralrs/examples/mcp_client)
- To use: add `mistralrs = { git = &quot;https://github.com/EricLBuehler/mistral.rs.git&quot; }` to your Cargo.toml
- **MCP Client**: Connect to external tools automatically - [Quick Start](examples/MCP_QUICK_START.md)

### Python API

Python API for mistral.rs.

- [Installation including PyPI](mistralrs-pyo3/_README.md)
- [Docs](mistralrs-pyo3/API.md)
- [Examples](examples/python) including [MCP client usage](examples/python/mcp_client.py)
- [Cookbook](examples/python/cookbook.ipynb)
- **MCP Client**: Full MCP integration - [Quick Start](examples/MCP_QUICK_START.md)

### HTTP Server

OpenAI API compatible API server

- [API Docs](docs/HTTP.md)
- [Launching the server or use the CLI](README.md#using-the-cli)
- [Example](examples/server/chat.py)
- [Use or extend the server in other axum projects](https://ericlbuehler.github.io/mistral.rs/mistralrs_server_core/)
- **MCP Client**: Configure via `--mcp-config` flag for automatic tool integration - [Quick Start](examples/MCP_QUICK_START.md)

### MCP Protocol

Serve the same models over the open [MCP](docs/mcp/server.md) (Model Context Protocol) in parallel to the HTTP API:

```bash
./mistralrs-server --mcp-port 4321 plain -m Qwen/Qwen3-4B
```

See the [docs](docs/mcp/server.md) for feature flags, examples and limitations.


### Llama Index integration

- Docs: https://docs.llamaindex.ai/en/stable/examples/llm/mistral_rs/

---

## Supported accelerators

| Accelerator              | Feature Flag  | Additional Flags       |
|--------------------------|---------------|------------------------|
| NVIDIA GPUs (CUDA)       | `cuda`        | `flash-attn`, `flash-attn-v3`, `cudnn`  |
| Apple Silicon GPU (Metal)| `metal`       |                        |
| CPU (Intel)              | `mkl`         |                        |
| CPU (Apple Accelerate)   | `accelerate`  |                        |
| Generic CPU (ARM/AVX)    | _none_        | ARM NEON / AVX enabled by default |

To enable one or more features, pass them to Cargo. For example:

```bash
cargo build --release --features &quot;cuda flash-attn cudnn&quot;
```

&gt; **Note for Linux users:** The `metal` feature is macOS-only and should not be used on Linux. Use `--features &quot;cuda flash-attn cudnn&quot;` for NVIDIA GPUs or `--features mkl` for Intel CPUs instead of `--all-features`.

## Installation and Build

&gt; Note: You can use our [Docker containers here](https://github.com/EricLBuehler/mistral.rs/pkgs/container/mistral.rs).
&gt; Learn more about running Docker containers: https://docs.docker.com/engine/reference/run/

- Install the [Python package here](mistralrs-pyo3/_README.md).
- The Python package has [wheels on PyPi](mistralrs-pyo3/_README.md#installation-from-pypi)!

1) Install required packages:
    - `OpenSSL` (*Example on Ubuntu:* `sudo apt install libssl-dev`)
    - &lt;b&gt;*Linux only:*&lt;/b&gt; `pkg-config` (*Example on Ubuntu:* `sudo apt install pkg-config`)

2) Install Rust: https://rustup.rs/

    *Example on Ubuntu:*
    ```bash
    curl --proto &#039;=https&#039; --tlsv1.2 -sSf https://sh.rustup.rs | sh
    source $HOME/.cargo/env
    ```

3) &lt;b&gt;*Optional:*&lt;/b&gt; Set HF token correctly (skip if already set or your model is not gated, or if you want to use the `token_source` parameters in Python or the command line.)
    - Note: you can install `huggingface-cli` as documented [here](https://huggingface.co/docs/huggingface_hub/en/installation). 
    ```bash
    huggingface-cli login
    ```

4) Download the code:
    ```bash
    git clone https://github.com/EricLBuehler/mistral.rs.git
    cd mistral.rs
    ```

5) Build or install `mistralrs-server`:
    - Build the `mistralrs-server` binary, which can be found at `target/release/mistralrs-server`.
        ```bash
        cargo build --release --features &lt;specify feature(s) here&gt;
        ```
    - Install with `cargo install` for easy command line usage

        Pass the same values to `--features` as you would for `cargo build`
        ```bash
        cargo install --path mistralrs-server --features &lt;specify feature(s) here&gt;
        ```

6) (*If you used `cargo build`*) The build process will output a binary `mistralrs-server` at `./target/release/mistralrs-server`. We can switch to that directory so that the binary can be accessed as `./mistralrs-server` with the following command:

    *Example on Ubuntu:*
    ```
    cd target/release
    ```

7) Use our APIs and integrations: 
    
    [APIs and integrations list](#apis-and-integrations)

## Getting models
&lt;details&gt;
&lt;summary&gt;Show: How to get models (Hub, local, GGUF, adapters, etc.)&lt;/summary&gt;

### Getting models from Hugging Face Hub
- **Default:** Downloads from Hugging Face Hub.
- For gated models, you can optionally set token source:
    - CLI: `./mistralrs-server --token-source env:HF_TOKEN ...`
    - Python: See [examples/python/token_source.py](examples/python/token_source.py)
    - If no token is found, tries `~/.cache/huggingface/token` or runs with no token.

### Loading models from local files
- Pass a path to a downloaded model from Hugging Face hub:
    - Example:  
      ```
      ./mistralrs-server -i run -m path/to/model
      ```

### Running GGUF models
- Minimal example:
  ```
  ./mistralrs-server gguf -m author/model-repo -f model-quant.gguf
  ```
- Specify tokenizer (if needed):
  ```
  ./mistralrs-server gguf -m author/model-repo -f file.gguf -t author/official-tokenizer
  ```
  (Or use the built-in GGUF tokenizer.)

### Adapters, X-LoRA, LoRA, Chat Templates
- Use the correct subcommand (`x-lora-*`, `lora-*`), pass model, adapter, or quant file as needed.
- See [docs/ADAPTER_MODELS.md](docs/ADAPTER_MODELS.md) for details.
- For chat templates: usually auto-detected, override with `--chat-template &lt;file&gt;`.  
  See [docs/CHAT_TOK.md](docs/CHAT_TOK.md).

### More model CLI examples
- See [Run with the CLI](#run-with-the-cli) below or [full documentation](docs/README.md).

&lt;/details&gt;

## Using the CLI

Mistral.rs uses subcommands to control the model type. Please run `./mistralrs-server --help` to see the subcommands which categorize the models by kind.

&gt; **ğŸš¨ Important:** The `run` subcommand (alias for `plain`/`vision-plain`) only auto-detects and runs **text** and **vision** models. It does **not** support **diffusion** or **speech** models. 
&gt; To run a diffusion model (e.g. FLUX series), use the `diffusion` subcommand:
&gt; ```bash
&gt; mistralrs-server -i diffusion -m &lt;model-id&gt; [options]
&gt; ```
&gt; To run a speech model (e.g. Dia), use the `speech` subcommand:
&gt; ```bash
&gt; mistralrs-server -i speech -m &lt;model-id&gt; [options]
&gt; ```
&gt; If you attempt to use `run` with diffusion or speech models, model loading will fail.

### Interactive mode

**Llama 3.2 3B running on an M3 Max with 8-bit ISQ:**

&lt;img src=&quot;./res/demo.gif&quot; alt=&quot;Interactive demo&quot; /&gt;

You can launch interactive mode, a simple chat application running in the terminal, by passing `-i`:

```bash
./mistralrs-server -i plain -m meta-llama/Llama-3.2-3B-Instruct
```

Vision models work seamlessly:

```bash
./mistralrs-server -i vision-plain -m lamm-mit/Cephalo-Llama-3.2-11B-Vision-Instruct-128k
```

Diffusion models can be run too (quantization and adapters are not yet supported):

```bash
./mistralrs-server -i diffusion -m black-forest-labs/FLUX.1-schnell -a flux
```

And you can run speech generation in your terminal!

```bash
./mistralrs-server -i speech -m nari-labs/Dia-1.6B -a dia
```

### OpenAI HTTP server

You can launch an HTTP server by replacing `-i` with `--port &lt;port&gt;`. For instance:

```bash
./mistralrs-server --port 1234 run -m microsoft/Phi-3.5-MoE-instruct
```

You can find documentation about the server itself [here](docs/HTTP.md).

### Multi-model support

Serve multiple models simultaneously from a single server instance. Perfect for comparing models, A/B testing, or serving different models for different use cases.

```bash
./mistralrs-server --port 1234 multi-model --config example-multi-model-config.json --default-model-id meta-llama/Llama-3.2-3B-Instruct
```

Select models in your requests using the `model` parameter:
```bash
curl http://localhost:1234/v1/chat/completions \
  -H &quot;Content-Type: application/json&quot; \
  -d &#039;{&quot;model&quot;: &quot;meta-llama/Llama-3.2-3B-Instruct&quot;, &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}]}&#039;
```

ğŸ“– **[Complete multi-model documentation â†’](docs/multi_model/README.md)**

### Structured selection with a `.toml` file

We provide a method to select models with a `.toml` file. The keys are the same as the command line, with `no_kv_cache` and `tokenizer_json` being &quot;global&quot; keys.

Example:
```bash
./mistralrs-server --port 1234 toml -f toml-selectors/gguf.toml
```

### Architecture for plain models

&gt; Note: for plain models, you can specify the data type to load and run in. This must be one of `f32`, `f16`, `bf16` or `auto` to choose based on the device. This is specified in the `--dype`/`-d` parameter after the model architecture (`plain`). For quantized models (gguf/ggml), you may specify data type of `f32` or `bf16` (`f16` is not recommended due to its lower precision in quantized inference).

If you do not specify the architecture, an attempt will be made to use the model&#039;s config. If this fails, please raise an issue.

&lt;details&gt;
  &lt;summary&gt;Show plain architectures&lt;/summary&gt;

- `mistral`
- `gemma`
- `mixtral`
- `llama`
- `phi2`
- `phi3`
- `phi3.5moe`
- `qwen2`
- `gemma2`
- `glm4`
- `starcoder2`
- `deepseekv2`
- `deepseekv3`
- `qwen3`
- `qwen3moe`
- `smollm3`

&lt;/details&gt;

### Architecture for vision models

&gt; Note: for vision models, you can specify the data type to load and run in. This must be one of `f32`, `f16`, `bf16` or `auto` to choose based on the device. This is specified in the `--dype`/`-d` parameter after the model architecture (`vision-plain`).

&lt;details&gt;
  &lt;summary&gt;Show vision architectures&lt;/summary&gt;

- `phi3v`
- `idefics2`
- `llava_next`
- `llava`
- `vllama`
- `qwen2vl`
- `idefics3`
- `minicpmo`
- `phi4mm`
- `qwen2_5vl`
- `gemma3`
- `mistral3`
- `llama4`
- `gemma3n`

&lt;/details&gt;

### Supported GGUF architectures

&lt;details&gt;
  &lt;summary&gt;Show supported GGUF architectures&lt;/summary&gt;

**Plain:**
- llama
- phi2
- phi3
- starcoder2
- qwen2
- qwen3

**With adapters:**
- llama
- phi3

&lt;/details&gt;

---

Please submit more benchmarks via raising an issue!

## Supported models

&lt;details&gt;
&lt;summary&gt;Show quantization support&lt;/summary&gt;

**Quantization support**
|Model|GGUF|GGML|ISQ|
|--|--|--|--|
|Mistral|âœ…| |âœ…|
|Gemma| | |âœ…|
|Llama|âœ…|âœ…|âœ…|
|Mixtral|âœ…| |âœ…|
|Phi 2|âœ…| |âœ…|
|Phi 3|âœ…| |âœ…|
|Phi 3.5 MoE| | |âœ…|
|Qwen 2.5| | |âœ…|
|Phi 3 Vision| | |âœ…|
|Idefics 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[rustdesk/rustdesk]]></title>
            <link>https://github.com/rustdesk/rustdesk</link>
            <guid>https://github.com/rustdesk/rustdesk</guid>
            <pubDate>Sat, 12 Jul 2025 00:05:25 GMT</pubDate>
            <description><![CDATA[An open-source remote desktop application designed for self-hosting, as an alternative to TeamViewer.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/rustdesk/rustdesk">rustdesk/rustdesk</a></h1>
            <p>An open-source remote desktop application designed for self-hosting, as an alternative to TeamViewer.</p>
            <p>Language: Rust</p>
            <p>Stars: 92,547</p>
            <p>Forks: 13,464</p>
            <p>Stars today: 110 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;res/logo-header.svg&quot; alt=&quot;RustDesk - Your remote desktop&quot;&gt;&lt;br&gt;
  &lt;a href=&quot;#raw-steps-to-build&quot;&gt;Build&lt;/a&gt; â€¢
  &lt;a href=&quot;#how-to-build-with-docker&quot;&gt;Docker&lt;/a&gt; â€¢
  &lt;a href=&quot;#file-structure&quot;&gt;Structure&lt;/a&gt; â€¢
  &lt;a href=&quot;#snapshot&quot;&gt;Snapshot&lt;/a&gt;&lt;br&gt;
  [&lt;a href=&quot;docs/README-UA.md&quot;&gt;Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°&lt;/a&gt;] | [&lt;a href=&quot;docs/README-CS.md&quot;&gt;Äesky&lt;/a&gt;] | [&lt;a href=&quot;docs/README-ZH.md&quot;&gt;ä¸­æ–‡&lt;/a&gt;] | [&lt;a href=&quot;docs/README-HU.md&quot;&gt;Magyar&lt;/a&gt;] | [&lt;a href=&quot;docs/README-ES.md&quot;&gt;EspaÃ±ol&lt;/a&gt;] | [&lt;a href=&quot;docs/README-FA.md&quot;&gt;ÙØ§Ø±Ø³ÛŒ&lt;/a&gt;] | [&lt;a href=&quot;docs/README-FR.md&quot;&gt;FranÃ§ais&lt;/a&gt;] | [&lt;a href=&quot;docs/README-DE.md&quot;&gt;Deutsch&lt;/a&gt;] | [&lt;a href=&quot;docs/README-PL.md&quot;&gt;Polski&lt;/a&gt;] | [&lt;a href=&quot;docs/README-ID.md&quot;&gt;Indonesian&lt;/a&gt;] | [&lt;a href=&quot;docs/README-FI.md&quot;&gt;Suomi&lt;/a&gt;] | [&lt;a href=&quot;docs/README-ML.md&quot;&gt;à´®à´²à´¯à´¾à´³à´‚&lt;/a&gt;] | [&lt;a href=&quot;docs/README-JP.md&quot;&gt;æ—¥æœ¬èª&lt;/a&gt;] | [&lt;a href=&quot;docs/README-NL.md&quot;&gt;Nederlands&lt;/a&gt;] | [&lt;a href=&quot;docs/README-IT.md&quot;&gt;Italiano&lt;/a&gt;] | [&lt;a href=&quot;docs/README-RU.md&quot;&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt;] | [&lt;a href=&quot;docs/README-PTBR.md&quot;&gt;PortuguÃªs (Brasil)&lt;/a&gt;] | [&lt;a href=&quot;docs/README-EO.md&quot;&gt;Esperanto&lt;/a&gt;] | [&lt;a href=&quot;docs/README-KR.md&quot;&gt;í•œêµ­ì–´&lt;/a&gt;] | [&lt;a href=&quot;docs/README-AR.md&quot;&gt;Ø§Ù„Ø¹Ø±Ø¨ÙŠ&lt;/a&gt;] | [&lt;a href=&quot;docs/README-VN.md&quot;&gt;Tiáº¿ng Viá»‡t&lt;/a&gt;] | [&lt;a href=&quot;docs/README-DA.md&quot;&gt;Dansk&lt;/a&gt;] | [&lt;a href=&quot;docs/README-GR.md&quot;&gt;Î•Î»Î»Î·Î½Î¹ÎºÎ¬&lt;/a&gt;] | [&lt;a href=&quot;docs/README-TR.md&quot;&gt;TÃ¼rkÃ§e&lt;/a&gt;] | [&lt;a href=&quot;docs/README-NO.md&quot;&gt;Norsk&lt;/a&gt;]&lt;br&gt;
  &lt;b&gt;We need your help to translate this README, &lt;a href=&quot;https://github.com/rustdesk/rustdesk/tree/master/src/lang&quot;&gt;RustDesk UI&lt;/a&gt; and &lt;a href=&quot;https://github.com/rustdesk/doc.rustdesk.com&quot;&gt;RustDesk Doc&lt;/a&gt; to your native language&lt;/b&gt;
&lt;/p&gt;

&gt; [!Caution]
&gt; **Misuse Disclaimer:** &lt;br&gt;
&gt; The developers of RustDesk do not condone or support any unethical or illegal use of this software. Misuse, such as unauthorized access, control or invasion of privacy, is strictly against our guidelines. The authors are not responsible for any misuse of the application.


Chat with us: [Discord](https://discord.gg/nDceKgxnkV) | [Twitter](https://twitter.com/rustdesk) | [Reddit](https://www.reddit.com/r/rustdesk) | [YouTube](https://www.youtube.com/@rustdesk)

[![ko-fi](https://ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/I2I04VU09)

Yet another remote desktop solution, written in Rust. Works out of the box with no configuration required. You have full control of your data, with no concerns about security. You can use our rendezvous/relay server, [set up your own](https://rustdesk.com/server), or [write your own rendezvous/relay server](https://github.com/rustdesk/rustdesk-server-demo).

![image](https://user-images.githubusercontent.com/71636191/171661982-430285f0-2e12-4b1d-9957-4a58e375304d.png)

RustDesk welcomes contribution from everyone. See [CONTRIBUTING.md](docs/CONTRIBUTING.md) for help getting started.

[**FAQ**](https://github.com/rustdesk/rustdesk/wiki/FAQ)

[**BINARY DOWNLOAD**](https://github.com/rustdesk/rustdesk/releases)

[**NIGHTLY BUILD**](https://github.com/rustdesk/rustdesk/releases/tag/nightly)

[&lt;img src=&quot;https://f-droid.org/badge/get-it-on.png&quot;
    alt=&quot;Get it on F-Droid&quot;
    height=&quot;80&quot;&gt;](https://f-droid.org/en/packages/com.carriez.flutter_hbb)
[&lt;img src=&quot;https://flathub.org/api/badge?svg&amp;locale=en&quot;
    alt=&quot;Get it on Flathub&quot;
    height=&quot;80&quot;&gt;](https://flathub.org/apps/com.rustdesk.RustDesk)

## Dependencies

Desktop versions use Flutter or Sciter (deprecated) for GUI, this tutorial is for Sciter only, since it is easier and more friendly to start. Check out our [CI](https://github.com/rustdesk/rustdesk/blob/master/.github/workflows/flutter-build.yml) for building Flutter version.

Please download Sciter dynamic library yourself.

[Windows](https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.win/x64/sciter.dll) |
[Linux](https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.lnx/x64/libsciter-gtk.so) |
[macOS](https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.osx/libsciter.dylib)

## Raw Steps to build

- Prepare your Rust development env and C++ build env

- Install [vcpkg](https://github.com/microsoft/vcpkg), and set `VCPKG_ROOT` env variable correctly

  - Windows: vcpkg install libvpx:x64-windows-static libyuv:x64-windows-static opus:x64-windows-static aom:x64-windows-static
  - Linux/macOS: vcpkg install libvpx libyuv opus aom

- run `cargo run`

## [Build](https://rustdesk.com/docs/en/dev/build/)

## How to Build on Linux

### Ubuntu 18 (Debian 10)

```sh
sudo apt install -y zip g++ gcc git curl wget nasm yasm libgtk-3-dev clang libxcb-randr0-dev libxdo-dev \
        libxfixes-dev libxcb-shape0-dev libxcb-xfixes0-dev libasound2-dev libpulse-dev cmake make \
        libclang-dev ninja-build libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev libpam0g-dev
```

### openSUSE Tumbleweed

```sh
sudo zypper install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libXfixes-devel cmake alsa-lib-devel gstreamer-devel gstreamer-plugins-base-devel xdotool-devel pam-devel
```

### Fedora 28 (CentOS 8)

```sh
sudo yum -y install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libxdo-devel libXfixes-devel pulseaudio-libs-devel cmake alsa-lib-devel gstreamer1-devel gstreamer1-plugins-base-devel pam-devel
```

### Arch (Manjaro)

```sh
sudo pacman -Syu --needed unzip git cmake gcc curl wget yasm nasm zip make pkg-config clang gtk3 xdotool libxcb libxfixes alsa-lib pipewire
```

### Install vcpkg

```sh
git clone https://github.com/microsoft/vcpkg
cd vcpkg
git checkout 2023.04.15
cd ..
vcpkg/bootstrap-vcpkg.sh
export VCPKG_ROOT=$HOME/vcpkg
vcpkg/vcpkg install libvpx libyuv opus aom
```

### Fix libvpx (For Fedora)

```sh
cd vcpkg/buildtrees/libvpx/src
cd *
./configure
sed -i &#039;s/CFLAGS+=-I/CFLAGS+=-fPIC -I/g&#039; Makefile
sed -i &#039;s/CXXFLAGS+=-I/CXXFLAGS+=-fPIC -I/g&#039; Makefile
make
cp libvpx.a $HOME/vcpkg/installed/x64-linux/lib/
cd
```

### Build

```sh
curl --proto &#039;=https&#039; --tlsv1.2 -sSf https://sh.rustup.rs | sh
source $HOME/.cargo/env
git clone --recurse-submodules https://github.com/rustdesk/rustdesk
cd rustdesk
mkdir -p target/debug
wget https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.lnx/x64/libsciter-gtk.so
mv libsciter-gtk.so target/debug
VCPKG_ROOT=$HOME/vcpkg cargo run
```

## How to build with Docker

Begin by cloning the repository and building the Docker container:

```sh
git clone https://github.com/rustdesk/rustdesk
cd rustdesk
git submodule update --init --recursive
docker build -t &quot;rustdesk-builder&quot; .
```

Then, each time you need to build the application, run the following command:

```sh
docker run --rm -it -v $PWD:/home/user/rustdesk -v rustdesk-git-cache:/home/user/.cargo/git -v rustdesk-registry-cache:/home/user/.cargo/registry -e PUID=&quot;$(id -u)&quot; -e PGID=&quot;$(id -g)&quot; rustdesk-builder
```

Note that the first build may take longer before dependencies are cached, subsequent builds will be faster. Additionally, if you need to specify different arguments to the build command, you may do so at the end of the command in the `&lt;OPTIONAL-ARGS&gt;` position. For instance, if you wanted to build an optimized release version, you would run the command above followed by `--release`. The resulting executable will be available in the target folder on your system, and can be run with:

```sh
target/debug/rustdesk
```

Or, if you&#039;re running a release executable:

```sh
target/release/rustdesk
```

Please ensure that you run these commands from the root of the RustDesk repository, or the application may not find the required resources. Also note that other cargo subcommands such as `install` or `run` are not currently supported via this method as they would install or run the program inside the container instead of the host.

## File Structure

- **[libs/hbb_common](https://github.com/rustdesk/rustdesk/tree/master/libs/hbb_common)**: video codec, config, tcp/udp wrapper, protobuf, fs functions for file transfer, and some other utility functions
- **[libs/scrap](https://github.com/rustdesk/rustdesk/tree/master/libs/scrap)**: screen capture
- **[libs/enigo](https://github.com/rustdesk/rustdesk/tree/master/libs/enigo)**: platform specific keyboard/mouse control
- **[libs/clipboard](https://github.com/rustdesk/rustdesk/tree/master/libs/clipboard)**: file copy and paste implementation for Windows, Linux, macOS.
- **[src/ui](https://github.com/rustdesk/rustdesk/tree/master/src/ui)**: obsolete Sciter UI (deprecated)
- **[src/server](https://github.com/rustdesk/rustdesk/tree/master/src/server)**: audio/clipboard/input/video services, and network connections
- **[src/client.rs](https://github.com/rustdesk/rustdesk/tree/master/src/client.rs)**: start a peer connection
- **[src/rendezvous_mediator.rs](https://github.com/rustdesk/rustdesk/tree/master/src/rendezvous_mediator.rs)**: Communicate with [rustdesk-server](https://github.com/rustdesk/rustdesk-server), wait for remote direct (TCP hole punching) or relayed connection
- **[src/platform](https://github.com/rustdesk/rustdesk/tree/master/src/platform)**: platform specific code
- **[flutter](https://github.com/rustdesk/rustdesk/tree/master/flutter)**: Flutter code for desktop and mobile
- **[flutter/web/js](https://github.com/rustdesk/rustdesk/tree/master/flutter/web/v1/js)**: JavaScript for Flutter web client

## Screenshots

![Connection Manager](https://github.com/rustdesk/rustdesk/assets/28412477/db82d4e7-c4bc-4823-8e6f-6af7eadf7651)

![Connected to a Windows PC](https://github.com/rustdesk/rustdesk/assets/28412477/9baa91e9-3362-4d06-aa1a-7518edcbd7ea)

![File Transfer](https://github.com/rustdesk/rustdesk/assets/28412477/39511ad3-aa9a-4f8c-8947-1cce286a46ad)

![TCP Tunneling](https://github.com/rustdesk/rustdesk/assets/28412477/78e8708f-e87e-4570-8373-1360033ea6c5)

</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[cloudflare/quiche]]></title>
            <link>https://github.com/cloudflare/quiche</link>
            <guid>https://github.com/cloudflare/quiche</guid>
            <pubDate>Sat, 12 Jul 2025 00:05:24 GMT</pubDate>
            <description><![CDATA[ğŸ¥§ Savoury implementation of the QUIC transport protocol and HTTP/3]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/cloudflare/quiche">cloudflare/quiche</a></h1>
            <p>ğŸ¥§ Savoury implementation of the QUIC transport protocol and HTTP/3</p>
            <p>Language: Rust</p>
            <p>Stars: 10,326</p>
            <p>Forks: 833</p>
            <p>Stars today: 4 stars today</p>
            <h2>README</h2><pre>![quiche](quiche.svg)

[![crates.io](https://img.shields.io/crates/v/quiche.svg)](https://crates.io/crates/quiche)
[![docs.rs](https://docs.rs/quiche/badge.svg)](https://docs.rs/quiche)
[![license](https://img.shields.io/github/license/cloudflare/quiche.svg)](https://opensource.org/licenses/BSD-2-Clause)
![build](https://img.shields.io/github/actions/workflow/status/cloudflare/quiche/stable.yml?branch=master)

[quiche] is an implementation of the QUIC transport protocol and HTTP/3 as
specified by the [IETF]. It provides a low level API for processing QUIC packets
and handling connection state. The application is responsible for providing I/O
(e.g. sockets handling) as well as an event loop with support for timers.

For more information on how quiche came about and some insights into its design
you can read a [post] on Cloudflare&#039;s blog that goes into some more detail.

[quiche]: https://docs.quic.tech/quiche/
[ietf]: https://quicwg.org/
[post]: https://blog.cloudflare.com/enjoy-a-slice-of-quic-and-rust/

Who uses quiche?
----------------

### Cloudflare

quiche powers Cloudflare edge network&#039;s [HTTP/3 support][cloudflare-http3]. The
[cloudflare-quic.com](https://cloudflare-quic.com) website can be used for
testing and experimentation.

### Android

Android&#039;s DNS resolver uses quiche to [implement DNS over HTTP/3][android-http3].

### curl

quiche can be [integrated into curl][curl-http3] to provide support for HTTP/3.

[cloudflare-http3]: https://blog.cloudflare.com/http3-the-past-present-and-future/
[android-http3]: https://security.googleblog.com/2022/07/dns-over-http3-in-android.html
[curl-http3]: https://github.com/curl/curl/blob/master/docs/HTTP3.md#quiche-version

Getting Started
---------------

### Command-line apps

Before diving into the quiche API, here are a few examples on how to use the
quiche tools provided as part of the [quiche-apps](apps/) crate.

After cloning the project according to the command mentioned in the [building](#building) section, the client can be run as follows:

```bash
 $ cargo run --bin quiche-client -- https://cloudflare-quic.com/
```

while the server can be run as follows:

```bash
 $ cargo run --bin quiche-server -- --cert apps/src/bin/cert.crt --key apps/src/bin/cert.key
```

(note that the certificate provided is self-signed and should not be used in
production)

Use the `--help` command-line flag to get a more detailed description of each
tool&#039;s options.

### Configuring connections

The first step in establishing a QUIC connection using quiche is creating a
[`Config`] object:

```rust
let mut config = quiche::Config::new(quiche::PROTOCOL_VERSION)?;
config.set_application_protos(&amp;[b&quot;example-proto&quot;]);

// Additional configuration specific to application and use case...
```

The [`Config`] object controls important aspects of the QUIC connection such
as QUIC version, ALPN IDs, flow control, congestion control, idle timeout
and other properties or features.

QUIC is a general-purpose transport protocol and there are several
configuration properties where there is no reasonable default value. For
example, the permitted number of concurrent streams of any particular type
is dependent on the application running over QUIC, and other use-case
specific concerns.

quiche defaults several properties to zero, applications most likely need
to set these to something else to satisfy their needs using the following:

- [`set_initial_max_streams_bidi()`]
- [`set_initial_max_streams_uni()`]
- [`set_initial_max_data()`]
- [`set_initial_max_stream_data_bidi_local()`]
- [`set_initial_max_stream_data_bidi_remote()`]
- [`set_initial_max_stream_data_uni()`]

[`Config`] also holds TLS configuration. This can be changed by mutators on
the an existing object, or by constructing a TLS context manually and
creating a configuration using [`with_boring_ssl_ctx_builder()`].

A configuration object can be shared among multiple connections.

### Connection setup

On the client-side the [`connect()`] utility function can be used to create
a new connection, while [`accept()`] is for servers:

```rust
// Client connection.
let conn = quiche::connect(Some(&amp;server_name), &amp;scid, local, peer, &amp;mut config)?;

// Server connection.
let conn = quiche::accept(&amp;scid, None, local, peer, &amp;mut config)?;
```

### Handling incoming packets

Using the connection&#039;s [`recv()`] method the application can process
incoming packets that belong to that connection from the network:

```rust
let to = socket.local_addr().unwrap();

loop {
    let (read, from) = socket.recv_from(&amp;mut buf).unwrap();

    let recv_info = quiche::RecvInfo { from, to };

    let read = match conn.recv(&amp;mut buf[..read], recv_info) {
        Ok(v) =&gt; v,

        Err(e) =&gt; {
            // An error occurred, handle it.
            break;
        },
    };
}
```

### Generating outgoing packets

Outgoing packet are generated using the connection&#039;s [`send()`] method
instead:

```rust
loop {
    let (write, send_info) = match conn.send(&amp;mut out) {
        Ok(v) =&gt; v,

        Err(quiche::Error::Done) =&gt; {
            // Done writing.
            break;
        },

        Err(e) =&gt; {
            // An error occurred, handle it.
            break;
        },
    };

    socket.send_to(&amp;out[..write], &amp;send_info.to).unwrap();
}
```

When packets are sent, the application is responsible for maintaining a
timer to react to time-based connection events. The timer expiration can be
obtained using the connection&#039;s [`timeout()`] method.

```rust
let timeout = conn.timeout();
```

The application is responsible for providing a timer implementation, which
can be specific to the operating system or networking framework used. When
a timer expires, the connection&#039;s [`on_timeout()`] method should be called,
after which additional packets might need to be sent on the network:

```rust
// Timeout expired, handle it.
conn.on_timeout();

// Send more packets as needed after timeout.
loop {
    let (write, send_info) = match conn.send(&amp;mut out) {
        Ok(v) =&gt; v,

        Err(quiche::Error::Done) =&gt; {
            // Done writing.
            break;
        },

        Err(e) =&gt; {
            // An error occurred, handle it.
            break;
        },
    };

    socket.send_to(&amp;out[..write], &amp;send_info.to).unwrap();
}
```

#### Pacing

It is recommended that applications [pace] sending of outgoing packets to
avoid creating packet bursts that could cause short-term congestion and
losses in the network.

quiche exposes pacing hints for outgoing packets through the [`at`] field
of the [`SendInfo`] structure that is returned by the [`send()`] method.
This field represents the time when a specific packet should be sent into
the network.

Applications can use these hints by artificially delaying the sending of
packets through platform-specific mechanisms (such as the [`SO_TXTIME`]
socket option on Linux), or custom methods (for example by using user-space
timers).

[pace]: https://datatracker.ietf.org/doc/html/rfc9002#section-7.7
[`SO_TXTIME`]: https://man7.org/linux/man-pages/man8/tc-etf.8.html

### Sending and receiving stream data

After some back and forth, the connection will complete its handshake and
will be ready for sending or receiving application data.

Data can be sent on a stream by using the [`stream_send()`] method:

```rust
if conn.is_established() {
    // Handshake completed, send some data on stream 0.
    conn.stream_send(0, b&quot;hello&quot;, true)?;
}
```

The application can check whether there are any readable streams by using
the connection&#039;s [`readable()`] method, which returns an iterator over all
the streams that have outstanding data to read.

The [`stream_recv()`] method can then be used to retrieve the application
data from the readable stream:

```rust
if conn.is_established() {
    // Iterate over readable streams.
    for stream_id in conn.readable() {
        // Stream is readable, read until there&#039;s no more data.
        while let Ok((read, fin)) = conn.stream_recv(stream_id, &amp;mut buf) {
            println!(&quot;Got {} bytes on stream {}&quot;, read, stream_id);
        }
    }
}
```

### HTTP/3

The quiche [HTTP/3 module] provides a high level API for sending and
receiving HTTP requests and responses on top of the QUIC transport protocol.

[`Config`]: https://docs.quic.tech/quiche/struct.Config.html
[`set_initial_max_streams_bidi()`]: https://docs.rs/quiche/latest/quiche/struct.Config.html#method.set_initial_max_streams_bidi
[`set_initial_max_streams_uni()`]: https://docs.rs/quiche/latest/quiche/struct.Config.html#method.set_initial_max_streams_uni
[`set_initial_max_data()`]: https://docs.rs/quiche/latest/quiche/struct.Config.html#method.set_initial_max_data
[`set_initial_max_stream_data_bidi_local()`]: https://docs.rs/quiche/latest/quiche/struct.Config.html#method.set_initial_max_stream_data_bidi_local
[`set_initial_max_stream_data_bidi_remote()`]: https://docs.rs/quiche/latest/quiche/struct.Config.html#method.set_initial_max_stream_data_bidi_remote
[`set_initial_max_stream_data_uni()`]: https://docs.rs/quiche/latest/quiche/struct.Config.html#method.set_initial_max_stream_data_uni
[`with_boring_ssl_ctx_builder()`]: https://docs.quic.tech/quiche/struct.Config.html#method.with_boring_ssl_ctx_builder
[`connect()`]: https://docs.quic.tech/quiche/fn.connect.html
[`accept()`]: https://docs.quic.tech/quiche/fn.accept.html
[`recv()`]: https://docs.quic.tech/quiche/struct.Connection.html#method.recv
[`send()`]: https://docs.quic.tech/quiche/struct.Connection.html#method.send
[`timeout()`]: https://docs.quic.tech/quiche/struct.Connection.html#method.timeout
[`on_timeout()`]: https://docs.quic.tech/quiche/struct.Connection.html#method.on_timeout
[`stream_send()`]: https://docs.quic.tech/quiche/struct.Connection.html#method.stream_send
[`readable()`]: https://docs.quic.tech/quiche/struct.Connection.html#method.readable
[`stream_recv()`]: https://docs.quic.tech/quiche/struct.Connection.html#method.stream_recv
[HTTP/3 module]: https://docs.quic.tech/quiche/h3/index.html

Have a look at the [quiche/examples/] directory for more complete examples on
how to use the quiche API, including examples on how to use quiche in C/C++
applications (see below for more information).

[examples/]: quiche/examples/

Calling quiche from C/C++
-------------------------

quiche exposes a [thin C API] on top of the Rust API that can be used to more
easily integrate quiche into C/C++ applications (as well as in other languages
that allow calling C APIs via some form of FFI). The C API follows the same
design of the Rust one, modulo the constraints imposed by the C language itself.

When running ``cargo build``, a static library called ``libquiche.a`` will be
built automatically alongside the Rust one. This is fully stand-alone and can
be linked directly into C/C++ applications.

Note that in order to enable the FFI API, the ``ffi`` feature must be enabled (it
is disabled by default), by passing ``--features ffi`` to ``cargo``.

[thin C API]: https://github.com/cloudflare/quiche/blob/master/quiche/include/quiche.h

Building
--------

quiche requires Rust 1.82 or later to build. The latest stable Rust release can
be installed using [rustup](https://rustup.rs/).

Once the Rust build environment is setup, the quiche source code can be fetched
using git:

```bash
 $ git clone --recursive https://github.com/cloudflare/quiche
```

and then built using cargo:

```bash
 $ cargo build --examples
```

cargo can also be used to run the testsuite:

```bash
 $ cargo test
```

Note that [BoringSSL], which is used to implement QUIC&#039;s cryptographic handshake
based on TLS, needs to be built and linked to quiche. This is done automatically
when building quiche using cargo, but requires the `cmake` command to be
available during the build process. On Windows you also need
[NASM](https://www.nasm.us/). The [official BoringSSL
documentation](https://github.com/google/boringssl/blob/master/BUILDING.md) has
more details.

In alternative you can use your own custom build of BoringSSL by configuring
the BoringSSL directory with the ``QUICHE_BSSL_PATH`` environment variable:

```bash
 $ QUICHE_BSSL_PATH=&quot;/path/to/boringssl&quot; cargo build --examples
```

Alternatively you can use [OpenSSL/quictls]. To enable quiche to use this vendor
the ``openssl`` feature can be added to the ``--feature`` list. Be aware that
``0-RTT`` is not supported if this vendor is used.

[BoringSSL]: https://boringssl.googlesource.com/boringssl/

[OpenSSL/quictls]: https://github.com/quictls/openssl

### Building for Android

Building quiche for Android (NDK version 19 or higher, 21 recommended), can be
done using [cargo-ndk] (v2.0 or later).

First the [Android NDK] needs to be installed, either using Android Studio or
directly, and the `ANDROID_NDK_HOME` environment variable needs to be set to the
NDK installation path, e.g.:

```bash
 $ export ANDROID_NDK_HOME=/usr/local/share/android-ndk
```

Then the Rust toolchain for the Android architectures needed can be installed as
follows:

```bash
 $ rustup target add aarch64-linux-android armv7-linux-androideabi i686-linux-android x86_64-linux-android
```

Note that the minimum API level is 21 for all target architectures.

[cargo-ndk] (v2.0 or later) also needs to be installed:

```bash
 $ cargo install cargo-ndk
```

Finally the quiche library can be built using the following procedure. Note that
the `-t &lt;architecture&gt;` and `-p &lt;NDK version&gt;` options are mandatory.

```bash
 $ cargo ndk -t arm64-v8a -p 21 -- build --features ffi
```

See [build_android_ndk19.sh] for more information.

[Android NDK]: https://developer.android.com/ndk
[cargo-ndk]: https://docs.rs/crate/cargo-ndk
[build_android_ndk19.sh]: https://github.com/cloudflare/quiche/blob/master/tools/android/build_android_ndk19.sh

### Building for iOS

To build quiche for iOS, you need the following:

- Install Xcode command-line tools. You can install them with Xcode or with the
  following command:

```bash
 $ xcode-select --install
```

- Install the Rust toolchain for iOS architectures:

```bash
 $ rustup target add aarch64-apple-ios x86_64-apple-ios
```

- Install `cargo-lipo`:

```bash
 $ cargo install cargo-lipo
```

To build libquiche, run the following command:

```bash
 $ cargo lipo --features ffi
```

or

```bash
 $ cargo lipo --features ffi --release
```

iOS build is tested in Xcode 10.1 and Xcode 11.2.

### Building Docker images

In order to build the Docker images, simply run the following command:

```bash
 $ make docker-build
```

You can find the quiche Docker images on the following Docker Hub repositories:

- [cloudflare/quiche](https://hub.docker.com/repository/docker/cloudflare/quiche)
- [cloudflare/quiche-qns](https://hub.docker.com/repository/docker/cloudflare/quiche-qns)

The `latest` tag will be updated whenever quiche master branch updates.

**cloudflare/quiche**

Provides a server and client installed in /usr/local/bin.

**cloudflare/quiche-qns**

Provides the script to test quiche within the [quic-interop-runner](https://github.com/marten-seemann/quic-interop-runner).

Copyright
---------

Copyright (C) 2018-2019, Cloudflare, Inc.

See [COPYING] for the license.

[COPYING]: https://github.com/cloudflare/quiche/tree/master/COPYING
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[zed-industries/zed]]></title>
            <link>https://github.com/zed-industries/zed</link>
            <guid>https://github.com/zed-industries/zed</guid>
            <pubDate>Sat, 12 Jul 2025 00:05:23 GMT</pubDate>
            <description><![CDATA[Code at the speed of thought â€“ Zed is a high-performance, multiplayer code editor from the creators of Atom and Tree-sitter.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/zed-industries/zed">zed-industries/zed</a></h1>
            <p>Code at the speed of thought â€“ Zed is a high-performance, multiplayer code editor from the creators of Atom and Tree-sitter.</p>
            <p>Language: Rust</p>
            <p>Stars: 62,354</p>
            <p>Forks: 4,678</p>
            <p>Stars today: 49 stars today</p>
            <h2>README</h2><pre># Zed

[![CI](https://github.com/zed-industries/zed/actions/workflows/ci.yml/badge.svg)](https://github.com/zed-industries/zed/actions/workflows/ci.yml)

Welcome to Zed, a high-performance, multiplayer code editor from the creators of [Atom](https://github.com/atom/atom) and [Tree-sitter](https://github.com/tree-sitter/tree-sitter).

---

### Installation

On macOS and Linux you can [download Zed directly](https://zed.dev/download) or [install Zed via your local package manager](https://zed.dev/docs/linux#installing-via-a-package-manager).

Other platforms are not yet available:

- Windows ([tracking issue](https://github.com/zed-industries/zed/issues/5394))
- Web ([tracking issue](https://github.com/zed-industries/zed/issues/5396))

### Developing Zed

- [Building Zed for macOS](./docs/src/development/macos.md)
- [Building Zed for Linux](./docs/src/development/linux.md)
- [Building Zed for Windows](./docs/src/development/windows.md)
- [Running Collaboration Locally](./docs/src/development/local-collaboration.md)

### Contributing

See [CONTRIBUTING.md](./CONTRIBUTING.md) for ways you can contribute to Zed.

Also... we&#039;re hiring! Check out our [jobs](https://zed.dev/jobs) page for open roles.

### Licensing

License information for third party dependencies must be correctly provided for CI to pass.

We use [`cargo-about`](https://github.com/EmbarkStudios/cargo-about) to automatically comply with open source licenses. If CI is failing, check the following:

- Is it showing a `no license specified` error for a crate you&#039;ve created? If so, add `publish = false` under `[package]` in your crate&#039;s Cargo.toml.
- Is the error `failed to satisfy license requirements` for a dependency? If so, first determine what license the project has and whether this system is sufficient to comply with this license&#039;s requirements. If you&#039;re unsure, ask a lawyer. Once you&#039;ve verified that this system is acceptable add the license&#039;s SPDX identifier to the `accepted` array in `script/licenses/zed-licenses.toml`.
- Is `cargo-about` unable to find the license for a dependency? If so, add a clarification field at the end of `script/licenses/zed-licenses.toml`, as specified in the [cargo-about book](https://embarkstudios.github.io/cargo-about/cli/generate/config.html#crate-configuration).
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[gfx-rs/wgpu]]></title>
            <link>https://github.com/gfx-rs/wgpu</link>
            <guid>https://github.com/gfx-rs/wgpu</guid>
            <pubDate>Sat, 12 Jul 2025 00:05:22 GMT</pubDate>
            <description><![CDATA[A cross-platform, safe, pure-Rust graphics API.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/gfx-rs/wgpu">gfx-rs/wgpu</a></h1>
            <p>A cross-platform, safe, pure-Rust graphics API.</p>
            <p>Language: Rust</p>
            <p>Stars: 14,567</p>
            <p>Forks: 1,098</p>
            <p>Stars today: 19 stars today</p>
            <h2>README</h2><pre># wgpu
&lt;img align=&quot;right&quot; width=&quot;20%&quot; src=&quot;logo.png&quot;&gt;

[![Matrix Space](https://img.shields.io/static/v1?label=Space&amp;message=%23Wgpu&amp;color=blue&amp;logo=matrix)](https://matrix.to/#/#Wgpu:matrix.org)
[![Dev Matrix](https://img.shields.io/static/v1?label=devs&amp;message=%23wgpu&amp;color=blueviolet&amp;logo=matrix)](https://matrix.to/#/#wgpu:matrix.org)
[![User Matrix](https://img.shields.io/static/v1?label=users&amp;message=%23wgpu-users&amp;color=blueviolet&amp;logo=matrix)](https://matrix.to/#/#wgpu-users:matrix.org)
[![Build Status](https://img.shields.io/github/actions/workflow/status/gfx-rs/wgpu/ci.yml?branch=trunk&amp;logo=github&amp;label=CI)](https://github.com/gfx-rs/wgpu/actions)
[![codecov.io](https://img.shields.io/codecov/c/github/gfx-rs/wgpu?logo=codecov&amp;logoColor=fff&amp;label=codecov&amp;token=84qJTesmeS)](https://codecov.io/gh/gfx-rs/wgpu)

`wgpu` is a cross-platform, safe, pure-rust graphics API. It runs natively on Vulkan, Metal, D3D12, and OpenGL; and on top of WebGL2 and WebGPU on wasm.

The API is based on the [WebGPU standard][webgpu]. It serves as the core of the WebGPU integration in Firefox, Servo, and Deno.

[webgpu]: https://gpuweb.github.io/gpuweb/

## Quick Links

| Docs                  | Examples                  | Changelog               |
|:---------------------:|:-------------------------:|:-----------------------:|
| [v26][rel-docs]       | [v26][rel-examples]       | [v26][rel-change]       |
| [`trunk`][trunk-docs] | [`trunk`][trunk-examples] | [`trunk`][trunk-change] |

Contributors are welcome! See [CONTRIBUTING.md][contrib] for more information.

[rel-docs]: https://docs.rs/wgpu/
[rel-examples]: https://github.com/gfx-rs/wgpu/tree/v26/examples#readme
[rel-change]: https://github.com/gfx-rs/wgpu/releases
[trunk-docs]: https://wgpu.rs/doc/wgpu/
[trunk-examples]: https://github.com/gfx-rs/wgpu/tree/trunk/examples#readme
[trunk-change]: https://github.com/gfx-rs/wgpu/blob/trunk/CHANGELOG.md#unreleased
[contrib]: CONTRIBUTING.md

## Repo Overview

The repository hosts the following libraries:

- [![Crates.io](https://img.shields.io/crates/v/wgpu.svg?label=wgpu)](https://crates.io/crates/wgpu) [![docs.rs](https://docs.rs/wgpu/badge.svg)](https://docs.rs/wgpu/) - User facing Rust API.
- [![Crates.io](https://img.shields.io/crates/v/wgpu-core.svg?label=wgpu-core)](https://crates.io/crates/wgpu-core) [![docs.rs](https://docs.rs/wgpu-core/badge.svg)](https://docs.rs/wgpu-core/) - Internal safe implementation.
- [![Crates.io](https://img.shields.io/crates/v/wgpu-hal.svg?label=wgpu-hal)](https://crates.io/crates/wgpu-hal) [![docs.rs](https://docs.rs/wgpu-hal/badge.svg)](https://docs.rs/wgpu-hal/) - Internal unsafe GPU API abstraction layer.
- [![Crates.io](https://img.shields.io/crates/v/wgpu-types.svg?label=wgpu-types)](https://crates.io/crates/wgpu-types) [![docs.rs](https://docs.rs/wgpu-types/badge.svg)](https://docs.rs/wgpu-types/) - Rust types shared between all crates.
- [![Crates.io](https://img.shields.io/crates/v/naga.svg?label=naga)](https://crates.io/crates/naga) [![docs.rs](https://docs.rs/naga/badge.svg)](https://docs.rs/naga/) - Stand-alone shader translation library.
- [![Crates.io](https://img.shields.io/crates/v/deno_webgpu.svg?label=deno_webgpu)](https://crates.io/crates/deno_webgpu) - WebGPU implementation for the Deno JavaScript/TypeScript runtime

The following binaries:

- [![Crates.io](https://img.shields.io/crates/v/naga-cli.svg?label=naga-cli)](https://crates.io/crates/naga-cli) - Tool for translating shaders between different languages using `naga`.
- [![Crates.io](https://img.shields.io/crates/v/wgpu-info.svg?label=wgpu-info)](https://crates.io/crates/wgpu-info) - Tool for getting information on GPUs in the system.
- `cts_runner` - WebGPU Conformance Test Suite runner using `deno_webgpu`.
- `player` - standalone application for replaying the API traces.

For an overview of all the components in the gfx-rs ecosystem, see [the big picture](./docs/big-picture.png).

## Getting Started

### Play with our Examples

Go to &lt;https://wgpu.rs/examples/&gt; to play with our examples in your browser. Requires a browser supporting WebGPU for the WebGPU examples.

### Rust

Rust examples can be found at [examples](examples). You can run the examples natively with `cargo run --bin wgpu-examples &lt;example&gt;`.

If you are new to wgpu and graphics programming, we recommend starting with https://sotrh.github.io/learn-wgpu/.

To run the examples in a browser, run `cargo xtask run-wasm`.
Then open `http://localhost:8000` in your browser, and you can choose an example to run.
Naturally, in order to display any of the WebGPU based examples, you need to make sure your browser supports it.

### C/C++

To use wgpu in C/C++, you need [wgpu-native](https://github.com/gfx-rs/wgpu-native).

If you are looking for a wgpu C++ tutorial, look at the following:

- https://eliemichel.github.io/LearnWebGPU/

### Others

If you want to use wgpu in other languages, there are many bindings to wgpu-native from languages such as Python, D, Julia, Kotlin, and more. See [the list](https://github.com/gfx-rs/wgpu-native#bindings).

## Community

We have the Matrix space [![Matrix Space](https://img.shields.io/static/v1?label=Space&amp;message=%23Wgpu&amp;color=blue&amp;logo=matrix)](https://matrix.to/#/#Wgpu:matrix.org) with a few different rooms that form the wgpu community:

- [![Wgpu Matrix](https://img.shields.io/static/v1?label=wgpu-devs&amp;message=%23wgpu&amp;color=blueviolet&amp;logo=matrix)](https://matrix.to/#/#wgpu:matrix.org) - discussion of the wgpu&#039;s development.
- [![Naga Matrix](https://img.shields.io/static/v1?label=naga-devs&amp;message=%23naga&amp;color=blueviolet&amp;logo=matrix)](https://matrix.to/#/#naga:matrix.org) - discussion of the naga&#039;s development.
- [![User Matrix](https://img.shields.io/static/v1?label=wgpu-users&amp;message=%23wgpu-users&amp;color=blueviolet&amp;logo=matrix)](https://matrix.to/#/#wgpu-users:matrix.org) - discussion of using the library and the surrounding ecosystem.
- [![Random Matrix](https://img.shields.io/static/v1?label=random&amp;message=%23wgpu-random&amp;color=blueviolet&amp;logo=matrix)](https://matrix.to/#/#wgpu-random:matrix.org) - discussion of everything else.

## Wiki

We have a [wiki](https://github.com/gfx-rs/wgpu/wiki) that serves as a knowledge base.

## Extension Specifications

While the core of wgpu is based on the WebGPU standard, we also support extensions that allow for features that the standard does not have yet.
For high-level documentation on how to use these extensions, see the individual specifications:

ğŸ§ªEXPERIMENTALğŸ§ª APIs are subject to change and may allow undefined behavior if used incorrectly.

- ğŸ§ªEXPERIMENTALğŸ§ª [Ray Tracing](./docs/api-specs/ray_tracing.md).
- ğŸ§ªEXPERIMENTALğŸ§ª [Mesh Shading](./docs/api-specs/mesh_shading.md).

## Supported Platforms

| API    | Windows            | Linux/Android      | macOS/iOS          | Web (wasm)         |
| ------ | ------------------ | ------------------ | ------------------ | ------------------ |
| Vulkan |         âœ…         |         âœ…         |         ğŸŒ‹         |                    |
| Metal  |                    |                    |         âœ…         |                    |
| DX12   |         âœ…         |                    |                    |                    |
| OpenGL |    ğŸ†— (GL 3.3+)    |  ğŸ†— (GL ES 3.0+)   |         ğŸ“         |    ğŸ†— (WebGL2)     |
| WebGPU |                    |                    |                    |         âœ…         |

âœ… = First Class Support  
ğŸ†— = Downlevel/Best Effort Support  
ğŸ“ = Requires the [ANGLE](#angle) translation layer (GL ES 3.0 only)  
ğŸŒ‹ = Requires the [MoltenVK](https://vulkan.lunarg.com/sdk/home#mac) translation layer  
ğŸ› ï¸ = Unsupported, though open to contributions

### Shader Support

wgpu supports shaders in [WGSL](https://gpuweb.github.io/gpuweb/wgsl/), SPIR-V, and GLSL.
Both [HLSL](https://github.com/Microsoft/DirectXShaderCompiler) and [GLSL](https://github.com/KhronosGroup/glslang)
have compilers to target SPIR-V. All of these shader languages can be used with any backend as we handle all of the conversions. Additionally, support for these shader inputs is not going away.

While WebGPU does not support any shading language other than WGSL, we will automatically convert your
non-WGSL shaders if you&#039;re running on WebGPU.

WGSL is always supported by default, but GLSL and SPIR-V need features enabled to compile in support.

Note that the WGSL specification is still under development,
so the [draft specification][wgsl spec] does not exactly describe what `wgpu` supports.
See [below](#tracking-the-webgpu-and-wgsl-draft-specifications) for details.

To enable SPIR-V shaders, enable the `spirv` feature of wgpu.
To enable GLSL shaders, enable the `glsl` feature of wgpu.

### Angle

[Angle](http://angleproject.org) is a translation layer from GLES to other backends developed by Google.
We support running our GLES3 backend over it in order to reach platforms DX11 support, which aren&#039;t accessible otherwise.
In order to run with Angle, the &quot;angle&quot; feature has to be enabled, and Angle libraries placed in a location visible to the application.
These binaries can be downloaded from [gfbuild-angle](https://github.com/DileSoft/gfbuild-angle) artifacts, [manual compilation](https://github.com/google/angle/blob/main/doc/DevSetup.md) may be required on Macs with Apple silicon.

On Windows, you generally need to copy them into the working directory, in the same directory as the executable, or somewhere in your path.
On Linux, you can point to them using `LD_LIBRARY_PATH` environment.

### MSRV policy

Due to complex dependants, we have two MSRV policies:

- `naga`, `wgpu-core`, `wgpu-hal`, and `wgpu-types`&#039;s MSRV is **1.76**.
- The rest of the workspace has an MSRV of **1.84**.

It is enforced on CI (in &quot;/.github/workflows/ci.yml&quot;) with the `CORE_MSRV` and `REPO_MSRV` variables.
This version can only be upgraded in breaking releases, though we release a breaking version every three months.

The `naga`, `wgpu-core`, `wgpu-hal`, and `wgpu-types` crates should never
require an MSRV ahead of Firefox&#039;s MSRV for nightly builds, as
determined by the value of `MINIMUM_RUST_VERSION` in
[`python/mozboot/mozboot/util.py`][util].

[util]: https://searchfox.org/mozilla-central/source/python/mozboot/mozboot/util.py

## Environment Variables

All testing and example infrastructure share the same set of environment variables that determine which Backend/GPU it will run on.

- `WGPU_ADAPTER_NAME` with a substring of the name of the adapter you want to use (ex. `1080` will match `NVIDIA GeForce 1080ti`).
- `WGPU_BACKEND` with a comma-separated list of the backends you want to use (`vulkan`, `metal`, `dx12`, or `gl`).
- `WGPU_POWER_PREF` with the power preference to choose when a specific adapter name isn&#039;t specified (`high`, `low` or `none`)
- `WGPU_DX12_COMPILER` with the DX12 shader compiler you wish to use (`dxc`, `static-dxc`, or `fxc`). Note that `dxc` requires `dxcompiler.dll` (min v1.8.2502) to be in the working directory, and `static-dxc` requires the `static-dxc` crate feature to be enabled. Otherwise, it will fall back to `fxc`.
- `WGPU_GLES_MINOR_VERSION` with the minor OpenGL ES 3 version number to request (`0`, `1`, `2` or `automatic`).
- `WGPU_ALLOW_UNDERLYING_NONCOMPLIANT_ADAPTER` with a boolean whether non-compliant drivers are enumerated (`0` for false, `1` for true).

When running the CTS, use the variables `DENO_WEBGPU_ADAPTER_NAME`, `DENO_WEBGPU_BACKEND`, `DENO_WEBGPU_POWER_PREFERENCE`.

## Testing

We have multiple methods of testing, each of which tests different qualities about wgpu. We automatically run our tests on CI. The current state of CI testing:

| Platform/Backend | Tests              | Notes                 |
| ---------------- | ------------------ | --------------------- |
| Windows/DX12     | :heavy_check_mark: | using WARP            |
| Windows/OpenGL   | :heavy_check_mark: | using llvmpipe        |
| MacOS/Metal      | :heavy_check_mark: | using hardware runner |
| Linux/Vulkan     | :heavy_check_mark: | using lavapipe        |
| Linux/OpenGL ES  | :heavy_check_mark: | using llvmpipe        |
| Chrome/WebGL     | :heavy_check_mark: | using swiftshader     |
| Chrome/WebGPU    | :x:                | not set up            |

### Core Test Infrastructure

We use a tool called [`cargo nextest`](https://github.com/nextest-rs/nextest) to run our tests.
To install it, run `cargo install cargo-nextest`.

To run the test suite:

```
cargo xtask test
```

To run the test suite on WebGL (currently incomplete):

```
cd wgpu
wasm-pack test --headless --chrome --no-default-features --features webgl --workspace
```

This will automatically run the tests using a packaged browser. Remove `--headless` to run the tests with whatever browser you wish at `http://localhost:8000`.

If you are a user and want a way to help contribute to wgpu, we always need more help writing test cases.

### WebGPU Conformance Test Suite

WebGPU includes a Conformance Test Suite to validate that implementations are
working correctly. We run cases from the CTS against wgpu using
[Deno](https://deno.com/). A [default list of enabled
tests](./cts_runner/test.lst) is automatically run on pull requests in CI.

To run the default set of CTS tests locally, run:

```
cargo xtask cts
```

You can also specify a test selector on the command line:

```
cargo xtask cts &#039;webgpu:api,operation,command_buffer,basic:*&#039;
```

Or supply your own test list in a file:

```
cargo xtask cts -f your_tests.lst
```

To find the full list of tests, go to the
[web version of the CTS](https://gpuweb.github.io/cts/standalone/?runnow=0&amp;worker=0&amp;debug=0&amp;q=webgpu:*).

The version of the CTS used by `cargo xtask cts` is specified in
[`cts_runner/revision.txt`](./cts_runner/revision.txt).

## Tracking the WebGPU and WGSL draft specifications

The `wgpu` crate is meant to be an idiomatic Rust translation of the [WebGPU API][webgpu spec].
That specification, along with its shading language, [WGSL][wgsl spec],
are both still in the &quot;Working Draft&quot; phase,
and while the general outlines are stable,
details change frequently.
Until the specification is stabilized, the `wgpu` crate and the version of WGSL it implements
will likely differ from what is specified,
as the implementation catches up.

Exactly which WGSL features `wgpu` supports depends on how you are using it:

- When running as native code, `wgpu` uses the [Naga][naga] crate
  to translate WGSL code into the shading language of your platform&#039;s native GPU API.
  Naga has [a milestone][naga wgsl milestone]
  for catching up to the WGSL specification,
  but in general, there is no up-to-date summary
  of the differences between Naga and the WGSL spec.

- When running in a web browser (by compilation to WebAssembly)
  without the `&quot;webgl&quot;` feature enabled,
  `wgpu` relies on the browser&#039;s own WebGPU implementation.
  WGSL shaders are simply passed through to the browser,
  so that determines which WGSL features you can use.

- When running in a web browser with `wgpu`&#039;s `&quot;webgl&quot;` feature enabled,
  `wgpu` uses Naga to translate WGSL programs into GLSL.
  This uses the same version of Naga as if you were running `wgpu` as native code.

[webgpu spec]: https://www.w3.org/TR/webgpu/
[wgsl spec]: https://gpuweb.github.io/gpuweb/wgsl/
[naga]: https://github.com/gfx-rs/naga/
[naga wgsl milestone]: https://github.com/gfx-rs/naga/milestone/4

## Coordinate Systems

wgpu uses the coordinate systems of D3D and Metal:

| Render                                              | Texture                                               |
| --------------------------------------------------- | ----------------------------------------------------- |
| ![render_coordinates](./docs/render_coordinates.png) | ![texture_coordinates](./docs/texture_coordinates.png) |
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[dandavison/delta]]></title>
            <link>https://github.com/dandavison/delta</link>
            <guid>https://github.com/dandavison/delta</guid>
            <pubDate>Sat, 12 Jul 2025 00:05:21 GMT</pubDate>
            <description><![CDATA[A syntax-highlighting pager for git, diff, grep, and blame output]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/dandavison/delta">dandavison/delta</a></h1>
            <p>A syntax-highlighting pager for git, diff, grep, and blame output</p>
            <p>Language: Rust</p>
            <p>Stars: 26,708</p>
            <p>Forks: 419</p>
            <p>Stars today: 32 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img width=400px src=&quot;https://user-images.githubusercontent.com/52205/147996902-9829bd3f-cd33-466e-833e-49a6f3ebd623.png&quot; alt=&quot;image&quot; /&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/dandavison/delta/actions&quot;&gt;
    &lt;img src=&quot;https://github.com/dandavison/delta/workflows/Continuous%20Integration/badge.svg&quot; alt=&quot;CI&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://coveralls.io/github/dandavison/delta?branch=main&quot;&gt;
    &lt;img src=&quot;https://coveralls.io/repos/github/dandavison/delta/badge.svg?branch=main&quot; alt=&quot;Coverage Status&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://gitter.im/dandavison-delta/community?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&quot;&gt;
    &lt;img src=&quot;https://badges.gitter.im/dandavison-delta/community.svg&quot; alt=&quot;Gitter&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

## Get Started

[Install it](https://dandavison.github.io/delta/installation.html) (the package is called &quot;git-delta&quot; in most package managers, but the executable is just `delta`) and add this to your `~/.gitconfig`:

```gitconfig
[core]
    pager = delta

[interactive]
    diffFilter = delta --color-only

[delta]
    navigate = true  # use n and N to move between diff sections
    dark = true      # or light = true, or omit for auto-detection

[merge]
    conflictstyle = zdiff3
```

Or run:

```sh
git config --global core.pager delta
git config --global interactive.diffFilter &#039;delta --color-only&#039;
git config --global delta.navigate true
git config --global merge.conflictStyle zdiff3
```

Delta has many features and is very customizable; please see `delta -h` (short help) or `delta --help` (full manual), or the [online user manual](https://dandavison.github.io/delta/).

## Features

- Language syntax highlighting with the same syntax-highlighting themes as [bat](https://github.com/sharkdp/bat#readme)
- Word-level diff highlighting using a Levenshtein edit inference algorithm
- Side-by-side view with line-wrapping
- Line numbering
- `n` and `N` keybindings to move between files in large diffs, and between diffs in `log -p` views (`--navigate`)
- Improved merge conflict display
- Improved `git blame` display (syntax highlighting; `--hyperlinks` formats commits as links to hosting provider etc. Supported hosting providers are: GitHub, GitLab, SourceHut, Codeberg)
- Syntax-highlights grep output from `rg`, `git grep`, `grep`, etc
- Support for Git&#039;s `--color-moved` feature.
- Code can be copied directly from the diff (`-/+` markers are removed by default).
- `diff-highlight` and `diff-so-fancy` emulation modes
- Commit hashes can be formatted as terminal [hyperlinks](https://gist.github.com/egmontkob/eb114294efbcd5adb1944c9f3cb5feda) to the hosting provider page (`--hyperlinks`).
  File paths can also be formatted as hyperlinks for opening in your OS.
- Stylable box/line decorations to draw attention to commit, file and hunk header sections.
- Style strings (foreground color, background color, font attributes) are supported for &gt;20 stylable elements, using the same color/style language as git
- Handles traditional unified diff output in addition to git output
- Automatic detection of light/dark terminal background

## A syntax-highlighting pager for git, diff, and grep output

Code evolves, and we all spend time studying diffs. Delta aims to make this both efficient and enjoyable: it allows you to make extensive changes to the layout and styling of diffs, as well as allowing you to stay arbitrarily close to the default git/diff output.

&lt;table align=&quot;center&quot;&gt;
  &lt;tr&gt;
    &lt;td&gt;
      &lt;img width=400px src=&quot;https://user-images.githubusercontent.com/52205/86275526-76792100-bba1-11ea-9e78-6be9baa80b29.png&quot; alt=&quot;image&quot; /&gt;
      &lt;br&gt;
      &lt;p align=&quot;center&quot;&gt;&lt;sub&gt;delta with &lt;code&gt;line-numbers&lt;/code&gt; activated&lt;/sub&gt;&lt;/p&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;table align=&quot;center&quot;&gt;
  &lt;tr&gt;
    &lt;td&gt;
      &lt;img width=800px src=&quot;https://user-images.githubusercontent.com/52205/87230973-412eb900-c381-11ea-8aec-cc200290bd1b.png&quot; alt=&quot;image&quot; /&gt;
      &lt;br&gt;
      &lt;p align=&quot;center&quot;&gt;&lt;sub&gt;delta with &lt;code&gt;side-by-side&lt;/code&gt; and &lt;code&gt;line-numbers&lt;/code&gt; activated&lt;/sub&gt;&lt;/p&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

Here&#039;s what `git show` can look like with git configured to use delta:

&lt;br&gt;

&lt;table align=&quot;center&quot;&gt;
  &lt;tr&gt;
    &lt;td&gt;
      &lt;img width=500px style=&quot;border: 1px solid black&quot;
           src=&quot;https://user-images.githubusercontent.com/52205/81058545-a5725f80-8e9c-11ea-912e-d21954586a44.png&quot;
           alt=&quot;image&quot; /&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;img width=500px style=&quot;border: 1px solid black&quot;
           src=&quot;https://user-images.githubusercontent.com/52205/81058911-6abcf700-8e9d-11ea-93be-e212824ec03d.png&quot;
           alt=&quot;image&quot; /&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;
      &quot;Dracula&quot; theme
    &lt;/td&gt;
    &lt;td&gt;
      &quot;GitHub&quot; theme
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;br&gt;
&lt;br&gt;

### Syntax-highlighting themes

**All the syntax-highlighting color themes that are available with [bat](https://github.com/sharkdp/bat/) are available with delta:**

&lt;br&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;
      &lt;img width=400px style=&quot;border: 1px solid black&quot;
           src=&quot;https://user-images.githubusercontent.com/52205/149431273-e3ad049d-771e-4186-869d-0e57967958a6.png&quot;
           alt=&quot;image&quot; /&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;img width=400px style=&quot;border: 1px solid black&quot;
           src=&quot;https://user-images.githubusercontent.com/52205/149431419-48836001-2afc-4fd0-97ad-561a69b71db7.png&quot;
           alt=&quot;image&quot; /&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;
      &lt;code&gt;delta --show-syntax-themes --dark&lt;/code&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;code&gt;delta --show-syntax-themes --light&lt;/code&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;br&gt;

### Side-by-side view

[[User manual](https://dandavison.github.io/delta/side-by-side-view.html)]

```gitconfig
[delta]
    side-by-side = true
```

By default, side-by-side view has line-numbers activated, and has syntax highlighting in both the left and right panels: [[config](#side-by-side-view-1)]

&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;img width=800px src=&quot;https://user-images.githubusercontent.com/52205/87230973-412eb900-c381-11ea-8aec-cc200290bd1b.png&quot; alt=&quot;image&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

Side-by-side view wraps long lines automatically:

&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;img width=600px src=&quot;https://user-images.githubusercontent.com/52205/139064537-f8479504-16d3-429a-b4f6-d0122438adaa.png&quot; alt=&quot;image&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

### Line numbers

[[User manual](https://dandavison.github.io/delta/line-numbers.html)]

```gitconfig
[delta]
    line-numbers = true
```

&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;img width=400px src=&quot;https://user-images.githubusercontent.com/52205/86275526-76792100-bba1-11ea-9e78-6be9baa80b29.png&quot; alt=&quot;image&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

### Merge conflicts

[[User manual](https://dandavison.github.io/delta/merge-conflicts.html)]

&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;img width=500px src=&quot;https://user-images.githubusercontent.com/52205/144783121-bb549100-69d8-41b8-ac62-1704f1f7b43e.png&quot; alt=&quot;image&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

### Git blame

[[User manual](https://dandavison.github.io/delta/git-blame.html)]

&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;img width=600px src=&quot;https://user-images.githubusercontent.com/52205/141891376-1fdb87dc-1d9c-4ad6-9d72-eeb19a8aeb0b.png&quot; alt=&quot;image&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

### Ripgrep, git grep

[[User manual](https://dandavison.github.io/delta/grep.html)]

&lt;table&gt;&lt;tr&gt;&lt;td&gt;
&lt;img width=&quot;600px&quot; alt=&quot;image&quot; src=&quot;https://github.com/dandavison/open-in-editor/assets/52205/d203d380-5acb-4296-aeb9-e38c73d6c27f&quot;&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

### Installation and usage

Please see the [user manual](https://dandavison.github.io/delta/) and `delta --help`.

### Maintainers

- [@dandavison](https://github.com/dandavison)
- [@th1000s](https://github.com/th1000s)
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[Wilfred/difftastic]]></title>
            <link>https://github.com/Wilfred/difftastic</link>
            <guid>https://github.com/Wilfred/difftastic</guid>
            <pubDate>Sat, 12 Jul 2025 00:05:20 GMT</pubDate>
            <description><![CDATA[a structural diff that understands syntax ğŸŸ¥ğŸŸ©]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Wilfred/difftastic">Wilfred/difftastic</a></h1>
            <p>a structural diff that understands syntax ğŸŸ¥ğŸŸ©</p>
            <p>Language: Rust</p>
            <p>Stars: 22,539</p>
            <p>Forks: 385</p>
            <p>Stars today: 21 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;#readme&quot;&gt;&lt;img src=&quot;img/logo.png&quot; alt=&quot;it&#039;s difftastic!&quot;/&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://difftastic.wilfred.me.uk/introduction.html&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/manual-en-brightgreen?style=flat-square&quot; alt=&quot;English manual&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://difftastic.wilfred.me.uk/zh-CN/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/manual-zh--CN-brightgreen?style=flat-square&quot; alt=&quot;Chinese manual&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://crates.io/crates/difftastic&quot;&gt;&lt;img src=&quot;https://img.shields.io/crates/v/difftastic.svg?style=flat-square&quot; alt=&quot;crates.io&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://codecov.io/gh/Wilfred/difftastic&quot;&gt;&lt;img src=&quot;https://img.shields.io/codecov/c/github/Wilfred/difftastic?style=flat-square&amp;token=dZzAZtQT2S&quot; alt=&quot;codecov.io&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

Difftastic is a structural diff tool that compares files based on
their syntax.

**For installation instructions, see
[Installation](https://difftastic.wilfred.me.uk/installation.html) in
[the manual](http://difftastic.wilfred.me.uk/).**

## Basic Example

![Screenshot of difftastic and JS](img/js.png)

In this JavaScript example, we can see:

(1) Difftastic understands nesting. It highlights the matching `{` and
`}`, but understands that `foo()` hasn&#039;t changed despite the leading
whitespace.

(2) Difftastic understands which lines should be aligned. It&#039;s aligned
`bar(1)` on the left with `bar(2)` on the right, even though the
textual content isn&#039;t identical.

(3) Difftastic understands that line-wrapping isn&#039;t
meaningful. `&quot;eric&quot;` is now on a new line, but it hasn&#039;t changed.

## One Minute Demo

[![asciicast](https://asciinema.org/a/480875.svg)](https://asciinema.org/a/480875)

This one minute screencast demonstrates difftastic usage with both
standalone files and git.

## Languages

Difftastic supports over 30 programming languages, see [the
manual](https://difftastic.wilfred.me.uk/languages_supported.html) for the full list.

If a file has an unrecognised extension, difftastic uses a
textual diff with word highlighting.

## Known Issues

Performance. Difftastic scales relatively poorly on files with a large
number of changes, and can use a lot of memory.

Display. Difftastic has a side-by-side display which usually works well, but can
be confusing.

Robustness. Difftastic regularly has releases that fix crashes.

## Non-goals

Patching. Difftastic output is intended for human consumption, and it
does not generate patches that you can apply later. Use `diff` if you
need a patch.

(Patch files are also line-oriented, which is too limited for
difftastic. Difftastic might find additions and removals on the same
line, and it tracks the relationship between line numbers in the old
and new file.)

Merging. AST merging is a hard problem that difftastic does not
address.

## FAQ

### Isn&#039;t this basically `--word-diff --ignore-all-space`?

Word diffing [can&#039;t do
this](https://twitter.com/_wilfredh/status/1510139929971421191/photo/1).

Difftastic parses your code. It understands when whitespace matters,
such as inside string literals or languages like Python. It understands
that `x-1` is three tokens in JS but one token in Lisp.

### Can I use difftastic with git?

You can! The difftastic manual [includes instructions for git
usage](https://difftastic.wilfred.me.uk/git.html). You can also use it
[with mercurial](https://difftastic.wilfred.me.uk/mercurial.html).

If you&#039;re an Emacs user, check out [this blog
post](https://tsdh.org/posts/2022-08-01-difftastic-diffing-with-magit.html)
showing one way to use difftastic with magit, as well as
[difftastic.el](https://github.com/pkryger/difftastic.el).

### Does difftastic integrate with my favourite tool?

Probably not. Difftastic is young. Consider writing a plugin for your
favourite tool, and I will link it in the README!

### What about parse errors?

By default, difftastic falls back to a line-oriented diff whenever
parse errors are encountered.

This is a conservative choice to ensure that difftastic never claims
two syntactically different files are the same.

Parse errors can occur if the file uses language features that the
parser does not understand, if the language relies on a preprocessor
before parsing (e.g. C++), or if the file has genuine syntactic
mistakes.

In practice, difftastic virtually always produces a good result when
there are a few minor parse errors. Consider allowing a small number
of parse errors when using difftastic.

```
$ export DFT_PARSE_ERROR_LIMIT=20
$ difft foo1.c foo2.c
```

### Can difftastic help me with merge conflicts?

Yes! As of version 0.50, difftastic understands merge conflict markers
(i.e. `&lt;&lt;&lt;&lt;&lt;&lt;&lt;`, `=======` and `&gt;&gt;&gt;&gt;&gt;&gt;&gt;`).

Pass your file with conflicts as a single argument to
difftastic. Difftastic will construct the two conflicting files and
diff those.

```
$ difft file_with_conflicts.js
```

### Can difftastic do merges?

No. AST merging is a hard problem that difftastic does not address.

AST diffing is a lossy process from the perspective of a text
diff. Difftastic will ignore whitespace that isn&#039;t syntactically
significant, but merging requires tracking whitespace.

The [mergiraf](https://mergiraf.org/) tool does offer merges based on
a tree-sitter AST however.

### Can difftastic ignore reordering?

No. Difftastic always considers order to be important, so diffing
e.g. `set(1, 2)` and `set(2, 1)` will show changes.

If you&#039;re diffing JSON, consider sorting the keys before passing them
to difftastic.

```
$ difft &lt;(jq --sort-keys &lt; file_1.json) &lt;(jq --sort-keys &lt; file_2.json)
```

See also [Tricky Cases: Unordered Data
Types](https://difftastic.wilfred.me.uk/tricky_cases.html#unordered-data-types)
in the manual.

### Can I use difftastic to check for syntactic changes without diffing?

Yes. Difftastic can check if the two files have the same AST, without
calculating a diff. This is much faster than normal diffing, and
useful for building tools that check for changes.

For example:

```
$ difft --check-only --exit-code before.js after.js
```

This will set the exit code to 0 if there are no syntactic changes, or
1 if there are changes found.

### Why aren&#039;t colours appearing in my terminal?

Difftastic uses ANSI bright colours by default, but some terminal
themes show bright colours as grey. Solarized is a popular theme that
does this.

If you&#039;re a Solarized user, use `export DFT_BACKGROUND=light` to
disable bright colours, or try a different terminal colour scheme.

### How does it work?

Difftastic treats structural diffing as a graph problem, and uses
Dijkstra&#039;s algorithm.

My [blog
post](https://www.wilfred.me.uk/blog/2022/09/06/difftastic-the-fantastic-diff/)
describes the design, and there is also an [internals section in the
manual](https://difftastic.wilfred.me.uk/diffing.html).

## Translation

+ [Chinese](./translation/zh-CN/README-zh-CN.md)

## License

Difftastic is open source under the MIT license, see LICENSE for more
details.

This repository also includes tree-sitter parsers by other authors in
the `vendored_parsers/` directory. These are a mix of the MIT license and the
Apache license. See `vendored_parsers/*/LICENSE` for more details.

Files in `sample_files/` are also under the MIT license unless stated
otherwise in their header.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[aws/amazon-q-developer-cli]]></title>
            <link>https://github.com/aws/amazon-q-developer-cli</link>
            <guid>https://github.com/aws/amazon-q-developer-cli</guid>
            <pubDate>Sat, 12 Jul 2025 00:05:19 GMT</pubDate>
            <description><![CDATA[âœ¨ Agentic chat experience in your terminal. Build applications using natural language.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/aws/amazon-q-developer-cli">aws/amazon-q-developer-cli</a></h1>
            <p>âœ¨ Agentic chat experience in your terminal. Build applications using natural language.</p>
            <p>Language: Rust</p>
            <p>Stars: 1,022</p>
            <p>Forks: 183</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre># Amazon Q CLI

## Installation

- **macOS**:
  - **DMG**: [Download now](https://desktop-release.q.us-east-1.amazonaws.com/latest/Amazon%20Q.dmg)
- **Linux**:
  - [Ubuntu/Debian](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/command-line-installing.html#command-line-installing-ubuntu)
  - [AppImage](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/command-line-installing.html#command-line-installing-appimage)
  - [Alternative Linux builds](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/command-line-installing.html#command-line-installing-alternative-linux)

## Contributing

Thank you so much for considering to contribute to Amazon Q.

Before getting started, see our [contributing docs](CONTRIBUTING.md#security-issue-notifications).

### Prerequisites

- MacOS
  - Xcode 13 or later
  - Brew

#### 1. Clone repo

```shell
git clone https://github.com/aws/amazon-q-developer-cli.git
```

#### 2. Install the Rust toolchain using [Rustup](https://rustup.rs):

```shell
curl --proto &#039;=https&#039; --tlsv1.2 -sSf https://sh.rustup.rs | sh
rustup default stable
rustup toolchain install nightly
cargo install typos-cli
```

#### 3. Develop locally

- To compile and run: `cargo run --bin chat_cli`.
- To run tests: `cargo test`.
- To run lints: `cargo clippy`.
- To format rust files: `cargo +nightly fmt`.
- To run subcommands: `cargo run --bin chat_cli -- {subcommand}`.
  - Login would then be: `cargo run --bin chat_cli -- login`

## Project Layout

- [`chat_cli`](crates/chat_cli/) - the `q` CLI, allows users to interface with Amazon Q Developer from
  the command line
- [`scripts/`](scripts/) - Contains ops and build related scripts
- [`crates/`](crates/) - Contains all rust crates
- [`docs/`](docs/) - Contains technical documentation

## Security

For security related concerns, see [here](SECURITY.md).

## Licensing

This repo is dual licensed under MIT and Apache 2.0 licenses.

Those licenses can be found [here](LICENSE.MIT) and [here](LICENSE.APACHE).

â€œAmazon Web Servicesâ€ and all related marks, including logos, graphic designs, and service names, are trademarks or trade dress of AWS in the U.S. and other countries. AWSâ€™s trademarks and trade dress may not be used in connection with any product or service that is not AWSâ€™s, in any manner that is likely to cause confusion among customers, or in any manner that disparages or discredits AWS.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
    </channel>
</rss>