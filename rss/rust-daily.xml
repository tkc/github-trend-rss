<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for rust - Rust Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for rust.</description>
        <lastBuildDate>Mon, 24 Mar 2025 00:05:58 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[Devolutions/IronRDP]]></title>
            <link>https://github.com/Devolutions/IronRDP</link>
            <guid>https://github.com/Devolutions/IronRDP</guid>
            <pubDate>Mon, 24 Mar 2025 00:05:58 GMT</pubDate>
            <description><![CDATA[Rust implementation of the Microsoft Remote Desktop Protocol (RDP)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Devolutions/IronRDP">Devolutions/IronRDP</a></h1>
            <p>Rust implementation of the Microsoft Remote Desktop Protocol (RDP)</p>
            <p>Language: Rust</p>
            <p>Stars: 1,513</p>
            <p>Forks: 75</p>
            <p>Stars today: 279 stars today</p>
            <h2>README</h2><pre># IronRDP

[![](https://docs.rs/ironrdp/badge.svg)](https://docs.rs/ironrdp/) [![](https://img.shields.io/crates/v/ironrdp)](https://crates.io/crates/ironrdp)

A collection of Rust crates providing an implementation of the Microsoft Remote Desktop Protocol, with a focus on security.

## Demonstration

&lt;https://user-images.githubusercontent.com/3809077/202049929-76f42471-aeb0-41da-9118-0dc6ea491bd2.mp4&gt;

## Video Codec Support

Supported codecs:

- Uncompressed raw bitmap
- Interleaved Run-Length Encoding (RLE) Bitmap Codec
- RDP 6.0 Bitmap Compression
- Microsoft RemoteFX (RFX)

## Examples

### [`ironrdp-client`](https://github.com/Devolutions/IronRDP/tree/master/crates/ironrdp-client)

A full-fledged RDP client based on IronRDP crates suite, and implemented using non-blocking, asynchronous I/O.

```shell
cargo run --bin ironrdp-client -- &lt;HOSTNAME&gt; --username &lt;USERNAME&gt; --password &lt;PASSWORD&gt;
```

### [`screenshot`](https://github.com/Devolutions/IronRDP/blob/master/crates/ironrdp/examples/screenshot.rs)

Example of utilizing IronRDP in a blocking, synchronous fashion.

This example showcases the use of IronRDP in a blocking manner. It
demonstrates how to create a basic RDP client with just a few hundred lines
of code by leveraging the IronRDP crates suite.

In this basic client implementation, the client establishes a connection
with the destination server, decodes incoming graphics updates, and saves the
resulting output as a BMP image file on the disk.

```shell
cargo run --example=screenshot -- --host &lt;HOSTNAME&gt; --username &lt;USERNAME&gt; --password &lt;PASSWORD&gt; --output out.bmp
```

### How to enable RemoteFX on server

Run the following PowerShell commands, and reboot.

```pwsh
Set-ItemProperty -Path &#039;HKLM:\Software\Policies\Microsoft\Windows NT\Terminal Services&#039; -Name &#039;ColorDepth&#039; -Type DWORD -Value 5
Set-ItemProperty -Path &#039;HKLM:\Software\Policies\Microsoft\Windows NT\Terminal Services&#039; -Name &#039;fEnableVirtualizedGraphics&#039; -Type DWORD -Value 1
```

Alternatively, you may change a few group policies using `gpedit.msc`:

1. Run `gpedit.msc`.

2. Enable `Computer Configuration/Administrative Templates/Windows Components/Remote Desktop Services/Remote Desktop Session Host/Remote Session Environment/RemoteFX for Windows Server 2008 R2/Configure RemoteFX`

3. Enable `Computer Configuration/Administrative Templates/Windows Components/Remote Desktop Services/Remote Desktop Session Host/Remote Session Environment/Enable RemoteFX encoding for RemoteFX clients designed for Windows Server 2008 R2 SP1`

4. Enable `Computer Configuration/Administrative Templates/Windows Components/Remote Desktop Services/Remote Desktop Session Host/Remote Session Environment/Limit maximum color depth`

5. Reboot.

## Architecture

See the [ARCHITECTURE.md](https://github.com/Devolutions/IronRDP/blob/master/ARCHITECTURE.md) document.

## Getting help

- Report bugs in the [issue tracker](https://github.com/Devolutions/IronRDP/issues)
- Discuss the project on the [matrix room](https://matrix.to/#/#IronRDP:matrix.org)
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[tracel-ai/burn]]></title>
            <link>https://github.com/tracel-ai/burn</link>
            <guid>https://github.com/tracel-ai/burn</guid>
            <pubDate>Mon, 24 Mar 2025 00:05:57 GMT</pubDate>
            <description><![CDATA[Burn is a new comprehensive dynamic Deep Learning Framework built using Rust with extreme flexibility, compute efficiency and portability as its primary goals.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/tracel-ai/burn">tracel-ai/burn</a></h1>
            <p>Burn is a new comprehensive dynamic Deep Learning Framework built using Rust with extreme flexibility, compute efficiency and portability as its primary goals.</p>
            <p>Language: Rust</p>
            <p>Stars: 10,065</p>
            <p>Forks: 523</p>
            <p>Stars today: 14 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/tracel-ai/burn/main/assets/logo-burn-neutral.webp&quot; width=&quot;350px&quot;/&gt;

[![Discord](https://img.shields.io/discord/1038839012602941528.svg?color=7289da&amp;&amp;logo=discord)](https://discord.gg/uPEBbYYDB6)
[![Current Crates.io Version](https://img.shields.io/crates/v/burn.svg)](https://crates.io/crates/burn)
[![Minimum Supported Rust Version](https://img.shields.io/crates/msrv/burn)](https://crates.io/crates/burn)
[![Documentation](https://img.shields.io/badge/docs-latest-blue)](https://burn.dev/docs/burn)
[![Test Status](https://github.com/tracel-ai/burn/actions/workflows/test.yml/badge.svg)](https://github.com/tracel-ai/burn/actions/workflows/test.yml)
[![CodeCov](https://codecov.io/gh/tracel-ai/burn/branch/main/graph/badge.svg)](https://codecov.io/gh/tracel-ai/burn)
[![Blaze](https://runblaze.dev/gh/114041730602611213183421653564341667516/badge.svg)](https://runblaze.dev)
![license](https://shields.io/badge/license-MIT%2FApache--2.0-blue)

---

**Burn is a new comprehensive dynamic Deep Learning Framework built using Rust &lt;br /&gt; with extreme
flexibility, compute efficiency and portability as its primary goals.**

&lt;br/&gt;
&lt;/div&gt;

&lt;div align=&quot;left&quot;&gt;

## Performance

&lt;div align=&quot;left&quot;&gt;
&lt;img align=&quot;right&quot; src=&quot;https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-blazingly-fast.png&quot; height=&quot;96px&quot;/&gt;

Because we believe the goal of a deep learning framework is to convert computation into useful
intelligence, we have made performance a core pillar of Burn. We strive to achieve top efficiency by
leveraging multiple optimization techniques described below.

**Click on each section for more details** üëá

&lt;/div&gt;

&lt;br /&gt;

&lt;details&gt;
&lt;summary&gt;
Automatic kernel fusion üí•
&lt;/summary&gt;
&lt;br /&gt;

Using Burn means having your models optimized on any backend. When possible, we provide a way to
automatically and dynamically create custom kernels that minimize data relocation between different
memory spaces, extremely useful when moving memory is the bottleneck.

As an example, you could write your own GELU activation function with the high level tensor api (see
Rust code snippet below).

```rust
fn gelu_custom&lt;B: Backend, const D: usize&gt;(x: Tensor&lt;B, D&gt;) -&gt; Tensor&lt;B, D&gt; {
    let x = x.clone() * ((x / SQRT_2).erf() + 1);
    x / 2
}
```

Then, at runtime, a custom low-level kernel will be automatically created for your specific
implementation and will rival a handcrafted GPU implementation. The kernel consists of about 60
lines of WGSL [WebGPU Shading Language](&quot;https://www.w3.org/TR/WGSL/https://www.w3.org/TR/WGSL/&quot;),
an extremely verbose lower level shader language you probably don&#039;t want to program your deep
learning models in!

&gt; As of now, our fusion strategy is only implemented for our own WGPU and CUDA backends and supports
&gt; only a subset of operations. We plan to add more operations very soon and extend this technique to
&gt; other future in-house backends.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Asynchronous execution ‚ù§Ô∏è‚Äçüî•
&lt;/summary&gt;
&lt;br /&gt;

For [backends developed from scratch by the Burn team](#backends), an asynchronous execution style
is used, which allows to perform various optimizations, such as the previously mentioned automatic
kernel fusion.

Asynchronous execution also ensures that the normal execution of the framework does not block the
model computations, which implies that the framework overhead won&#039;t impact the speed of execution
significantly. Conversely, the intense computations in the model do not interfere with the
responsiveness of the framework. For more information about our asynchronous backends, see
[this blog post](https://burn.dev/blog/creating-high-performance-asynchronous-backends-with-burn-compute).

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Thread-safe building blocks ü¶û
&lt;/summary&gt;
&lt;br /&gt;

Burn emphasizes thread safety by leveraging the
[ownership system of Rust](https://doc.rust-lang.org/book/ch04-00-understanding-ownership.html).
With Burn, each module is the owner of its weights. It is therefore possible to send a module to
another thread for computing the gradients, then send the gradients to the main thread that can
aggregate them, and _voil√†_, you get multi-device training.

This is a very different approach from what PyTorch does, where backpropagation actually mutates the
_grad_ attribute of each tensor parameter. This is not a thread-safe operation and therefore
requires lower level synchronization primitives, see
[distributed training](https://pytorch.org/docs/stable/distributed.html) for reference. Note that
this is still very fast, but not compatible across different backends and quite hard to implement.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Intelligent memory management ü¶Ä
&lt;/summary&gt;
&lt;br /&gt;

One of the main roles of a deep learning framework is to reduce the amount of memory necessary to
run models. The naive way of handling memory is that each tensor has its own memory space, which is
allocated when the tensor is created then deallocated as the tensor gets out of scope. However,
allocating and deallocating data is very costly, so a memory pool is often required to achieve good
throughput. Burn offers an infrastructure that allows for easily creating and selecting memory
management strategies for backends. For more details on memory management in Burn, see
[this blog post](https://burn.dev/blog/creating-high-performance-asynchronous-backends-with-burn-compute).

Another very important memory optimization of Burn is that we keep track of when a tensor can be
mutated in-place just by using the ownership system well. Even though it is a rather small memory
optimization on its own, it adds up considerably when training or running inference with larger
models and contributes to reduce the memory usage even more. For more information, see
[this blog post about tensor handling](https://burn.dev/blog/burn-rusty-approach-to-tensor-handling).

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Automatic kernel selection üéØ
&lt;/summary&gt;
&lt;br /&gt;

A good deep learning framework should ensure that models run smoothly on all hardware. However, not
all hardware share the same behavior in terms of execution speed. For instance, a matrix
multiplication kernel can be launched with many different parameters, which are highly sensitive to
the size of the matrices and the hardware. Using the wrong configuration could reduce the speed of
execution by a large factor (10 times or even more in extreme cases), so choosing the right kernels
becomes a priority.

With our home-made backends, we run benchmarks automatically and choose the best configuration for
the current hardware and matrix sizes with a reasonable caching strategy.

This adds a small overhead by increasing the warmup execution time, but stabilizes quickly after a
few forward and backward passes, saving lots of time in the long run. Note that this feature isn&#039;t
mandatory, and can be disabled when cold starts are a priority over optimized throughput.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Hardware specific features üî•
&lt;/summary&gt;
&lt;br /&gt;

It is no secret that deep learning is mostly relying on matrix multiplication as its core operation,
since this is how fully-connected neural networks are modeled.

More and more, hardware manufacturers optimize their chips specifically for matrix multiplication
workloads. For instance, Nvidia has its _Tensor Cores_ and today most cellphones have AI specialized
chips. As of this moment, we support Tensor Cores with our LibTorch, Candle, CUDA and WGPU/SPIR-V
backends, but not other accelerators yet. We hope
[this issue](https://github.com/gpuweb/gpuweb/issues/4195) gets resolved at some point to bring
support to our WGPU backend.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Custom Backend Extension üéí
&lt;/summary&gt;
&lt;br /&gt;

Burn aims to be the most flexible deep learning framework. While it&#039;s crucial to maintain
compatibility with a wide variety of backends, Burn also provides the ability to extend the
functionalities of a backend implementation to suit your personal modeling requirements.

This versatility is advantageous in numerous ways, such as supporting custom operations like flash
attention or manually writing your own kernel for a specific backend to enhance performance. See
[this section](https://burn.dev/burn-book/advanced/backend-extension/index.html) in the Burn Book üî•
for more details.

&lt;/details&gt;

&lt;br /&gt;

## Training &amp; Inference

&lt;div align=&quot;left&quot;&gt;
&lt;img align=&quot;right&quot; src=&quot;https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-wall.png&quot; height=&quot;96px&quot;/&gt;

The whole deep learning workflow is made easy with Burn, as you can monitor your training progress
with an ergonomic dashboard, and run inference everywhere from embedded devices to large GPU
clusters.

Burn was built from the ground up with training and inference in mind. It&#039;s also worth noting how
Burn, in comparison to frameworks like PyTorch, simplifies the transition from training to
deployment, eliminating the need for code changes.

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

&lt;br /&gt;

&lt;a href=&quot;https://www.youtube.com/watch?v=N9RM5CQbNQc&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/tracel-ai/burn/main/assets/burn-train-tui.png&quot; alt=&quot;Burn Train TUI&quot; width=&quot;75%&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

&lt;br /&gt;

**Click on the following sections to expand üëá**

&lt;details&gt;
&lt;summary&gt;
Training Dashboard üìà
&lt;/summary&gt;
&lt;br /&gt;

As you can see in the previous video (click on the picture!), a new terminal UI dashboard based on
the [Ratatui](https://github.com/ratatui-org/ratatui) crate allows users to follow their training
with ease without having to connect to any external application.

You can visualize your training and validation metrics updating in real-time and analyze the
lifelong progression or recent history of any registered metrics using only the arrow keys. Break
from the training loop without crashing, allowing potential checkpoints to be fully written or
important pieces of code to complete without interruption üõ°

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
ONNX Support üê´
&lt;/summary&gt;
&lt;br /&gt;

ONNX (Open Neural Network Exchange) is an open-standard format that exports both the architecture
and the weights of a deep learning model.

Burn supports the importation of models that follow the ONNX standard so you can easily port a model
you have written in another framework like TensorFlow or PyTorch to Burn to benefit from all the
advantages our framework offers.

Our ONNX support is further described in
[this section of the Burn Book üî•](https://burn.dev/burn-book/import/onnx-model.html).

&gt; **Note**: This crate is in active development and currently supports a
&gt; [limited set of ONNX operators](./crates/burn-import/SUPPORTED-ONNX-OPS.md).

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Importing PyTorch Models üöö
&lt;/summary&gt;
&lt;br /&gt;

Support for loading of PyTorch model weights into Burn‚Äôs native model architecture, ensuring
seamless integration. See
[Burn Book üî• section on importing PyTorch](https://burn.dev/burn-book/import/pytorch-model.html)

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Inference in the Browser üåê
&lt;/summary&gt;
&lt;br /&gt;

Several of our backends can compile to Web Assembly: Candle and NdArray for CPU, and WGPU for GPU.
This means that you can run inference directly within a browser. We provide several examples of
this:

- [MNIST](./examples/mnist-inference-web) where you can draw digits and a small convnet tries to
  find which one it is! 2Ô∏è‚É£ 7Ô∏è‚É£ üò∞
- [Image Classification](./examples/image-classification-web) where you can upload images and
  classify them! üåÑ

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Embedded: &lt;i&gt;no_std&lt;/i&gt; support ‚öôÔ∏è
&lt;/summary&gt;
&lt;br /&gt;

Burn&#039;s core components support [no_std](https://docs.rust-embedded.org/book/intro/no-std.html). This
means it can run in bare metal environment such as embedded devices without an operating system.

&gt; As of now, only the NdArray backend can be used in a _no_std_ environment.

&lt;/details&gt;

&lt;br /&gt;

## Backends

&lt;div align=&quot;left&quot;&gt;
&lt;img align=&quot;right&quot; src=&quot;https://raw.githubusercontent.com/tracel-ai/burn/main/assets/backend-chip.png&quot; height=&quot;96px&quot;/&gt;
Burn strives to be as fast as possible on as many hardwares as possible, with robust implementations.
We believe this flexibility is crucial for modern needs where you may train your models in the cloud,
then deploy on customer hardwares, which vary from user to user.
&lt;/div&gt;

&lt;br /&gt;

Compared to other frameworks, Burn has a very different approach to supporting many backends. By
design, most code is generic over the Backend trait, which allows us to build Burn with swappable
backends. This makes composing backend possible, augmenting them with additional functionalities
such as autodifferentiation and automatic kernel fusion.

**We already have many backends implemented, all listed below üëá**

&lt;details&gt;
&lt;summary&gt;
WGPU (WebGPU): Cross-Platform GPU Backend üåê
&lt;/summary&gt;
&lt;br /&gt;

**The go-to backend for running on any GPU.**

Based on the most popular and well-supported Rust graphics library, [WGPU](https://wgpu.rs), this
backend automatically targets Vulkan, OpenGL, Metal, Direct X11/12, and WebGPU, by using the WebGPU
shading language [WGSL](https://www.w3.org/TR/WGSL/), or optionally
[SPIR-V](https://www.khronos.org/spir/) when targeting Vulkan. It can also be compiled to Web
Assembly to run in the browser while leveraging the GPU, see
[this demo](https://antimora.github.io/image-classification/). For more information on the benefits
of this backend, see [this blog](https://burn.dev/blog/cross-platform-gpu-backend).

The WGPU backend is our first &quot;in-house backend&quot;, which means we have complete control over its
implementation details. It is fully optimized with the
[performance characteristics mentioned earlier](#performance), as it serves as our research
playground for a variety of optimizations. We&#039;ve since added CUDA, ROCm and SPIR-V support using the
same compiler infrastructure, so a kernel written for burn once, can run anywhere.

See the [WGPU Backend README](./crates/burn-wgpu/README.md) and
[CUDA Backend README](./crates/burn-cuda/README.md) for more details.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Candle: Backend using the Candle bindings üïØ
&lt;/summary&gt;
&lt;br /&gt;

Based on [Candle by Hugging Face](https://github.com/huggingface/candle), a minimalist ML framework
for Rust with a focus on performance and ease of use, this backend can run on CPU with support for
Web Assembly or on Nvidia GPUs using CUDA.

See the [Candle Backend README](./crates/burn-candle/README.md) for more details.

&gt; _Disclaimer:_ This backend is not fully completed yet, but can work in some contexts like
&gt; inference.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
LibTorch: Backend using the LibTorch bindings üéÜ
&lt;/summary&gt;
&lt;br /&gt;

PyTorch doesn&#039;t need an introduction in the realm of deep learning. This backend leverages
[PyTorch Rust bindings](https://github.com/LaurentMazare/tch-rs), enabling you to use LibTorch C++
kernels on CPU, CUDA and Metal.

See the [LibTorch Backend README](./crates/burn-tch/README.md) for more details.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
NdArray: Backend using the NdArray primitive as data structure ü¶ê
&lt;/summary&gt;
&lt;br /&gt;

This CPU backend is admittedly not our fastest backend, but offers extreme portability.

It is our only backend supporting _no_std_.

See the [NdArray Backend README](./crates/burn-ndarray/README.md) for more details.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Autodiff: Backend decorator that brings backpropagation to any backend üîÑ
&lt;/summary&gt;
&lt;br /&gt;

Contrary to the aforementioned backends, Autodiff is actually a backend _decorator_. This means that
it cannot exist by itself; it must encapsulate another backend.

The simple act of wrapping a base backend with Autodiff transparently equips it with
autodifferentiation support, making it possible to call backward on your model.

```rust
use burn::backend::{Autodiff, Wgpu};
use burn::tensor::{Distribution, Tensor};

fn main() {
    type Backend = Autodiff&lt;Wgpu&gt;;

    let x: Tensor&lt;Backend, 2&gt; = Tensor::random([32, 32], Distribution::Default);
    let y: Tensor&lt;Backend, 2&gt; = Tensor::random([32, 32], Distribution::Default).require_grad();

    let tmp = x.clone() + y.clone();
    let tmp = tmp.matmul(x);
    let tmp = tmp.exp();

    let grads = tmp.backward();
    let y_grad = y.grad(&amp;grads).unwrap();
    println!(&quot;{y_grad}&quot;);
}
```

Of note, it is impossible to make the mistake of calling backward on a model that runs on a backend
that does not support autodiff (for inference), as this method is only offered by an Autodiff
backend.

See the [Autodiff Backend README](./crates/burn-autodiff/README.md) for more details.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Fusion: Backend decorator that brings kernel fusion to backends that support it üí•
&lt;/summary&gt;
&lt;br /&gt;

This backend decorator enhances a backend with kernel fusion, provided that the inner backend
supports it. Note that you can compose this backend with other backend decorators such as Autodiff.
For now, only the WGPU and CUDA backends have support for fused kernels.

```rust
use burn::backend::{Autodiff, Fusion, Wgpu};
use burn::tensor::{Distribution, Tensor};

fn main() {
    type Backend = Autodiff&lt;Fusion&lt;Wgpu&gt;&gt;;

    let x: Tensor&lt;Backend, 2&gt; = Tensor::random([32, 32], Distribution::Default);
    let y: Tensor&lt;Backend, 2&gt; = Tensor::random([32, 32], Distribution::Default).require_grad();

    let tmp = x.clone() + y.clone();
    let tmp = tmp.matmul(x);
    let tmp = tmp.exp();

    let grads = tmp.backward();
    let y_grad = y.grad(&amp;grads).unwrap();
    println!(&quot;{y_grad}&quot;);
}

```

Of note, we plan to implement automatic gradient checkpointing based on compute bound and memory
bound operations, which will work gracefully with the fusion backend to make your code run even
faster during training, see [this issue](https://github.com/tracel-ai/burn/issues/936).

See the [Fusion Backend README](./crates/burn-fusion/README.md) for more details.

&lt;/details&gt;

&lt;br /&gt;

## Getting Started

&lt;div align=&quot;left&quot;&gt;
&lt;img align=&quot;right&quot; src=&quot;https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-walking.png&quot; height=&quot;96px&quot;/&gt;

Just heard of Burn? You are at the right place! Just continue reading this section and we hope you
can get on board really quickly.

&lt;/div&gt;

&lt;details&gt;
&lt;summary&gt;
The Burn Book üî•
&lt;/summary&gt;
&lt;br /&gt;

To begin working effectively with Burn, it is crucial to understand its key components and
philosophy. This is why we highly recommend new users to read the first sections of
[The Burn Book üî•](https://burn.dev/burn-book/). It provides detailed examples and explanations
covering every facet of the framework, including building blocks like tensors, modules, and
optimizers, all the way to advanced usage, like coding your own GPU kernels.

&gt; The project is constantly evolving, and we try as much as possible to keep the book up to date
&gt; with new additions. However, we might miss some details sometimes, so if you see something weird,
&gt; let us know! We also gladly accept Pull Requests üòÑ

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Examples üôè
&lt;/summary&gt;
&lt;br /&gt;

Let&#039;s start with a code snippet that shows how intuitive the framework is to use! In the following,
we declare a neural network module with some parameters along with its forward pass.

```rust
use burn::nn;
use burn::module::Module;
use burn::tensor::backend::Backend;

#[derive(Module, Debug)]
pub struct PositionWiseFeedForward&lt;B: Backend&gt; {
    linear_inner: nn::Linear&lt;B&gt;,
    linear_outer: nn::Linear&lt;B&gt;,
    dropout: nn::Dropout,
    gelu: nn::Gelu,
}

impl&lt;B: Backend&gt; PositionWiseFeedForward&lt;B&gt; {
    pub fn forward&lt;const D: usize&gt;(&amp;self, input: Tensor&lt;B, D&gt;) -&gt; Tensor&lt;B, D&gt; {
        let x = self.linear_inner.forward(input);
        let x = self.gelu.forward(x);
        let x = self.dropout.forward(x);

        self.linear_outer.forward(x)
    }
}
```

We have a somewhat large amount of [examples](./examples) in the repository that shows how to use
the framework in different scenarios.

Following [the book](https://burn.dev/burn-book/):

- [Basic Workflow](./examples/guide) 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[bevyengine/bevy]]></title>
            <link>https://github.com/bevyengine/bevy</link>
            <guid>https://github.com/bevyengine/bevy</guid>
            <pubDate>Mon, 24 Mar 2025 00:05:56 GMT</pubDate>
            <description><![CDATA[A refreshingly simple data-driven game engine built in Rust]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/bevyengine/bevy">bevyengine/bevy</a></h1>
            <p>A refreshingly simple data-driven game engine built in Rust</p>
            <p>Language: Rust</p>
            <p>Stars: 38,596</p>
            <p>Forks: 3,797</p>
            <p>Stars today: 23 stars today</p>
            <h2>README</h2><pre># [![Bevy](assets/branding/bevy_logo_light_dark_and_dimmed.svg)](https://bevyengine.org)

[![License](https://img.shields.io/badge/license-MIT%2FApache-blue.svg)](https://github.com/bevyengine/bevy#license)
[![Crates.io](https://img.shields.io/crates/v/bevy.svg)](https://crates.io/crates/bevy)
[![Downloads](https://img.shields.io/crates/d/bevy.svg)](https://crates.io/crates/bevy)
[![Docs](https://docs.rs/bevy/badge.svg)](https://docs.rs/bevy/latest/bevy/)
[![CI](https://github.com/bevyengine/bevy/workflows/CI/badge.svg)](https://github.com/bevyengine/bevy/actions)
[![Discord](https://img.shields.io/discord/691052431525675048.svg?label=&amp;logo=discord&amp;logoColor=ffffff&amp;color=7389D8&amp;labelColor=6A7EC2)](https://discord.gg/bevy)

## What is Bevy?

Bevy is a refreshingly simple data-driven game engine built in Rust. It is free and open-source forever!

## WARNING

Bevy is still in the early stages of development. Important features are missing. Documentation is sparse. A new version of Bevy containing breaking changes to the API is released [approximately once every 3 months](https://bevyengine.org/news/bevy-0-6/#the-train-release-schedule). We provide [migration guides](https://bevyengine.org/learn/migration-guides/), but we can&#039;t guarantee migrations will always be easy. Use only if you are willing to work in this environment.

**MSRV:** Bevy relies heavily on improvements in the Rust language and compiler.
As a result, the Minimum Supported Rust Version (MSRV) is generally close to &quot;the latest stable release&quot; of Rust.

## Design Goals

* **Capable**: Offer a complete 2D and 3D feature set
* **Simple**: Easy for newbies to pick up, but infinitely flexible for power users
* **Data Focused**: Data-oriented architecture using the Entity Component System paradigm
* **Modular**: Use only what you need. Replace what you don&#039;t like
* **Fast**: App logic should run quickly, and when possible, in parallel
* **Productive**: Changes should compile quickly ... waiting isn&#039;t fun

## About

* **[Features](https://bevyengine.org):** A quick overview of Bevy&#039;s features.
* **[News](https://bevyengine.org/news/)**: A development blog that covers our progress, plans and shiny new features.

## Docs

* **[Quick Start Guide](https://bevyengine.org/learn/quick-start/introduction):** Bevy&#039;s official Quick Start Guide. The best place to start learning Bevy.
* **[Bevy Rust API Docs](https://docs.rs/bevy):** Bevy&#039;s Rust API docs, which are automatically generated from the doc comments in this repo.
* **[Official Examples](https://github.com/bevyengine/bevy/tree/latest/examples):** Bevy&#039;s dedicated, runnable examples, which are great for digging into specific concepts.
* **[Community-Made Learning Resources](https://bevyengine.org/assets/#learning)**: More tutorials, documentation, and examples made by the Bevy community.

## Community

Before contributing or participating in discussions with the community, you should familiarize yourself with our [**Code of Conduct**](./CODE_OF_CONDUCT.md).

* **[Discord](https://discord.gg/bevy):** Bevy&#039;s official discord server.
* **[Reddit](https://reddit.com/r/bevy):** Bevy&#039;s official subreddit.
* **[GitHub Discussions](https://github.com/bevyengine/bevy/discussions):** The best place for questions about Bevy, answered right here!
* **[Bevy Assets](https://bevyengine.org/assets/):** A collection of awesome Bevy projects, tools, plugins and learning materials.

### Contributing

If you&#039;d like to help build Bevy, check out the **[Contributor&#039;s Guide](https://bevyengine.org/learn/contribute/introduction)**.
For simple problems, feel free to [open an issue](https://github.com/bevyengine/bevy/issues) or
[PR](https://github.com/bevyengine/bevy/pulls) and tackle it yourself!

For more complex architecture decisions and experimental mad science, please open an [RFC](https://github.com/bevyengine/rfcs) (Request For Comments) so we can brainstorm together effectively!

## Getting Started

We recommend checking out the [Quick Start Guide](https://bevyengine.org/learn/quick-start/introduction) for a brief introduction.

Follow the [Setup guide](https://bevyengine.org/learn/quick-start/getting-started/setup) to ensure your development environment is set up correctly.
Once set up, you can quickly try out the [examples](https://github.com/bevyengine/bevy/tree/latest/examples) by cloning this repo and running the following commands:

```sh
# Switch to the correct version (latest release, default is main development branch)
git checkout latest
# Runs the &quot;breakout&quot; example
cargo run --example breakout
```

To draw a window with standard functionality enabled, use:

```rust
use bevy::prelude::*;

fn main(){
  App::new()
    .add_plugins(DefaultPlugins)
    .run();
}
```

### Fast Compiles

Bevy can be built just fine using default configuration on stable Rust. However for really fast iterative compiles, you should enable the &quot;fast compiles&quot; setup by [following the instructions here](https://bevyengine.org/learn/quick-start/getting-started/setup).

## [Bevy Cargo Features][cargo_features]

This [list][cargo_features] outlines the different cargo features supported by Bevy. These allow you to customize the Bevy feature set for your use-case.

[cargo_features]: docs/cargo_features.md

## Thanks

Bevy is the result of the hard work of many people. A huge thanks to all Bevy contributors, the many open source projects that have come before us, the [Rust gamedev ecosystem](https://arewegameyet.rs/), and the many libraries we build on.

A huge thanks to Bevy&#039;s [generous sponsors](https://bevyengine.org). Bevy will always be free and open source, but it isn&#039;t free to make. Please consider [sponsoring our work](https://bevyengine.org/donate/) if you like what we&#039;re building.

&lt;!-- This next line need to stay exactly as is. It is required for BrowserStack sponsorship. --&gt;
This project is tested with BrowserStack.

## License

Bevy is free, open source and permissively licensed!
Except where noted (below and/or in individual files), all code in this repository is dual-licensed under either:

* MIT License ([LICENSE-MIT](LICENSE-MIT) or [http://opensource.org/licenses/MIT](http://opensource.org/licenses/MIT))
* Apache License, Version 2.0 ([LICENSE-APACHE](LICENSE-APACHE) or [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0))

at your option.
This means you can select the license you prefer!
This dual-licensing approach is the de-facto standard in the Rust ecosystem and there are [very good reasons](https://github.com/bevyengine/bevy/issues/2373) to include both.

Some of the engine&#039;s code carries additional copyright notices and license terms due to their external origins.
These are generally BSD-like, but exact details vary by crate:
If the README of a crate contains a &#039;License&#039; header (or similar), the additional copyright notices and license terms applicable to that crate will be listed.
The above licensing requirement still applies to contributions to those crates, and sections of those crates will carry those license terms.
The [license](https://doc.rust-lang.org/cargo/reference/manifest.html#the-license-and-license-file-fields) field of each crate will also reflect this.
For example, [`bevy_mikktspace`](./crates/bevy_mikktspace/README.md#license-agreement) has code under the Zlib license (as well as a copyright notice when choosing the MIT license).

The [assets](assets) included in this repository (for our [examples](./examples/README.md)) typically fall under different open licenses.
These will not be included in your game (unless copied in by you), and they are not distributed in the published bevy crates.
See [CREDITS.md](CREDITS.md) for the details of the licenses of those files.

### Your contributions

Unless you explicitly state otherwise,
any contribution intentionally submitted for inclusion in the work by you,
as defined in the Apache-2.0 license,
shall be dual licensed as above,
without any additional terms or conditions.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[shadowsocks/shadowsocks-rust]]></title>
            <link>https://github.com/shadowsocks/shadowsocks-rust</link>
            <guid>https://github.com/shadowsocks/shadowsocks-rust</guid>
            <pubDate>Mon, 24 Mar 2025 00:05:55 GMT</pubDate>
            <description><![CDATA[A Rust port of shadowsocks]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/shadowsocks/shadowsocks-rust">shadowsocks/shadowsocks-rust</a></h1>
            <p>A Rust port of shadowsocks</p>
            <p>Language: Rust</p>
            <p>Stars: 9,199</p>
            <p>Forks: 1,270</p>
            <p>Stars today: 32 stars today</p>
            <h2>README</h2><pre># shadowsocks

[![License](https://img.shields.io/github/license/zonyitoo/shadowsocks-rust.svg)](https://github.com/zonyitoo/shadowsocks-rust)
[![Build &amp; Test](https://github.com/shadowsocks/shadowsocks-rust/actions/workflows/build-and-test.yml/badge.svg)](https://github.com/shadowsocks/shadowsocks-rust/actions/workflows/build-and-test.yml)
[![Build MSRV](https://github.com/shadowsocks/shadowsocks-rust/actions/workflows/build-msrv.yml/badge.svg)](https://github.com/shadowsocks/shadowsocks-rust/actions/workflows/build-msrv.yml)
[![Build Releases](https://github.com/shadowsocks/shadowsocks-rust/actions/workflows/build-release.yml/badge.svg?event=push)](https://github.com/shadowsocks/shadowsocks-rust/actions/workflows/build-release.yml)
[![Build Nightly Releases](https://github.com/shadowsocks/shadowsocks-rust/actions/workflows/build-nightly-release.yml/badge.svg)](https://github.com/shadowsocks/shadowsocks-rust/actions/workflows/build-nightly-release.yml)
[![Gurubase](https://img.shields.io/badge/Gurubase-Ask%20shadowsocks%20Guru-006BFF)](https://gurubase.io/g/shadowsocks)

[![crates.io](https://img.shields.io/crates/v/shadowsocks-rust.svg)](https://crates.io/crates/shadowsocks-rust)
[![Release](https://img.shields.io/github/release/shadowsocks/shadowsocks-rust.svg)](https://github.com/shadowsocks/shadowsocks-rust/releases)
[![shadowsocks-rust](https://img.shields.io/archlinux/v/extra/x86_64/shadowsocks-rust)](https://archlinux.org/packages/extra/x86_64/shadowsocks-rust/)
[![aur shadowsocks-rust-git](https://img.shields.io/aur/version/shadowsocks-rust-git)](https://aur.archlinux.org/packages/shadowsocks-rust-git)
[![NixOS](https://img.shields.io/badge/NixOS-shadowsocks--rust-blue?logo=nixos)](https://github.com/NixOS/nixpkgs/tree/master/pkgs/tools/networking/shadowsocks-rust)
[![snap shadowsocks-rust](https://snapcraft.io/shadowsocks-rust/badge.svg)](https://snapcraft.io/shadowsocks-rust)
[![homebrew shadowsocks-rust](https://img.shields.io/homebrew/v/shadowsocks-rust)](https://formulae.brew.sh/formula/shadowsocks-rust#default)
[![MacPorts shadowsocks-rust](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fports.macports.org%2Fapi%2Fv1%2Fports%2Fshadowsocks-rust%2F&amp;query=%24.version&amp;label=macports)](https://ports.macports.org/port/shadowsocks-rust/)

This is a port of [shadowsocks](https://github.com/shadowsocks/shadowsocks).

shadowsocks is a fast tunnel proxy that helps you bypass firewalls.

| Library                                                                 | Description                                                                                                                                                                                                                                                 |
| ----------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [**shadowsocks**](https://crates.io/crates/shadowsocks)                 | [![crates.io](https://img.shields.io/crates/v/shadowsocks.svg)](https://crates.io/crates/shadowsocks) [![docs.rs](https://img.shields.io/docsrs/shadowsocks)](https://docs.rs/shadowsocks) shadowsocks core protocol                                        |
| [**shadowsocks-service**](https://crates.io/crates/shadowsocks-service) | [![crates.io](https://img.shields.io/crates/v/shadowsocks-service.svg)](https://crates.io/crates/shadowsocks-service) [![docs.rs](https://img.shields.io/docsrs/shadowsocks-service)](https://docs.rs/shadowsocks-service) Services for serving shadowsocks |
| [**shadowsocks-rust**](https://crates.io/crates/shadowsocks-rust)       | [![crates.io](https://img.shields.io/crates/v/shadowsocks-rust.svg)](https://crates.io/crates/shadowsocks-rust) Binaries running common shadowsocks services                                                                                                |

Related Projects:

- [spyophobia/shadowsocks-gtk-rs](https://github.com/spyophobia/shadowsocks-gtk-rs) A GUI on Linux for `sslocal` using GTK, [discussion](https://github.com/shadowsocks/shadowsocks-rust/issues/664)
- [honwen/openwrt-shadowsocks-rust](https://github.com/honwen/openwrt-shadowsocks-rust) OpenWRT solution for `sslocal`, [discussion](https://github.com/honwen/openwrt-shadowsocks-rust)
- [cg31/shadowsocks-windows-gui-rust](https://github.com/cg31/shadowsocks-windows-gui-rust) Windows GUI client, [discussion](https://github.com/shadowsocks/shadowsocks-rust/issues/375)

## Build &amp; Install

### Optional Features

- `hickory-dns` - Uses [`hickory-resolver`](https://crates.io/crates/hickory-resolver) as DNS resolver instead of `tokio`&#039;s builtin.

- `local-http` - Allow using HTTP protocol for `sslocal`

  - `local-http-native-tls` - Support HTTPS with [`native-tls`](https://crates.io/crates/native-tls)

  - `local-http-rustls` - Support HTTPS with [`rustls`](https://crates.io/crates/rustls)

- `local-tunnel` - Allow using tunnel protocol for `sslocal`

- `local-socks4` - Allow using SOCKS4/4a protocol for `sslocal`

- `local-redir` - Allow using redir (transparent proxy) protocol for `sslocal`

- `local-dns` - Allow using dns protocol for `sslocal`, serves as a DNS server proxying queries to local or remote DNS servers by ACL rules

- `local-fake-dns` - FakeDNS, allocating an IP address for each individual Query from a specific IP pool

- `local-tun` - [TUN](https://en.wikipedia.org/wiki/TUN/TAP) interface support for `sslocal`

- `local-online-config` - [SIP008](https://shadowsocks.org/doc/sip008.html) Online Configuration Delivery

- `stream-cipher` - Enable deprecated stream ciphers. WARN: stream ciphers are UNSAFE!

- `aead-cipher-extra` - Enable non-standard AEAD ciphers

- `aead-cipher-2022` - Enable AEAD-2022 ciphers ([SIP022](https://github.com/shadowsocks/shadowsocks-org/issues/196))

- `aead-cipher-2022-extra` - Enable AEAD-2022 extra ciphers (non-standard ciphers)

#### Memory Allocators

This project uses system (libc) memory allocator (Rust&#039;s default). But it also allows you to use other famous allocators by features:

- `jemalloc` - Uses [jemalloc](http://jemalloc.net/) as global memory allocator
- `mimalloc` - Uses [mi-malloc](https://microsoft.github.io/mimalloc/) as global memory allocator
- `tcmalloc` - Uses [TCMalloc](https://google.github.io/tcmalloc/overview.html) as global memory allocator. It tries to link system-wide tcmalloc by default, use vendored from source with `tcmalloc-vendored`.
- `snmalloc` - Uses [snmalloc](https://github.com/microsoft/snmalloc) as global memory allocator
- `rpmalloc` - Uses [rpmalloc](https://github.com/mjansson/rpmalloc) as global memory allocator

### **crates.io**

Install from [crates.io](https://crates.io/crates/shadowsocks-rust):

```bash
# Install from crates.io
cargo install shadowsocks-rust
```

then you can find `sslocal` and `ssserver` in `$CARGO_HOME/bin`.

### **Install using Homebrew**

For macOS and Linux, you can install it using [Homebrew](https://brew.sh/):

```bash
brew install shadowsocks-rust
```

### **Install using snap**

```bash
# Install from snapstore
snap install shadowsocks-rust

# List services
snap services shadowsocks-rust

# Enable and start shadowsocks-rust.sslocal-daemon snap service
snap start --enable shadowsocks-rust.sslocal-daemon

# Show generated systemd service status
systemctl status snap.shadowsocks-rust.sslocal-daemon.service

# Override generated systemd service (configure startup options)
systemctl edit snap.shadowsocks-rust.sslocal-daemon.service

## NOTE: you can pass args to sslocal:
##  [Service]
##  ExecStart=
##  ExecStart=/usr/bin/snap run shadowsocks-rust.sslocal-daemon -b &quot;127.0.0.1:1080&quot; --server-url &quot;ss://....&quot;

# Restart generated systemd service to apply changes
systemctl restart snap.shadowsocks-rust.sslocal-daemon.service

# ... and show service status
systemctl status snap.shadowsocks-rust.sslocal-daemon.service
```

### **Download release**

Download static-linked build [here](https://github.com/shadowsocks/shadowsocks-rust/releases).

- `build-windows`: Build for `x86_64-pc-windows-msvc`
- `build-linux`: Build for `x86_64-unknown-linux-gnu`, Debian 9 (Stretch), GLIBC 2.18
- `build-docker`: Build for `x86_64-unknown-linux-musl`, `x86_64-pc-windows-gnu`, ... (statically linked)

### **Docker**

This project provided Docker images for the `linux/i386` and `linux/amd64` and `linux/arm64/v8` architectures.

&gt; :warning: **Docker containers do not have access to IPv6 by default**: Make sure to disable IPv6 Route in the client or [enable IPv6 access to docker containers](https://docs.docker.com/config/daemon/ipv6/#use-ipv6-for-the-default-bridge-network).

#### Pull from GitHub Container Registry

Docker will pull the image of the appropriate architecture from our [GitHub Packages](https://github.com/orgs/shadowsocks/packages?repo_name=shadowsocks-rust).

```bash
docker pull ghcr.io/shadowsocks/sslocal-rust:latest
docker pull ghcr.io/shadowsocks/ssserver-rust:latest
```

#### Build on the local machineÔºàOptionalÔºâ

If you want to build the Docker image yourself, you need to use the [BuildX](https://docs.docker.com/buildx/working-with-buildx/).

```bash
docker buildx build -t shadowsocks/ssserver-rust:latest -t shadowsocks/ssserver-rust:v1.15.2 --target ssserver .
docker buildx build -t shadowsocks/sslocal-rust:latest -t shadowsocks/sslocal-rust:v1.15.2 --target sslocal .
```

#### Run the container

You need to mount the configuration file into the container and create an external port map for the container to connect to it.

```bash
docker run --name sslocal-rust \
  --restart always \
  -p 1080:1080/tcp \
  -v /path/to/config.json:/etc/shadowsocks-rust/config.json \
  -dit ghcr.io/shadowsocks/sslocal-rust:latest

docker run --name ssserver-rust \
  --restart always \
  -p 8388:8388/tcp \
  -p 8388:8388/udp \
  -v /path/to/config.json:/etc/shadowsocks-rust/config.json \
  -dit ghcr.io/shadowsocks/ssserver-rust:latest
```

### **Deploy to Kubernetes**

This project provided yaml manifests for deploying to Kubernetes.

You can leverage k8s Service to expose traffic outside, like LoadBalancer or NodePort which gains more fine-grained compared with fixed host or port.

For a more interesting use case, you can use a Ingress(Istio, nginx, etc.) which routes the matched traffic to shadowsocks along with the real web service.

#### Using `kubectl`

`kubectl apply -f https://github.com/shadowsocks/shadowsocks-rust/raw/master/k8s/shadowsocks-rust.yaml`

You can change the config via editing the ConfigMap named `shadowsocks-rust`.

For more fine-grained control, use `helm`.

#### Using `helm`

`helm install my-release k8s/chart -f my-values.yaml`

Below is the common default values you can change:

```yaml
# This is the shadowsocks config which will be mount to /etc/shadowocks-rust.
# You can put arbitrary yaml here, and it will be translated to json before mounting.
servers:
- server: &quot;::&quot;
  server_port: 8388
  service_port: 80 # the k8s service port, default to server_port
  password: mypassword
  method: aes-256-gcm
  fast_open: true
  mode: tcp_and_udp
  # plugin: v2ray-plugin
  # plugin_opts: server;tls;host=github.com

# Whether to download v2ray and xray plugin.
downloadPlugins: false

# Name of the ConfigMap with config.json configuration for shadowsocks-rust.
configMapName: &quot;&quot;

service:
  # Change to LoadBalancer if you are behind a cloud provider like aws, gce, or tke.
  type: ClusterIP

# Bind shadowsocks port port to host, i.e., we can use host:port to access shawdowsocks server.
hostPort: false

replicaCount: 1

image:
  repository: ghcr.io/shadowsocks/ssserver-rust
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: &quot;latest&quot;
```

### **Build from source**

Use cargo to build. NOTE: **RAM &gt;= 2GiB**

```bash
cargo build --release
```

Then `sslocal` and `ssserver` will appear in `./target/(debug|release)/`, it works similarly as the two binaries in the official ShadowSocks&#039; implementation.

```bash
make install TARGET=release
```

Then `sslocal`, `ssserver`, `ssmanager` and `ssurl` will be installed to `/usr/local/bin` (variable PREFIX).

For Windows users, if you have encountered any problem in building, check and discuss in [#102](https://github.com/shadowsocks/shadowsocks-rust/issues/102).

### **target-cpu optimization**

If you are building for your current CPU platform (for example, build and run on your personal computer), it is recommended to set `target-cpu=native` feature to let `rustc` generate and optimize code for the CPU running the compiler.

```bash
export RUSTFLAGS=&quot;-C target-cpu=native&quot;
```

### **Build standalone binaries**

Requirements:

- Docker

```bash
./build/build-release
```

Then `sslocal`, `ssserver`, `ssmanager` and `ssurl` will be packaged in

- `./build/shadowsocks-${VERSION}-stable.x86_64-unknown-linux-musl.tar.xz`
- `./build/shadowsocks-${VERSION}-stable.x86_64-pc-windows-gnu.zip`

Read `Cargo.toml` for more details.

## Getting Started

Generate a safe and secured password for a specific encryption method (`aes-128-gcm` in the example) with:

```bash
ssservice genkey -m &quot;aes-128-gcm&quot;
```

Create a ShadowSocks&#039; configuration file. Example

```jsonc
{
    &quot;server&quot;: &quot;my_server_ip&quot;,
    &quot;server_port&quot;: 8388,
    &quot;password&quot;: &quot;rwQc8qPXVsRpGx3uW+Y3Lj4Y42yF9Bs0xg1pmx8/+bo=&quot;,
    &quot;method&quot;: &quot;aes-256-gcm&quot;,
    // ONLY FOR `sslocal`
    // Delete these lines if you are running `ssserver` or `ssmanager`
    &quot;local_address&quot;: &quot;127.0.0.1&quot;,
    &quot;local_port&quot;: 1080
}
```

Detailed explanation of the configuration file could be found in [shadowsocks&#039; documentation](https://github.com/shadowsocks/shadowsocks/wiki). (Link to original project, not maintained anymore !)

&gt; :warning: For snap installations, configuration file is most probably located in `/var/snap/shadowsocks-rust/common/etc/shadowsocks-rust/config.json` (see https://github.com/shadowsocks/shadowsocks-rust/issues/621 / https://github.com/shadowsocks/shadowsocks-rust/issues/1146)

In shadowsocks-rust, we also have an extended configuration file format, which is able to define more than one server. You can also disable individual servers.

```jsonc
{
    &quot;servers&quot;: [
        {
            &quot;server&quot;: &quot;127.0.0.1&quot;,
            &quot;server_port&quot;: 8388,
            &quot;password&quot;: &quot;rwQc8qPXVsRpGx3uW+Y3Lj4Y42yF9Bs0xg1pmx8/+bo=&quot;,
            &quot;method&quot;: &quot;aes-256-gcm&quot;,
            &quot;timeout&quot;: 7200
        },
        {
            &quot;server&quot;: &quot;127.0.0.1&quot;,
            &quot;server_port&quot;: 8389,
            &quot;password&quot;: &quot;/dliNXn5V4jg6vBW4MnC1I8Jljg9x7vSihmk6UZpRBM=&quot;,
            &quot;method&quot;: &quot;chacha20-ietf-poly1305&quot;
        },
        {
            &quot;disabled&quot;: true,
            &quot;server&quot;: &quot;eg.disable.me&quot;,
            &quot;server_port&quot;: 8390,
            &quot;password&quot;: &quot;mGvbWWay8ueP9IHnV5F1uWGN2BRToiVCAWJmWOTLU24=&quot;,
            &quot;method&quot;: &quot;chacha20-ietf-poly1305&quot;
        }
    ],
    // ONLY FOR `sslocal`
    // Delete these lines if you are running `ssserver` or `ssmanager`
    &quot;local_port&quot;: 1080,
    &quot;local_address&quot;: &quot;127.0.0.1&quot;
}
```

`sslocal` automatically selects the best server with the lowest latency and the highest availability.

Start Shadowsocks client and server with:

```bash
sslocal -c config.json
ssserver -c config.json
```

If you Build it with Cargo:

```bash
cargo run --bin sslocal -- -c config.json
cargo run --bin ssserver -- -c config.json
```

List all available arguments with `-h`.

## Usage

Start local client with configuration file

```bash
# Read local client configuration from file
sslocal -c /path/to/shadowsocks.json
```

### Socks5 Local client

```bash
# Pass all parameters via command line
sslocal -b &quot;127.0.0.1:1080&quot; -s &quot;[::1]:8388&quot; -m &quot;aes-256-gcm&quot; -k &quot;hello-kitty&quot; --plugin &quot;v2ray-plugin&quot; --plugin-opts &quot;server;tls;host=github.com&quot;

# Pass server with SIP002 URL
sslocal -b &quot;127.0.0.1:1080&quot; --server-url &quot;ss://YWVzLTI1Ni1nY206cGFzc3dvcmQ@127.0.0.1:8388/?plugin=v2ray-plugin%3Bserver%3Btls%3Bhost%3Dgithub.com&quot;
```

### HTTP Local client

```bash
sslocal -b &quot;127.0.0.1:3128&quot; --protocol http -s &quot;[::1]:8388&quot; -m &quot;aes-256-gcm&quot; -k &quot;hello-kitty&quot;
```

All parameters are the same as Socks5 client, except `--protocol http`.

### Tunnel Local client

```bash
# Set 127.0.0.1:8080 as the target for forwarding to
sslocal --protocol tunnel -b &quot;127.0.0.1:3128&quot; -f &quot;127.0.0.1:8080&quot; -s &quot;[::1]:8388&quot; -m &quot;aes-256-gcm&quot; -k &quot;hello-kitty&quot;
```

- `--protocol tunnel` enables local client Tunnel mode
- `-f &quot;127.0.0.1:8080` sets the tunnel target address

### Transparent Proxy Local client

**NOTE**: It currently only supports

- Linux (with `iptables` targets `REDIRECT` and `TPROXY`)
- BSDs (with `pf`), such as OS X 10.10+, FreeBSD, ...

```bash
sslocal -b &quot;127.0.0.1:60080&quot; --protocol redir -s &quot;[::1]:8388&quot; -m &quot;aes-256-gcm&quot; -k &quot;hello-kitty&quot; --tcp-redir &quot;redirect&quot; --udp-redir &quot;tproxy&quot;
```

Redirects connections with `iptables` configurations to the port that `sslocal` is listening on.

- `--protocol redir` enables local client Redir mode
- (optional) `--tcp-redir` sets TCP mode to `REDIRECT` (Linux)
- (optional) `--udp-redir` sets UDP mode to `TPROXY` (Linux)

### Tun interface client

**NOTE**: It currently only supports

- Linux, Android
- macOS, iOS
- Windows

#### Linux

Create a Tun interface with name `tun0`

```bash
ip tuntap add mode tun tun0
ifconfig tun0 inet 10.255.0.1 netmask 255.255.255.0 up
```

Start `sslocal` with `--protocol tun` and binds to `tun0`

```bash
sslocal --protocol tun -s &quot;[::1]:8388&quot; -m &quot;aes-256-gcm&quot; -k &quot;hello-kitty&quot; --outbound-bind-interface lo0 --tun-interface-name tun0
```

#### macOS

```bash
sslocal --protocol tun -s &quot;[::1]:8388&quot; -m &quot;aes-256-gcm&quot; -k &quot;hello-kitty&quot; --outbound-bind-interface lo0 --tun-interface-address 10.255.0.1/24
```

It will create a Tun interface with address `10.255.0.1` and netmask `255.255.255.0`.

#### Windows

Download `wintun.dll` from [Wintun](https://www.wintun.net/), and place it in the folder with shadowsocks&#039; runnable binaries, or in the system PATH.

```powershell
sslocal --protocol tun -s &quot;[::1]:8388&quot; -m &quot;aes-256-gcm&quot; -k &quot;hello-kitty&quot; --outbound-bind-interface &quot;Ethernet 0&quot; --tun-interface-name &quot;shadowsocks&quot;
```

### Local client for Windows Service

Compile it by enabling `--features &quot;winservice&quot;` (not included in the default build):

```bash
cargo build --release --bin &quot;sswinservice&quot; --features &quot;winservice&quot;
```

Install it as a Windows Service (PowerShell):

```powershell
New-Service -Name &quot;shadowsocks-local-service&quot; `
            -DisplayName &quot;Shadowsocks Local Service&quot; `
            -BinaryPathName &quot;&lt;Path\to&gt;\sswinservice.exe local -c &lt;Path\to&gt;\local_config.json&quot;
```

There are other ways to install `sswinservice` as a Windows Service, for example, the `sc` command.

As you may have noticed that the `-BinaryPathName` contains not only just the `sswinservice.exe`, but `local -c local_config.json`. These command line parameters will be used as the default parameter when the Windows Service starts. You can also start the service with customized parameters.

Learn more from [Microsoft&#039;s Document](https://learn.microsoft.com/en-us/dotnet/framework/windows-services/introduction-to-windows-service-applications).

The `sswinservice`&#039;s parameter works exactly the same as `ssservice`. It supports `local`, `server` and `manager` subcommands.

### Server

```bash
# Read server configuration from file
ssserver -c /path/to/shadowsocks.json

# Pass all parameters via command line
ssserver -s &quot;[::]:8388&quot; -m &quot;aes-256-gcm&quot; -k &quot;hello-kitty&quot; --plugin &quot;v2ray-plugin&quot; --plugin-opts &quot;server;tls;host=github.com&quot;
```

### Server Manager

Supported [Manage Multiple Users](https://github.com/shadowsocks/shadowsocks/wiki/Manage-Multiple-Users) API:

- `add` - Starts a server instance
- `remove` - Deletes an existing server instance
- `list` - Lists all current running 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[zellij-org/zellij]]></title>
            <link>https://github.com/zellij-org/zellij</link>
            <guid>https://github.com/zellij-org/zellij</guid>
            <pubDate>Mon, 24 Mar 2025 00:05:54 GMT</pubDate>
            <description><![CDATA[A terminal workspace with batteries included]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/zellij-org/zellij">zellij-org/zellij</a></h1>
            <p>A terminal workspace with batteries included</p>
            <p>Language: Rust</p>
            <p>Stars: 23,611</p>
            <p>Forks: 713</p>
            <p>Stars today: 14 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
  &lt;br&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/zellij-org/zellij/main/assets/logo.png&quot; alt=&quot;logo&quot; width=&quot;200&quot;&gt;
  &lt;br&gt;
  Zellij
  &lt;br&gt;
  &lt;br&gt;
&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://discord.gg/CrUAFH3&quot;&gt;&lt;img alt=&quot;Discord Chat&quot; src=&quot;https://img.shields.io/discord/771367133715628073?color=5865F2&amp;label=discord&amp;style=flat-square&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://matrix.to/#/#zellij_general:matrix.org&quot;&gt;&lt;img alt=&quot;Matrix Chat&quot; src=&quot;https://img.shields.io/matrix/zellij_general:matrix.org?color=1d7e64&amp;label=matrix%20chat&amp;style=flat-square&amp;logo=matrix&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://zellij.dev/documentation/&quot;&gt;&lt;img alt=&quot;Zellij documentation&quot; src=&quot;https://img.shields.io/badge/zellij-documentation-fc0060?style=flat-square&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/zellij-org/zellij/main/assets/demo.gif&quot; alt=&quot;demo&quot;&gt;
&lt;/p&gt;

&lt;h4 align=&quot;center&quot;&gt;
  [&lt;a href=&quot;https://zellij.dev/documentation/installation&quot;&gt;Installation&lt;/a&gt;]
  [&lt;a href=&quot;https://zellij.dev/screencasts/&quot;&gt;Screencasts &amp; Tutorials&lt;/a&gt;]
  [&lt;a href=&quot;https://zellij.dev/documentation/configuration&quot;&gt;Configuration&lt;/a&gt;]
  [&lt;a href=&quot;https://zellij.dev/documentation/layouts&quot;&gt;Layouts&lt;/a&gt;]
  [&lt;a href=&quot;https://zellij.dev/documentation/faq&quot;&gt;FAQ&lt;/a&gt;]
&lt;/h4&gt;

# What is this?

[Zellij](#origin-of-the-name) is a workspace aimed at developers, ops-oriented people and anyone who loves the terminal. Similar programs are sometimes called &quot;Terminal Multiplexers&quot;.

Zellij is designed around the philosophy that one must not sacrifice simplicity for power, taking pride in its great experience out of the box as well as the advanced features it places at its users&#039; fingertips.

Zellij is geared toward beginner and power users alike - allowing deep customizability, personal automation through [layouts](https://zellij.dev/documentation/layouts.html), true multiplayer collaboration, unique UX features such as floating and stacked panes, and a [plugin system](https://zellij.dev/documentation/plugins.html) allowing one to create plugins in any language that compiles to WebAssembly.

You can get started by [installing](https://zellij.dev/documentation/installation.html) Zellij and checking out the [Screencasts &amp; Tutorials](https://zellij.dev/screencasts/).

For more details about our future plans, read about upcoming features in our [roadmap](#roadmap).

## How do I install it?

The easiest way to install Zellij is through a [package for your OS](./docs/THIRD_PARTY_INSTALL.md).

If one is not available for your OS, you could download a prebuilt binary from the [latest release](https://github.com/zellij-org/zellij/releases/latest) and place it in your `$PATH`. If you&#039;d like, we could [automatically choose one for you](#try-zellij-without-installing).

You can also install (compile) with `cargo`:

```
cargo install --locked zellij
```

#### Try Zellij without installing

bash/zsh:
```bash
bash &lt;(curl -L https://zellij.dev/launch)
```
fish/xonsh:
```bash
bash -c &#039;bash &lt;(curl -L https://zellij.dev/launch)&#039;
```

#### Installing from `main`
Installing Zellij from the `main` branch is not recommended. This branch represents pre-release code, is constantly being worked on and may contain broken or unusable features. In addition, using it may corrupt the cache for future versions, forcing users to clear it before they can use the officially released version.

That being said - no-one will stop you from using it (and bug reports involving new features are greatly appreciated), but please consider using the latest release instead as detailed at the top of this section.

## How do I start a development environment?

* Clone the project
* In the project folder, for debug builds run: `cargo xtask run`
* To run all tests: `cargo xtask test`

For more build commands, see [CONTRIBUTING.md](CONTRIBUTING.md).

## Configuration
For configuring Zellij, please see the [Configuration Documentation](https://zellij.dev/documentation/configuration.html).

## About issues in this repository
Issues in this repository, whether open or closed, do not necessarily indicate a problem or a bug in the software. They only indicate that the reporter wanted to communicate their experiences or thoughts to the maintainers. The Zellij maintainers do their best to go over and reply to all issue reports, but unfortunately cannot promise these will always be dealt with or even read. Your understanding is appreciated.

## Roadmap
Presented here is the project roadmap, divided into three main sections.

These are issues that are either being actively worked on or are planned for the near future.

***If you&#039;ll click on the image, you&#039;ll be led to an SVG version of it on the website where you can directly click on every issue***

[![roadmap](https://github.com/zellij-org/zellij/assets/795598/9c5b573b-20f5-41c6-908b-6b21c5fd456e)](https://zellij.dev/roadmap)

## Origin of the Name
[From Wikipedia, the free encyclopedia](https://en.wikipedia.org/wiki/Zellij)

Zellij (Arabic: ÿßŸÑÿ≤ŸÑŸäÿ¨, romanized: zillƒ´j; also spelled zillij or zellige) is a style of mosaic tilework made from individually hand-chiseled tile pieces. The pieces were typically of different colours and fitted together to form various patterns on the basis of tessellations, most notably elaborate Islamic geometric motifs such as radiating star patterns composed of various polygons. This form of Islamic art is one of the main characteristics of architecture in the western Islamic world. It is found in the architecture of Morocco, the architecture of Algeria, early Islamic sites in Tunisia, and in the historic monuments of al-Andalus (in the Iberian Peninsula).

## License

MIT

## Sponsored by
&lt;a href=&quot;https://terminaltrove.com/&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/121595180?s=200&amp;v=4&quot; width=&quot;80px&quot;&gt;&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[huggingface/candle]]></title>
            <link>https://github.com/huggingface/candle</link>
            <guid>https://github.com/huggingface/candle</guid>
            <pubDate>Mon, 24 Mar 2025 00:05:53 GMT</pubDate>
            <description><![CDATA[Minimalist ML framework for Rust]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/huggingface/candle">huggingface/candle</a></h1>
            <p>Minimalist ML framework for Rust</p>
            <p>Language: Rust</p>
            <p>Stars: 16,865</p>
            <p>Forks: 1,062</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre># candle
[![discord server](https://dcbadge.vercel.app/api/server/hugging-face-879548962464493619)](https://discord.gg/hugging-face-879548962464493619)
[![Latest version](https://img.shields.io/crates/v/candle-core.svg)](https://crates.io/crates/candle-core)
[![Documentation](https://docs.rs/candle-core/badge.svg)](https://docs.rs/candle-core)
[![License](https://img.shields.io/github/license/base-org/node?color=blue)](https://github.com/huggingface/candle/blob/main/LICENSE-MIT)
[![License](https://img.shields.io/badge/license-Apache%202.0-blue?style=flat-square)](https://github.com/huggingface/candle/blob/main/LICENSE-APACHE)

Candle is a minimalist ML framework for Rust with a focus on performance (including GPU support) 
and ease of use. Try our online demos: 
[whisper](https://huggingface.co/spaces/lmz/candle-whisper),
[LLaMA2](https://huggingface.co/spaces/lmz/candle-llama2),
[T5](https://huggingface.co/spaces/radames/Candle-T5-Generation-Wasm),
[yolo](https://huggingface.co/spaces/lmz/candle-yolo),
[Segment
Anything](https://huggingface.co/spaces/radames/candle-segment-anything-wasm).

## Get started

Make sure that you have [`candle-core`](https://github.com/huggingface/candle/tree/main/candle-core) correctly installed as described in [**Installation**](https://huggingface.github.io/candle/guide/installation.html).

Let&#039;s see how to run a simple matrix multiplication.
Write the following to your `myapp/src/main.rs` file:
```rust
use candle_core::{Device, Tensor};

fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    let device = Device::Cpu;

    let a = Tensor::randn(0f32, 1., (2, 3), &amp;device)?;
    let b = Tensor::randn(0f32, 1., (3, 4), &amp;device)?;

    let c = a.matmul(&amp;b)?;
    println!(&quot;{c}&quot;);
    Ok(())
}
```

`cargo run` should display a tensor of shape `Tensor[[2, 4], f32]`.


Having installed `candle` with Cuda support, simply define the `device` to be on GPU:

```diff
- let device = Device::Cpu;
+ let device = Device::new_cuda(0)?;
```

For more advanced examples, please have a look at the following section.

## Check out our examples

These online demos run entirely in your browser:
- [yolo](https://huggingface.co/spaces/lmz/candle-yolo): pose estimation and
  object recognition.
- [whisper](https://huggingface.co/spaces/lmz/candle-whisper): speech recognition.
- [LLaMA2](https://huggingface.co/spaces/lmz/candle-llama2): text generation.
- [T5](https://huggingface.co/spaces/radames/Candle-T5-Generation-Wasm): text generation.
- [Phi-1.5, and Phi-2](https://huggingface.co/spaces/radames/Candle-Phi-1.5-Wasm): text generation.
- [Segment Anything Model](https://huggingface.co/spaces/radames/candle-segment-anything-wasm): Image segmentation.
- [BLIP](https://huggingface.co/spaces/radames/Candle-BLIP-Image-Captioning): image captioning.

We also provide a some command line based examples using state of the art models:

- [LLaMA v1, v2, and v3](./candle-examples/examples/llama/): general LLM, includes
  the SOLAR-10.7B variant.
- [Falcon](./candle-examples/examples/falcon/): general LLM.
- [Codegeex4](./candle-examples/examples/codegeex4-9b/): Code completion,code interpreter,web search,fuction calling,repository-level
- [GLM4](./candle-examples/examples/glm4/): Open Multilingual Multimodal Chat LMs by THUDM
- [Gemma v1 and v2](./candle-examples/examples/gemma/): 2b and 7b+/9b general LLMs from Google Deepmind.
- [RecurrentGemma](./candle-examples/examples/recurrent-gemma/): 2b and 7b
  Griffin based models from Google that mix attention with a RNN like state.
- [Phi-1, Phi-1.5, Phi-2, and Phi-3](./candle-examples/examples/phi/): 1.3b,
  2.7b, and 3.8b general LLMs with performance on par with 7b models.
- [StableLM-3B-4E1T](./candle-examples/examples/stable-lm/): a 3b general LLM
  pre-trained on 1T tokens of English and code datasets. Also supports
  StableLM-2, a 1.6b LLM trained on 2T tokens, as well as the code variants.
- [Mamba](./candle-examples/examples/mamba/): an inference only
  implementation of the Mamba state space model.
- [Mistral7b-v0.1](./candle-examples/examples/mistral/): a 7b general LLM with
  better performance than all publicly available 13b models as of 2023-09-28.
- [Mixtral8x7b-v0.1](./candle-examples/examples/mixtral/): a sparse mixture of
  experts 8x7b general LLM with better performance than a Llama 2 70B model with
  much faster inference.
- [StarCoder](./candle-examples/examples/bigcode/) and
  [StarCoder2](./candle-examples/examples/starcoder2/): LLM specialized to code generation.
- [Qwen1.5](./candle-examples/examples/qwen/): Bilingual (English/Chinese) LLMs.
- [RWKV v5 and v6](./candle-examples/examples/rwkv/): An RNN with transformer level LLM
  performance.
- [Replit-code-v1.5](./candle-examples/examples/replit-code/): a 3.3b LLM specialized for code completion.
- [Yi-6B / Yi-34B](./candle-examples/examples/yi/): two bilingual
  (English/Chinese) general LLMs with 6b and 34b parameters.
- [Quantized LLaMA](./candle-examples/examples/quantized/): quantized version of
  the LLaMA model using the same quantization techniques as
  [llama.cpp](https://github.com/ggerganov/llama.cpp).

&lt;img src=&quot;https://github.com/huggingface/candle/raw/main/candle-examples/examples/quantized/assets/aoc.gif&quot; width=&quot;600&quot;&gt;
  
- [Stable Diffusion](./candle-examples/examples/stable-diffusion/): text to
  image generative model, support for the 1.5, 2.1, SDXL 1.0 and Turbo versions.

&lt;img src=&quot;https://github.com/huggingface/candle/raw/main/candle-examples/examples/stable-diffusion/assets/stable-diffusion-xl.jpg&quot; width=&quot;200&quot;&gt;

- [Wuerstchen](./candle-examples/examples/wuerstchen/): another text to
  image generative model.

&lt;img src=&quot;https://github.com/huggingface/candle/raw/main/candle-examples/examples/wuerstchen/assets/cat.jpg&quot; width=&quot;200&quot;&gt;

- [yolo-v3](./candle-examples/examples/yolo-v3/) and
  [yolo-v8](./candle-examples/examples/yolo-v8/): object detection and pose
  estimation models.

&lt;img src=&quot;https://github.com/huggingface/candle/raw/main/candle-examples/examples/yolo-v8/assets/bike.od.jpg&quot; width=&quot;200&quot;&gt;&lt;img src=&quot;https://github.com/huggingface/candle/raw/main/candle-examples/examples/yolo-v8/assets/bike.pose.jpg&quot; width=&quot;200&quot;&gt;
- [segment-anything](./candle-examples/examples/segment-anything/): image
  segmentation model with prompt.

&lt;img src=&quot;https://github.com/huggingface/candle/raw/main/candle-examples/examples/segment-anything/assets/sam_merged.jpg&quot; width=&quot;200&quot;&gt;

- [SegFormer](./candle-examples/examples/segformer/): transformer based semantic segmentation model.
- [Whisper](./candle-examples/examples/whisper/): speech recognition model.
- [EnCodec](./candle-examples/examples/encodec/): high-quality audio compression
  model using residual vector quantization.
- [MetaVoice](./candle-examples/examples/metavoice/): foundational model for
  text-to-speech.
- [Parler-TTS](./candle-examples/examples/parler-tts/): large text-to-speech
  model.
- [T5](./candle-examples/examples/t5), [Bert](./candle-examples/examples/bert/),
  [JinaBert](./candle-examples/examples/jina-bert/) : useful for sentence embeddings.
- [DINOv2](./candle-examples/examples/dinov2/): computer vision model trained
  using self-supervision (can be used for imagenet classification, depth
  evaluation, segmentation).
- [VGG](./candle-examples/examples/vgg/),
  [RepVGG](./candle-examples/examples/repvgg): computer vision models.
- [BLIP](./candle-examples/examples/blip/): image to text model, can be used to
  generate captions for an image.
- [CLIP](./candle-examples/examples/clip/): multi-model vision and language
  model.
- [TrOCR](./candle-examples/examples/trocr/): a transformer OCR model, with
  dedicated submodels for hand-writing and printed recognition.
- [Marian-MT](./candle-examples/examples/marian-mt/): neural machine translation
  model, generates the translated text from the input text.
- [Moondream](./candle-examples/examples/moondream/): tiny computer-vision model 
  that can answer real-world questions about images.

Run them using commands like:
```
cargo run --example quantized --release
```

In order to use **CUDA** add `--features cuda` to the example command line. If
you have cuDNN installed, use `--features cudnn` for even more speedups.

There are also some wasm examples for whisper and
[llama2.c](https://github.com/karpathy/llama2.c). You can either build them with
`trunk` or try them online:
[whisper](https://huggingface.co/spaces/lmz/candle-whisper),
[llama2](https://huggingface.co/spaces/lmz/candle-llama2),
[T5](https://huggingface.co/spaces/radames/Candle-T5-Generation-Wasm),
[Phi-1.5, and Phi-2](https://huggingface.co/spaces/radames/Candle-Phi-1.5-Wasm),
[Segment Anything Model](https://huggingface.co/spaces/radames/candle-segment-anything-wasm).

For LLaMA2, run the following command to retrieve the weight files and start a
test server:
```bash
cd candle-wasm-examples/llama2-c
wget https://huggingface.co/spaces/lmz/candle-llama2/resolve/main/model.bin
wget https://huggingface.co/spaces/lmz/candle-llama2/resolve/main/tokenizer.json
trunk serve --release --port 8081
```
And then head over to
[http://localhost:8081/](http://localhost:8081/).

&lt;!--- ANCHOR: useful_libraries ---&gt;

## Useful External Resources
- [`candle-tutorial`](https://github.com/ToluClassics/candle-tutorial): A
  very detailed tutorial showing how to convert a PyTorch model to Candle.
- [`candle-lora`](https://github.com/EricLBuehler/candle-lora): Efficient and
  ergonomic LoRA implementation for Candle. `candle-lora` has      
  out-of-the-box LoRA support for many models from Candle, which can be found
  [here](https://github.com/EricLBuehler/candle-lora/tree/master/candle-lora-transformers/examples).
- [`optimisers`](https://github.com/KGrewal1/optimisers): A collection of optimisers
  including SGD with momentum, AdaGrad, AdaDelta, AdaMax, NAdam, RAdam, and RMSprop.
- [`candle-vllm`](https://github.com/EricLBuehler/candle-vllm): Efficient platform for inference and
  serving local LLMs including an OpenAI compatible API server.
- [`candle-ext`](https://github.com/mokeyish/candle-ext): An extension library to Candle that provides PyTorch functions not currently available in Candle.
- [`candle-coursera-ml`](https://github.com/vishpat/candle-coursera-ml): Implementation of ML algorithms from Coursera&#039;s [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) course.
- [`kalosm`](https://github.com/floneum/floneum/tree/master/interfaces/kalosm): A multi-modal meta-framework in Rust for interfacing with local pre-trained models with support for controlled generation, custom samplers, in-memory vector databases, audio transcription, and more.
- [`candle-sampling`](https://github.com/EricLBuehler/candle-sampling): Sampling techniques for Candle.
- [`gpt-from-scratch-rs`](https://github.com/jeroenvlek/gpt-from-scratch-rs): A port of Andrej Karpathy&#039;s _Let&#039;s build GPT_ tutorial on YouTube showcasing the Candle API on a toy problem.
- [`candle-einops`](https://github.com/tomsanbear/candle-einops): A pure rust implementation of the python [einops](https://github.com/arogozhnikov/einops) library.
- [`atoma-infer`](https://github.com/atoma-network/atoma-infer): A Rust library for fast inference at scale, leveraging FlashAttention2 for efficient attention computation, PagedAttention for efficient KV-cache memory management, and multi-GPU support. It is OpenAI api compatible.
- [`llms-from-scratch-rs`](https://github.com/nerdai/llms-from-scratch-rs): A comprehensive Rust translation of the code from Sebastian Raschka&#039;s Build an LLM from Scratch book.

If you have an addition to this list, please submit a pull request.

&lt;!--- ANCHOR_END: useful_libraries ---&gt;

&lt;!--- ANCHOR: features ---&gt;

## Features

- Simple syntax, looks and feels like PyTorch.
    - Model training.
    - Embed user-defined ops/kernels, such as [flash-attention v2](https://github.com/huggingface/candle/blob/89ba005962495f2bfbda286e185e9c3c7f5300a3/candle-flash-attn/src/lib.rs#L152).
- Backends.
    - Optimized CPU backend with optional MKL support for x86 and Accelerate for macs.
    - CUDA backend for efficiently running on GPUs, multiple GPU distribution via NCCL.
    - WASM support, run your models in a browser.
- Included models.
    - Language Models.
        - LLaMA v1, v2, and v3 with variants such as SOLAR-10.7B.
        - Falcon.
        - StarCoder, StarCoder2.
        - Phi 1, 1.5, 2, and 3.
        - Mamba, Minimal Mamba
        - Gemma v1 2b and 7b+, v2 2b and 9b.
        - Mistral 7b v0.1.
        - Mixtral 8x7b v0.1.
        - StableLM-3B-4E1T, StableLM-2-1.6B, Stable-Code-3B.
        - Replit-code-v1.5-3B.
        - Bert.
        - Yi-6B and Yi-34B.
        - Qwen1.5, Qwen1.5 MoE.
        - RWKV v5 and v6.
    - Quantized LLMs.
        - Llama 7b, 13b, 70b, as well as the chat and code variants.
        - Mistral 7b, and 7b instruct.
        - Mixtral 8x7b.
        - Zephyr 7b a and b (Mistral-7b based).
        - OpenChat 3.5 (Mistral-7b based).
    - Text to text.
        - T5 and its variants: FlanT5, UL2, MADLAD400 (translation), CoEdit (Grammar correction).
        - Marian MT (Machine Translation).
    - Text to image.
        - Stable Diffusion v1.5, v2.1, XL v1.0.
        - Wurstchen v2.
    - Image to text.
        - BLIP.
        - TrOCR.
    - Audio.
        - Whisper, multi-lingual speech-to-text.
        - EnCodec, audio compression model.
        - MetaVoice-1B, text-to-speech model.
        - Parler-TTS, text-to-speech model.
    - Computer Vision Models.
        - DINOv2, ConvMixer, EfficientNet, ResNet, ViT, VGG, RepVGG, ConvNeXT,
          ConvNeXTv2, MobileOne, EfficientVit (MSRA), MobileNetv4, Hiera, FastViT.
        - yolo-v3, yolo-v8.
        - Segment-Anything Model (SAM).
        - SegFormer.
- File formats: load models from safetensors, npz, ggml, or PyTorch files.
- Serverless (on CPU), small and fast deployments.
- Quantization support using the llama.cpp quantized types.

&lt;!--- ANCHOR_END: features ---&gt;

## How to use

&lt;!--- ANCHOR: cheatsheet ---&gt;
Cheatsheet:

|            | Using PyTorch                            | Using Candle                                                     |
|------------|------------------------------------------|------------------------------------------------------------------|
| Creation   | `torch.Tensor([[1, 2], [3, 4]])`         | `Tensor::new(&amp;[[1f32, 2.], [3., 4.]], &amp;Device::Cpu)?`           |
| Creation   | `torch.zeros((2, 2))`                    | `Tensor::zeros((2, 2), DType::F32, &amp;Device::Cpu)?`               |
| Indexing   | `tensor[:, :4]`                          | `tensor.i((.., ..4))?`                                           |
| Operations | `tensor.view((2, 2))`                    | `tensor.reshape((2, 2))?`                                        |
| Operations | `a.matmul(b)`                            | `a.matmul(&amp;b)?`                                                  |
| Arithmetic | `a + b`                                  | `&amp;a + &amp;b`                                                        |
| Device     | `tensor.to(device=&quot;cuda&quot;)`               | `tensor.to_device(&amp;Device::new_cuda(0)?)?`                            |
| Dtype      | `tensor.to(dtype=torch.float16)`         | `tensor.to_dtype(&amp;DType::F16)?`                                  |
| Saving     | `torch.save({&quot;A&quot;: A}, &quot;model.bin&quot;)`      | `candle::safetensors::save(&amp;HashMap::from([(&quot;A&quot;, A)]), &quot;model.safetensors&quot;)?` |
| Loading    | `weights = torch.load(&quot;model.bin&quot;)`      | `candle::safetensors::load(&quot;model.safetensors&quot;, &amp;device)`        |

&lt;!--- ANCHOR_END: cheatsheet ---&gt;


## Structure

- [candle-core](./candle-core): Core ops, devices, and `Tensor` struct definition
- [candle-nn](./candle-nn/): Tools to build real models
- [candle-examples](./candle-examples/): Examples of using the library in realistic settings
- [candle-kernels](./candle-kernels/): CUDA custom kernels
- [candle-datasets](./candle-datasets/): Datasets and data loaders.
- [candle-transformers](./candle-transformers): transformers-related utilities.
- [candle-flash-attn](./candle-flash-attn): Flash attention v2 layer.
- [candle-onnx](./candle-onnx/): ONNX model evaluation.

## FAQ

### Why should I use Candle?

Candle&#039;s core goal is to *make serverless inference possible*. Full machine learning frameworks like PyTorch
are very large, which makes creating instances on a cluster slow. Candle allows deployment of lightweight
binaries.

Secondly, Candle lets you *remove Python* from production workloads. Python overhead can seriously hurt performance,
and the [GIL](https://www.backblaze.com/blog/the-python-gil-past-present-and-future/) is a notorious source of headaches.

Finally, Rust is cool! A lot of the HF ecosystem already has Rust crates, like [safetensors](https://github.com/huggingface/safetensors) and [tokenizers](https://github.com/huggingface/tokenizers).


### Other ML frameworks

- [dfdx](https://github.com/coreylowman/dfdx) is a formidable crate, with shapes being included
  in types. This prevents a lot of headaches by getting the compiler to complain about shape mismatches right off the bat.
  However, we found that some features still require nightly, and writing code can be a bit daunting for non rust experts.

  We&#039;re leveraging and contributing to other core crates for the runtime so hopefully both crates can benefit from each
  other.

- [burn](https://github.com/burn-rs/burn) is a general crate that can leverage multiple backends so you can choose the best
  engine for your workload.

- [tch-rs](https://github.com/LaurentMazare/tch-rs.git) Bindings to the torch library in Rust. Extremely versatile, but they 
  bring in the entire torch library into the runtime. The main contributor of `tch-rs` is also involved in the development
  of `candle`.

### Common Errors

#### Missing symbols when compiling with the mkl feature.

If you get some missing symbols when compiling binaries/tests using the mkl
or accelerate features, e.g. for mkl you get:
```
  = note: /usr/bin/ld: (....o): in function `blas::sgemm&#039;:
          .../blas-0.22.0/src/lib.rs:1944: undefined reference to `sgemm_&#039; collect2: error: ld returned 1 exit status

  = note: some `extern` functions couldn&#039;t be found; some native libraries may need to be installed or have their path specified
  = note: use the `-l` flag to specify native libraries to link
  = note: use the `cargo:rustc-link-lib` directive to specify the native libraries to link with Cargo
```
or for accelerate:
```
Undefined symbols for architecture arm64:
            &quot;_dgemm_&quot;, referenced from:
                candle_core::accelerate::dgemm::h1b71a038552bcabe in libcandle_core...
            &quot;_sgemm_&quot;, referenced from:
                candle_core::accelerate::sgemm::h2cf21c592cba3c47 in libcandle_core...
          ld: symbol(s) not found for architecture arm64
```

This is likely due to a missing linker flag that was needed to enable the mkl library. You
can try adding the following for mkl at the top of your binary:
```rust
extern crate intel_mkl_src;
```
or for accelerate:
```rust
extern crate accelerate_src;
```

#### Cannot run the LLaMA examples: access to source requires login credentials

```
Error: request error: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/tokenizer.json: status code 401
```

This is likely because you&#039;re not permissioned for the LLaMA-v2 model. To fix
this, you have to register on the huggingface-hub, accept the [LLaMA-v2 model
conditions](https://huggingface.co/meta-llama/Llama-2-7b-hf), and set up your
authentication token. See issue
[#350](https://github.com/huggingface/candle/issues/350) for more details.

#### Missing cute/cutlass headers when compiling flash-attn

```
  In file included from kernels/flash_fwd_launch_template.h:11:0,
                   from kernels/flash_fwd_hdim224_fp16_sm80.cu:5:
  kernels/flash_fwd_kernel.h:8:10: fatal error: cute/algorithm/copy.hpp: No such file or directory
   #include

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[emilk/egui]]></title>
            <link>https://github.com/emilk/egui</link>
            <guid>https://github.com/emilk/egui</guid>
            <pubDate>Mon, 24 Mar 2025 00:05:52 GMT</pubDate>
            <description><![CDATA[egui: an easy-to-use immediate mode GUI in Rust that runs on both web and native]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/emilk/egui">emilk/egui</a></h1>
            <p>egui: an easy-to-use immediate mode GUI in Rust that runs on both web and native</p>
            <p>Language: Rust</p>
            <p>Stars: 24,126</p>
            <p>Forks: 1,694</p>
            <p>Stars today: 17 stars today</p>
            <h2>README</h2><pre># üñå egui: an easy-to-use GUI in pure Rust

[&lt;img alt=&quot;github&quot; src=&quot;https://img.shields.io/badge/github-emilk/egui-8da0cb?logo=github&quot; height=&quot;20&quot;&gt;](https://github.com/emilk/egui)
[![Latest version](https://img.shields.io/crates/v/egui.svg)](https://crates.io/crates/egui)
[![Documentation](https://docs.rs/egui/badge.svg)](https://docs.rs/egui)
[![unsafe forbidden](https://img.shields.io/badge/unsafe-forbidden-success.svg)](https://github.com/rust-secure-code/safety-dance/)
[![Build Status](https://github.com/emilk/egui/workflows/CI/badge.svg)](https://github.com/emilk/egui/actions?workflow=CI)
[![MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/emilk/egui/blob/master/LICENSE-MIT)
[![Apache](https://img.shields.io/badge/license-Apache-blue.svg)](https://github.com/emilk/egui/blob/master/LICENSE-APACHE)
[![Discord](https://img.shields.io/discord/900275882684477440?label=egui%20discord)](https://discord.gg/JFcEma9bJq)


&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://www.rerun.io/&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/78e79463-4357-461b-bbd1-31aa5ef5e1a2&quot; width=&quot;250&quot;&gt;&lt;/a&gt;

egui development is sponsored by [Rerun](https://www.rerun.io/), a startup building&lt;br&gt;
an SDK for visualizing streams of multimodal data.
&lt;/div&gt;

---

üëâ [Click to run the web demo](https://www.egui.rs/#demo) üëà

egui (pronounced &quot;e-gooey&quot;) is a simple, fast, and highly portable immediate mode GUI library for Rust. egui runs on the web, natively, and [in your favorite game engine](#integrations).

egui aims to be the easiest-to-use Rust GUI library, and the simplest way to make a web app in Rust.

egui can be used anywhere you can draw textured triangles, which means you can easily integrate it into your game engine of choice.

[`eframe`](https://github.com/emilk/egui/tree/master/crates/eframe) is the official egui framework, which supports writing apps for Web, Linux, Mac, Windows, and Android.


## Example

``` rust
ui.heading(&quot;My egui Application&quot;);
ui.horizontal(|ui| {
    ui.label(&quot;Your name: &quot;);
    ui.text_edit_singleline(&amp;mut name);
});
ui.add(egui::Slider::new(&amp;mut age, 0..=120).text(&quot;age&quot;));
if ui.button(&quot;Increment&quot;).clicked() {
    age += 1;
}
ui.label(format!(&quot;Hello &#039;{name}&#039;, age {age}&quot;));
ui.image(egui::include_image!(&quot;ferris.png&quot;));
```

&lt;img alt=&quot;Dark mode&quot; src=&quot;https://github.com/user-attachments/assets/3b446d29-99d8-4c82-86bb-4d8ef0516017&quot;&gt; &amp;nbsp; &amp;nbsp; &lt;img alt=&quot;Light mode&quot; src=&quot;https://github.com/user-attachments/assets/a5e7da93-89a8-4ba0-86b8-0fa2228a4f62&quot; height=&quot;278&quot;&gt;

## Sections:

* [Example](#example)
* [Quick start](#quick-start)
* [Demo](#demo)
* [Goals](#goals)
* [State / features](#state)
* [Dependencies](#dependencies)
* [Who is egui for?](#who-is-egui-for)
* [Integrations](#integrations)
* [Why immediate mode](#why-immediate-mode)
* [FAQ](#faq)
* [Other](#other)
* [Credits](#credits)

([egui ÁöÑ‰∏≠ÊñáÁøªËØëÊñáÊ°£ / chinese translation](https://github.com/Re-Ch-Love/egui-doc-cn/blob/main/README_zh-hans.md))


## Quick start

There are simple examples in [the `examples/` folder](https://github.com/emilk/egui/blob/master/examples/). If you want to write a web app, then go to &lt;https://github.com/emilk/eframe_template/&gt; and follow the instructions. The official docs are at &lt;https://docs.rs/egui&gt;. For inspiration and more examples, check out the [the egui web demo](https://www.egui.rs/#demo) and follow the links in it to its source code.

If you want to integrate egui into an existing engine, go to the [Integrations](#integrations) section.

If you have questions, use [GitHub Discussions](https://github.com/emilk/egui/discussions). There is also [an egui discord server](https://discord.gg/JFcEma9bJq). If you want to contribute to egui, please read the [Contributing Guidelines](https://github.com/emilk/egui/blob/master/CONTRIBUTING.md).

## Demo

[Click to run egui web demo](https://www.egui.rs/#demo) (works in any browser with Wasm and WebGL support). Uses [`eframe`](https://github.com/emilk/egui/tree/master/crates/eframe).

To test the demo app locally, run `cargo run --release -p egui_demo_app`.

The native backend is [`egui_glow`](https://github.com/emilk/egui/tree/master/crates/egui_glow) (using [`glow`](https://crates.io/crates/glow)) and should work out-of-the-box on Mac and Windows, but on Linux you need to first run:

`sudo apt-get install -y libclang-dev libgtk-3-dev libxcb-render0-dev libxcb-shape0-dev libxcb-xfixes0-dev libxkbcommon-dev libssl-dev`

On Fedora Rawhide you need to run:

`dnf install clang clang-devel clang-tools-extra libxkbcommon-devel pkg-config openssl-devel libxcb-devel gtk3-devel atk fontconfig-devel`

**NOTE**: This is just for the demo app - egui itself is completely platform agnostic!

## Goals

* The easiest to use GUI library
* Responsive: target 60 Hz in debug build
* Friendly: difficult to make mistakes, and shouldn&#039;t panic
* Portable: the same code works on the web and as a native app
* Easy to integrate into any environment
* A simple 2D graphics API for custom painting ([`epaint`](https://docs.rs/epaint)).
* Pure immediate mode: no callbacks
* Extensible: [easy to write your own widgets for egui](https://github.com/emilk/egui/blob/master/crates/egui_demo_lib/src/demo/toggle_switch.rs)
* Modular: You should be able to use small parts of egui and combine them in new ways
* Safe: there is no `unsafe` code in egui
* Minimal dependencies

egui is *not* a framework. egui is a library you call into, not an environment you program for.

**NOTE**: egui does not claim to have reached all these goals yet! egui is still work in progress.

### Non-goals

* Become the most powerful GUI library
* Native looking interface

## State

egui is in active development. It works well for what it does, but it lacks many features and the interfaces are still in flux. New releases will have breaking changes.

Still, egui can be used to create professional looking applications, like [the Rerun Viewer](https://app.rerun.io/).

### Features

* Widgets: label, text button, hyperlink, checkbox, radio button, slider, draggable value, text editing, color picker, spinner
* Images
* Layouts: horizontal, vertical, columns, automatic wrapping
* Text editing: multiline, copy/paste, undo, emoji supports
* Windows: move, resize, name, minimize and close. Automatically sized and positioned.
* Regions: resizing, vertical scrolling, collapsing headers (sections), panels
* Rendering: Anti-aliased rendering of lines, circles, text and convex polygons.
* Tooltips on hover
* Accessibility via [AccessKit](https://accesskit.dev/)
* Label text selection
* And more!

Check out the [3rd party egui crates wiki](https://github.com/emilk/egui/wiki/3rd-party-egui-crates) for even more
widgets and features, maintained by the community.

&lt;img src=&quot;https://github.com/user-attachments/assets/13e73b76-e456-42bd-8ec9-220802834268&quot; width=&quot;50%&quot;&gt;

Light Theme:

&lt;img src=&quot;https://github.com/user-attachments/assets/2e38972c-a444-4894-b32f-47a2719cf369&quot; width=&quot;50%&quot;&gt;

## Dependencies
`egui` has a minimal set of default dependencies:

* [`ab_glyph`](https://crates.io/crates/ab_glyph)
* [`ahash`](https://crates.io/crates/ahash)
* [`bitflags`](https://crates.io/crates/bitflags)
* [`nohash-hasher`](https://crates.io/crates/nohash-hasher)
* [`parking_lot`](https://crates.io/crates/parking_lot)

Heavier dependencies are kept out of `egui`, even as opt-in.
All code in `egui` is Wasm-friendly (even outside a browser).

To load images into `egui` you can use the official [`egui_extras`](https://github.com/emilk/egui/tree/master/crates/egui_extras) crate.

[`eframe`](https://github.com/emilk/egui/tree/master/crates/eframe) on the other hand has a lot of dependencies, including [`winit`](https://crates.io/crates/winit), [`image`](https://crates.io/crates/image), graphics crates, clipboard crates, etc,

## Who is egui for?

egui aims to be the best choice when you want a simple way to create a GUI, or you want to add a GUI to a game engine.

If you are not using Rust, egui is not for you. If you want a GUI that looks native, egui is not for you. If you want something that doesn&#039;t break when you upgrade it, egui isn&#039;t for you (yet).

But if you are writing something interactive in Rust that needs a simple GUI, egui may be for you.


## Integrations

egui is built to be easy to integrate into any existing game engine or platform you are working on.
egui itself doesn&#039;t know or care on what OS it is running or how to render things to the screen - that is the job of the egui integration.

An integration needs to do the following each frame:

* **Input**: Gather input (mouse, touches, keyboard, screen size, etc) and give it to egui
* Call into the application GUI code
* **Output**: Handle egui output (cursor changes, paste, texture allocations, ‚Ä¶)
* **Painting**: Render the triangle mesh egui produces (see [OpenGL example](https://github.com/emilk/egui/blob/master/crates/egui_glow/src/painter.rs))

### Official integrations

These are the official egui integrations:

* [`eframe`](https://github.com/emilk/egui/tree/master/crates/eframe) for compiling the same app to web/wasm and desktop/native. Uses `egui-winit` and `egui_glow` or `egui-wgpu`
* [`egui_glow`](https://github.com/emilk/egui/tree/master/crates/egui_glow) for rendering egui with [glow](https://github.com/grovesNL/glow) on native and web, and for making native apps
* [`egui-wgpu`](https://github.com/emilk/egui/tree/master/crates/egui-wgpu) for [wgpu](https://crates.io/crates/wgpu) (WebGPU API)
* [`egui-winit`](https://github.com/emilk/egui/tree/master/crates/egui-winit) for integrating with [winit](https://github.com/rust-windowing/winit)

### 3rd party integrations

Check the wiki to find [3rd party integrations](https://github.com/emilk/egui/wiki/3rd-party-integrations)
and [egui crates](https://github.com/emilk/egui/wiki/3rd-party-egui-crates).

### Writing your own egui integration
Missing an integration for the thing you&#039;re working on? Create one, it&#039;s easy!
See &lt;https://docs.rs/egui/latest/egui/#integrating-with-egui&gt;.


## Why immediate mode

`egui` is an [immediate mode GUI library](https://en.wikipedia.org/wiki/Immediate_mode_GUI), as opposed to a *retained mode* GUI library. The difference between retained mode and immediate mode is best illustrated with the example of a button: In a retained GUI you create a button, add it to some UI and install some on-click handler (callback). The button is retained in the UI, and to change the text on it you need to store some sort of reference to it. By contrast, in immediate mode you show the button and interact with it immediately, and you do so every frame (e.g. 60 times per second). This means there is no need for any on-click handler, nor to store any reference to it. In `egui` this looks like this: `if ui.button(&quot;Save file&quot;).clicked() { save(file); }`.

A more detailed description of immediate mode can be found [in the `egui` docs](https://docs.rs/egui/latest/egui/#understanding-immediate-mode).

There are advantages and disadvantages to both systems.

The short of it is this: immediate mode GUI libraries are easier to use, but less powerful.

### Advantages of immediate mode
#### Usability
The main advantage of immediate mode is that the application code becomes vastly simpler:

* You never need to have any on-click handlers and callbacks that disrupts your code flow.
* You don&#039;t have to worry about a lingering callback calling something that is gone.
* Your GUI code can easily live in a simple function (no need for an object just for the UI).
* You don&#039;t have to worry about app state and GUI state being out-of-sync (i.e. the GUI showing something outdated), because the GUI isn&#039;t storing any state - it is showing the latest state *immediately*.

In other words, a whole lot of code, complexity and bugs are gone, and you can focus your time on something more interesting than writing GUI code.

### Disadvantages of immediate mode

#### Layout
The main disadvantage of immediate mode is it makes layout more difficult. Say you want to show a small dialog window in the center of the screen. To position the window correctly the GUI library must first know the size of it. To know the size of the window the GUI library must first layout the contents of the window. In retained mode this is easy: the GUI library does the window layout, positions the window, then checks for interaction (&quot;was the OK button clicked?&quot;).

In immediate mode you run into a paradox: to know the size of the window, we must do the layout, but the layout code also checks for interaction (&quot;was the OK button clicked?&quot;) and so it needs to know the window position *before* showing the window contents. This means we must decide where to show the window *before* we know its size!

This is a fundamental shortcoming of immediate mode GUIs, and any attempt to resolve it comes with its own downsides.

One workaround is to store the size and use it the next frame. This produces a frame-delay for the correct layout, producing occasional flickering the first frame something shows up. `egui` does this for some things such as windows and grid layouts.

The &quot;first-frame jitter&quot; can be covered up with an extra _pass_, which egui supports via `Context::request_discard`.
The downside of this is the added CPU cost of a second pass, so egui only does this in very rare circumstances (the majority of frames are single-pass).

For &quot;atomic&quot; widgets (e.g. a button) `egui` knows the size before showing it, so centering buttons, labels etc is possible in `egui` without any special workarounds.

See [this issue](https://github.com/emilk/egui/issues/4378) for more.

#### CPU usage
Since an immediate mode GUI does a full layout each frame, the layout code needs to be quick. If you have a very complex GUI this can tax the CPU. In particular, having a very large UI in a scroll area (with very long scrollback) can be slow, as the content needs to be laid out each frame.

If you design the GUI with this in mind and refrain from huge scroll areas (or only lay out the part that is in view) then the performance hit is generally pretty small. For most cases you can expect `egui` to take up 1-2 ms per frame, but `egui` still has a lot of room for optimization (it&#039;s not something I&#039;ve focused on yet). `egui` only repaints when there is interaction (e.g. mouse movement) or an animation, so if your app is idle, no CPU is wasted.

If your GUI is highly interactive, then immediate mode may actually be more performant compared to retained mode. Go to any web page and resize the browser window, and you&#039;ll notice that the browser is very slow to do the layout and eats a lot of CPU doing it. Resize a window in `egui` by contrast, and you&#039;ll get smooth 60 FPS at no extra CPU cost.


#### IDs
There are some GUI state that you want the GUI library to retain, even in an immediate mode library such as `egui`. This includes position and sizes of windows and how far the user has scrolled in some UI. In these cases you need to provide `egui` with a seed of a unique identifier (unique within the parent UI). For instance: by default `egui` uses the window titles as unique IDs to store window positions. If you want two windows with the same name (or one window with a dynamic name) you must provide some other ID source to `egui` (some unique integer or string).

`egui` also needs to track which widget is being interacted with (e.g. which slider is being dragged). `egui` uses unique IDs for this as well, but in this case the IDs are automatically generated, so there is no need for the user to worry about it. In particular, having two buttons with the same name is no problem (this is in contrast with [`Dear ImGui`](https://github.com/ocornut/imgui)).

Overall, ID handling is a rare inconvenience, and not a big disadvantage.


## FAQ

Also see [GitHub Discussions](https://github.com/emilk/egui/discussions/categories/q-a).

### Can I use `egui` with non-latin characters?
Yes! But you need to install your own font (`.ttf` or `.otf`) using [`Context::set_fonts`](https://docs.rs/egui/latest/egui/struct.Context.html#method.set_fonts).

### Can I customize the look of egui?
Yes! You can customize the colors, spacing, fonts and sizes of everything using `Context::set_style`.

This is not yet as powerful as say CSS, [but this is going to improve](https://github.com/emilk/egui/issues/3284).

Here is an example (from https://github.com/a-liashenko/TinyPomodoro):

&lt;img src=&quot;https://github.com/user-attachments/assets/e6107237-2547-41d6-996b-9a20ae0345ab&quot; width=&quot;50%&quot;&gt;

### How do I use egui with `async`?
If you call `.await` in your GUI code, the UI will freeze, which is very bad UX. Instead, keep the GUI thread non-blocking and communicate with any concurrent tasks (`async` tasks or other threads) with something like:
* Channels (e.g. [`std::sync::mpsc::channel`](https://doc.rust-lang.org/std/sync/mpsc/fn.channel.html)). Make sure to use [`try_recv`](https://doc.rust-lang.org/std/sync/mpsc/struct.Receiver.html#method.try_recv) so you don&#039;t block the gui thread!
* `Arc&lt;Mutex&lt;Value&gt;&gt;` (background thread sets a value; GUI thread reads it)
* [`poll_promise::Promise`](https://docs.rs/poll-promise)
* [`eventuals::Eventual`](https://docs.rs/eventuals/latest/eventuals/struct.Eventual.html)
* [`tokio::sync::watch::channel`](https://docs.rs/tokio/latest/tokio/sync/watch/fn.channel.html)

### How do I create a file dialog?

The async version of [rfd](https://docs.rs/rfd/latest/rfd/) supports both native and Wasm. See example app here https://github.com/woelper/egui_pick_file which also has a demo available via [gitub pages](https://woelper.github.io/egui_pick_file/).

### What about accessibility, such as screen readers?
egui includes optional support for [AccessKit](https://accesskit.dev/), which currently implements the native accessibility APIs on Windows and macOS. This feature is enabled by default in eframe. For platforms that AccessKit doesn&#039;t yet support, including web, there is an experimental built-in screen reader; in [the web demo](https://www.egui.rs/#demo) you can enable it in the &quot;Backend&quot; tab.

The original discussion of accessibility in egui is at &lt;https://github.com/emilk/egui/issues/167&gt;. Now that AccessKit support is merged, providing a strong foundation for future accessibility work, please open new issues on specific accessibility problems.

### What is the difference between [egui](https://docs.rs/egui) and [eframe](https://github.com/emilk/egui/tree/master/crates/eframe)?

`egui` is a 2D user interface library for laying out and interacting with buttons, sliders, etc.
`egui` has no idea if it is running on the web or natively, and does not know how to collect input or show things on screen.
That is the job of *the integration* or *backend*.

It is common to use `egui` from a game engine (using e.g. [`bevy_egui`](https://docs.rs/bevy_egui)),
but you can also use `egui` stand-alone using `eframe`. `eframe` has integration for web and native, and handles input and rendering.
The _frame_ in `eframe` stands both for the frame in which your egui app resides and also for &quot;framework&quot; (`eframe` is a framework, `egui` is a library).

### How do I render 3D stuff in an egui area?
There are multiple ways to combine egui with 3D. The simplest way is to use a 3D library and have egui sit on top of the 3D view. See for instance [`bevy_egui`](https://github.com/mvlabat/bevy_egui) or [`three-d`](https://github.com/asny/three-d).

If you want to embed 3D into an egui view there are two options:

#### `Shape::Callback`
Example:
* &lt;https://github.com/emilk/egui/blob/master/examples/custom_3d_glow/src/main.rs&gt;

`Shape::Callback` will call your code when egui gets painted, to show anything using whatever the background rendering context is. When using [`eframe`](https://github.com/emilk/egui/tree/master/crates/eframe) this will be [`glow`](https://github.com/grovesNL/glow). Other integrations will give you other rendering contexts, if they support `Shape:

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[EricLBuehler/mistral.rs]]></title>
            <link>https://github.com/EricLBuehler/mistral.rs</link>
            <guid>https://github.com/EricLBuehler/mistral.rs</guid>
            <pubDate>Mon, 24 Mar 2025 00:05:51 GMT</pubDate>
            <description><![CDATA[Blazingly fast LLM inference.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/EricLBuehler/mistral.rs">EricLBuehler/mistral.rs</a></h1>
            <p>Blazingly fast LLM inference.</p>
            <p>Language: Rust</p>
            <p>Stars: 5,290</p>
            <p>Forks: 384</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre>&lt;a name=&quot;top&quot;&gt;&lt;/a&gt;
&lt;h1 align=&quot;center&quot;&gt;
  mistral.rs
&lt;/h1&gt;

&lt;h3 align=&quot;center&quot;&gt;
Blazingly fast LLM inference.
&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
| &lt;a href=&quot;https://ericlbuehler.github.io/mistral.rs/mistralrs/&quot;&gt;&lt;b&gt;Rust Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs-pyo3/API.md&quot;&gt;&lt;b&gt;Python Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://discord.gg/SZrecqK8qw&quot;&gt;&lt;b&gt;Discord&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://matrix.to/#/#mistral.rs:matrix.org&quot;&gt;&lt;b&gt;Matrix&lt;/b&gt;&lt;/a&gt; |
&lt;/p&gt;

Please submit requests for new models [here](https://github.com/EricLBuehler/mistral.rs/issues/156).

## Get started fast üöÄ

1) [Install](#installation-and-build)

2) [Get models](#getting-models)

3) Deploy with our easy to use APIs
    - [Python](examples/python)
    - [Rust](mistralrs/examples)
    - [OpenAI compatible HTTP server](docs/HTTP.md)

## Quick examples

*After following installation instructions*

- Check out UQFF for prequantized models of various methods!
    - Models can be found [here](https://huggingface.co/collections/EricB/uqff-670e4a49d56ecdd3f7f0fd4c).

- üíéüíéüíé Run the entire **Gemma 3** Model family (1b, 4b, 12b, 27b) with 128k context length and vision support: [documentation](docs/GEMMA3.md)

    ```
    ./mistralrs-server -i vision-plain -m google/gemma-3-4b-it -a gemma3
    ```

- Run the **Mistral 3** Model with 128k context length and strong vision support: [documentation](docs/MISTRAL3.md)

    ```
    ./mistralrs-server -i --isq q4k vision-plain -m mistralai/Mistral-Small-3.1-24B-Instruct-2503 -a mistral3
    ```

- üêãüêãüêã Run the **Deepseek R1/V3** model with automatic **tensor parallelism**: [documentation](docs/DEEPSEEKV3.md)

    ```
    ./mistralrs-server -i --isq Q4K plain -m deepseek-ai/DeepSeek-R1
    ```

- üêãüêãüêã Run the **Deepseek R1** [distillations](https://huggingface.co/collections/deepseek-ai/deepseek-r1-678e1e131c0169c0bc89728d) out of the box

    ```
    ./mistralrs-server -i --isq Q4K plain -m deepseek-ai/DeepSeek-R1-Distill-Llama-8B
    ./mistralrs-server -i --isq Q4K plain -m deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
    ./mistralrs-server -i --isq Q4K plain -m deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
    ```

- ü¶ôüì∑ Run the **Llama 3.2 Vision** Model: [documentation and guide here](docs/VLLAMA.md)

    &lt;img src=&quot;https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg&quot; alt=&quot;Mount Washington&quot; width = &quot;400&quot; height = &quot;267&quot;&gt;
    &lt;h6&gt;&lt;a href = &quot;https://www.nhmagazine.com/mount-washington/&quot;&gt;Credit&lt;/a&gt;&lt;/h6&gt;

    ```
    ./mistralrs-server -i vision-plain -m lamm-mit/Cephalo-Llama-3.2-11B-Vision-Instruct-128k -a vllama
    ```

- œÜ‚Å¥ üì∑ Run the **Phi 4 Multimodal** model: [documentation and guide here](docs/PHI4MM.md)

    ```
    ./mistralrs-server -i vision-plain -m microsoft/Phi-4-multimodal-instruct -a phi4mm
    ```

- œÜ‚Å¥ Run the new **Phi 4/Phi 4 Mini** models with 128K context window

    ```
    ./mistralrs-server -i plain -m microsoft/Phi-4-mini-instruct -a phi3
    ```

- üßÆ Enhance ISQ by collecting an imatrix from calibration data: [documentation](docs/IMATRIX.md)

    ```
    ./mistralrs-server -i --isq Q4K plain -m meta-llama/Llama-3.2-3B-Instruct --calibration-file calibration_data/calibration_datav3_small.txt
    ```

- üå≤üì∑ Run the **FLUX.1** diffusion model: [documentation and guide here](docs/FLUX.md)

    &lt;img src=&quot;https://github.com/user-attachments/assets/82bf5009-e3e9-402b-acf9-c48a52c7721b&quot; width = &quot;400&quot; height = &quot;267&quot;&gt;

    ```
    ./mistralrs-server --port 1234 diffusion-plain -m black-forest-labs/FLUX.1-schnell -a flux
    ```

- Other models: [see a support matrix](#support-matrix) and [how to run them](#run-with-the-cli)

Mistral.rs supports several model categories:
- Text to Text
- Text+Image to Text: Vision (see [the docs](docs/VISION_MODELS.md))
- Text to Image: Image Generation (see [the docs](docs/IMAGEGEN_MODELS.md))

## Description
**Easy**:
- Lightweight OpenAI API compatible HTTP server
- Python API
- Grammar support with JSON Schema, Regex, Lark, and Guidance via [LLGuidance library](https://github.com/microsoft/llguidance)
- [ISQ](docs/ISQ.md) (In situ quantization): run `.safetensors` models directly from ü§ó Hugging Face by quantizing in-place
    - Enhance performance with an [imatrix](docs/IMATRIX.md)!
- Automatic [device mapping](docs/DEVICE_MAPPING.md) to easily load and run models across multiple GPUs and CPU.
- Specify custom chat templates easily: [chat templates](docs/CHAT_TOK.md)

**Fast**:
- Apple silicon support: ARM NEON, Accelerate, Metal
- Accelerated CPU inference with MKL, AVX support
- CUDA support with FlashAttention and cuDNN.
- Automatic tensor-parallelism support with NCCL: [distributed documentation](docs/DISTRIBUTED.md)

**Quantization**:
- [Details](docs/QUANTS.md)
- GGML: 2-bit, 3-bit, 4-bit, 5-bit, 6-bit and 8-bit, with imatrix support
- GPTQ: 2-bit, 3-bit, 4-bit and 8-bit, with [Marlin](https://github.com/IST-DASLab/marlin) kernel support in 4-bit and 8-bit.
- HQQ: 4-bit and 8 bit, with ISQ support
- FP8
- BNB: bitsandbytes int8, fp4, nf4 support

**Powerful**:
- LoRA support with weight merging
- First X-LoRA inference platform with first class support
- [AnyMoE](docs/ANYMOE.md): Build a memory-efficient MoE model from anything, in seconds
- Various [sampling and penalty](docs/SAMPLING.mds) methods
- Native tool calling support for Llama, Mistral Small, Mistral Nemo, and Hermes models: [docs](docs/TOOL_CALLING.md)
- Prompt chunking: process large prompts in a more manageable way

**Advanced features**:
- [PagedAttention](docs/PAGED_ATTENTION.md) and continuous batching (CUDA and Metal support)
- [FlashAttention](docs/FLASH_ATTENTION.md) V2/V3
- Prefix caching
- [Topology](docs/TOPOLOGY.md): Configure ISQ and device mapping easily
- [UQFF](docs/UQFF.md): Quantized file format for easy mixing of quants, [collection here](https://huggingface.co/collections/EricB/uqff-670e4a49d56ecdd3f7f0fd4c).
- Speculative Decoding: Mix supported models as the draft model or the target model
- Dynamic LoRA adapter activation with adapter preloading: [examples and docs](docs/ADAPTER_MODELS.md#adapter-model-dynamic-adapter-activation)

**Documentation for mistral.rs can be found [here](docs/README.md).**

This is a demo of interactive mode with streaming running Phi 3 128k mini with quantization via ISQ to Q4K.

&lt;!-- Mistral GGUF demo, old API --&gt;
&lt;!-- https://github.com/EricLBuehler/mistral.rs/assets/65165915/3396abcd-8d44-4bf7-95e6-aa532db09415 --&gt;

https://github.com/EricLBuehler/mistral.rs/assets/65165915/09d9a30f-1e22-4b9a-9006-4ec6ebc6473c

## Architecture Support matrix

&gt; Note: See [supported models](#supported-models) for more information

|Model|Supports quantization|Supports adapters|Supports device mapping|Supported by AnyMoE|
|--|--|--|--|--|
|Mistral v0.1/v0.2/v0.3|‚úÖ|‚úÖ|‚úÖ|‚úÖ|
|Gemma|‚úÖ|‚úÖ|‚úÖ|‚úÖ|
|Llama 3.1/3.2|‚úÖ|‚úÖ|‚úÖ|‚úÖ|
|Mixtral|‚úÖ|‚úÖ|‚úÖ| |
|Phi 2|‚úÖ|‚úÖ|‚úÖ|‚úÖ|
|Phi 3|‚úÖ|‚úÖ|‚úÖ|‚úÖ|
|Phi 3.5 MoE|‚úÖ| |‚úÖ| |
|Qwen 2.5|‚úÖ| |‚úÖ|‚úÖ|
|Phi 3 Vision|‚úÖ| |‚úÖ|‚úÖ|
|Idefics 2|‚úÖ| |‚úÖ|‚úÖ|
|Gemma 2|‚úÖ|‚úÖ|‚úÖ|‚úÖ|
|Starcoder 2|‚úÖ|‚úÖ|‚úÖ|‚úÖ|
|LLaVa Next|‚úÖ| |‚úÖ|‚úÖ|
|LLaVa|‚úÖ| |‚úÖ|‚úÖ|
|Llama 3.2 Vision|‚úÖ| |‚úÖ| |
|Qwen2-VL|‚úÖ| |‚úÖ| |
|Idefics 3|‚úÖ| |‚úÖ|‚úÖ|
|DeepseekV2|‚úÖ| |‚úÖ| |
|DeepseekV3|‚úÖ| |‚úÖ| |
|MinCPM-O 2.6|‚úÖ| |‚úÖ| |
|Phi 4 Multimodal|‚úÖ| |‚úÖ| |
|Qwen2.5-VL|‚úÖ| |‚úÖ| |
|Gemma 3|‚úÖ| |‚úÖ|‚úÖ|
|Mistral 3|‚úÖ| |‚úÖ|‚úÖ|

## APIs and Integrations

### Rust Crate

Rust multithreaded/async API for easy integration into any application.

- [Docs](https://ericlbuehler.github.io/mistral.rs/mistralrs/)
- [Examples](mistralrs/examples/)
- To install: Add `mistralrs = { git = &quot;https://github.com/EricLBuehler/mistral.rs.git&quot; }`

### Python API

Python API for mistral.rs.

- [Installation including PyPI](mistralrs-pyo3/_README.md)
- [Docs](mistralrs-pyo3/API.md)
- [Examples](examples/python)
- [Cookbook](examples/python/cookbook.ipynb)


### HTTP Server

OpenAI API compatible API server

- [API Docs](docs/HTTP.md).
- [Running](README.md#run-with-the-cli)
- [Example](examples/server/chat.py)


### Llama Index integration (Python)

- Docs: https://docs.llamaindex.ai/en/stable/examples/llm/mistral_rs/

---

## Supported accelerators
- CUDA:
  - Compile with the `cuda` feature: `--features cuda`
  - FlashAttention support: compile with the `flash-attn` feature
  - cuDNN support: compile with the`cudnn` feature: `--features cudnn`
- Metal:
  - Compile with the `metal` feature: `--features metal`
- CPU:
  - Intel MKL: compile with the `mkl` feature: `--features mkl`
  - Apple Accelerate: compile with the `accelerate` feature: `--features accelerate`
  - ARM NEON and AVX are used automatically

Enabling features is done by passing `--features ...` to the build system. When using `cargo run` or `maturin develop`, pass the `--features` flag before the `--` separating build flags from runtime flags.

- To enable a single feature like `metal`: `cargo build --release --features metal`.
- To enable multiple features, specify them in quotes: `cargo build --release --features &quot;cuda flash-attn cudnn&quot;`.

## Installation and Build

&gt; Note: You can use our [Docker containers here](https://github.com/EricLBuehler/mistral.rs/pkgs/container/mistral.rs).
&gt; Learn more about running Docker containers: https://docs.docker.com/engine/reference/run/

- Install the [Python package here](mistralrs-pyo3/_README.md).
- The Python package has [wheels on PyPi](mistralrs-pyo3/_README.md#installation-from-pypi)!

1) Install required packages:
    - `OpenSSL` (*Example on Ubuntu:* `sudo apt install libssl-dev`)
    - &lt;b&gt;*Linux only:*&lt;/b&gt; `pkg-config` (*Example on Ubuntu:* `sudo apt install pkg-config`)

2) Install Rust: https://rustup.rs/

    *Example on Ubuntu:*
    ```bash
    curl --proto &#039;=https&#039; --tlsv1.2 -sSf https://sh.rustup.rs | sh
    source $HOME/.cargo/env
    ```

3) &lt;b&gt;*Optional:*&lt;/b&gt; Set HF token correctly (skip if already set or your model is not gated, or if you want to use the `token_source` parameters in Python or the command line.)
    - Note: you can install `huggingface-cli` as documented [here](https://huggingface.co/docs/huggingface_hub/en/installation). 
    ```bash
    huggingface-cli login
    ```

4) Download the code:
    ```bash
    git clone https://github.com/EricLBuehler/mistral.rs.git
    cd mistral.rs
    ```

5) Build or install:
    - Base build command
        ```bash
        cargo build --release
        ```
    - Build with CUDA support
        ```bash
        cargo build --release --features cuda
        ```
    - Build with CUDA and Flash Attention V2 support
        ```bash
        cargo build --release --features &quot;cuda flash-attn&quot;
        ```
    - Build with Metal support
        ```bash
        cargo build --release --features metal
        ```
    - Build with Accelerate support
        ```bash
        cargo build --release --features accelerate
        ```
    - Build with MKL support
        ```bash
        cargo build --release --features mkl
        ```
    - Install with `cargo install` for easy command line usage

        Pass the same values to `--features` as you would for `cargo build`
        ```bash
        cargo install --path mistralrs-server --features cuda
        ```
6) The build process will output a binary `mistralrs-server` at `./target/release/mistralrs-server`. We can switch to that directory so that the binary can be accessed as `./mistralrs-server` with the following command:

    *Example on Ubuntu:*
    ```
    cd target/release
    ```

7) Use our APIs and integrations: 
    
    [APIs and integrations list](#apis-and-integrations)

## Getting models

There are 2 ways to get models with mistral.rs:
- From Hugging Face Hub (easiest)
- From local files
    - Running a GGUF model
    - Specify local paths

### Getting models from Hugging Face Hub

Mistral.rs can automatically download models from HF Hub. To access gated models, you should provide a token source. They may be one of:
- `literal:&lt;value&gt;`: Load from a specified literal
- `env:&lt;value&gt;`: Load from a specified environment variable
- `path:&lt;value&gt;`: Load from a specified file
- `cache`: **default**: Load from the HF token at ~/.cache/huggingface/token or equivalent.
- `none`: Use no HF token

This is passed in the following ways:
- Command line:
```bash
./mistralrs-server --token-source none -i plain -m microsoft/Phi-3-mini-128k-instruct -a phi3
```
- Python:

[Here](examples/python/token_source.py) is an example of setting the token source.

If token cannot be loaded, no token will be used (i.e. effectively using `none`).

### Loading models from local files:

You can also instruct mistral.rs to load models fully locally by modifying the `*_model_id` arguments or options:
```bash
./mistralrs-server --port 1234 plain -m . -a mistral
```

Throughout mistral.rs, any model ID argument or option may be a local path and should contain the following files for each model ID option:
- `--model-id` (server) or `model_id` (python/rust) or `--tok-model-id` (server) or `tok_model_id` (python/rust): 
  - `config.json`
  - `tokenizer_config.json`
  - `tokenizer.json` (if not specified separately)
  - `.safetensors`/`.bin`/`.pth`/`.pt` files (defaults to `.safetensors`)
  - `preprocessor_config.json` (required for vision models).
  - `processor_config.json` (optional for vision models).
- `--quantized-model-id` (server) or `quantized_model_id` (python/rust):
  - Specified `.gguf` or `.ggml` file.
- `--x-lora-model-id` (server) or `xlora_model_id` (python/rust):
  - `xlora_classifier.safetensors`
  - `xlora_config.json`
  - Adapters `.safetensors` and `adapter_config.json` files in their respective directories
- `--adapters-model-id` (server) or `adapters_model_id` (python/rust):
  - Adapters `.safetensors` and `adapter_config.json` files in their respective directories

### Running GGUF models

To run GGUF models, the only mandatory arguments are the quantized model ID and the quantized filename. The quantized model ID can be a HF model ID.

You must also specify either `-i` for interactive mode or `--port` to launch a server, just like when [running a non-GGUF model with the CLI](#run-with-the-cli)

GGUF models contain a tokenizer. However, mistral.rs allows you to run the model with a tokenizer from a specified model, typically the official one. This means there are two options:
1) [With a specified tokenizer](#with-a-specified-tokenizer)
1) [With the builtin tokenizer](#with-the-builtin-tokenizer)

#### With a specified tokenizer

Running with a tokenizer model ID enables you to specify the model ID to source the tokenizer from:

```bash
./mistralrs-server gguf -m bartowski/Phi-3.5-mini-instruct-GGUF -f Phi-3.5-mini-instruct-Q4_K_M.gguf -t microsoft/Phi-3.5-mini-instruct
```

If the specified tokenizer model ID contains a `tokenizer.json`, then it will be used over the GGUF tokenizer.

#### With the builtin tokenizer

Using the builtin tokenizer:

```bash
./mistralrs-server gguf -m bartowski/Phi-3.5-mini-instruct-GGUF -f Phi-3.5-mini-instruct-Q4_K_M.gguf
```

(or using a local file):

```bash
./mistralrs-server gguf -m path/to/files -f Phi-3.5-mini-instruct-Q4_K_M.gguf
```

There are a few more ways to configure:

**Chat template:**

The chat template can be automatically detected and loaded from the GGUF file if no other chat template source is specified including the tokenizer model ID.

If that does not work, you can either [provide a tokenizer](#with-a-specified-tokenizer) (recommended), or specify a custom chat template.

```bash
./mistralrs-server --chat-template &lt;chat_template&gt; gguf -m . -f Phi-3.5-mini-instruct-Q4_K_M.gguf
```

**Tokenizer**

The following tokenizer model types are currently supported. If you would like one to be added, please raise an issue. Otherwise,
please consider using the method demonstrated in examples below, where the tokenizer is sourced from Hugging Face.

**Supported GGUF tokenizer types**
- `llama` (sentencepiece)
- `gpt2` (BPE)

## Run with the CLI

Mistral.rs uses subcommands to control the model type. Please run `./mistralrs-server --help` to see the subcommands which categorize the models by kind.

### Architecture for plain models

&gt; Note: for plain models, you can specify the data type to load and run in. This must be one of `f32`, `f16`, `bf16` or `auto` to choose based on the device. This is specified in the `--dype`/`-d` parameter after the model architecture (`plain`). For quantized models (gguf/ggml), you may specify data type of `f32` or `bf16` (`f16` is not recommended due to its lower precision in quantized inference).

If you do not specify the architecture, an attempt will be made to use the model&#039;s config. If this fails, please raise an issue.

- `mistral`
- `gemma`
- `mixtral`
- `llama`
- `phi2`
- `phi3`
- `phi3.5moe`
- `qwen2`
- `gemma2`
- `starcoder2`
- `deepseekv2`
- `deepseekv3`

### Architecture for vision models

&gt; Note: for vision models, you can specify the data type to load and run in. This must be one of `f32`, `f16`, `bf16` or `auto` to choose based on the device. This is specified in the `--dype`/`-d` parameter after the model architecture (`vision-plain`).

- `phi3v`
- `idefics2`
- `llava_next`
- `llava`
- `vllama`
- `qwen2vl`
- `idefics3`
- `minicpmo`
- `phi4mm`
- `qwen2_5vl`
- `gemma3`
- `mistral3`

### Supported GGUF architectures

**Plain:**

- `llama`
- `phi2`
- `phi3`
- `starcoder2`
- `qwen2`

**With adapters:**

- `llama`
- `phi3`

### Interactive mode

You can launch interactive mode, a simple chat application running in the terminal, by passing `-i`:

```bash
./mistralrs-server -i plain -m microsoft/Phi-3-mini-128k-instruct -a phi3
```

Vision models work too:

```bash
./mistralrs-server -i vision-plain -m lamm-mit/Cephalo-Llama-3.2-11B-Vision-Instruct-128k -a vllama
```

And even diffusion models:

```bash
./mistralrs-server -i diffusion-plain -m black-forest-labs/FLUX.1-schnell -a flux
```

On Apple Silicon (`Metal`), run with throughput log, settings of paged attention (maximum usage of 4GB for kv cache) and dtype (bf16 for kv cache and attention)

```bash
cargo build --release --features metal
./target/release/mistralrs-server -i --throughput --paged-attn --pa-gpu-mem 4096 gguf --dtype bf16 -m /Users/Downloads/ -f Phi-3.5-mini-instruct-Q4_K_M.gguf
```

### OpenAI HTTP server

You can an HTTP server

```bash
./mistralrs-server --port 1234 plain -m microsoft/Phi-3.5-MoE-instruct -a phi3.5moe
```

### Structured selection with a `.toml` file

We provide a method to select models with a `.toml` file. The keys are the same as the command line, with `no_kv_cache` and `tokenizer_json` being &quot;global&quot; keys.

Example:
```bash
./mistralrs-server --port 1234 toml -f toml-selectors/gguf.toml
```

---

## Benchmarks
|Device|Mistral.rs Completion T/s|Llama.cpp Completion T/s|Model|Quant|
|-|-|-|-|-|
|A10 GPU, CUDA|86|83|[mistral-7b](TheBloke/Mistral-7B-Instruct-v0.1-GGUF)|4_K_M|
|Intel Xeon 8358 CPU, AVX|11|23|[mistral-7b](TheBloke/Mistral-7B-Instruct-v0.1-GGUF)|4_K_M|
|Raspberry Pi 5 (8GB), Neon|2|3|[mistral-7b](TheBloke/Mistral-7B-Instruct-v0.1-GGUF)|2_K|
|A100 GPU, CUDA|131|134|[mistral-7b](TheBloke/Mistral-7B-Instruct-v0.1-GGUF)|4_K_M|
|RTX 6000 GPU, CUDA|103|96|[mistral-7b](TheBloke/Mistral-7B-Instruct-v0.1-GGUF)|4_K_M|

&gt; Note: All CUDA tests for mistral.rs conducted with PagedAttention enabled, block size = 32

Please submit more benchmarks via raising an issue!

## Supported models

**Quantization support**
|Model|GGUF|GGML|ISQ|
|--|--|--|--|
|Mistral|‚úÖ| |‚úÖ|
|Gemma| | |‚úÖ|
|Llama|‚úÖ|‚úÖ|‚úÖ|
|Mixtral|‚úÖ| |‚úÖ|
|Phi 2|‚úÖ| |‚úÖ|
|Phi 3|‚úÖ| |‚úÖ|
|Phi 3.5 MoE| | |‚úÖ|
|Qwen 2.5| | |‚úÖ|
|Phi 3 Vision| | |‚úÖ|
|Idefics 2| | |‚úÖ|
|Gemma 2| | |‚úÖ|
|Starcoder 2| |‚úÖ|‚úÖ|
|LLaVa Next| | |‚úÖ|
|LLaVa| | |‚úÖ|
|Llama 3.2 Vision| | |‚úÖ|
|Qwen2-VL| | |‚úÖ|
|Idefics 3| | |‚úÖ|
|Deepseek V2| | |‚úÖ|
|Deepseek V3| | |‚úÖ|
|MiniCPM-O 2.6| | |‚úÖ|
|Qwen2.5-VL| | |‚úÖ|
|Gemma 3| | |‚úÖ|
|Mistral 3| | |‚úÖ|

**Device mapping support**
|Model category|Supported|
|--|--|
|Plain|‚úÖ|
|GGUF|‚úÖ|
|GGML| |
|Vision Plain|‚úÖ|

**X-LoRA and LoRA support**
|Model|X-LoRA|X-LoRA+GGUF|X-LoRA+GGML|
|--|--|

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[tikv/tikv]]></title>
            <link>https://github.com/tikv/tikv</link>
            <guid>https://github.com/tikv/tikv</guid>
            <pubDate>Mon, 24 Mar 2025 00:05:50 GMT</pubDate>
            <description><![CDATA[Distributed transactional key-value database, originally created to complement TiDB]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/tikv/tikv">tikv/tikv</a></h1>
            <p>Distributed transactional key-value database, originally created to complement TiDB</p>
            <p>Language: Rust</p>
            <p>Stars: 15,644</p>
            <p>Forks: 2,169</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre>&lt;img src=&quot;images/tikv-logo.png&quot; alt=&quot;tikv_logo&quot; width=&quot;300&quot;/&gt;

## [Website](https://tikv.org) | [Documentation](https://tikv.org/docs/latest/concepts/overview/) | [Community Chat](https://slack.tidb.io/invite?team=tikv-wg&amp;channel=general)

[![Build Status](https://ci.pingcap.net/buildStatus/icon?job=tikv_ghpr_build_master)](https://ci.pingcap.net/blue/organizations/jenkins/tikv_ghpr_build_master/activity)
[![Coverage Status](https://codecov.io/gh/tikv/tikv/branch/master/graph/badge.svg)](https://codecov.io/gh/tikv/tikv)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/2574/badge)](https://bestpractices.coreinfrastructure.org/projects/2574)

TiKV is an open-source, distributed, and transactional key-value database. Unlike other traditional NoSQL systems, TiKV not only provides classical key-value APIs, but also transactional APIs with ACID compliance. Built in Rust and powered by Raft, TiKV was originally created by [PingCAP](https://en.pingcap.com) to complement [TiDB](https://github.com/pingcap/tidb), a distributed HTAP database compatible with the MySQL protocol.

The design of TiKV (&#039;Ti&#039; stands for titanium) is inspired by some great distributed systems from Google, such as BigTable, Spanner, and Percolator, and some of the latest achievements in academia in recent years, such as the Raft consensus algorithm.

If you&#039;re interested in contributing to TiKV, or want to build it from source, see [CONTRIBUTING.md](./CONTRIBUTING.md).

![cncf_logo](images/cncf.png#gh-light-mode-only)
![cncf_logo](images/cncf-white.png#gh-dark-mode-only)

TiKV is a graduated project of the [Cloud Native Computing Foundation](https://cncf.io/) (CNCF). If you are an organization that wants to help shape the evolution of technologies that are container-packaged, dynamically-scheduled and microservices-oriented, consider joining the CNCF. For details about who&#039;s involved and how TiKV plays a role, read the CNCF [announcement](https://www.cncf.io/announcements/2020/09/02/cloud-native-computing-foundation-announces-tikv-graduation/).

---

With the implementation of the Raft consensus algorithm in Rust and consensus state stored in RocksDB, TiKV guarantees data consistency. [Placement Driver (PD)](https://github.com/pingcap/pd/), which is introduced to implement auto-sharding, enables automatic data migration. The transaction model is similar to Google&#039;s Percolator with some performance improvements. TiKV also provides snapshot isolation (SI), snapshot isolation with lock (SQL: `SELECT ... FOR UPDATE`), and externally consistent reads and writes in distributed transactions.

TiKV has the following key features:

- **Geo-Replication**

    TiKV uses [Raft](http://raft.github.io/) and the Placement Driver to support Geo-Replication.

- **Horizontal scalability**

    With PD and carefully designed Raft groups, TiKV excels in horizontal scalability and can easily scale to 100+ TBs of data.

- **Consistent distributed transactions**

    Similar to Google&#039;s Spanner, TiKV supports externally-consistent distributed transactions.

- **Coprocessor support**

    Similar to HBase, TiKV implements a coprocessor framework to support distributed computing.

- **Cooperates with [TiDB](https://github.com/pingcap/tidb)**

    Thanks to the internal optimization, TiKV and TiDB can work together to be a compelling database solution with high horizontal scalability, externally-consistent transactions, support for RDBMS, and NoSQL design patterns.

## Governance

See [Governance](https://github.com/tikv/community/blob/master/GOVERNANCE.md).

## Documentation

For instructions on deployment, configuration, and maintenance of TiKV,see TiKV documentation on our [website](https://tikv.org/docs/4.0/tasks/introduction/). For more details on concepts and designs behind TiKV, see [Deep Dive TiKV](https://tikv.org/deep-dive/introduction/).

&gt; **Note:**
&gt;
&gt; We have migrated our documentation from the [TiKV&#039;s wiki page](https://github.com/tikv/tikv/wiki/) to the [official website](https://tikv.org/docs). The original Wiki page is discontinued. If you have any suggestions or issues regarding documentation, offer your feedback [here](https://github.com/tikv/website).

## TiKV adopters

You can view the list of [TiKV Adopters](https://tikv.org/adopters/).

## TiKV software stack

![The TiKV software stack](images/tikv_stack.png)

- **Placement Driver:** PD is the cluster manager of TiKV, which periodically checks replication constraints to balance load and data automatically.
- **Store:** There is a RocksDB within each Store and it stores data into the local disk.
- **Region:** Region is the basic unit of Key-Value data movement. Each Region is replicated to multiple Nodes. These multiple replicas form a Raft group.
- **Node:** A physical node in the cluster. Within each node, there are one or more Stores. Within each Store, there are many Regions.

When a node starts, the metadata of the Node, Store and Region are recorded into PD. The status of each Region and Store is reported to PD regularly.

## Quick start

### Deploy a playground with TiUP

The most quickest to try out TiKV with TiDB is using TiUP, a component manager for TiDB.

You can see [this page](https://docs.pingcap.com/tidb/stable/quick-start-with-tidb#deploy-a-local-test-environment-using-tiup-playground) for a step by step tutorial.

### Deploy a playground with binary

TiKV is able to run separately with PD, which is the minimal deployment required.

1. Download and extract binaries.

```bash
$ export TIKV_VERSION=v7.5.0
$ export GOOS=darwin  # only {darwin, linux} are supported
$ export GOARCH=amd64 # only {amd64, arm64} are supported
$ curl -O  https://tiup-mirrors.pingcap.com/tikv-$TIKV_VERSION-$GOOS-$GOARCH.tar.gz
$ curl -O  https://tiup-mirrors.pingcap.com/pd-$TIKV_VERSION-$GOOS-$GOARCH.tar.gz
$ tar -xzf tikv-$TIKV_VERSION-$GOOS-$GOARCH.tar.gz
$ tar -xzf pd-$TIKV_VERSION-$GOOS-$GOARCH.tar.gz
```

2. Start PD instance.

```bash
$ ./pd-server --name=pd --data-dir=/tmp/pd/data --client-urls=&quot;http://127.0.0.1:2379&quot; --peer-urls=&quot;http://127.0.0.1:2380&quot; --initial-cluster=&quot;pd=http://127.0.0.1:2380&quot; --log-file=/tmp/pd/log/pd.log
```

3. Start TiKV instance.

```bash
$ ./tikv-server --pd-endpoints=&quot;127.0.0.1:2379&quot; --addr=&quot;127.0.0.1:20160&quot; --data-dir=/tmp/tikv/data --log-file=/tmp/tikv/log/tikv.log
```

4. Install TiKV Client(Python) and verify the deployment, required Python 3.5+.

```bash
$ pip3 install -i https://test.pypi.org/simple/ tikv-client
```

```python
from tikv_client import RawClient

client = RawClient.connect(&quot;127.0.0.1:2379&quot;)

client.put(b&#039;foo&#039;, b&#039;bar&#039;)
print(client.get(b&#039;foo&#039;)) # b&#039;bar&#039;

client.put(b&#039;foo&#039;, b&#039;baz&#039;)
print(client.get(b&#039;foo&#039;)) # b&#039;baz&#039;
```

### Deploy a cluster with TiUP

You can see [this manual](./doc/deploy.md) of production-like cluster deployment presented by @c4pt0r.

### Build from source

See [CONTRIBUTING.md](./CONTRIBUTING.md).

## Client drivers

- [Go](https://github.com/tikv/client-go) (The most stable and widely used)
- [Java](https://github.com/tikv/client-java)
- [Rust](https://github.com/tikv/client-rust)
- [C](https://github.com/tikv/client-c)

If you want to try the Go client, see [Go Client](https://tikv.org/docs/4.0/reference/clients/go/).

## Security

### Security audit

A third-party security auditing was performed by Cure53. See the full report [here](./security/Security-Audit.pdf).

### Reporting Security Vulnerabilities

To report a security vulnerability, please send an email to [TiKV-security](mailto:tikv-security@lists.cncf.io) group.

See [Security](SECURITY.md) for the process and policy followed by the TiKV project.

## Communication

Communication within the TiKV community abides by [TiKV Code of Conduct](./CODE_OF_CONDUCT.md). Here is an excerpt:

&gt; In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to making participation in our project and
our community a harassment-free experience for everyone, regardless of age, body
size, disability, ethnicity, sex characteristics, gender identity and expression,
level of experience, education, socio-economic status, nationality, personal
appearance, race, religion, or sexual identity and orientation.

### Social Media

- [Twitter](https://twitter.com/tikvproject)
- [Blog](https://tikv.org/blog/)
- [Reddit](https://www.reddit.com/r/TiKV)
- Post questions or help answer them on [Stack Overflow](https://stackoverflow.com/questions/tagged/tikv)

### Slack

Join the TiKV community on [Slack](https://slack.tidb.io/invite?team=tikv-wg&amp;channel=general) - Sign up and join channels on TiKV topics that interest you.

## License

TiKV is under the Apache 2.0 license. See the [LICENSE](./LICENSE) file for details.

## Acknowledgments

- Thanks [etcd](https://github.com/coreos/etcd) for providing some great open source tools.
- Thanks [RocksDB](https://github.com/facebook/rocksdb) for their powerful storage engines.
- Thanks [rust-clippy](https://github.com/rust-lang/rust-clippy). We do love the great project.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[launchbadge/sqlx]]></title>
            <link>https://github.com/launchbadge/sqlx</link>
            <guid>https://github.com/launchbadge/sqlx</guid>
            <pubDate>Mon, 24 Mar 2025 00:05:49 GMT</pubDate>
            <description><![CDATA[üß∞ The Rust SQL Toolkit. An async, pure Rust SQL crate featuring compile-time checked queries without a DSL. Supports PostgreSQL, MySQL, and SQLite.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/launchbadge/sqlx">launchbadge/sqlx</a></h1>
            <p>üß∞ The Rust SQL Toolkit. An async, pure Rust SQL crate featuring compile-time checked queries without a DSL. Supports PostgreSQL, MySQL, and SQLite.</p>
            <p>Language: Rust</p>
            <p>Stars: 14,383</p>
            <p>Forks: 1,355</p>
            <p>Stars today: 22 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;SQLx&lt;/h1&gt;
&lt;div align=&quot;center&quot;&gt;
 &lt;strong&gt;
   üß∞ The Rust SQL Toolkit
 &lt;/strong&gt;
&lt;/div&gt;

&lt;br /&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;!-- Github Actions --&gt;
  &lt;a href=&quot;https://github.com/launchbadge/sqlx/actions/workflows/sqlx.yml?query=branch%3Amain&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/launchbadge/sqlx/sqlx.yml?branch=main&amp;style=flat-square&quot; alt=&quot;actions status&quot; /&gt;&lt;/a&gt;
  &lt;!-- Version --&gt;
  &lt;a href=&quot;https://crates.io/crates/sqlx&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/crates/v/sqlx.svg?style=flat-square&quot;
    alt=&quot;Crates.io version&quot; /&gt;&lt;/a&gt;
  &lt;!-- Discord --&gt;
  &lt;a href=&quot;https://discord.gg/uuruzJ7&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/discord/665528275556106240?style=flat-square&quot; alt=&quot;chat&quot; /&gt;&lt;/a&gt;
  &lt;!-- Docs --&gt;
  &lt;a href=&quot;https://docs.rs/sqlx&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/docs-latest-blue.svg?style=flat-square&quot; alt=&quot;docs.rs docs&quot; /&gt;&lt;/a&gt;
  &lt;!-- Downloads --&gt;
  &lt;a href=&quot;https://crates.io/crates/sqlx&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/crates/d/sqlx.svg?style=flat-square&quot; alt=&quot;Download&quot; /&gt;
  &lt;/a&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;h4&gt;
    &lt;a href=&quot;#install&quot;&gt;
      Install
    &lt;/a&gt;
    &lt;span&gt; | &lt;/span&gt;
    &lt;a href=&quot;#usage&quot;&gt;
      Usage
    &lt;/a&gt;
    &lt;span&gt; | &lt;/span&gt;
    &lt;a href=&quot;https://docs.rs/sqlx&quot;&gt;
      Docs
    &lt;/a&gt;
    &lt;span&gt; | &lt;/span&gt;
    &lt;a href=&quot;https://github.com/launchbadge/sqlx/wiki/Ecosystem&quot;&gt;
      Ecosystem
    &lt;/a&gt;    
    &lt;span&gt; | &lt;/span&gt;
    &lt;a href=&quot;https://discord.gg/uuruzJ7&quot;&gt;
      Discord
    &lt;/a&gt;
  &lt;/h4&gt;
&lt;/div&gt;

&lt;br /&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;small&gt;Built with ‚ù§Ô∏è by &lt;a href=&quot;https://launchbadge.com&quot;&gt;The LaunchBadge team&lt;/a&gt;&lt;/small&gt;
&lt;/div&gt;

&lt;br /&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;h5&gt;Have a question? Be sure to &lt;a href=&quot;FAQ.md&quot;&gt;check the FAQ first!&lt;/a&gt;&lt;/h5&gt;
&lt;/div&gt;

&lt;br /&gt;

SQLx is an async, pure Rust&lt;sub&gt;‚Ä†&lt;/sub&gt; SQL crate featuring compile-time checked queries without a DSL.

-   **Truly Asynchronous**. Built from the ground-up using async/await for maximum concurrency.

-   **Compile-time checked queries** (if you want). See [SQLx is not an ORM](#sqlx-is-not-an-orm).

-   **Database Agnostic**. Support for [PostgreSQL], [MySQL], [MariaDB], [SQLite].
    -   [MSSQL] was supported prior to version 0.7, but has been removed pending a full rewrite of the driver as part of our [SQLx Pro initiative].

-   **Pure Rust**. The Postgres and MySQL/MariaDB drivers are written in pure Rust using **zero** unsafe&lt;sub&gt;‚Ä†‚Ä†&lt;/sub&gt; code.

-   **Runtime Agnostic**. Works on different runtimes ([`async-std`] / [`tokio`] / [`actix`]) and TLS backends ([`native-tls`], [`rustls`]).

&lt;small&gt;&lt;small&gt;

‚Ä† The SQLite driver uses the libsqlite3 C library as SQLite is an embedded database (the only way
we could be pure Rust for SQLite is by porting _all_ of SQLite to Rust).

‚Ä†‚Ä† SQLx uses `#![forbid(unsafe_code)]` unless the `sqlite` feature is enabled.
The SQLite driver directly invokes the SQLite3 API via `libsqlite3-sys`, which requires `unsafe`.

&lt;/small&gt;&lt;/small&gt;

[postgresql]: http://postgresql.org/
[sqlite]: https://sqlite.org/
[mysql]: https://www.mysql.com/
[mariadb]: https://www.mariadb.org/
[mssql]: https://www.microsoft.com/en-us/sql-server
[SQLx Pro initiative]: https://github.com/launchbadge/sqlx/discussions/1616

---

-   Cross-platform. Being native Rust, SQLx will compile anywhere Rust is supported.

-   Built-in connection pooling with `sqlx::Pool`.

-   Row streaming. Data is read asynchronously from the database and decoded on demand.

-   Automatic statement preparation and caching. When using the high-level query API (`sqlx::query`), statements are
    prepared and cached per connection.

-   Simple (unprepared) query execution including fetching results into the same `Row` types used by
    the high-level API. Supports batch execution and returns results from all statements.

-   Transport Layer Security (TLS) where supported ([MySQL], [MariaDB] and [PostgreSQL]).

-   Asynchronous notifications using `LISTEN` and `NOTIFY` for [PostgreSQL].

-   Nested transactions with support for save points.

-   `Any` database driver for changing the database driver at runtime. An `AnyPool` connects to the driver indicated by the URL scheme.

## Install

SQLx is compatible with the [`async-std`], [`tokio`], and [`actix`] runtimes; and, the [`native-tls`] and [`rustls`] TLS backends. When adding the dependency, you must choose a runtime feature that is `runtime` + `tls`.

[`async-std`]: https://github.com/async-rs/async-std
[`tokio`]: https://github.com/tokio-rs/tokio
[`actix`]: https://github.com/actix/actix-net
[`native-tls`]: https://crates.io/crates/native-tls
[`rustls`]: https://crates.io/crates/rustls

```toml
# Cargo.toml
[dependencies]
# PICK ONE OF THE FOLLOWING:

# tokio (no TLS)
sqlx = { version = &quot;0.8&quot;, features = [ &quot;runtime-tokio&quot; ] }
# tokio + native-tls
sqlx = { version = &quot;0.8&quot;, features = [ &quot;runtime-tokio&quot;, &quot;tls-native-tls&quot; ] }
# tokio + rustls with ring and WebPKI CA certificates
sqlx = { version = &quot;0.8&quot;, features = [ &quot;runtime-tokio&quot;, &quot;tls-rustls-ring-webpki&quot; ] }
# tokio + rustls with ring and platform&#039;s native CA certificates
sqlx = { version = &quot;0.8&quot;, features = [ &quot;runtime-tokio&quot;, &quot;tls-rustls-ring-native-roots&quot; ] }
# tokio + rustls with aws-lc-rs
sqlx = { version = &quot;0.8&quot;, features = [ &quot;runtime-tokio&quot;, &quot;tls-rustls-aws-lc-rs&quot; ] }

# async-std (no TLS)
sqlx = { version = &quot;0.8&quot;, features = [ &quot;runtime-async-std&quot; ] }
# async-std + native-tls
sqlx = { version = &quot;0.8&quot;, features = [ &quot;runtime-async-std&quot;, &quot;tls-native-tls&quot; ] }
# async-std + rustls with ring and WebPKI CA certificates
sqlx = { version = &quot;0.8&quot;, features = [ &quot;runtime-async-std&quot;, &quot;tls-rustls-ring-webpki&quot; ] }
# async-std + rustls with ring and platform&#039;s native CA certificates
sqlx = { version = &quot;0.8&quot;, features = [ &quot;runtime-async-std&quot;, &quot;tls-rustls-ring-native-roots&quot; ] }
# async-std + rustls with aws-lc-rs
sqlx = { version = &quot;0.8&quot;, features = [ &quot;runtime-async-std&quot;, &quot;tls-rustls-aws-lc-rs&quot; ] }
```

#### Cargo Feature Flags

For backward-compatibility reasons, the runtime and TLS features can either be chosen together as a single feature,
or separately.

For forward compatibility, you should use the separate runtime and TLS features as the combination features may
be removed in the future.

-   `runtime-async-std`: Use the `async-std` runtime without enabling a TLS backend.

-   `runtime-async-std-native-tls`: Use the `async-std` runtime and `native-tls` TLS backend (SOFT-DEPRECATED).

-   `runtime-async-std-rustls`: Use the `async-std` runtime and `rustls` TLS backend (SOFT-DEPRECATED).

-   `runtime-tokio`: Use the `tokio` runtime without enabling a TLS backend.

-   `runtime-tokio-native-tls`: Use the `tokio` runtime and `native-tls` TLS backend (SOFT-DEPRECATED).

-   `runtime-tokio-rustls`: Use the `tokio` runtime and `rustls` TLS backend (SOFT-DEPRECATED).

    - Actix-web is fully compatible with Tokio and so a separate runtime feature is no longer needed.

-   `tls-native-tls`: Use the `native-tls` TLS backend (OpenSSL on *nix, SChannel on Windows, Secure Transport on macOS).

-   `tls-rustls`: Use the `rustls` TLS backend (cross-platform backend, only supports TLS 1.2 and 1.3).

-   `postgres`: Add support for the Postgres database server.

-   `mysql`: Add support for the MySQL/MariaDB database server.

-   `mssql`: Add support for the MSSQL database server.

-   `sqlite`: Add support for the self-contained [SQLite](https://sqlite.org/) database engine with SQLite bundled and statically-linked.

-   `sqlite-unbundled`: The same as above (`sqlite`), but link SQLite from the system instead of the bundled version.
    * Allows updating SQLite independently of SQLx or using forked versions.
    * You must have SQLite installed on the system or provide a path to the library at build time.
       See [the `rusqlite` README](https://github.com/rusqlite/rusqlite?tab=readme-ov-file#notes-on-building-rusqlite-and-libsqlite3-sys) for details.
    * May result in link errors if the SQLite version is too old. Version `3.20.0` or newer is recommended.
    * Can increase build time due to the use of bindgen.

-   `sqlite-preupdate-hook`: enables SQLite&#039;s [preupdate hook](https://sqlite.org/c3ref/preupdate_count.html) API.
    * Exposed as a separate feature because it&#039;s generally not enabled by default.
    * Using this feature with `sqlite-unbundled` may cause linker failures if the system SQLite version does not support it.

-   `any`: Add support for the `Any` database driver, which can proxy to a database driver at runtime.

-   `derive`: Add support for the derive family macros, those are `FromRow`, `Type`, `Encode`, `Decode`.

-   `macros`: Add support for the `query*!` macros, which allows compile-time checked queries.

-   `migrate`: Add support for the migration management and `migrate!` macro, which allow compile-time embedded migrations.

-   `uuid`: Add support for UUID.

-   `chrono`: Add support for date and time types from `chrono`.

-   `time`: Add support for date and time types from `time` crate (alternative to `chrono`, which is preferred by `query!` macro, if both enabled)

-   `bstr`: Add support for `bstr::BString`.

-   `bigdecimal`: Add support for `NUMERIC` using the `bigdecimal` crate.

-   `rust_decimal`: Add support for `NUMERIC` using the `rust_decimal` crate.

-   `ipnetwork`: Add support for `INET` and `CIDR` (in postgres) using the `ipnetwork` crate.

-   `json`: Add support for `JSON` and `JSONB` (in postgres) using the `serde_json` crate.

-   Offline mode is now always enabled. See [sqlx-cli/README.md][readme-offline].

[readme-offline]: sqlx-cli/README.md#enable-building-in-offline-mode-with-query

## SQLx is not an ORM!

SQLx supports **compile-time checked queries**. It does not, however, do this by providing a Rust
API or DSL (domain-specific language) for building queries. Instead, it provides macros that take
regular SQL as input and ensure that it is valid for your database. The way this works is that
SQLx connects to your development DB at compile time to have the database itself verify (and return
some info on) your SQL queries. This has some potentially surprising implications:

- Since SQLx never has to parse the SQL string itself, any syntax that the development DB accepts
  can be used (including things added by database extensions)
- Due to the different amount of information databases let you retrieve about queries, the extent of
  SQL verification you get from the query macros depends on the database

**If you are looking for an (asynchronous) ORM,** you can check out our new [Ecosystem wiki page](https://github.com/launchbadge/sqlx/wiki/Ecosystem#orms)!

[`ormx`]: https://crates.io/crates/ormx
[`SeaORM`]: https://github.com/SeaQL/sea-orm
## Usage

See the `examples/` folder for more in-depth usage.

### Quickstart

```rust
use sqlx::postgres::PgPoolOptions;
// use sqlx::mysql::MySqlPoolOptions;
// etc.

#[async_std::main] // Requires the `attributes` feature of `async-std`
// or #[tokio::main]
// or #[actix_web::main]
async fn main() -&gt; Result&lt;(), sqlx::Error&gt; {
    // Create a connection pool
    //  for MySQL/MariaDB, use MySqlPoolOptions::new()
    //  for SQLite, use SqlitePoolOptions::new()
    //  etc.
    let pool = PgPoolOptions::new()
        .max_connections(5)
        .connect(&quot;postgres://postgres:password@localhost/test&quot;).await?;

    // Make a simple query to return the given parameter (use a question mark `?` instead of `$1` for MySQL/MariaDB)
    let row: (i64,) = sqlx::query_as(&quot;SELECT $1&quot;)
        .bind(150_i64)
        .fetch_one(&amp;pool).await?;

    assert_eq!(row.0, 150);

    Ok(())
}
```


### Connecting

A single connection can be established using any of the database connection types and calling `connect()`.

```rust
use sqlx::Connection;

let conn = SqliteConnection::connect(&quot;sqlite::memory:&quot;).await?;
```

Generally, you will want to instead create a connection pool (`sqlx::Pool`) for the application to
regulate how many server-side connections it&#039;s using.

```rust
let pool = MySqlPool::connect(&quot;mysql://user:pass@host/database&quot;).await?;
```

### Querying

In SQL, queries can be separated into prepared (parameterized) or unprepared (simple). Prepared queries have their
query plan _cached_, use a binary mode of communication (lower bandwidth and faster decoding), and utilize parameters
to avoid SQL injection. Unprepared queries are simple and intended only for use where a prepared statement
will not work, such as various database commands (e.g., `PRAGMA` or `SET` or `BEGIN`).

SQLx supports all operations with both types of queries. In SQLx, a `&amp;str` is treated as an unprepared query,
and a `Query` or `QueryAs` struct is treated as a prepared query.

```rust
// low-level, Executor trait
conn.execute(&quot;BEGIN&quot;).await?; // unprepared, simple query
conn.execute(sqlx::query(&quot;DELETE FROM table&quot;)).await?; // prepared, cached query
```

We should prefer to use the high-level `query` interface whenever possible. To make this easier, there are finalizers
on the type to avoid the need to wrap with an executor.

```rust
sqlx::query(&quot;DELETE FROM table&quot;).execute(&amp;mut conn).await?;
sqlx::query(&quot;DELETE FROM table&quot;).execute(&amp;pool).await?;
```

The `execute` query finalizer returns the number of affected rows, if any, and drops all received results.
In addition, there are `fetch`, `fetch_one`, `fetch_optional`, and `fetch_all` to receive results.

The `Query` type returned from `sqlx::query` will return `Row&lt;&#039;conn&gt;` from the database. Column values can be accessed
by ordinal or by name with `row.get()`. As the `Row` retains an immutable borrow on the connection, only one
`Row` may exist at a time.

The `fetch` query finalizer returns a stream-like type that iterates through the rows in the result sets.

```rust
// provides `try_next`
use futures::TryStreamExt;
// provides `try_get`
use sqlx::Row;

let mut rows = sqlx::query(&quot;SELECT * FROM users WHERE email = ?&quot;)
    .bind(email)
    .fetch(&amp;mut conn);

while let Some(row) = rows.try_next().await? {
    // map the row into a user-defined domain type
    let email: &amp;str = row.try_get(&quot;email&quot;)?;
}
```

To assist with mapping the row into a domain type, one of two idioms may be used:

```rust
let mut stream = sqlx::query(&quot;SELECT * FROM users&quot;)
    .map(|row: PgRow| {
        // map the row into a user-defined domain type
    })
    .fetch(&amp;mut conn);
```

```rust
#[derive(sqlx::FromRow)]
struct User { name: String, id: i64 }

let mut stream = sqlx::query_as::&lt;_, User&gt;(&quot;SELECT * FROM users WHERE email = ? OR name = ?&quot;)
    .bind(user_email)
    .bind(user_name)
    .fetch(&amp;mut conn);
```

Instead of a stream of results, we can use `fetch_one` or `fetch_optional` to request one required or optional result
from the database.

### Compile-time verification

We can use the macro, `sqlx::query!` to achieve compile-time syntactic and semantic verification of the SQL, with
an output to an anonymous record type where each SQL column is a Rust field (using raw identifiers where needed).

```rust
let countries = sqlx::query!(
        &quot;
SELECT country, COUNT(*) as count
FROM users
GROUP BY country
WHERE organization = ?
        &quot;,
        organization
    )
    .fetch_all(&amp;pool) // -&gt; Vec&lt;{ country: String, count: i64 }&gt;
    .await?;

// countries[0].country
// countries[0].count
```

Differences from `query()`:

-   The input (or bind) parameters must be given all at once (and they are compile-time validated to be
    the right number and the right type).

-   The output type is an anonymous record. In the above example the type would be similar to:

    ```rust
    { country: String, count: i64 }
    ```

-   The `DATABASE_URL` environment variable must be set at build time to a database which it can prepare
    queries against; the database does not have to contain any data but must be the same
    kind (MySQL, Postgres, etc.) and have the same schema as the database you will be connecting to at runtime.

    For convenience, you can use [a `.env` file][dotenv]&lt;sup&gt;1&lt;/sup&gt; to set DATABASE_URL so that you don&#039;t have to pass it every time:

    ```
    DATABASE_URL=mysql://localhost/my_database
    ```

[dotenv]: https://github.com/dotenv-rs/dotenv#examples

The biggest downside to `query!()` is that the output type cannot be named (due to Rust not
officially supporting anonymous records). To address that, there is a `query_as!()` macro that is
mostly identical except that you can name the output type.

```rust
// no traits are needed
struct Country { country: String, count: i64 }

let countries = sqlx::query_as!(Country,
        &quot;
SELECT country, COUNT(*) as count
FROM users
GROUP BY country
WHERE organization = ?
        &quot;,
        organization
    )
    .fetch_all(&amp;pool) // -&gt; Vec&lt;Country&gt;
    .await?;

// countries[0].country
// countries[0].count
```

To avoid the need of having a development database around to compile the project even when no
modifications (to the database-accessing parts of the code) are done, you can enable &quot;offline mode&quot;
to cache the results of the SQL query analysis using the `sqlx` command-line tool. See
[sqlx-cli/README.md](./sqlx-cli/README.md#enable-building-in-offline-mode-with-query).

Compile-time verified queries do quite a bit of work at compile time. Incremental actions like
`cargo check` and `cargo build` can be significantly faster when using an optimized build by
putting the following in your `Cargo.toml` (More information in the
[Profiles section](https://doc.rust-lang.org/cargo/reference/profiles.html) of The Cargo Book)

```toml
[profile.dev.package.sqlx-macros]
opt-level = 3
```

&lt;sup&gt;1&lt;/sup&gt; The `dotenv` crate itself appears abandoned as of [December 2021](https://github.com/dotenv-rs/dotenv/issues/74)
so we now use the `dotenvy` crate instead. The file format is the same.

## Safety

This crate uses `#![forbid(unsafe_code)]` to ensure everything is implemented in 100% Safe Rust.

If the `sqlite` feature is enabled, this is downgraded to `#![deny(unsafe_code)]` with `#![allow(unsafe_code)]` on the
`sqlx::sqlite` module. There are several places where we interact with the C SQLite API. We try to document each call for the invariants we&#039;re assuming. We absolutely welcome auditing of, and feedback on, our unsafe code usage.

## License

Licensed under either of

-   Apache License, Version 2.0
    ([LICENSE-APACHE](LICENSE-APACHE) or http://www.apache.org/licenses/LICENSE-2.0)
-   MIT license
    ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)

at your option.

## Contribution

Unless you explicitly state otherwise, any Contribution intentionally submitted
for inclusion in the work by you, as defined in the Apache-2.0 license, shall be
dual licensed as above, without any additional terms or conditions.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[H-M-H/Weylus]]></title>
            <link>https://github.com/H-M-H/Weylus</link>
            <guid>https://github.com/H-M-H/Weylus</guid>
            <pubDate>Mon, 24 Mar 2025 00:05:48 GMT</pubDate>
            <description><![CDATA[Use your tablet as graphic tablet/touch screen on your computer.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/H-M-H/Weylus">H-M-H/Weylus</a></h1>
            <p>Use your tablet as graphic tablet/touch screen on your computer.</p>
            <p>Language: Rust</p>
            <p>Stars: 7,638</p>
            <p>Forks: 300</p>
            <p>Stars today: 17 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[sxyazi/yazi]]></title>
            <link>https://github.com/sxyazi/yazi</link>
            <guid>https://github.com/sxyazi/yazi</guid>
            <pubDate>Mon, 24 Mar 2025 00:05:47 GMT</pubDate>
            <description><![CDATA[üí• Blazing fast terminal file manager written in Rust, based on async I/O.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sxyazi/yazi">sxyazi/yazi</a></h1>
            <p>üí• Blazing fast terminal file manager written in Rust, based on async I/O.</p>
            <p>Language: Rust</p>
            <p>Stars: 23,337</p>
            <p>Forks: 511</p>
            <p>Stars today: 38 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;sup&gt;Special thanks to:&lt;/sup&gt;
  &lt;br&gt;&lt;br&gt;
  &lt;a href=&quot;https://www.warp.dev/?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=yazi&quot; target=&quot;_blank&quot;&gt;
    &lt;img alt=&quot;Warp sponsorship&quot; width=&quot;300&quot; src=&quot;https://github.com/user-attachments/assets/c7f141e7-9751-407d-bb0e-d6f2c487b34f&quot;&gt;
    &lt;br&gt;
    &lt;b&gt;Warp, the intelligent terminal&lt;/b&gt;
    &lt;br&gt;
    &lt;sup&gt;Yazi&#039;s AI-powered terminal of choice!&lt;br&gt;Available for macOS, Linux and Windows&lt;/sup&gt;
  &lt;/a&gt;
&lt;/div&gt;

&lt;br&gt;

## Yazi - ‚ö°Ô∏è Blazing Fast Terminal File Manager

Yazi (means &quot;duck&quot;) is a terminal file manager written in Rust, based on non-blocking async I/O. It aims to provide an efficient, user-friendly, and customizable file management experience.

üí° A new article explaining its internal workings: [Why is Yazi Fast?](https://yazi-rs.github.io/blog/why-is-yazi-fast)

- üöÄ **Full Asynchronous Support**: All I/O operations are asynchronous, CPU tasks are spread across multiple threads, making the most of available resources.
- üí™ **Powerful Async Task Scheduling and Management**: Provides real-time progress updates, task cancellation, and internal task priority assignment.
- üñºÔ∏è **Built-in Support for Multiple Image Protocols**: Also integrated with √úberzug++ and Chafa, covering almost all terminals.
- üåü **Built-in Code Highlighting and Image Decoding**: Combined with the pre-loading mechanism, greatly accelerates image and normal file loading.
- üîå **Concurrent Plugin System**: UI plugins (rewriting most of the UI), functional plugins, custom previewer/preloader/spotter/fetcher; Just some pieces of Lua.
- üì° **Data Distribution Service**: Built on a client-server architecture (no additional server process required), integrated with a Lua-based publish-subscribe model, achieving cross-instance communication and state persistence.
- üì¶ **Package Manager**: Install plugins and themes with one command, keeping them up-to-date, or pin them to a specific version.
- üß∞ Integration with ripgrep, fd, fzf, zoxide
- üí´ Vim-like input/pick/confirm/which/notify component, auto-completion for cd paths
- üè∑Ô∏è Multi-Tab Support, Cross-directory selection, Scrollable Preview (for videos, PDFs, archives, code, directories, etc.)
- üîÑ Bulk Renaming, Archive Extraction, Visual Mode, File Chooser, [Git Integration](https://github.com/yazi-rs/plugins/tree/main/git.yazi), [Mount Manager](https://github.com/yazi-rs/plugins/tree/main/mount.yazi)
- üé® Theme System, Mouse Support, Trash Bin, Custom Layouts, CSI u, OSC 52
- ... and more!

https://github.com/sxyazi/yazi/assets/17523360/92ff23fa-0cd5-4f04-b387-894c12265cc7

## Project status

Public beta, can be used as a daily driver.

Yazi is currently in heavy development, expect breaking changes.

## Documentation

- Usage: https://yazi-rs.github.io/docs/installation
- Features: https://yazi-rs.github.io/features

## Discussion

- Discord Server (English mainly): https://discord.gg/qfADduSdJu
- Telegram Group (Chinese mainly): https://t.me/yazi_rs

## Image Preview

| Platform                                                                     | Protocol                               | Support                                               |
| ---------------------------------------------------------------------------- | -------------------------------------- | ----------------------------------------------------- |
| [kitty](https://github.com/kovidgoyal/kitty) (&gt;= 0.28.0)                     | [Kitty unicode placeholders][kgp]      | ‚úÖ Built-in                                           |
| [iTerm2](https://iterm2.com)                                                 | [Inline images protocol][iip]          | ‚úÖ Built-in                                           |
| [WezTerm](https://github.com/wez/wezterm)                                    | [Inline images protocol][iip]          | ‚úÖ Built-in                                           |
| [Konsole](https://invent.kde.org/utilities/konsole)                          | [Kitty old protocol][kgp-old]          | ‚úÖ Built-in                                           |
| [foot](https://codeberg.org/dnkl/foot)                                       | [Sixel graphics format][sixel]         | ‚úÖ Built-in                                           |
| [Ghostty](https://github.com/ghostty-org/ghostty)                            | [Kitty unicode placeholders][kgp]      | ‚úÖ Built-in                                           |
| [Windows Terminal](https://github.com/microsoft/terminal) (&gt;= v1.22.10352.0) | [Sixel graphics format][sixel]         | ‚úÖ Built-in                                           |
| [st with Sixel patch](https://github.com/bakkeby/st-flexipatch)              | [Sixel graphics format][sixel]         | ‚úÖ Built-in                                           |
| [Tabby](https://github.com/Eugeny/tabby)                                     | [Inline images protocol][iip]          | ‚úÖ Built-in                                           |
| [VSCode](https://github.com/microsoft/vscode)                                | [Inline images protocol][iip]          | ‚úÖ Built-in                                           |
| [Rio](https://github.com/raphamorim/rio)                                     | [Inline images protocol][iip]          | ‚ùå Rio doesn&#039;t correctly clear images [#709][rio-bug] |
| [Black Box](https://gitlab.gnome.org/raggesilver/blackbox)                   | [Sixel graphics format][sixel]         | ‚úÖ Built-in                                           |
| [Hyper](https://github.com/vercel/hyper)                                     | [Inline images protocol][iip]          | ‚úÖ Built-in                                           |
| [Bobcat](https://github.com/ismail-yilmaz/Bobcat)                            | [Inline images protocol][iip]          | ‚úÖ Built-in                                           |
| X11 / Wayland                                                                | Window system protocol                 | ‚òëÔ∏è [√úberzug++][ueberzug] required                     |
| Fallback                                                                     | [ASCII art (Unicode block)][ascii-art] | ‚òëÔ∏è [Chafa][chafa] required                            |

See https://yazi-rs.github.io/docs/image-preview for details.

&lt;!-- Protocols --&gt;

[kgp]: https://sw.kovidgoyal.net/kitty/graphics-protocol/#unicode-placeholders
[kgp-old]: https://github.com/sxyazi/yazi/blob/main/yazi-adapter/src/drivers/kgp_old.rs
[iip]: https://iterm2.com/documentation-images.html
[sixel]: https://www.vt100.net/docs/vt3xx-gp/chapter14.html
[ascii-art]: https://en.wikipedia.org/wiki/ASCII_art

&lt;!-- Dependencies --&gt;

[ueberzug]: https://github.com/jstkdng/ueberzugpp
[chafa]: https://hpjansson.org/chafa/

&lt;!-- Rio bug --&gt;

[rio-bug]: https://github.com/raphamorim/rio/issues/709

## License

Yazi is MIT-licensed. For more information check the [LICENSE](LICENSE) file.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[aws/amazon-q-developer-cli]]></title>
            <link>https://github.com/aws/amazon-q-developer-cli</link>
            <guid>https://github.com/aws/amazon-q-developer-cli</guid>
            <pubDate>Mon, 24 Mar 2025 00:05:46 GMT</pubDate>
            <description><![CDATA[Add autocomplete and AI to your existing terminal on macOS & Linux]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/aws/amazon-q-developer-cli">aws/amazon-q-developer-cli</a></h1>
            <p>Add autocomplete and AI to your existing terminal on macOS & Linux</p>
            <p>Language: Rust</p>
            <p>Stars: 174</p>
            <p>Forks: 34</p>
            <p>Stars today: 8 stars today</p>
            <h2>README</h2><pre># Amazon Q Developer for command line Monorepo

The **`amazon-q-developer-cli`** monorepo houses most of the core code for the Amazon Q Developer desktop
app and CLI.

## Installation

To install Amazon Q Developer for command line see the AWS public documentation [here](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/command-line-installing.html).

## Overview

Several projects live here:

- [`autocomplete`](packages/autocomplete/) - The autocomplete react app
- [`dashboard`](packages/dashboard/) - The dashboard react app
- [`figterm`](crates/figterm/) - figterm, our headless terminal/pseudoterminal that
  intercepts the user‚Äôs terminal edit buffer.
- [`q_cli`](crates/q_cli/) - the `q` CLI, allows users to interface with Amazon Q Developer from
  the command line
- [`fig_desktop`](crates/fig_desktop/) - the Rust desktop app, uses
  [`tao`](https://docs.rs/tao/latest/tao/)/[`wry`](https://docs.rs/wry/latest/wry/)
  for windowing/webviews
- [`fig_input_method`](crates/fig_input_method/) - The input method used to get cursor
  position on macOS
- [`vscode`](extensions/vscode/) - Contains the VSCode plugin needed
  for the Amazon Q Developer for command line to work in VSCode
- [`jetbrains`](extensions/jetbrains/) - Contains the VSCode plugin
  needed for the Amazon Q Developer for command line to work in Jetbrains IDEs

Other folder to be aware of

- [`build-scripts/`](build-scripts/) - Contains all python scripts to build,
  sign, and test the project on macOS and Linux
- [`crates/`](crates/) - Contains all internal rust crates
- [`packages/`](packages/) - Contains all internal npm packages
- [`proto/`](proto/) -
  [protocol buffer](https://developers.google.com/protocol-buffers/) message
  specification for inter-process communication
- [`tests/`](tests/) - Contain integration tests for the projects

Below is a high level architecture of how the different components of the app and
their IPC:

![architecture](docs/assets/architecture.svg)

## Setup

### Prerequisites

- MacOS
  - Xcode 13 or later
  - Brew

### 1. Clone repo

```bash
git clone https://github.com/aws/amazon-q-for-command-line.git
```

### 2. Install platform dependencies

This is all the dep

For Debian/Ubuntu:

```bash
sudo apt update
sudo apt install build-essential pkg-config jq dpkg curl wget cmake clang libssl-dev libgtk-3-dev libayatana-appindicator3-dev librsvg2-dev libdbus-1-dev libwebkit2gtk-4.1-dev libjavascriptcoregtk-4.1-dev valac libibus-1.0-dev libglib2.0-dev sqlite3 libxdo-dev protobuf-compiler
```

For Arch:

```bash
sudo pacman -Syu
sudo pacman -S --needed webkit2gtk base-devel curl wget openssl appmenu-gtk-module gtk3 libappindicator-gtk3 librsvg libvips cmake jq pkgconf
```

For Fedora:

```bash
sudo dnf check-update
sudo dnf install webkit2gtk3-devel.x86_64 openssl-devel curl wget libappindicator-gtk3 librsvg2-devel jq
sudo dnf group install &quot;C Development Tools and Libraries&quot;
```

For MacOS:

```shell
xcode-select --install
brew install mise pnpm protobuf zsh bash fish shellcheck jq
```

### 2. Install Rust toolchain using [Rustup](https://rustup.rs):

```shell
curl --proto &#039;=https&#039; --tlsv1.2 -sSf https://sh.rustup.rs | sh
rustup default stable
# for pre-commit hooks the two following commands are required
rustup toolchain install nightly
cargo install typos-cli
```

For MacOS development make sure the right targets are installed:

```bash
rustup target add x86_64-apple-darwin
rustup target add aarch64-apple-darwin
```

### 3. Setup Python and Node using [`mise`](https://mise.jdx.dev)

Add mise integrations to your shell shell

```shell
# zsh
echo &#039;eval &quot;$(mise activate zsh)&quot;&#039; &gt;&gt; &quot;${ZDOTDIR-$HOME}/.zshrc&quot;

# bash
echo &#039;eval &quot;$(mise activate bash)&quot;&#039; &gt;&gt; ~/.bashrc

# fish
echo &#039;mise activate fish | source&#039; &gt;&gt; ~/.config/fish/config.fish
```

Install the Python and Node toolchains using:

```shell
mise trust
mise install
```

### 4. Setup precommit hooks

```shell
# Run `pnpm` in root directory to add pre-commit hooks
pnpm install --ignore-scripts &amp;&amp; pnpm husky install
```

## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

## Licensing

This repo is dual licensed under MIT and Apache 2.0 licenses.

‚ÄúAmazon Web Services‚Äù and all related marks, including logos, graphic designs, and service names, are trademarks or trade dress of AWS in the U.S. and other countries. AWS‚Äôs trademarks and trade dress may not be used in connection with any product or service that is not AWS‚Äôs, in any manner that is likely to cause confusion among customers, or in any manner that disparages or discredits AWS.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[iced-rs/iced]]></title>
            <link>https://github.com/iced-rs/iced</link>
            <guid>https://github.com/iced-rs/iced</guid>
            <pubDate>Mon, 24 Mar 2025 00:05:45 GMT</pubDate>
            <description><![CDATA[A cross-platform GUI library for Rust, inspired by Elm]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/iced-rs/iced">iced-rs/iced</a></h1>
            <p>A cross-platform GUI library for Rust, inspired by Elm</p>
            <p>Language: Rust</p>
            <p>Stars: 26,023</p>
            <p>Forks: 1,255</p>
            <p>Stars today: 14 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;img src=&quot;docs/logo.svg&quot; width=&quot;140px&quot; /&gt;

# Iced

[![Documentation](https://docs.rs/iced/badge.svg)][documentation]
[![Crates.io](https://img.shields.io/crates/v/iced.svg)](https://crates.io/crates/iced)
[![License](https://img.shields.io/crates/l/iced.svg)](https://github.com/iced-rs/iced/blob/master/LICENSE)
[![Downloads](https://img.shields.io/crates/d/iced.svg)](https://crates.io/crates/iced)
[![Test Status](https://img.shields.io/github/actions/workflow/status/iced-rs/iced/test.yml?branch=master&amp;event=push&amp;label=test)](https://github.com/iced-rs/iced/actions)
[![Discourse](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscourse.iced.rs%2Fsite%2Fstatistics.json&amp;query=%24.users_count&amp;suffix=%20users&amp;label=discourse&amp;color=5e7ce2)](https://discourse.iced.rs/)
[![Discord Server](https://img.shields.io/discord/628993209984614400?label=&amp;labelColor=6A7EC2&amp;logo=discord&amp;logoColor=ffffff&amp;color=7389D8)](https://discord.gg/3xZJ65GAhd)

A cross-platform GUI library for Rust focused on simplicity and type-safety.
Inspired by [Elm].

&lt;a href=&quot;https://github.com/squidowl/halloy&quot;&gt;
  &lt;img src=&quot;https://iced.rs/showcase/halloy.gif&quot; width=&quot;460px&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/hecrj/icebreaker&quot;&gt;
  &lt;img src=&quot;https://iced.rs/showcase/icebreaker.gif&quot; width=&quot;360px&quot;&gt;
&lt;/a&gt;

&lt;/div&gt;

## Features

* Simple, easy-to-use, batteries-included API
* Type-safe, reactive programming model
* [Cross-platform support] (Windows, macOS, Linux, and the Web)
* Responsive layout
* Built-in widgets (including [text inputs], [scrollables], and more!)
* Custom widget support (create your own!)
* [Debug overlay with performance metrics]
* First-class support for async actions (use futures!)
* Modular ecosystem split into reusable parts:
  * A [renderer-agnostic native runtime] enabling integration with existing systems
  * Two built-in renderers leveraging [`wgpu`] and [`tiny-skia`]
    * [`iced_wgpu`] supporting Vulkan, Metal and DX12
    * [`iced_tiny_skia`] offering a software alternative as a fallback
  * A [windowing shell]

__Iced is currently experimental software.__ [Take a look at the roadmap] and
[check out the issues].

[Cross-platform support]: https://raw.githubusercontent.com/iced-rs/iced/master/docs/images/todos_desktop.jpg
[text inputs]: https://iced.rs/examples/text_input.mp4
[scrollables]: https://iced.rs/examples/scrollable.mp4
[Debug overlay with performance metrics]: https://iced.rs/examples/debug.mp4
[renderer-agnostic native runtime]: runtime/
[`wgpu`]: https://github.com/gfx-rs/wgpu
[`tiny-skia`]: https://github.com/RazrFalcon/tiny-skia
[`iced_wgpu`]: wgpu/
[`iced_tiny_skia`]: tiny_skia/
[windowing shell]: winit/
[Take a look at the roadmap]: ROADMAP.md
[check out the issues]: https://github.com/iced-rs/iced/issues

## Overview

Inspired by [The Elm Architecture], Iced expects you to split user interfaces
into four different concepts:

* __State__ ‚Äî the state of your application
* __Messages__ ‚Äî user interactions or meaningful events that you care
  about
* __View logic__ ‚Äî a way to display your __state__ as widgets that
  may produce __messages__ on user interaction
* __Update logic__ ‚Äî a way to react to __messages__ and update your
  __state__

We can build something to see how this works! Let&#039;s say we want a simple counter
that can be incremented and decremented using two buttons.

We start by modelling the __state__ of our application:

```rust
#[derive(Default)]
struct Counter {
    value: i32,
}
```

Next, we need to define the possible user interactions of our counter:
the button presses. These interactions are our __messages__:

```rust
#[derive(Debug, Clone, Copy)]
pub enum Message {
    Increment,
    Decrement,
}
```

Now, let&#039;s show the actual counter by putting it all together in our
__view logic__:

```rust
use iced::widget::{button, column, text, Column};

impl Counter {
    pub fn view(&amp;self) -&gt; Column&lt;Message&gt; {
        // We use a column: a simple vertical layout
        column![
            // The increment button. We tell it to produce an
            // `Increment` message when pressed
            button(&quot;+&quot;).on_press(Message::Increment),

            // We show the value of the counter here
            text(self.value).size(50),

            // The decrement button. We tell it to produce a
            // `Decrement` message when pressed
            button(&quot;-&quot;).on_press(Message::Decrement),
        ]
    }
}
```

Finally, we need to be able to react to any produced __messages__ and change our
__state__ accordingly in our __update logic__:

```rust
impl Counter {
    // ...

    pub fn update(&amp;mut self, message: Message) {
        match message {
            Message::Increment =&gt; {
                self.value += 1;
            }
            Message::Decrement =&gt; {
                self.value -= 1;
            }
        }
    }
}
```

And that&#039;s everything! We just wrote a whole user interface. Let&#039;s run it:

```rust
fn main() -&gt; iced::Result {
    iced::run(&quot;A cool counter&quot;, Counter::update, Counter::view)
}
```

Iced will automatically:

  1. Take the result of our __view logic__ and layout its widgets.
  1. Process events from our system and produce __messages__ for our
     __update logic__.
  1. Draw the resulting user interface.

Read the [book], the [documentation], and the [examples] to learn more!

## Implementation details

Iced was originally born as an attempt at bringing the simplicity of [Elm] and
[The Elm Architecture] into [Coffee], a 2D game library I am working on.

The core of the library was implemented during May 2019 in [this pull request].
[The first alpha version] was eventually released as
[a renderer-agnostic GUI library]. The library did not provide a renderer and
implemented the current [tour example] on top of [`ggez`], a game library.

Since then, the focus has shifted towards providing a batteries-included,
end-user-oriented GUI library, while keeping the ecosystem modular.

[this pull request]: https://github.com/hecrj/coffee/pull/35
[The first alpha version]: https://github.com/iced-rs/iced/tree/0.1.0-alpha
[a renderer-agnostic GUI library]: https://www.reddit.com/r/rust/comments/czzjnv/iced_a_rendereragnostic_gui_library_focused_on/
[tour example]: examples/README.md#tour
[`ggez`]: https://github.com/ggez/ggez

## Contributing / Feedback

If you want to contribute, please read our [contributing guidelines] for more details.

Feedback is also welcome! You can create a new topic in [our Discourse forum] or
come chat to [our Discord server].

## Sponsors

The development of Iced is sponsored by the [Cryptowatch] team at [Kraken.com]

[book]: https://book.iced.rs/
[documentation]: https://docs.rs/iced/
[examples]: https://github.com/iced-rs/iced/tree/master/examples#examples
[Coffee]: https://github.com/hecrj/coffee
[Elm]: https://elm-lang.org/
[The Elm Architecture]: https://guide.elm-lang.org/architecture/
[the current issues]: https://github.com/iced-rs/iced/issues
[contributing guidelines]: https://github.com/iced-rs/iced/blob/master/CONTRIBUTING.md
[our Discourse forum]: https://discourse.iced.rs/
[our Discord server]: https://discord.gg/3xZJ65GAhd
[Cryptowatch]: https://cryptowat.ch/charts
[Kraken.com]: https://kraken.com/
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[katanemo/archgw]]></title>
            <link>https://github.com/katanemo/archgw</link>
            <guid>https://github.com/katanemo/archgw</guid>
            <pubDate>Mon, 24 Mar 2025 00:05:44 GMT</pubDate>
            <description><![CDATA[The AI-native proxy server for agents. Arch handles the pesky heavy lifting in building agentic apps - routing prompts to agents or specifc tools, clarifying user inputs, unifying access and observability to any LLM - so you can build smarter and ship faster.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/katanemo/archgw">katanemo/archgw</a></h1>
            <p>The AI-native proxy server for agents. Arch handles the pesky heavy lifting in building agentic apps - routing prompts to agents or specifc tools, clarifying user inputs, unifying access and observability to any LLM - so you can build smarter and ship faster.</p>
            <p>Language: Rust</p>
            <p>Stars: 2,064</p>
            <p>Forks: 104</p>
            <p>Stars today: 19 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/source/_static/img/arch-logo.png&quot; alt=&quot;Arch Logo&quot; width=&quot;75%&quot; heigh=auto&gt;
&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt;


_The intelligent (edge and LLM) proxy server for agentic applications._&lt;br&gt;&lt;br&gt;
Move faster by letting Arch handle the **pesky** heavy lifting in building agents: fast input clarification, agent routing, seamless integration of prompts with tools for common tasks, and unified access and observability of LLMs.

[Quickstart](#Quickstart) ‚Ä¢
[Demos](#Demos) ‚Ä¢
[Build agentic apps with Arch](#Build-AI-Agent-with-Arch-Gateway) ‚Ä¢
[Use Arch as an LLM router](#Use-Arch-Gateway-as-LLM-Router) ‚Ä¢
[Documentation](https://docs.archgw.com) ‚Ä¢
[Contact](#Contact)

[![pre-commit](https://github.com/katanemo/arch/actions/workflows/pre-commit.yml/badge.svg)](https://github.com/katanemo/arch/actions/workflows/pre-commit.yml)
[![rust tests (prompt and llm gateway)](https://github.com/katanemo/arch/actions/workflows/rust_tests.yml/badge.svg)](https://github.com/katanemo/arch/actions/workflows/rust_tests.yml)
[![e2e tests](https://github.com/katanemo/arch/actions/workflows/e2e_tests.yml/badge.svg)](https://github.com/katanemo/arch/actions/workflows/e2e_tests.yml)
[![Build and Deploy Documentation](https://github.com/katanemo/arch/actions/workflows/static.yml/badge.svg)](https://github.com/katanemo/arch/actions/workflows/static.yml)


&lt;/div&gt;

# Overview
&lt;a href=&quot;https://www.producthunt.com/posts/arch-3?embed=true&amp;utm_source=badge-top-post-badge&amp;utm_medium=badge&amp;utm_souce=badge-arch&amp;#0045;3&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=565761&amp;theme=dark&amp;period=daily&amp;t=1742359429995&quot; alt=&quot;Arch - Build&amp;#0032;fast&amp;#0044;&amp;#0032;hyper&amp;#0045;personalized&amp;#0032;agents&amp;#0032;with&amp;#0032;intelligent&amp;#0032;infra | Product Hunt&quot; style=&quot;width: 188px; height: 41px;&quot; width=&quot;188&quot; height=&quot;41&quot; /&gt;&lt;/a&gt;

Past the thrill of an AI demo, have you found yourself hitting these walls? You know, the all too familiar ones:

- You go from one BIG prompt to specialized prompts, but get stuck building **routing and handoff** code?
- You want use new LLMs, but struggle to **quickly and safely add LLMs** without writing integration code?
- You&#039;re bogged down with prompt engineering just to **clarify user intent and validate inputs** effectively?
- You&#039;re wasting cycles choosing and integrating code for **observability** instead of it happening transparently?

And you think to youself, can&#039;t I move faster by focusing on higher-level objectives in a language/framework agnostic way? Well, you can! **Arch Gateway** was built by the contributors of [Envoy Proxy](https://www.envoyproxy.io/) with the belief that:

&gt;Prompts are nuanced and opaque user requests, which require the same capabilities as traditional HTTP requests including secure handling, intelligent routing, robust observability, and integration with backend (API) systems to improve speed and accuracy for common agentic scenarios  ‚Äì all outside core application logic.*

**Core Features**:

  - `üö¶ Routing`. Engineered with purpose-built [LLMs](https://huggingface.co/collections/katanemo/arch-function-66f209a693ea8df14317ad68) for fast (&lt;100ms) agent routing and hand-off scenarios
  - `‚ö° Tools Use`: For common agentic scenarios let Arch instantly clarfiy and convert prompts to tools/API calls
  - `‚õ® Guardrails`: Centrally configure and prevent harmful outcomes and ensure safe user interactions
  - `üîó Access to LLMs`: Centralize access and traffic to LLMs with smart retries for continuous availability
  - `üïµ Observability`: W3C compatible request tracing and LLM metrics that instantly plugin with popular tools
  - `üß± Built on Envoy`: Arch runs alongside app servers as a containerized process, and builds on top of [Envoy&#039;s](https://envoyproxy.io) proven HTTP management and scalability features to handle ingress and egress traffic related to prompts and LLMs.

**High-Level Sequence Diagram**:
![alt text](docs/source/_static/img/arch_network_diagram_high_level.png)

**Jump to our [docs](https://docs.archgw.com)** to learn how you can use Arch to improve the speed, security and personalization of your GenAI apps.

&gt; [!IMPORTANT]
&gt; Today, the function calling LLM (Arch-Function) designed for the agentic and RAG scenarios is hosted free of charge in the US-central region. To offer consistent latencies and throughput, and to manage our expenses, we will enable access to the hosted version via developers keys soon, and give you the option to run that LLM locally. For more details see this issue [#258](https://github.com/katanemo/archgw/issues/258)

## Contact
To get in touch with us, please join our [discord server](https://discord.gg/pGZf2gcwEc). We will be monitoring that actively and offering support there.

## Demos
* [Sample App: Weather Forecast Agent](demos/samples_python/weather_forecast/README.md) - A sample agentic weather forecasting app that highlights core function calling capabilities of Arch.
* [Sample App: Network Operator Agent](demos/samples_python/network_switch_operator_agent/README.md) - A simple network device switch operator agent that can retrive device statistics and reboot them.
* [User Case: Connecting to SaaS APIs](demos/use_cases/spotify_bearer_auth) - Connect 3rd party SaaS APIs to your agentic chat experience.

## Quickstart

Follow this quickstart guide to use arch gateway to build a simple AI agent. Laster in the section we will see how you can Arch Gateway to manage access keys, provide unified access to upstream LLMs and to provide e2e observability.

### Prerequisites

Before you begin, ensure you have the following:

1. [Docker System](https://docs.docker.com/get-started/get-docker/) (v24)
2. [Docker compose](https://docs.docker.com/compose/install/) (v2.29)
3. [Python](https://www.python.org/downloads/) (v3.12)

Arch&#039;s CLI allows you to manage and interact with the Arch gateway efficiently. To install the CLI, simply run the following command:

&gt; [!TIP]
&gt; We recommend that developers create a new Python virtual environment to isolate dependencies before installing Arch. This ensures that archgw and its dependencies do not interfere with other packages on your system.

```console
$ python -m venv venv
$ source venv/bin/activate   # On Windows, use: venv\Scripts\activate
$ pip install archgw==0.2.4
```

### Build AI Agent with Arch Gateway

In following quickstart we will show you how easy it is to build AI agent with Arch gateway. We will build a currency exchange agent using following simple steps. For this demo we will use `https://api.frankfurter.dev/` to fetch latest price for currencies and assume USD as base currency.

#### Step 1. Create arch config file

Create `arch_config.yaml` file with following content,

```yaml
version: v0.1

listener:
  address: 0.0.0.0
  port: 10000
  message_format: huggingface
  connect_timeout: 0.005s

llm_providers:
  - name: gpt-4o
    access_key: $OPENAI_API_KEY
    provider: openai
    model: gpt-4o

system_prompt: |
  You are a helpful assistant.

prompt_guards:
  input_guards:
    jailbreak:
      on_exception:
        message: Looks like you&#039;re curious about my abilities, but I can only provide assistance for currency exchange.

prompt_targets:
  - name: currency_exchange
    description: Get currency exchange rate from USD to other currencies
    parameters:
      - name: currency_symbol
        description: the currency that needs conversion
        required: true
        type: str
        in_path: true
    endpoint:
      name: frankfurther_api
      path: /v1/latest?base=USD&amp;symbols={currency_symbol}
    system_prompt: |
      You are a helpful assistant. Show me the currency symbol you want to convert from USD.

  - name: get_supported_currencies
    description: Get list of supported currencies for conversion
    endpoint:
      name: frankfurther_api
      path: /v1/currencies

endpoints:
  frankfurther_api:
    endpoint: api.frankfurter.dev:443
    protocol: https
```

#### Step 2. Start arch gateway with currency conversion config

```sh

$ archgw up arch_config.yaml
2024-12-05 16:56:27,979 - cli.main - INFO - Starting archgw cli version: 0.1.5
...
2024-12-05 16:56:28,485 - cli.utils - INFO - Schema validation successful!
2024-12-05 16:56:28,485 - cli.main - INFO - Starging arch model server and arch gateway
...
2024-12-05 16:56:51,647 - cli.core - INFO - Container is healthy!

```

Once the gateway is up you can start interacting with at port 10000 using openai chat completion API.

Some of the sample queries you can ask could be `what is currency rate for gbp?` or `show me list of currencies for conversion`.

#### Step 3. Interacting with gateway using curl command

Here is a sample curl command you can use to interact,

```bash
$ curl --header &#039;Content-Type: application/json&#039; \
  --data &#039;{&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;,&quot;content&quot;: &quot;what is exchange rate for gbp&quot;}]}&#039; \
  http://localhost:10000/v1/chat/completions | jq &quot;.choices[0].message.content&quot;

&quot;As of the date provided in your context, December 5, 2024, the exchange rate for GBP (British Pound) from USD (United States Dollar) is 0.78558. This means that 1 USD is equivalent to 0.78558 GBP.&quot;

```

And to get list of supported currencies,

```bash
$ curl --header &#039;Content-Type: application/json&#039; \
  --data &#039;{&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;,&quot;content&quot;: &quot;show me list of currencies that are supported for conversion&quot;}]}&#039; \
  http://localhost:10000/v1/chat/completions | jq &quot;.choices[0].message.content&quot;

&quot;Here is a list of the currencies that are supported for conversion from USD, along with their symbols:\n\n1. AUD - Australian Dollar\n2. BGN - Bulgarian Lev\n3. BRL - Brazilian Real\n4. CAD - Canadian Dollar\n5. CHF - Swiss Franc\n6. CNY - Chinese Renminbi Yuan\n7. CZK - Czech Koruna\n8. DKK - Danish Krone\n9. EUR - Euro\n10. GBP - British Pound\n11. HKD - Hong Kong Dollar\n12. HUF - Hungarian Forint\n13. IDR - Indonesian Rupiah\n14. ILS - Israeli New Sheqel\n15. INR - Indian Rupee\n16. ISK - Icelandic Kr√≥na\n17. JPY - Japanese Yen\n18. KRW - South Korean Won\n19. MXN - Mexican Peso\n20. MYR - Malaysian Ringgit\n21. NOK - Norwegian Krone\n22. NZD - New Zealand Dollar\n23. PHP - Philippine Peso\n24. PLN - Polish Z≈Çoty\n25. RON - Romanian Leu\n26. SEK - Swedish Krona\n27. SGD - Singapore Dollar\n28. THB - Thai Baht\n29. TRY - Turkish Lira\n30. USD - United States Dollar\n31. ZAR - South African Rand\n\nIf you want to convert USD to any of these currencies, you can select the one you are interested in.&quot;

```

### Use Arch Gateway as LLM Router

#### Step 1. Create arch config file

Arch operates based on a configuration file where you can define LLM providers, prompt targets, guardrails, etc. Below is an example configuration that defines openai and mistral LLM providers.

Create `arch_config.yaml` file with following content:

```yaml
version: v0.1

listener:
  address: 0.0.0.0
  port: 10000
  message_format: huggingface
  connect_timeout: 0.005s

llm_providers:
  - name: gpt-4o
    access_key: $OPENAI_API_KEY
    provider: openai
    model: gpt-4o
    default: true

  - name: ministral-3b
    access_key: $MISTRAL_API_KEY
    provider: openai
    model: ministral-3b-latest
```

#### Step 2. Start arch gateway

Once the config file is created ensure that you have env vars setup for `MISTRAL_API_KEY` and `OPENAI_API_KEY` (or these are defined in `.env` file).

Start arch gateway,

```
$ archgw up arch_config.yaml
2024-12-05 11:24:51,288 - cli.main - INFO - Starting archgw cli version: 0.1.5
2024-12-05 11:24:51,825 - cli.utils - INFO - Schema validation successful!
2024-12-05 11:24:51,825 - cli.main - INFO - Starting arch model server and arch gateway
...
2024-12-05 11:25:16,131 - cli.core - INFO - Container is healthy!
```

### Step 3: Interact with LLM

#### Step 3.1: Using OpenAI python client

Make outbound calls via Arch gateway

```python
from openai import OpenAI

# Use the OpenAI client as usual
client = OpenAI(
  # No need to set a specific openai.api_key since it&#039;s configured in Arch&#039;s gateway
  api_key = &#039;--&#039;,
  # Set the OpenAI API base URL to the Arch gateway endpoint
  base_url = &quot;http://127.0.0.1:12000/v1&quot;
)

response = client.chat.completions.create(
    # we select model from arch_config file
    model=&quot;None&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is the capital of France?&quot;}],
)

print(&quot;OpenAI Response:&quot;, response.choices[0].message.content)

```

#### Step 3.2: Using curl command
```
$ curl --header &#039;Content-Type: application/json&#039; \
  --data &#039;{&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;,&quot;content&quot;: &quot;What is the capital of France?&quot;}]}&#039; \
  http://localhost:12000/v1/chat/completions

{
  ...
  &quot;model&quot;: &quot;gpt-4o-2024-08-06&quot;,
  &quot;choices&quot;: [
    {
      ...
      &quot;message&quot;: {
        &quot;role&quot;: &quot;assistant&quot;,
        &quot;content&quot;: &quot;The capital of France is Paris.&quot;,
      },
    }
  ],
...
}

```

You can override model selection using `x-arch-llm-provider-hint` header. For example if you want to use mistral using following curl command,

```
$ curl --header &#039;Content-Type: application/json&#039; \
  --header &#039;x-arch-llm-provider-hint: ministral-3b&#039; \
  --data &#039;{&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;,&quot;content&quot;: &quot;What is the capital of France?&quot;}]}&#039; \
  http://localhost:12000/v1/chat/completions
{
  ...
  &quot;model&quot;: &quot;ministral-3b-latest&quot;,
  &quot;choices&quot;: [
    {
      &quot;message&quot;: {
        &quot;role&quot;: &quot;assistant&quot;,
        &quot;content&quot;: &quot;The capital of France is Paris. It is the most populous city in France and is known for its iconic landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral. Paris is also a major global center for art, fashion, gastronomy, and culture.&quot;,
      },
      ...
    }
  ],
  ...
}

```

## [Observability](https://docs.archgw.com/guides/observability/observability.html)
Arch is designed to support best-in class observability by supporting open standards. Please read our [docs](https://docs.archgw.com/guides/observability/observability.html) on observability for more details on tracing, metrics, and logs. The screenshot below is from our integration with Signoz (among others)

![alt text](docs/source/_static/img/tracing.png)

## Contribution
We would love feedback on our [Roadmap](https://github.com/orgs/katanemo/projects/1) and we welcome contributions to **Arch**!
Whether you&#039;re fixing bugs, adding new features, improving documentation, or creating tutorials, your help is much appreciated.
Please visit our [Contribution Guide](CONTRIBUTING.md) for more details
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[rust-unofficial/awesome-rust]]></title>
            <link>https://github.com/rust-unofficial/awesome-rust</link>
            <guid>https://github.com/rust-unofficial/awesome-rust</guid>
            <pubDate>Mon, 24 Mar 2025 00:05:43 GMT</pubDate>
            <description><![CDATA[A curated list of Rust code and resources.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/rust-unofficial/awesome-rust">rust-unofficial/awesome-rust</a></h1>
            <p>A curated list of Rust code and resources.</p>
            <p>Language: Rust</p>
            <p>Stars: 49,408</p>
            <p>Forks: 2,873</p>
            <p>Stars today: 24 stars today</p>
            <h2>README</h2><pre># Awesome Rust [![lint badge](https://github.com/rust-unofficial/awesome-rust/actions/workflows/lint.yml/badge.svg)](https://github.com/rust-unofficial/awesome-rust/actions/workflows/lint.yml) [![build badge](https://github.com/rust-unofficial/awesome-rust/actions/workflows/rust.yml/badge.svg?branch=main)](https://github.com/rust-unofficial/awesome-rust/actions/workflows/rust.yml) [![Track Awesome List](https://www.trackawesomelist.com/badge.svg)](https://www.trackawesomelist.com/rust-unofficial/awesome-rust/)

A curated list of Rust code and resources.

If you want to contribute, please read [this](CONTRIBUTING.md).

## Table of contents

&lt;!-- toc --&gt;

- [Applications](#applications)
  * [Audio and Music](#audio-and-music)
  * [Blockchain](#blockchain)
  * [Database](#database)
  * [Embedded](#embedded)
  * [Emulators](#emulators)
  * [File manager](#file-manager)
  * [Finance](#finance)
  * [Games](#games)
  * [Graphics](#graphics)
  * [Image processing](#image-processing)
  * [Industrial automation](#industrial-automation)
  * [Message Queue](#message-queue)
  * [MLOps](#mlops)
  * [Observability](#observability)
  * [Operating systems](#operating-systems)
  * [Package Managers](#package-managers)
  * [Payments](#payments)
  * [Productivity](#productivity)
  * [Routing protocols](#routing-protocols)
  * [Security tools](#security-tools)
  * [Social networks](#social-networks)
  * [System tools](#system-tools)
  * [Task scheduling](#task-scheduling)
  * [Text editors](#text-editors)
  * [Text processing](#text-processing)
  * [Utilities](#utilities)
  * [Video](#video)
  * [Virtualization](#virtualization)
  * [Web](#web)
  * [Web Servers](#web-servers)
- [Development tools](#development-tools)
  * [Build system](#build-system)
  * [Debugging](#debugging)
  * [Deployment](#deployment)
  * [Embedded](#embedded-1)
  * [FFI](#ffi)
  * [Formatters](#formatters)
  * [IDEs](#ides)
  * [Profiling](#profiling)
  * [Services](#services)
  * [Static analysis](#static-analysis)
  * [Testing](#testing)
  * [Transpiling](#transpiling)
- [Libraries](#libraries)
  * [Artificial Intelligence](#artificial-intelligence)
    + [Genetic algorithms](#genetic-algorithms)
    + [Machine learning](#machine-learning)
    + [OpenAI](#openai)
  * [Astronomy](#astronomy)
  * [Asynchronous](#asynchronous)
  * [Audio and Music](#audio-and-music-1)
  * [Authentication](#authentication)
  * [Automotive](#automotive)
  * [Bioinformatics](#bioinformatics)
  * [Caching](#caching)
  * [Cloud](#cloud)
  * [Command-line](#command-line)
  * [Compression](#compression)
  * [Computation](#computation)
  * [Concurrency](#concurrency)
  * [Configuration](#configuration)
  * [Cryptography](#cryptography)
  * [Data processing](#data-processing)
  * [Data streaming](#data-streaming)
  * [Data structures](#data-structures)
  * [Data visualization](#data-visualization)
  * [Database](#database-1)
  * [Date and time](#date-and-time)
  * [Distributed systems](#distributed-systems)
  * [Domain driven design](#domain-driven-design)
  * [eBPF](#ebpf)
  * [Email](#email)
  * [Encoding](#encoding)
  * [Filesystem](#filesystem)
  * [Finance](#finance-1)
  * [Functional Programming](#functional-programming)
  * [Game development](#game-development)
  * [Geospatial](#geospatial)
  * [Graph algorithms](#graph-algorithms)
  * [Graphics](#graphics-1)
  * [GUI](#gui)
  * [Image processing](#image-processing-1)
  * [Language specification](#language-specification)
  * [Logging](#logging)
  * [Macro](#macro)
  * [Markup language](#markup-language)
  * [Mobile](#mobile)
  * [Network programming](#network-programming)
  * [Parsing](#parsing)
  * [Peripherals](#peripherals)
  * [Platform specific](#platform-specific)
  * [Reverse engineering](#reverse-engineering)
  * [Scripting](#scripting)
  * [Simulation](#simulation)
  * [Social networks](#social-networks-1)
  * [System](#system)
  * [Task scheduling](#task-scheduling-1)
  * [Template engine](#template-engine)
  * [Text processing](#text-processing-1)
  * [Text search](#text-search)
  * [Unsafe](#unsafe)
  * [Video](#video-1)
  * [Virtualization](#virtualization-1)
  * [Web programming](#web-programming)
- [Registries](#registries)
- [Resources](#resources)
- [License](#license)

&lt;!-- tocstop --&gt;

## Applications

See also [Rust - Production](https://www.rust-lang.org/production) organizations running Rust in production.

* [alacritty](https://github.com/alacritty/alacritty) - A cross-platform, GPU enhanced terminal emulator
* [Arti](https://gitlab.torproject.org/tpo/core/arti) - An implementation of Tor. (So far, it&#039;s a not-very-complete client. But watch this space!) [![Crates.io](https://img.shields.io/crates/v/arti.svg)](https://crates.io/crates/arti)
* [asm-cli-rust](https://github.com/cch123/asm-cli-rust) - An interactive assembly shell.
* [cloudflare/boringtun](https://github.com/cloudflare/boringtun) - A Userspace WireGuard VPN Implementation [![build badge](https://img.shields.io/crates/v/boringtun.svg)](https://crates.io/crates/boringtun)
* [defguard](https://github.com/defguard/defguard) - Enterprise Open Source SSO &amp; WireGuard VPN with real 2FA/MFA
* [denoland/deno](https://github.com/denoland/deno) - A secure JavaScript/TypeScript runtime built with V8 and Tokio [![Build Status](https://github.com/denoland/deno/actions/workflows/ci.yml/badge.svg)](https://github.com/denoland/deno/actions)
* [doprz/dipc](https://github.com/doprz/dipc) - Convert your favorite images and wallpapers with your favorite color palettes/themes [![crates.io](https://img.shields.io/crates/v/dipc)](https://crates.io/crates/dipc)
* [EasyTier](https://github.com/EasyTier/EasyTier) - A simple, full-featured and decentralized mesh VPN with WireGuard support. [![crates.io](https://img.shields.io/crates/v/easytier)](https://crates.io/crates/easytier) [![GitHub actions](https://github.com/EasyTier/EasyTier/actions/workflows/core.yml/badge.svg)](https://github.com/EasyTier/EasyTier/actions/)[![GitHub actions](https://github.com/EasyTier/EasyTier/actions/workflows/gui.yml/badge.svg)](https://github.com/EasyTier/EasyTier/actions/)
* [fcsonline/drill](https://github.com/fcsonline/drill) - A HTTP load testing application inspired by Ansible syntax
* [fend](https://github.com/printfn/fend) - Arbitrary-precision unit-aware calculator [![build](https://github.com/printfn/fend/workflows/build/badge.svg)](https://github.com/printfn/fend/actions/workflows/actions.yml)
* [Fractalide](https://github.com/fractalide/fractalide) - Simple microservices
* [habitat](https://github.com/habitat-sh/habitat) - A tool created by Chef to build, deploy, and manage applications.
* [Herd](https://github.com/imjacobclark/Herd) - an experimental HTTP load testing application
* [hickory-dns](https://crates.io/crates/trust-dns) - A DNS-server [![Build Status](https://github.com/hickory-dns/hickory-dns/actions/workflows/test.yml/badge.svg)](https://github.com/hickory-dns/hickory-dns/actions?query=workflow%3Atest)
* [innernet](https://github.com/tonarino/innernet) - An overlay or private mesh network that uses Wireguard under the hood
* [jedisct1/flowgger](https://github.com/awslabs/flowgger) - A fast, simple and lightweight data collector
* [kalker](https://github.com/PaddiM8/kalker) - A scientific calculator that supports math-like syntax with user-defined variables, functions, derivation, integration, and complex numbers. Cross-platform + WASM support [![Build Status](https://github.com/PaddiM8/kalker/workflows/Release/badge.svg)](https://github.com/PaddiM8/kalker/actions)
* [kftray](https://github.com/hcavarsan/kftray) - A cross-platform system tray app for managing and sharing multiple kubectl port-forward configurations. [![Build Status](https://github.com/hcavarsan/kftray/workflows/Release/badge.svg)](https://github.com/hcavarsan/kftray/actions)
* [kytan](https://github.com/changlan/kytan) - High Performance Peer-to-Peer VPN
* [linkerd/linkerd2-proxy](https://github.com/linkerd/linkerd2-proxy) - Ultralight service mesh for Kubernetes.
* [MaidSafe](https://github.com/maidsafe) - A decentralized platform.
* [mdBook](https://github.com/rust-lang/mdBook) - A command line utility to create books from markdown files [![Build Status](https://github.com/rust-lang/mdBook/actions/workflows/main.yml/badge.svg)](https://github.com/rust-lang/mdBook/actions)
* [Mega](https://github.com/web3infra-foundation/mega) - A monorepo &amp; monolithic codebase management system that supports Git, also is an unofficial open source implementation of Google Piper.
* [mirrord](https://github.com/metalbear-co/mirrord) - Connect your local process and your cloud environment, and run local code in cloud conditions
* [nicohman/eidolon](https://github.com/nicohman/eidolon) - A steam and drm-free game registry and launcher for linux and macosx
* [Pijul](https://pijul.org) - A patch-based distributed version control system
* [Rauthy](https://github.com/sebadob/rauthy) - OpenID Connect Single Sign-On Identity &amp; Access Management
* [Rio](https://github.com/raphamorim/rio) - A hardware-accelerated GPU terminal emulator powered by WebGPU, focusing to run in desktops and browsers.
* [Rust Iot Platform](https://github.com/iot-ecology/rust-iot-platform) - A high-performance IoT development platform built with Rust, designed for multi-protocol support and real-time data processing. This platform supports MQTT, WebSockets (WS), TCP, and CoAP protocols, making it highly flexible for diverse IoT applications.
* [rx](https://github.com/cloudhead/rx) - Vi inspired Modern Pixel Art Editor
* [Ryot](https://github.com/ignisda/ryot) - A self hosted application to track media consumption, fitness, etc.
* [Servo](https://github.com/servo/servo) - A prototype web browser engine
* [shoes](https://github.com/cfal/shoes) - A multi-protocol proxy server
* [shuttle](https://github.com/shuttle-hq/shuttle) - A serverless platform.
* [Sniffnet](https://github.com/GyulyVGC/sniffnet) - Cross-platform application to monitor your network traffic with ease [![build badge](https://img.shields.io/github/actions/workflow/status/gyulyvgc/sniffnet/rust.yml?logo=github)](https://github.com/GyulyVGC/sniffnet/blob/main/.github/workflows/rust.yml) [![crate](https://img.shields.io/crates/v/sniffnet?logo=rust)](https://crates.io/crates/sniffnet)
* [SWC](https://github.com/swc-project/swc) - super-fast TypeScript / JavaScript compiler
* [tiny](https://github.com/osa1/tiny) - A terminal IRC client
* [UpVPN](https://github.com/upvpn/upvpn-app) - WireGuard VPN client for macOS, Linux, and Windows built on Tauri.
* [wasmer](https://github.com/wasmerio/wasmer) - A safe and fast WebAssembly runtime supporting WASI and Emscripten [![Build Status](https://github.com/wasmerio/wasmer/actions/workflows/build.yml/badge.svg)](https://github.com/wasmerio/wasmer/actions)
* [Weld](https://github.com/serayuzgur/weld) - Full fake REST API generator
* [wezterm](https://github.com/wezterm/wezterm) - A GPU-accelerated cross-platform terminal emulator and multiplexer
* [WinterJS](https://github.com/wasmerio/winterjs) - A secure JavaScript runtime built with SpiderMonkey and Axum
* [zellij](https://github.com/zellij-org/zellij) - A terminal multiplexer (workspace) with batteries included

### Audio and Music

* [dano](https://github.com/kimono-koans/dano) - A hashdeep/md5tree (but much more) for media files
* [enginesound](https://github.com/DasEtwas/enginesound) - A GUI and command line application used to procedurally generate semi-realistic engine sounds. Featuring in-depth configuration, variable sample rate and a frequency analysis window.
* [Festival](https://github.com/hinto-janai/festival) - A local music player/server/client [![build-badge](https://github.com/hinto-janai/festival/actions/workflows/ci.yml/badge.svg)](https://github.com/hinto-janai/festival/actions/workflows/ci.yml)
* [figsoda/mmtc](https://github.com/figsoda/mmtc) [[mmtc](https://crates.io/crates/mmtc)] - Minimal mpd terminal client that aims to be simple yet highly configurable [![build-badge](https://github.com/figsoda/mmtc/actions/workflows/ci.yml/badge.svg)](https://github.com/figsoda/mmtc/actions/workflows/ci.yml)
* [Glicol](https://github.com/chaosprint/glicol) - Graph-oriented live coding language, for collaborative musicking in browsers.
* [ncspot](https://github.com/hrkfdn/ncspot) - Cross-platform ncurses Spotify client, inspired by ncmpc and the likes. [![build badge](https://github.com/hrkfdn/ncspot/actions/workflows/ci.yml/badge.svg)](https://github.com/hrkfdn/ncspot/actions?query=workflow%3ABuild)
* [Pinepods](https://github.com/madeofpendletonwool/PinePods) - A rust based podcast management system with multi-user support. Pinepods utilizes a central database so aspects like listen time and themes follow from device to device. With clients built using Tauri, it&#039;s a full cross-platform listening solution! [![Docker Container Build](https://github.com/madeofpendletonwool/PinePods/actions/workflows/docker-publish.yml/badge.svg)](https://github.com/madeofpendletonwool/PinePods/actions/workflows/docker-publish.yml)
* [Polaris](https://github.com/agersant/polaris) - A music streaming application.
* [Spotify Player](https://github.com/aome510/spotify-player) - A Spotify player in the terminal with full feature parity.
* [Spotifyd](https://github.com/Spotifyd/spotifyd) - An open source Spotify client running as a UNIX daemon. [![Continuous Integration](https://github.com/Spotifyd/spotifyd/actions/workflows/ci.yml/badge.svg)](https://github.com/Spotifyd/spotifyd/actions/workflows/ci.yml)
* [termusic](https://github.com/tramhao/termusic) - Music Player TUI written
* [WhatBPM](https://github.com/sergree/whatbpm) - A daily statically generated information resource for electronic dance music producers. Provides daily analytics on the most frequently used values for each EDM genre: tempos, keys, root notes, and so on, using publicly available data such as Beatport and Spotify.
  
### Blockchain

* [artemis](https://github.com/paradigmxyz/artemis) - A simple, modular, and fast framework for writing MEV bots.
* [beerus](https://github.com/eigerco/beerus) - Beerus is a trustless StarkNet Light Client, ‚ö°blazing fast ‚ö° [![GitHub Workflow Status](https://github.com/eigerco/beerus/actions/workflows/check.yml/badge.svg)](https://github.com/eigerco/beerus/actions/workflows/check.yml)
* [Bitcoin Satoshi&#039;s Vision](https://github.com/brentongunning/rust-sv) [[sv](https://crates.io/crates/sv)] - A library for working with Bitcoin SV.
* [cairo](https://github.com/starkware-libs/cairo) - Cairo is the first Turing-complete language for creating provable programs for general computation. This is also the native language of [StarkNet](https://www.starknet.io), a ZK-Rollup using STARK proofs ![GitHub Workflow Status](https://img.shields.io/github/workflow/status/starkware-libs/cairo/CI?style=flat-square&amp;logo=github)
* [cairo-vm](https://github.com/lambdaclass/cairo-vm) - Implementation of the Cairo VM [![rust](https://github.com/lambdaclass/cairo-vm/actions/workflows/rust.yml/badge.svg)](https://github.com/lambdaclass/cairo-vm/actions/workflows/rust.yml)
* [ChainX](https://github.com/chainx-org/ChainX) - Fully Decentralized Interchain Crypto Asset Management on Polkadot.
* [CITA](https://github.com/citahub/cita) - A high performance blockchain kernel for enterprise users.
* [coinbase-pro-rs](https://github.com/inv2004/coinbase-pro-rs) - Coinbase pro client, supports sync/async/websocket
* [Diem](https://github.com/diem/diem) - Diem‚Äôs mission is to enable a simple global currency and financial infrastructure that empowers billions of people.
* [dusk-network/rusk](https://github.com/dusk-network/rusk) - Reference implementation of Dusk, a privacy-focused, scalable FMI for real-world assets (RWA) and compliant financial applications. [![Build Status](https://github.com/dusk-network/rusk/actions/workflows/rusk_ci.yml/badge.svg)](https://github.com/dusk-network/rusk/actions/workflows/rusk_ci.yml)
* [electrumrs](https://github.com/romanz/electrs) - An efficient re-implementation of Electrum Server.
* [ethabi](https://github.com/rust-ethereum/ethabi) - Encode and decode smart contract invocations.
* [ethaddrgen](https://github.com/Limeth/ethaddrgen) - Custom Ethereum vanity address generator
* [etk](https://github.com/quilt/etk) - etk is a collection of tools for writing, reading, and analyzing EVM bytecode.
* [Forest](https://github.com/ChainSafe/forest) - Filecoin implementation [![Build Status](https://img.shields.io/circleci/build/gh/ChainSafe/forest/main?branch=master)](https://app.circleci.com/pipelines/github/ChainSafe/forest?branch=main)
* [Foundry](https://github.com/foundry-rs/foundry) - Foundry is a blazing fast, portable and modular toolkit for Ethereum application development. ![Build Status](https://img.shields.io/github/workflow/status/foundry-rs/foundry/test?style=flat-square)
* [Grin](https://github.com/mimblewimble/grin/) - Evolution of the MimbleWimble protocol
* [hdwallet](https://github.com/jjyr/hdwallet) [[hdwallet](https://crates.io/crates/hdwallet)] - BIP-32 HD wallet related key derivation utilities.
* [Holochain](https://github.com/holochain/holochain) - Scalable P2P alternative to blockchain for all those distributed apps you always wanted to build. [![detect critical check failures](https://github.com/holochain/holochain/actions/workflows/autorebase.yml/badge.svg)](https://github.com/holochain/holochain/actions/)
* [Hyperlane](https://github.com/hyperlane-xyz/hyperlane-monorepo) - Framework for permissionless, modular interoperability. The offchain clients are written in Rust, as well as the smart contracts for Solana VM and CosmWasm.
* [ibc-rs](https://github.com/informalsystems/hermes) - Implementation of the [Interblockchain Communication](https://ibc.cosmos.network/) protocol
* [infincia/bip39-rs](https://github.com/infincia/bip39-rs) [[bip39](https://crates.io/crates/bip39)] - Implementation of BIP39.
* [interBTC](https://github.com/interlay/interbtc) - Trustless and fully decentralized Bitcoin bridge to Polkadot and Kusama.
* [Joystream](https://github.com/Joystream/joystream) - A user governed video platform
* [Lighthouse](https://github.com/sigp/lighthouse) - Ethereum Consensus Layer (CL) Client [![Build Status](https://github.com/sigp/lighthouse/actions/workflows/test-suite.yml/badge.svg)](https://github.com/sigp/lighthouse/actions)
* [near/nearcore](https://github.com/near/nearcore) - decentralized smart-contract platform for low-end mobile devices.
* [Nervos CKB](https://github.com/nervosnetwork/ckb) - Nervos CKB is a public permissionless blockchain, the common knowledge layer of Nervos network.
* [opensea-rs](https://github.com/gakonst/opensea-rs) - Bindings &amp; CLI to the Opensea API and Contracts.
* [Parity-Bitcoin](https://github.com/paritytech/parity-bitcoin) - The Parity Bitcoin client
* [Phala-Network/phala-blockchain](https://github.com/Phala-Network/phala-blockchain) - Confidential smart contract blockchain based on Intel SGX and Substrate
* [polkadot-sdk](https://github.com/paritytech/polkadot-sdk) - The Parity Polkadot Blockchain SDK
* [reth](https://github.com/paradigmxyz/reth) - Modular, contributor-friendly and blazing-fast implementation of the Ethereum protocol.
* [revm](https://github.com/bluealloy/revm) - Revolutionary Machine (revm) is a fast Ethereum virtual machine.
* [rust-bitcoin](https://github.com/rust-bitcoin/rust-bitcoin) - Library with support for de/serialization, parsing and executing on data structures and network messages related to Bitcoin.
* [rust-lightning](https://github.com/lightningdevkit/rust-lightning) [![Crate](https://img.shields.io/crates/v/lightning.svg?logo=rust)](https://crates.io/crates/lightning) - Bitcoin Lightning library. The main crate,`lightning`, does not handle networking, persistence, or any other I/O. Thus,it is runtime-agnostic, but users must implement basic networking logic, chain interactions, and disk storage.po on linking crate.

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[influxdata/influxdb]]></title>
            <link>https://github.com/influxdata/influxdb</link>
            <guid>https://github.com/influxdata/influxdb</guid>
            <pubDate>Mon, 24 Mar 2025 00:05:42 GMT</pubDate>
            <description><![CDATA[Scalable datastore for metrics, events, and real-time analytics]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/influxdata/influxdb">influxdata/influxdb</a></h1>
            <p>Scalable datastore for metrics, events, and real-time analytics</p>
            <p>Language: Rust</p>
            <p>Stars: 29,706</p>
            <p>Forks: 3,593</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
 &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/influxdb-logo.png&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/influxdb-logo-dark.png&quot;&gt;
    &lt;img src=&quot;assets/influxdb-logo.png&quot; alt=&quot;InfluxDB Logo&quot; width=&quot;600&quot;&gt;
  &lt;/picture&gt;
 &lt;p&gt;
&lt;/div&gt;

InfluxDB Core is a database built to collect, process, transform, and store event and time series data. It is ideal for use cases that require real-time ingest and fast query response times to build user interfaces, monitoring, and automation solutions.

Common use cases include:

- Monitoring sensor data
- Server monitoring
- Application performance monitoring
- Network monitoring
- Financial market and trading analytics
- Behavioral analytics

InfluxDB is optimized for scenarios where near real-time data monitoring is essential and queries 
need to return quickly to support user experiences such as dashboards and interactive user interfaces.

InfluxDB 3 Core‚Äôs feature highlights include:

- Diskless architecture with object storage support (or local disk with no dependencies)
- Fast query response times (under 10ms for last-value queries, or 30ms for distinct metadata)
- Embedded Python VM for plugins and triggers
- Parquet file persistence
- Compatibility with InfluxDB 1.x and 2.x write APIs
- Compatability with InfluxDB 1.x query API (InfluxQL)
- SQL query engine with support for FlightSQL and HTTP query API

## Project Status

InfluxDB 3 Core is in public beta and available for testing and feedback, but is not meant for 
production use. During the beta period we will be adding a couple of security and operational 
features to Enterprise. Otherwise, we will be focused on testing, robustness, performance, and 
operational tooling. Both the product and this documentation are works in progress. New builds get created on 
every merge into main, however we will have weekly beta builds along with a changelog for uses 
that want to move at a slower pace.

All of the APIs outside of the processing engine can be considered stable. They are what we will GA the 3.0 
version with. We will also not be making any changes to file formats that don&#039;t also come with an in-place 
upgrade path. This means that you will be able to keep the data in the database when upgrading beta versions 
and into the GA. We anticpate the general availability of InfluxDB 3 Core in April 2025.

We welcome and encourage your input about your experience with the beta. Join the [InfluxDB3 Discord](https://discord.gg/vZe2w2Ds8B) 
or the public channels below.

See the [InfluxDB 3 beta release announcement here](https://www.influxdata.com/blog/influxdb3-open-source-public-beta/) 
or dig into the [InfluxDB 3 getting started guide here](https://docs.influxdata.com/influxdb3/core/get-started/).

## Learn InfluxDB
[Documentation](https://docs.influxdata.com/) | [Community Forum](https://community.influxdata.com/) | [Community Slack](https://www.influxdata.com/slack/) | [Blog](https://www.influxdata.com/blog/) | [InfluxDB University](https://university.influxdata.com/) | [YouTube](https://www.youtube.com/@influxdata8893)

Try **InfluxDB Cloud** for free and get started fast with no local setup required. Click [here](https://cloud2.influxdata.com/signup) to start building your application on InfluxDB Cloud.


## Installation
We have nightly and versioned Docker images, Debian packages, RPM packages, and tarballs of InfluxDB available on the [InfluxData downloads page](https://portal.influxdata.com/downloads/). We also provide the InfluxDB command line interface (CLI) client as a separate binary available at the same location.

- For v1 installation, use the [main 1.x branch](https://github.com/influxdata/influxdb/tree/master-1.x) or [install InfluxDB OSS directly](https://docs.influxdata.com/influxdb/v1/introduction/install/#installing-influxdb-oss).
- For v2 installation, use the [main 2.x branch](https://github.com/influxdata/influxdb/tree/main-2.x).
- For InfluxDB 3 Core beta see the [InfluxDB 3 Core getting started guide](https://docs.influxdata.com/influxdb3/core/get-started/).

If you are interested in building from source, see the [building from source](https://github.com/influxdata/influxdb/blob/main-2.x/CONTRIBUTING.md#building-from-source) guide for contributors.

To begin using InfluxDB, visit our [Getting Started with InfluxDB](https://docs.influxdata.com/influxdb/v1/introduction/get-started/) documentation.


## License
The open source software we build is licensed under the permissive MIT or Apache 2 licenses at the user&#039;s choosing. We‚Äôve long held the view that our open source code should be truly open and our commercial code should be separate and closed. 


## Interested in joining the team building InfluxDB?
Check out current job openings at [www.influxdata.com/careers](https://www.influxdata.com/careers) today!</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[leptos-rs/leptos]]></title>
            <link>https://github.com/leptos-rs/leptos</link>
            <guid>https://github.com/leptos-rs/leptos</guid>
            <pubDate>Mon, 24 Mar 2025 00:05:41 GMT</pubDate>
            <description><![CDATA[Build fast web applications with Rust.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/leptos-rs/leptos">leptos-rs/leptos</a></h1>
            <p>Build fast web applications with Rust.</p>
            <p>Language: Rust</p>
            <p>Stars: 17,983</p>
            <p>Forks: 725</p>
            <p>Stars today: 15 stars today</p>
            <h2>README</h2><pre>&lt;picture&gt;
    &lt;source srcset=&quot;https://raw.githubusercontent.com/leptos-rs/leptos/main/docs/logos/Leptos_logo_pref_dark_RGB.svg&quot; media=&quot;(prefers-color-scheme: dark)&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/leptos-rs/leptos/main/docs/logos/Leptos_logo_RGB.svg&quot; alt=&quot;Leptos Logo&quot;&gt;
&lt;/picture&gt;

[![crates.io](https://img.shields.io/crates/v/leptos.svg)](https://crates.io/crates/leptos)
[![docs.rs](https://docs.rs/leptos/badge.svg)](https://docs.rs/leptos)
![Crates.io MSRV](https://img.shields.io/crates/msrv/leptos)
[![Discord](https://img.shields.io/discord/1031524867910148188?color=%237289DA&amp;label=discord)](https://discord.gg/YdRAhS7eQB)
[![Matrix](https://img.shields.io/badge/Matrix-leptos-grey?logo=matrix&amp;labelColor=white&amp;logoColor=black)](https://matrix.to/#/#leptos:matrix.org)

[Website](https://leptos.dev) | [Book](https://leptos-rs.github.io/leptos/) | [Docs.rs](https://docs.rs/leptos/latest/leptos/) | [Playground](https://codesandbox.io/p/sandbox/leptos-rtfggt?file=%2Fsrc%2Fmain.rs%3A1%2C1) | [Discord](https://discord.gg/YdRAhS7eQB)

You can find a list of useful libraries and example projects at [`awesome-leptos`](https://github.com/leptos-rs/awesome-leptos).

# Leptos

```rust
use leptos::*;

#[component]
pub fn SimpleCounter(initial_value: i32) -&gt; impl IntoView {
    // create a reactive signal with the initial value
    let (value, set_value) = signal(initial_value);

    // create event handlers for our buttons
    // note that `value` and `set_value` are `Copy`, so it&#039;s super easy to move them into closures
    let clear = move |_| set_value(0);
    let decrement = move |_| set_value.update(|value| *value -= 1);
    let increment = move |_| set_value.update(|value| *value += 1);

    // create user interfaces with the declarative `view!` macro
    view! {
        &lt;div&gt;
            &lt;button on:click=clear&gt;Clear&lt;/button&gt;
            &lt;button on:click=decrement&gt;-1&lt;/button&gt;
            // text nodes can be quoted or unquoted
            &lt;span&gt;&quot;Value: &quot; {value} &quot;!&quot;&lt;/span&gt;
            &lt;button on:click=increment&gt;+1&lt;/button&gt;
        &lt;/div&gt;
    }
}

// we also support a builder syntax rather than the JSX-like `view` macro
#[component]
pub fn SimpleCounterWithBuilder(initial_value: i32) -&gt; impl IntoView {
    use leptos::html::*;

    let (value, set_value) = signal(initial_value);
    let clear = move |_| set_value(0);
    let decrement = move |_| set_value.update(|value| *value -= 1);
    let increment = move |_| set_value.update(|value| *value += 1);

    // the `view` macro above expands to this builder syntax
    div().child((
        button().on(ev::click, clear).child(&quot;Clear&quot;),
        button().on(ev::click, decrement).child(&quot;-1&quot;),
        span().child((&quot;Value: &quot;, value, &quot;!&quot;)),
        button().on(ev::click, increment).child(&quot;+1&quot;)
    ))
}

// Easy to use with Trunk (trunkrs.dev) or with a simple wasm-bindgen setup
pub fn main() {
    mount_to_body(|| view! {
        &lt;SimpleCounter initial_value=3 /&gt;
    })
}
```

## About the Framework

Leptos is a full-stack, isomorphic Rust web framework leveraging fine-grained reactivity to build declarative user interfaces.

## What does that mean?

- **Full-stack**: Leptos can be used to build apps that run in the browser (client-side rendering), on the server (server-side rendering), or by rendering HTML on the server and then adding interactivity in the browser (server-side rendering with hydration). This includes support for HTTP streaming of both data ([`Resource`s](https://docs.rs/leptos/latest/leptos/struct.Resource.html)) and HTML (out-of-order or in-order streaming of [`&lt;Suspense/&gt;`](https://docs.rs/leptos/latest/leptos/fn.Suspense.html) components.)
- **Isomorphic**: Leptos provides primitives to write isomorphic [server functions](https://docs.rs/leptos_server/0.2.5/leptos_server/index.html), i.e., functions that can be called with the ‚Äúsame shape‚Äù on the client or server, but only run on the server. This means you can write your server-only logic (database requests, authentication etc.) alongside the client-side components that will consume it, and call server functions as if they were running in the browser, without needing to create and maintain a separate REST or other API.
- **Web**: Leptos is built on the Web platform and Web standards. The [router](https://docs.rs/leptos_router/latest/leptos_router/) is designed to use Web fundamentals (like links and forms) and build on top of them rather than trying to replace them.
- **Framework**: Leptos provides most of what you need to build a modern web app: a reactive system, templating library, and a router that works on both the server and client side.
- **Fine-grained reactivity**: The entire framework is built from reactive primitives. This allows for extremely performant code with minimal overhead: when a reactive signal‚Äôs value changes, it can update a single text node, toggle a single class, or remove an element from the DOM without any other code running. (So, no virtual DOM overhead!)
- **Declarative**: Tell Leptos how you want the page to look, and let the framework tell the browser how to do it.

## Learn more

Here are some resources for learning more about Leptos:

- [Book](https://leptos-rs.github.io/leptos/) (work in progress)
- [Examples](https://github.com/leptos-rs/leptos/tree/main/examples)
- [API Documentation](https://docs.rs/leptos/latest/leptos/)
- [Common Bugs](https://github.com/leptos-rs/leptos/tree/main/docs/COMMON_BUGS.md) (and how to fix them!)

## `nightly` Note

Most of the examples assume you‚Äôre using `nightly` version of Rust and the `nightly` feature of Leptos. To use `nightly` Rust, you can either set your toolchain globally or on per-project basis.

To set `nightly` as a default toolchain for all projects (and add the ability to compile Rust to WebAssembly, if you haven‚Äôt already):

```
rustup toolchain install nightly
rustup default nightly
rustup target add wasm32-unknown-unknown
```

If you&#039;d like to use `nightly` only in your Leptos project however, add [`rust-toolchain.toml`](https://rust-lang.github.io/rustup/overrides.html#the-toolchain-file) file with the following content:

```toml
[toolchain]
channel = &quot;nightly&quot;
targets = [&quot;wasm32-unknown-unknown&quot;]
```

The `nightly` feature enables the function call syntax for accessing and setting signals, as opposed to `.get()` and `.set()`. This leads to a consistent mental model in which accessing a reactive value of any kind (a signal, memo, or derived signal) is always represented as a function call. This is only possible with nightly Rust and the `nightly` feature.

## `cargo-leptos`

[`cargo-leptos`](https://github.com/leptos-rs/cargo-leptos) is a build tool that&#039;s designed to make it easy to build apps that run on both the client and the server, with seamless integration. The best way to get started with a real Leptos project right now is to use `cargo-leptos` and our starter templates for [Actix](https://github.com/leptos-rs/start) or [Axum](https://github.com/leptos-rs/start-axum).

```bash
cargo install cargo-leptos
cargo leptos new --git https://github.com/leptos-rs/start
cd [your project name]
cargo leptos watch
```

Open browser to [http://localhost:3000/](http://localhost:3000/).

## FAQs

### What‚Äôs up with the name?

_Leptos_ (ŒªŒµœÄœÑœåœÇ) is an ancient Greek word meaning ‚Äúthin, light, refined, fine-grained.‚Äù To me, a classicist and not a dog owner, it evokes the lightweight reactive system that powers the framework. I&#039;ve since learned the same word is at the root of the medical term ‚Äúleptospirosis,‚Äù a blood infection that affects humans and animals... My bad. No dogs were harmed in the creation of this framework.

### Is it production ready?

People usually mean one of three things by this question.

1. **Are the APIs stable?** i.e., will I have to rewrite my whole app from Leptos 0.1 to 0.2 to 0.3 to 0.4, or can I write it now and benefit from new features and updates as new versions come?

The APIs are basically settled. We‚Äôre adding new features, but we‚Äôre very happy with where the type system and patterns have landed. I would not expect major breaking changes to your code to adapt to future releases, in terms of architecture.

2. **Are there bugs?**

Yes, I‚Äôm sure there are. You can see from the state of our issue tracker over time that there aren‚Äôt that _many_ bugs and they‚Äôre usually resolved pretty quickly. But for sure, there may be moments where you encounter something that requires a fix at the framework level, which may not be immediately resolved.

3. **Am I a consumer or a contributor?**

This may be the big one: ‚Äúproduction ready‚Äù implies a certain orientation to a library: that you can basically use it, without any special knowledge of its internals or ability to contribute. Everyone has this at some level in their stack: for example I (@gbj) don‚Äôt have the capacity or knowledge to contribute to something like `wasm-bindgen` at this point: I simply rely on it to work.

There are several people in the community using Leptos right now for internal apps at work, who have also become significant contributors. I think this is the right level of production use for now. There may be missing features that you need, and you may end up building them! But for internal apps, if you‚Äôre willing to build and contribute missing pieces along the way, the framework is definitely usable right now.

### Can I use this for native GUI?

Sure! Obviously the `view` macro is for generating DOM nodes but you can use the reactive system to drive any native GUI toolkit that uses the same kind of object-oriented, event-callback-based framework as the DOM pretty easily. The principles are the same:

- Use signals, derived signals, and memos to create your reactive system
- Create GUI widgets
- Use event listeners to update signals
- Create effects to update the UI

The 0.7 update originally set out to create a &quot;generic rendering&quot; approach that would allow us to reuse most of the same view logic to do all of the above. Unfortunately, this has had to be shelved for now due to difficulties encountered by the Rust compiler when building larger-scale applications with the number of generics spread throughout the codebase that this required. It&#039;s an approach I&#039;m looking forward to exploring again in the future; feel free to reach out if you&#039;re interested in this kind of work.

### How is this different from Yew?

Yew is the most-used library for Rust web UI development, but there are several differences between Yew and Leptos, in philosophy, approach, and performance.

- **VDOM vs. fine-grained:** Yew is built on the virtual DOM (VDOM) model: state changes cause components to re-render, generating a new virtual DOM tree. Yew diffs this against the previous VDOM, and applies those patches to the actual DOM. Component functions rerun whenever state changes. Leptos takes an entirely different approach. Components run once, creating (and returning) actual DOM nodes and setting up a reactive system to update those DOM nodes.
- **Performance:** This has huge performance implications: Leptos is simply much faster at both creating and updating the UI than Yew is.
- **Server integration:** Yew was created in an era in which browser-rendered single-page apps (SPAs) were the dominant paradigm. While Leptos supports client-side rendering, it also focuses on integrating with the server side of your application via server functions and multiple modes of serving HTML, including out-of-order streaming.

### How is this different from Dioxus?

Like Leptos, Dioxus is a framework for building UIs using web technologies. However, there are significant differences in approach and features.

- **VDOM vs. fine-grained:** While Dioxus has a performant virtual DOM (VDOM), it still uses coarse-grained/component-scoped reactivity: changing a stateful value reruns the component function and diffs the old UI against the new one. Leptos components use a different mental model, creating (and returning) actual DOM nodes and setting up a reactive system to update those DOM nodes.
- **Web vs. desktop priorities:** Dioxus uses Leptos server functions in its fullstack mode, but does not have the same `&lt;Suspense&gt;`-based support for things like streaming HTML rendering, or share the same focus on holistic web performance. Leptos tends to prioritize holistic web performance (streaming HTML rendering, smaller WASM binary sizes, etc.), whereas Dioxus has an unparalleled experience when building desktop apps, because your application logic runs as a native Rust binary.

### How is this different from Sycamore?

Sycamore and Leptos are both heavily influenced by SolidJS. At this point, Leptos has a larger community and ecosystem and is more actively developed. Other differences:

- **Templating DSLs:** Sycamore uses a custom templating language for its views, while Leptos uses a JSX-like template format.
- **`&#039;static` signals:** One of Leptos‚Äôs main innovations was the creation of `Copy + &#039;static` signals, which have excellent ergonomics. Sycamore is in the process of adopting the same pattern, but this is not yet released.
- **Perseus vs. server functions:** The Perseus metaframework provides an opinionated way to build Sycamore apps that include server functionality. Leptos instead provides primitives like server functions in the core of the framework.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[dani-garcia/vaultwarden]]></title>
            <link>https://github.com/dani-garcia/vaultwarden</link>
            <guid>https://github.com/dani-garcia/vaultwarden</guid>
            <pubDate>Mon, 24 Mar 2025 00:05:40 GMT</pubDate>
            <description><![CDATA[Unofficial Bitwarden compatible server written in Rust, formerly known as bitwarden_rs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/dani-garcia/vaultwarden">dani-garcia/vaultwarden</a></h1>
            <p>Unofficial Bitwarden compatible server written in Rust, formerly known as bitwarden_rs</p>
            <p>Language: Rust</p>
            <p>Stars: 42,830</p>
            <p>Forks: 2,027</p>
            <p>Stars today: 22 stars today</p>
            <h2>README</h2><pre>![Vaultwarden Logo](./resources/vaultwarden-logo-auto.svg)

An alternative server implementation of the Bitwarden Client API, written in Rust and compatible with [official Bitwarden clients](https://bitwarden.com/download/) [[disclaimer](#disclaimer)], perfect for self-hosted deployment where running the official resource-heavy service might not be ideal.

---

[![GitHub Release](https://img.shields.io/github/release/dani-garcia/vaultwarden.svg?style=for-the-badge&amp;logo=vaultwarden&amp;color=005AA4)](https://github.com/dani-garcia/vaultwarden/releases/latest)
[![ghcr.io Pulls](https://img.shields.io/badge/dynamic/json?style=for-the-badge&amp;logo=github&amp;logoColor=fff&amp;color=005AA4&amp;url=https%3A%2F%2Fipitio.github.io%2Fbackage%2Fdani-garcia%2Fvaultwarden%2Fvaultwarden.json&amp;query=%24.downloads&amp;label=ghcr.io%20pulls&amp;cacheSeconds=14400)](https://github.com/dani-garcia/vaultwarden/pkgs/container/vaultwarden)
[![Docker Pulls](https://img.shields.io/docker/pulls/vaultwarden/server.svg?style=for-the-badge&amp;logo=docker&amp;logoColor=fff&amp;color=005AA4&amp;label=docker.io%20pulls)](https://hub.docker.com/r/vaultwarden/server)
[![Quay.io](https://img.shields.io/badge/quay.io-download-005AA4?style=for-the-badge&amp;logo=redhat&amp;cacheSeconds=14400)](https://quay.io/repository/vaultwarden/server) &lt;br&gt;
[![Contributors](https://img.shields.io/github/contributors-anon/dani-garcia/vaultwarden.svg?style=flat-square&amp;logo=vaultwarden&amp;color=005AA4)](https://github.com/dani-garcia/vaultwarden/graphs/contributors)
[![Forks](https://img.shields.io/github/forks/dani-garcia/vaultwarden.svg?style=flat-square&amp;logo=github&amp;logoColor=fff&amp;color=005AA4)](https://github.com/dani-garcia/vaultwarden/network/members)
[![Stars](https://img.shields.io/github/stars/dani-garcia/vaultwarden.svg?style=flat-square&amp;logo=github&amp;logoColor=fff&amp;color=005AA4)](https://github.com/dani-garcia/vaultwarden/stargazers)
[![Issues Open](https://img.shields.io/github/issues/dani-garcia/vaultwarden.svg?style=flat-square&amp;logo=github&amp;logoColor=fff&amp;color=005AA4&amp;cacheSeconds=300)](https://github.com/dani-garcia/vaultwarden/issues)
[![Issues Closed](https://img.shields.io/github/issues-closed/dani-garcia/vaultwarden.svg?style=flat-square&amp;logo=github&amp;logoColor=fff&amp;color=005AA4&amp;cacheSeconds=300)](https://github.com/dani-garcia/vaultwarden/issues?q=is%3Aissue+is%3Aclosed)
[![AGPL-3.0 Licensed](https://img.shields.io/github/license/dani-garcia/vaultwarden.svg?style=flat-square&amp;logo=vaultwarden&amp;color=944000&amp;cacheSeconds=14400)](https://github.com/dani-garcia/vaultwarden/blob/main/LICENSE.txt) &lt;br&gt;
[![Dependency Status](https://img.shields.io/badge/dynamic/xml?url=https%3A%2F%2Fdeps.rs%2Frepo%2Fgithub%2Fdani-garcia%2Fvaultwarden%2Fstatus.svg&amp;query=%2F*%5Blocal-name()%3D&#039;svg&#039;%5D%2F*%5Blocal-name()%3D&#039;g&#039;%5D%5B2%5D%2F*%5Blocal-name()%3D&#039;text&#039;%5D%5B4%5D&amp;style=flat-square&amp;logo=rust&amp;label=dependencies&amp;color=005AA4)](https://deps.rs/repo/github/dani-garcia/vaultwarden)
[![GHA Release](https://img.shields.io/github/actions/workflow/status/dani-garcia/vaultwarden/release.yml?style=flat-square&amp;logo=github&amp;logoColor=fff&amp;label=Release%20Workflow)](https://github.com/dani-garcia/vaultwarden/actions/workflows/release.yml)
[![GHA Build](https://img.shields.io/github/actions/workflow/status/dani-garcia/vaultwarden/build.yml?style=flat-square&amp;logo=github&amp;logoColor=fff&amp;label=Build%20Workflow)](https://github.com/dani-garcia/vaultwarden/actions/workflows/build.yml) &lt;br&gt;
[![Matrix Chat](https://img.shields.io/matrix/vaultwarden:matrix.org.svg?style=flat-square&amp;logo=matrix&amp;logoColor=fff&amp;color=953B00&amp;cacheSeconds=14400)](https://matrix.to/#/#vaultwarden:matrix.org)
[![GitHub Discussions](https://img.shields.io/github/discussions/dani-garcia/vaultwarden?style=flat-square&amp;logo=github&amp;logoColor=fff&amp;color=953B00&amp;cacheSeconds=300)](https://github.com/dani-garcia/vaultwarden/discussions)
[![Discourse Discussions](https://img.shields.io/discourse/topics?server=https%3A%2F%2Fvaultwarden.discourse.group%2F&amp;style=flat-square&amp;logo=discourse&amp;color=953B00)](https://vaultwarden.discourse.group/)

&gt; [!IMPORTANT]
&gt; **When using this server, please report any bugs or suggestions directly to us (see [Get in touch](#get-in-touch)), regardless of whatever clients you are using (mobile, desktop, browser...). DO NOT use the official Bitwarden support channels.**

&lt;br&gt;

## Features

A nearly complete implementation of the Bitwarden Client API is provided, including:

 * [Personal Vault](https://bitwarden.com/help/managing-items/)
 * [Send](https://bitwarden.com/help/about-send/)
 * [Attachments](https://bitwarden.com/help/attachments/)
 * [Website icons](https://bitwarden.com/help/website-icons/)
 * [Personal API Key](https://bitwarden.com/help/personal-api-key/)
 * [Organizations](https://bitwarden.com/help/getting-started-organizations/)
   - [Collections](https://bitwarden.com/help/about-collections/),
     [Password Sharing](https://bitwarden.com/help/sharing/),
     [Member Roles](https://bitwarden.com/help/user-types-access-control/),
     [Groups](https://bitwarden.com/help/about-groups/),
     [Event Logs](https://bitwarden.com/help/event-logs/),
     [Admin Password Reset](https://bitwarden.com/help/admin-reset/),
     [Directory Connector](https://bitwarden.com/help/directory-sync/),
     [Policies](https://bitwarden.com/help/policies/)
 * [Multi/Two Factor Authentication](https://bitwarden.com/help/bitwarden-field-guide-two-step-login/)
   - [Authenticator](https://bitwarden.com/help/setup-two-step-login-authenticator/),
     [Email](https://bitwarden.com/help/setup-two-step-login-email/),
     [FIDO2 WebAuthn](https://bitwarden.com/help/setup-two-step-login-fido/),
     [YubiKey](https://bitwarden.com/help/setup-two-step-login-yubikey/),
     [Duo](https://bitwarden.com/help/setup-two-step-login-duo/)
 * [Emergency Access](https://bitwarden.com/help/emergency-access/)
 * [Vaultwarden Admin Backend](https://github.com/dani-garcia/vaultwarden/wiki/Enabling-admin-page)
 * [Modified Web Vault client](https://github.com/dani-garcia/bw_web_builds) (Bundled within our containers)

&lt;br&gt;

## Usage

&gt; [!IMPORTANT]
&gt; Most modern web browsers disallow the use of Web Crypto APIs in insecure contexts. In this case, you might get an error like `Cannot read property &#039;importKey&#039;`. To solve this problem, you need to access the web vault via HTTPS or localhost.
&gt;
&gt;This can be configured in [Vaultwarden directly](https://github.com/dani-garcia/vaultwarden/wiki/Enabling-HTTPS) or using a third-party reverse proxy ([some examples](https://github.com/dani-garcia/vaultwarden/wiki/Proxy-examples)).
&gt;
&gt;If you have an available domain name, you can get HTTPS certificates with [Let&#039;s Encrypt](https://letsencrypt.org/), or you can generate self-signed certificates with utilities like [mkcert](https://github.com/FiloSottile/mkcert). Some proxies automatically do this step, like Caddy or Traefik (see examples linked above).

&gt; [!TIP]
&gt;**For more detailed examples on how to install, use and configure Vaultwarden you can check our [Wiki](https://github.com/dani-garcia/vaultwarden/wiki).**

The main way to use Vaultwarden is via our container images which are published to [ghcr.io](https://github.com/dani-garcia/vaultwarden/pkgs/container/vaultwarden), [docker.io](https://hub.docker.com/r/vaultwarden/server) and [quay.io](https://quay.io/repository/vaultwarden/server).

There are also [community driven packages](https://github.com/dani-garcia/vaultwarden/wiki/Third-party-packages) which can be used, but those might be lagging behind the latest version or might deviate in the way Vaultwarden is configured, as described in our [Wiki](https://github.com/dani-garcia/vaultwarden/wiki).

### Docker/Podman CLI

Pull the container image and mount a volume from the host for persistent storage.&lt;br&gt;
You can replace `docker` with `podman` if you prefer to use podman.

```shell
docker pull vaultwarden/server:latest
docker run --detach --name vaultwarden \
  --env DOMAIN=&quot;https://vw.domain.tld&quot; \
  --volume /vw-data/:/data/ \
  --restart unless-stopped \
  --publish 80:80 \
  vaultwarden/server:latest
```

This will preserve any persistent data under `/vw-data/`, you can adapt the path to whatever suits you.

### Docker Compose

To use Docker compose you need to create a `compose.yaml` which will hold the configuration to run the Vaultwarden container.

```yaml
services:
  vaultwarden:
    image: vaultwarden/server:latest
    container_name: vaultwarden
    restart: unless-stopped
    environment:
      DOMAIN: &quot;https://vw.domain.tld&quot;
    volumes:
      - ./vw-data/:/data/
    ports:
      - 80:80
```

&lt;br&gt;

## Get in touch

Have a question, suggestion or need help? Join our community on [Matrix](https://matrix.to/#/#vaultwarden:matrix.org), [GitHub Discussions](https://github.com/dani-garcia/vaultwarden/discussions) or [Discourse Forums](https://vaultwarden.discourse.group/).

Encountered a bug or crash? Please search our issue tracker and discussions to see if it&#039;s already been reported. If not, please [start a new discussion](https://github.com/dani-garcia/vaultwarden/discussions) or [create a new issue](https://github.com/dani-garcia/vaultwarden/issues/). Ensure you&#039;re using the latest version of Vaultwarden and there aren&#039;t any similar issues open or closed!

&lt;br&gt;

## Contributors

Thanks for your contribution to the project!

[![Contributors Count](https://img.shields.io/github/contributors-anon/dani-garcia/vaultwarden?style=for-the-badge&amp;logo=vaultwarden&amp;color=005AA4)](https://github.com/dani-garcia/vaultwarden/graphs/contributors)&lt;br&gt;
[![Contributors Avatars](https://contributors-img.web.app/image?repo=dani-garcia/vaultwarden)](https://github.com/dani-garcia/vaultwarden/graphs/contributors)

&lt;br&gt;

## Disclaimer

**This project is not associated with [Bitwarden](https://bitwarden.com/) or Bitwarden, Inc.**

However, one of the active maintainers for Vaultwarden is employed by Bitwarden and is allowed to contribute to the project on their own time. These contributions are independent of Bitwarden and are reviewed by other maintainers.

The maintainers work together to set the direction for the project, focusing on serving the self-hosting community, including individuals, families, and small organizations, while ensuring the project&#039;s sustainability.

**Please note:** We cannot be held liable for any data loss that may occur while using Vaultwarden. This includes passwords, attachments, and other information handled by the application. We highly recommend performing regular backups of your files and database. However, should you experience data loss, we encourage you to contact us immediately.

&lt;br&gt;

## Bitwarden_RS

This project was known as Bitwarden_RS and has been renamed to separate itself from the official Bitwarden server in the hopes of avoiding confusion and trademark/branding issues.&lt;br&gt;
Please see [#1642 - v1.21.0 release and project rename to Vaultwarden](https://github.com/dani-garcia/vaultwarden/discussions/1642) for more explanation.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[ratatui/ratatui]]></title>
            <link>https://github.com/ratatui/ratatui</link>
            <guid>https://github.com/ratatui/ratatui</guid>
            <pubDate>Mon, 24 Mar 2025 00:05:39 GMT</pubDate>
            <description><![CDATA[A Rust crate for cooking up terminal user interfaces (TUIs) üë®‚Äçüç≥üêÄ https://ratatui.rs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ratatui/ratatui">ratatui/ratatui</a></h1>
            <p>A Rust crate for cooking up terminal user interfaces (TUIs) üë®‚Äçüç≥üêÄ https://ratatui.rs</p>
            <p>Language: Rust</p>
            <p>Stars: 12,314</p>
            <p>Forks: 385</p>
            <p>Stars today: 17 stars today</p>
            <h2>README</h2><pre>&lt;details&gt;
&lt;summary&gt;Table of Contents&lt;/summary&gt;

- [Quickstart](#quickstart)
- [Documentation](#documentation)
- [Templates](#templates)
- [Built with Ratatui](#built-with-ratatui)
- [Alternatives](#alternatives)
- [Contributing](#contributing)
- [Acknowledgements](#acknowledgements)
- [License](#license)

&lt;/details&gt;

![Demo](https://github.com/ratatui/ratatui/blob/87ae72dbc756067c97f6400d3e2a58eeb383776e/examples/demo2-destroy.gif?raw=true)

&lt;div align=&quot;center&quot;&gt;

[![Crate Badge]][Crate] [![Repo Badge]][Repo] [![Docs Badge]][Docs] [![License Badge]][License]  \
[![CI Badge]][CI] [![Deps Badge]][Deps] [![Codecov Badge]][Codecov] [![Sponsors Badge]][Sponsors]  \
[Ratatui Website] ¬∑ [Docs] ¬∑ [Widget Examples] ¬∑ [App Examples] ¬∑ [Changelog]  \
[Breaking Changes] ¬∑ [Contributing] ¬∑ [Report a bug] ¬∑ [Request a Feature]

&lt;/div&gt;

[Ratatui][Ratatui Website] (_Àår√¶.t…ôÀàtu.i_) is a Rust crate for cooking up terminal user interfaces
(TUIs). It provides a simple and flexible way to create text-based user interfaces in the terminal,
which can be used for command-line applications, dashboards, and other interactive console programs.

## Quickstart

Ratatui has [templates] available to help you get started quickly. You can use the
[`cargo-generate`] command to create a new project with Ratatui:

```shell
cargo install --locked cargo-generate
cargo generate ratatui/templates
```

Selecting the Hello World template produces the following application:

```rust
use color_eyre::Result;
use crossterm::event::{self, Event};
use ratatui::{DefaultTerminal, Frame};

fn main() -&gt; Result&lt;()&gt; {
    color_eyre::install()?;
    let terminal = ratatui::init();
    let result = run(terminal);
    ratatui::restore();
    result
}

fn run(mut terminal: DefaultTerminal) -&gt; Result&lt;()&gt; {
    loop {
        terminal.draw(render)?;
        if matches!(event::read()?, Event::Key(_)) {
            break Ok(());
        }
    }
}

fn render(frame: &amp;mut Frame) {
    frame.render_widget(&quot;hello world&quot;, frame.area());
}
```

## Documentation

- [Docs] - the full API documentation for the library on docs.rs.
- [Ratatui Website] - explains the library&#039;s concepts and provides step-by-step tutorials.
- [Ratatui Forum] - a place to ask questions and discuss the library.
- [Widget Examples] - a collection of examples that demonstrate how to use the library.
- [App Examples] - a collection of more complex examples that demonstrate how to build apps.
- [Changelog] - generated by [git-cliff] utilizing [Conventional Commits].
- [Breaking Changes] - a list of breaking changes in the library.

You can also watch the [EuroRust 2024 talk] to learn about common concepts in Ratatui and what&#039;s
possible to build with it.

## Templates

If you&#039;re looking to get started quickly, you can use one of the available templates from the
[templates] repository using [`cargo-generate`]:

```shell
cargo generate ratatui/templates
```

## Built with Ratatui

[![Awesome](https://awesome.re/badge-flat2.svg)][awesome-ratatui]

Check out the [showcase] section of the website, or the [awesome-ratatui] repository for a curated
list of awesome apps and libraries built with Ratatui!

## Alternatives

- [Cursive](https://crates.io/crates/cursive) - a ncurses-based TUI library.
- [iocraft](https://crates.io/crates/iocraft) - a declarative TUI library.

## Contributing

[![Discord Badge]][Discord Server] [![Matrix Badge]][Matrix] [![Forum Badge]][Ratatui Forum]

Feel free to join our [Discord server](https://discord.gg/pMCEU9hNEj) for discussions and questions!
There is also a [Matrix](https://matrix.org/) bridge available at
[#ratatui:matrix.org](https://matrix.to/#/#ratatui:matrix.org). We have also recently launched the
[Ratatui Forum].

We rely on GitHub for [bugs][Report a bug] and [feature requests][Request a Feature].

Please make sure you read the [contributing](./CONTRIBUTING.md) guidelines before [creating a pull
request][Create a Pull Request].

## Acknowledgements

Ratatui was forked from the [tui-rs] crate in 2023 in order to continue its development. None of
this could be possible without [Florian Dehau] who originally created [tui-rs] which inspired many
Rust TUIs.

Special thanks to [Pavel Fomchenkov] for his work in designing an awesome logo for the Ratatui
project and organization.

## License

This project is licensed under the [MIT License][License].

[Repo]: https://github.com/ratatui/ratatui
[Ratatui Website]: https://ratatui.rs/
[Ratatui Forum]: https://forum.ratatui.rs
[Docs]: https://docs.rs/ratatui
[Widget Examples]: https://github.com/ratatui/ratatui/tree/main/ratatui-widgets/examples
[App Examples]: https://github.com/ratatui/ratatui/tree/main/examples
[Changelog]: https://github.com/ratatui/ratatui/blob/main/CHANGELOG.md
[git-cliff]: https://git-cliff.org
[Conventional Commits]: https://www.conventionalcommits.org
[Breaking Changes]: https://github.com/ratatui/ratatui/blob/main/BREAKING-CHANGES.md
[EuroRust 2024 talk]: https://www.youtube.com/watch?v=hWG51Mc1DlM
[Report a bug]: https://github.com/ratatui/ratatui/issues/new?labels=bug&amp;projects=&amp;template=bug_report.md
[Request a Feature]: https://github.com/ratatui/ratatui/issues/new?labels=enhancement&amp;projects=&amp;template=feature_request.md
[Create a Pull Request]: https://github.com/ratatui/ratatui/compare
[Contributing]: https://github.com/ratatui/ratatui/blob/main/CONTRIBUTING.md
[Crate]: https://crates.io/crates/ratatui
[tui-rs]: https://crates.io/crates/tui
[Sponsors]: https://github.com/sponsors/ratatui
[Crate Badge]: https://img.shields.io/crates/v/ratatui?logo=rust&amp;style=flat-square&amp;color=E05D44
[Repo Badge]: https://img.shields.io/badge/repo-ratatui/ratatui-1370D3?style=flat-square&amp;logo=github
[License Badge]: https://img.shields.io/crates/l/ratatui?style=flat-square&amp;color=1370D3
[CI Badge]: https://img.shields.io/github/actions/workflow/status/ratatui/ratatui/ci.yml?style=flat-square&amp;logo=github
[CI]: https://github.com/ratatui/ratatui/actions/workflows/ci.yml
[Codecov Badge]: https://img.shields.io/codecov/c/github/ratatui/ratatui?logo=codecov&amp;style=flat-square&amp;token=BAQ8SOKEST&amp;color=C43AC3
[Codecov]: https://app.codecov.io/gh/ratatui/ratatui
[Deps Badge]: https://deps.rs/repo/github/ratatui/ratatui/status.svg?path=ratatui&amp;style=flat-square
[Deps]: https://deps.rs/repo/github/ratatui/ratatui?path=ratatui
[Discord Badge]: https://img.shields.io/discord/1070692720437383208?label=discord&amp;logo=discord&amp;style=flat-square&amp;color=1370D3&amp;logoColor=1370D3
[Discord Server]: https://discord.gg/pMCEU9hNEj
[Docs Badge]: https://img.shields.io/badge/docs-ratatui-1370D3?style=flat-square&amp;logo=rust
[Matrix Badge]: https://img.shields.io/matrix/ratatui-general%3Amatrix.org?style=flat-square&amp;logo=matrix&amp;label=Matrix&amp;color=C43AC3
[Matrix]: https://matrix.to/#/#ratatui:matrix.org
[Forum Badge]: https://img.shields.io/discourse/likes?server=https%3A%2F%2Fforum.ratatui.rs&amp;style=flat-square&amp;logo=discourse&amp;label=forum&amp;color=C43AC3
[Sponsors Badge]: https://img.shields.io/github/sponsors/ratatui?logo=github&amp;style=flat-square&amp;color=1370D3
[templates]: https://github.com/ratatui/templates/
[showcase]: https://ratatui.rs/showcase/
[awesome-ratatui]: https://github.com/ratatui/awesome-ratatui
[Pavel Fomchenkov]: https://github.com/nawok
[Florian Dehau]: https://github.com/fdehau
[`cargo-generate`]: https://crates.io/crates/cargo-generate
[License]: ./LICENSE
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[gitbutlerapp/gitbutler]]></title>
            <link>https://github.com/gitbutlerapp/gitbutler</link>
            <guid>https://github.com/gitbutlerapp/gitbutler</guid>
            <pubDate>Mon, 24 Mar 2025 00:05:38 GMT</pubDate>
            <description><![CDATA[The GitButler version control client, backed by Git, powered by Tauri/Rust/Svelte]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/gitbutlerapp/gitbutler">gitbutlerapp/gitbutler</a></h1>
            <p>The GitButler version control client, backed by Git, powered by Tauri/Rust/Svelte</p>
            <p>Language: Rust</p>
            <p>Stars: 14,777</p>
            <p>Forks: 582</p>
            <p>Stars today: 44 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
   &lt;img align=&quot;center&quot; width=&quot;128px&quot; src=&quot;crates/gitbutler-tauri/icons/128x128@2x.png&quot; /&gt;
	&lt;h1 align=&quot;center&quot;&gt;&lt;b&gt;GitButler&lt;/b&gt;&lt;/h1&gt;
	&lt;p align=&quot;center&quot;&gt;
		Git branch management tool, built from the ground up for modern workflows
    &lt;br /&gt;
    &lt;a href=&quot;https://gitbutler.com&quot;&gt;&lt;strong&gt;gitbutler.com ¬ª&lt;/strong&gt;&lt;/a&gt;
    &lt;br /&gt;
    &lt;br /&gt;
    &lt;b&gt;Download for &lt;/b&gt;
    macOS (&lt;a href=&quot;https://app.gitbutler.com/downloads/release/darwin/aarch64/dmg&quot;&gt;Apple Silicon&lt;/a&gt; |
      &lt;a href=&quot;https://app.gitbutler.com/downloads/release/darwin/x86_64/dmg&quot;&gt;Intel&lt;/a&gt;) ¬∑
		Linux (&lt;a href=&quot;https://app.gitbutler.com/downloads/release/linux/x86_64/gz&quot;&gt;AppImage&lt;/a&gt; |
       &lt;a href=&quot;https://app.gitbutler.com/downloads/release/linux/x86_64/deb&quot;&gt;deb&lt;/a&gt;)
      ¬∑
		Windows (&lt;a href=&quot;https://app.gitbutler.com/downloads/release/windows/x86_64/msi&quot;&gt;msi&lt;/a&gt;)
    &lt;br /&gt;
    &lt;br /&gt;
    (Unstable Nightly releases can be found &lt;a href=&quot;https://app.gitbutler.com/downloads&quot;&gt;here&lt;/a&gt;)
  &lt;/p&gt;
&lt;/div&gt;

&lt;br/&gt;

![gitbutler_client](https://github.com/gitbutlerapp/gitbutler/assets/16262535/0d180bfd-7a3c-4a66-a923-30534c5a8197)

[![CI][s0]][l0] [![BADGE][s6]][l6] [![TWEET][s1]][l1] [![DISCORD][s2]][l2] [![INSTA][s3]][l3] [![YOUTUBE][s5]][l5]

[s0]: https://github.com/gitbutlerapp/gitbutler/actions/workflows/push.yaml/badge.svg
[l0]: https://github.com/gitbutlerapp/gitbutler/actions/workflows/push.yaml
[s1]: https://img.shields.io/badge/Twitter-black?logo=x&amp;logoColor=white
[l1]: https://twitter.com/intent/follow?screen_name=gitbutler
[s2]: https://img.shields.io/discord/1060193121130000425?label=Discord&amp;color=5865F2
[l2]: https://discord.gg/MmFkmaJ42D
[s3]: https://img.shields.io/badge/Instagram-E4405F?logo=instagram&amp;logoColor=white
[l3]: https://www.instagram.com/gitbutler/
[s5]: https://img.shields.io/youtube/channel/subscribers/UCEwkZIHGqsTGYvX8wgD0LoQ
[l5]: https://www.youtube.com/@gitbutlerapp
[s6]: https://img.shields.io/badge/GitButler-%23B9F4F2?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB3aWR0aD0iMzkiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCAzOSAyOCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTI1LjIxNDUgMTIuMTk5N0wyLjg3MTA3IDEuMzg5MTJDMS41NDI5NSAwLjc0NjUzMiAwIDEuNzE0MDYgMCAzLjE4OTQ3VjI0LjgxMDVDMCAyNi4yODU5IDEuNTQyOTUgMjcuMjUzNSAyLjg3MTA3IDI2LjYxMDlMMjUuMjE0NSAxNS44MDAzQzI2LjcxOTcgMTUuMDcyMSAyNi43MTk3IDEyLjkyNzkgMjUuMjE0NSAxMi4xOTk3WiIgZmlsbD0iYmxhY2siLz4KPHBhdGggZD0iTTEzLjc4NTUgMTIuMTk5N0wzNi4xMjg5IDEuMzg5MTJDMzcuNDU3MSAwLjc0NjUzMiAzOSAxLjcxNDA2IDM5IDMuMTg5NDdWMjQuODEwNUMzOSAyNi4yODU5IDM3LjQ1NzEgMjcuMjUzNSAzNi4xMjg5IDI2LjYxMDlMMTMuNzg1NSAxNS44MDAzQzEyLjI4MDMgMTUuMDcyMSAxMi4yODAzIDEyLjkyNzkgMTMuNzg1NSAxMi4xOTk3WiIgZmlsbD0idXJsKCNwYWludDBfcmFkaWFsXzMxMF8xMjkpIi8%2BCjxkZWZzPgo8cmFkaWFsR3JhZGllbnQgaWQ9InBhaW50MF9yYWRpYWxfMzEwXzEyOSIgY3g9IjAiIGN5PSIwIiByPSIxIiBncmFkaWVudFVuaXRzPSJ1c2VyU3BhY2VPblVzZSIgZ3JhZGllbnRUcmFuc2Zvcm09InRyYW5zbGF0ZSgxNi41NzAxIDE0KSBzY2FsZSgxOS44NjQxIDE5LjgzODMpIj4KPHN0b3Agb2Zmc2V0PSIwLjMwMTA1NiIgc3RvcC1vcGFjaXR5PSIwIi8%2BCjxzdG9wIG9mZnNldD0iMSIvPgo8L3JhZGlhbEdyYWRpZW50Pgo8L2RlZnM%2BCjwvc3ZnPgo%3D
[l6]: https://gitbutler.com/

![Alt](https://repobeats.axiom.co/api/embed/fb23382bcf57c609832661874d3019a43555d6ae.svg &#039;Repobeats analytics for GitButler&#039;)

GitButler is a git client that lets you work on multiple branches at the same time.
It allows you to quickly organize file changes into separate branches while still having them applied to your working directory.
You can then push branches individually to your remote, or directly create pull requests.

In a nutshell, it&#039;s a more flexible version of `git add -p` and `git rebase -i`, allowing you to efficiently multitask across branches.

## How Does It Work?

GitButler keeps track of uncommitted changes in a layer on top of Git. Changes to files or parts of files can be grouped into what we call virtual branches. Whenever you are happy with the contents of a virtual branch, you can push it to a remote. GitButler makes sure that the state of other virtual branches is kept separate.

## How Do GB&#039;s Virtual Branches Differ From Git Branches?

The branches that we know and love in Git are separate universes, and switching between them is a full context switch. GitButler allows you to work with multiple branches in parallel in the same working directory. This effectively means having the content of multiple branches available at the same time.

GitButler is aware of changes before they are committed. This allows it to keep a record of which virtual branch each individual diff belongs to. Effectively, this means that you can separate out individual branches with their content at any time to push them to a remote or to unapply them from your working directory.

And finally, while in Git it is preferable that you create your desired branch ahead of time, using GitButler you can move changes between virtual branches at any point during development.

## Why GitButler?

We love Git. Our own [@schacon](https://github.com/schacon) has even published the [Pro Git](https://git-scm.com/book/en/v2) book. At the same time, Git&#039;s user interface hasn&#039;t been fundamentally changed for 15 years. While it was written for Linux kernel devs sending patches to each other over mailing lists, most developers today have different workflows and needs.

Instead of trying to fit the semantics of the Git CLI into a graphical interface, we are starting with the developer workflow and mapping it back to Git.

## Tech

GitButler is a [Tauri](https://tauri.app/)-based application. Its UI is written in [Svelte](https://svelte.dev/) using [TypeScript](https://www.typescriptlang.org) and its backend is written in [Rust](https://www.rust-lang.org/).

## Main Features

- **Virtual Branches**
  - Organize work on multiple branches simultaneously, rather than constantly switching branches
  - Automatically create new branches when needed
- **Easy Commit Management**
  - Undo, Amend and Squash commits by dragging and dropping
- **Undo Timeline**
  - Logs all operations and changes and allows you to easily undo or revert any operation
- **GitHub Integration**
  - Authenticate to GitHub to open Pull Requests, list branches and statuses and more
- **Easy SSH Key Management**
  - GitButler can generate an SSH key to upload to GitHub automatically
- **AI Tooling**
  - Automatically write commit messages based on your work in progress
  - Automatically create descriptive branch names
- **Commit Signing**
  - Easy commit signing with GPG or SSH

## Example Uses

### Fixing a Bug While Working on a Feature

&gt; Say that while developing a feature, you encounter a bug that you wish to fix. It&#039;s often desirable that you ship the fix as a separate contribution (Pull request).

Using Git you can stash your changes and switch to another branch, where you can commit, and push your fix.

_With GitButler_ you simply assign your fix to a separate virtual branch, which you can individually push (or directly create a PR). An additional benefit is that you can retain the fix in your working directory while waiting for CI and/or code review.

### Trying Someone Else&#039;s Branch Together With My Work in Progress

&gt; Say you want to test a branch from someone else for the purpose of code review.

Using Git trying out someone else&#039;s branch is a full context switch away from your own work.
_With GitButler_ you can apply and unapply (add / remove) any remote branch directly into your working directory.

## Documentation

You can find our end user documentation at: https://docs.gitbutler.com

## Bugs and Feature Requests

If you have a bug or feature request, feel free to open an [issue](https://github.com/gitbutlerapp/gitbutler/issues/new),
or [join our Discord server](https://discord.gg/MmFkmaJ42D).

## AI Commit Message Generation

Commit message generation is an opt-in feature. You can enable it while adding your repository for the first time or later in the project settings.

Currently, GitButler uses OpenAI&#039;s API for diff summarization, which means that if enabled, code diffs would be sent to OpenAI&#039;s servers.

Our goal is to make this feature more modular such that in the future you can modify the prompt as well as plug a different LLM endpoints (including local ones).

## Contributing

So you want to help out? Please check out the [CONTRIBUTING.md](CONTRIBUTING.md)
document.

If you want to skip right to getting the code to actually compile, take a look
at the [DEVELOPMENT.md](DEVELOPMENT.md) file.

Want to show your support? Add a GitButler badge to your project&#039;s README:

```md
[![GitButler](https://img.shields.io/badge/GitButler-%23B9F4F2?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB3aWR0aD0iMzkiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCAzOSAyOCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTI1LjIxNDUgMTIuMTk5N0wyLjg3MTA3IDEuMzg5MTJDMS41NDI5NSAwLjc0NjUzMiAwIDEuNzE0MDYgMCAzLjE4OTQ3VjI0LjgxMDVDMCAyNi4yODU5IDEuNTQyOTUgMjcuMjUzNSAyLjg3MTA3IDI2LjYxMDlMMjUuMjE0NSAxNS44MDAzQzI2LjcxOTcgMTUuMDcyMSAyNi43MTk3IDEyLjkyNzkgMjUuMjE0NSAxMi4xOTk3WiIgZmlsbD0iYmxhY2siLz4KPHBhdGggZD0iTTEzLjc4NTUgMTIuMTk5N0wzNi4xMjg5IDEuMzg5MTJDMzcuNDU3MSAwLjc0NjUzMiAzOSAxLjcxNDA2IDM5IDMuMTg5NDdWMjQuODEwNUMzOSAyNi4yODU5IDM3LjQ1NzEgMjcuMjUzNSAzNi4xMjg5IDI2LjYxMDlMMTMuNzg1NSAxNS44MDAzQzEyLjI4MDMgMTUuMDcyMSAxMi4yODAzIDEyLjkyNzkgMTMuNzg1NSAxMi4xOTk3WiIgZmlsbD0idXJsKCNwYWludDBfcmFkaWFsXzMxMF8xMjkpIi8%2BCjxkZWZzPgo8cmFkaWFsR3JhZGllbnQgaWQ9InBhaW50MF9yYWRpYWxfMzEwXzEyOSIgY3g9IjAiIGN5PSIwIiByPSIxIiBncmFkaWVudFVuaXRzPSJ1c2VyU3BhY2VPblVzZSIgZ3JhZGllbnRUcmFuc2Zvcm09InRyYW5zbGF0ZSgxNi41NzAxIDE0KSBzY2FsZSgxOS44NjQxIDE5LjgzODMpIj4KPHN0b3Agb2Zmc2V0PSIwLjMwMTA1NiIgc3RvcC1vcGFjaXR5PSIwIi8%2BCjxzdG9wIG9mZnNldD0iMSIvPgo8L3JhZGlhbEdyYWRpZW50Pgo8L2RlZnM%2BCjwvc3ZnPgo%3D)](https://gitbutler.com/)
```

[![BADGE][s6]][l6]
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
    </channel>
</rss>