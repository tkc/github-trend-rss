<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for rust - Rust Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for rust.</description>
        <lastBuildDate>Sun, 01 Feb 2026 00:07:24 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2026, GitHub</copyright>
        <item>
            <title><![CDATA[jj-vcs/jj]]></title>
            <link>https://github.com/jj-vcs/jj</link>
            <guid>https://github.com/jj-vcs/jj</guid>
            <pubDate>Sun, 01 Feb 2026 00:07:24 GMT</pubDate>
            <description><![CDATA[A Git-compatible VCS that is both simple and powerful]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jj-vcs/jj">jj-vcs/jj</a></h1>
            <p>A Git-compatible VCS that is both simple and powerful</p>
            <p>Language: Rust</p>
            <p>Stars: 25,346</p>
            <p>Forks: 901</p>
            <p>Stars today: 48 stars today</p>
            <h2>README</h2><pre>&lt;div class=&quot;title-block&quot; style=&quot;text-align: center;&quot; align=&quot;center&quot;&gt;

# Jujutsuâ€”a version control system

&lt;p&gt;&lt;img title=&quot;jj logo&quot; src=&quot;docs/images/jj-logo.svg&quot; width=&quot;320&quot; height=&quot;320&quot;&gt;&lt;/p&gt;

[![Release](https://img.shields.io/github/v/release/martinvonz/jj)](https://github.com/jj-vcs/jj/releases)
[![Release date](https://img.shields.io/github/release-date/martinvonz/jj)](https://github.com/jj-vcs/jj/releases)
&lt;br/&gt;
[![License](https://img.shields.io/github/license/martinvonz/jj)](https://github.com/jj-vcs/jj/blob/main/LICENSE)
[![Discord](https://img.shields.io/discord/968932220549103686.svg?label=&amp;logo=discord&amp;logoColor=ffffff&amp;color=7389D8&amp;labelColor=6A7EC2)](https://discord.gg/dkmfj3aGQN)
[![IRC](https://img.shields.io/badge/irc-%23jujutsu-blue.svg)](https://web.libera.chat/?channel=#jujutsu)

**[Homepage] &amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;**
**[Installation] &amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;**
**[Getting Started] &amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;**
**[Development Roadmap] &amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;**
**[Contributing](#contributing)**

[Homepage]: https://www.jj-vcs.dev
[Installation]: https://docs.jj-vcs.dev/latest/install-and-setup
[Getting Started]: https://docs.jj-vcs.dev/latest/tutorial
[Development Roadmap]: https://docs.jj-vcs.dev/latest/roadmap

&lt;/div&gt;

## Introduction

Jujutsu is a powerful [version control system](https://en.wikipedia.org/wiki/Version_control)
for software projects. You use it to get a copy of your code, track changes
to the code, and finally publish those changes for others to see and use.
It is designed from the ground up to be easy to useâ€”whether you&#039;re new or
experienced, working on brand new projects alone, or large scale software
projects with large histories and teams.

Jujutsu is unlike most other systems, because internally it abstracts the user
interface and version control algorithms from the *storage systems* used to
serve your content. This allows it to serve as a VCS with many possible physical
backends, that may have their own data or networking modelsâ€”like [Mercurial] or
[Breezy], or hybrid systems like Google&#039;s cloud-based design, [Piper/CitC].

[Mercurial]: https://www.mercurial-scm.org/
[Breezy]: https://www.breezy-vcs.org/
[Piper/CitC]: https://youtu.be/W71BTkUbdqE?t=645

Today, we use Git repositories as a storage layer to serve and track content,
making it **compatible with many of your favorite Git-based tools, right now!**
However, note that only commits and files are stored in Git; bookmarks
(branches) and other higher-level metadata are stored in custom storage outside
of Git. All core developers use Jujutsu to develop Jujutsu, right here on
GitHub. But it should hopefully work with your favorite Git forges, too.

We combine many distinct design choices and concepts from other version control
systems into a single tool. Some of those sources of inspiration include:

- **Git**: We make an effort to [be fast][perf]â€”with a snappy UX, efficient
  algorithms, correct data structures, and good-old-fashioned attention to
  detail. The default storage backend uses Git repositories for &quot;physical
  storage&quot;, for wide interoperability and ease of onboarding.

- **Mercurial &amp; Sapling**: There are many Mercurial-inspired features, such as
  the [revset] language to select commits. There is [no explicit index][no-index]
  or staging area. Branches are &quot;anonymous&quot; like Mercurial, so you don&#039;t need
  to make up a name for each small change. Primitives for rewriting history are
  powerful and simple. Formatting output is done with a robust template language
  that can be configured by the user.

- **Darcs**: Jujutsu keeps track of conflicts as [first-class
  objects][conflicts] in its model; they are first-class in the same way commits
  are, while alternatives like Git simply think of conflicts as textual diffs.
  While not as rigorous as systems like Darcs (which is based on a formalized
  theory of patches, as opposed to snapshots), the effect is that many forms of
  conflict resolution can be performed and propagated automatically.

[perf]: https://github.com/jj-vcs/jj/discussions/49
[revset]: https://docs.jj-vcs.dev/latest/revsets/
[no-index]: https://docs.jj-vcs.dev/latest/git-comparison/#the-index
[conflicts]: https://docs.jj-vcs.dev/latest/conflicts/

And it adds several innovative, useful features of its own:

- **Working-copy-as-a-commit**: Changes to files are [recorded automatically][wcc]
  as normal commits, and amended on every subsequent change. This &quot;snapshot&quot;
  design simplifies the user-facing data model (commits are the only visible
  object), simplifies internal algorithms, and completely subsumes features like
  Git&#039;s stashes or the index/staging-area.

- **Operation log &amp; undo**: Jujutsu records every operation that is performed on the
  repository, from commits, to pulls, to pushes. This makes debugging problems like
  &quot;what just happened?&quot; or &quot;how did I end up here?&quot; easier, *especially* when
  you&#039;re helping your coworker answer those questions about their repository!
  And because everything is recorded, you can undo that mistake you just made
  with ease. Version control has finally entered [the 1960s][undo-history]!

- **Automatic rebase and conflict resolution**: When you modify a commit, every
  descendent is automatically rebased on top of the freshly-modified one. This
  makes &quot;patch-based&quot; workflows a breeze. If you resolve a conflict in a commit,
  the _resolution_ of that conflict is also propagated through descendants as
  well. In effect, this is a completely transparent version of `git rebase
  --update-refs` combined with `git rerere`, supported by design.

&gt; [!WARNING]
&gt; The following features are available for use, but experimental; they may have
&gt; bugs, backwards incompatible storage changes, and user-interface changes!

- **Safe, concurrent replication**: Have you ever wanted to store your version
  controlled repositories inside a Dropbox folder? Or continuously backup
  repositories to S3? No? Well, now you can!

  The fundamental problem with using filesystems like Dropbox and backup tools
  like `rsync` on your typical Git/Mercurial repositories is that they rely
  on *local filesystem operations* being atomic, serialized, and non-concurrent
  with respect to other reads and writesâ€”which is _not_ true when operating on
  distributed file systems, or when operations like concurrent file copies (for
  backup) happen while lock files are being held.

  Jujutsu is instead designed to be [safe under concurrent scenarios][conc-safety];
  simply using rsync or Dropbox and then using that resulting repository
  should never result in a repository in a *corrupt state*. The worst that
  _should_ happen is that it will expose conflicts between the local and remote
  state, leaving you to resolve them.

[wcc]: https://docs.jj-vcs.dev/latest/working-copy/
[undo-history]: https://en.wikipedia.org/wiki/Undo#History
[conc-safety]: https://docs.jj-vcs.dev/latest/technical/concurrency/

The command-line tool is called `jj` for now because it&#039;s easy to type and easy
to replace (rare in English). The project is called &quot;Jujutsu&quot; because it matches
&quot;jj&quot;.

Jujutsu is relatively young, with lots of work to still be done. If you have any
questions, or want to talk about future plans, please join us on Discord
[![Discord](https://img.shields.io/discord/968932220549103686.svg?label=&amp;logo=discord&amp;logoColor=ffffff&amp;color=7389D8&amp;labelColor=6A7EC2)](https://discord.gg/dkmfj3aGQN),
start a [GitHub Discussion](https://github.com/jj-vcs/jj/discussions), or
send an IRC message to [`#jujutsu` on Libera
Chat](https://web.libera.chat/?channel=#jujutsu). The developers monitor all of
these channels[^bridge].

[^bridge]: To be more precise, the `#jujutsu` Libera IRC channel is bridged to
one of the channels on jj&#039;s Discord. Some of the developers stay on Discord and
use the bridge to follow IRC.

### News and Updates ğŸ“£

- **December 2024**: The `jj` Repository has moved to the `jj-vcs` GitHub
  organization.
- **November 2024**: Version 0.24 is released which adds `jj file annotate`,
  which is equivalent to `git blame` or `hg annotate`.
- **September 2024**: Martin gave a [presentation about Jujutsu][merge-vid-2024] at
  Git Merge 2024.
- **Feb 2024**: Version 0.14 is released, which deprecates [&quot;jj checkout&quot; and &quot;jj merge&quot;](CHANGELOG.md#0140---2024-02-07),
  as well as `jj init --git`, which is now just called `jj git init`.
- **Oct 2023**: Version 0.10.0 is released! Now includes a bundled merge and
  diff editor for all platforms, &quot;immutable revsets&quot; to avoid accidentally
  `edit`-ing the wrong revisions, and lots of polish.
- **Jan 2023**: Martin gave a presentation about Google&#039;s plans for Jujutsu at
  Git Merge 2022!
  See the [slides][merge-slides] or the [recording][merge-talk].

### Related Media

- **Mar 2024**: Chris Krycho started [a YouTube series about Jujutsu][krycho-yt].
- **Feb 2024**: Chris Krycho published an article about Jujutsu called [jj init][krycho]
  and Steve Klabnik followed up with the [Jujutsu Tutorial][klabnik].
- **Jan 2024**: Jujutsu was featured in an LWN.net article called
  [Jujutsu: a new, Git-compatible version control system][lwn].
- **Jan 2023**: Martin&#039;s Talk about Jujutsu at Git Merge 2022, [video][merge-talk]
  and the associated [slides][merge-slides].

The wiki also contains a more extensive list of [media references][wiki-media].

[krycho-yt]: https://www.youtube.com/playlist?list=PLelyiwKWHHAq01Pvmpf6x7J0y-yQpmtxp
[krycho]: https://v5.chriskrycho.com/essays/jj-init/
[klabnik]: https://steveklabnik.github.io/jujutsu-tutorial/
[lwn]: https://lwn.net/Articles/958468/
[merge-talk]: https://www.youtube.com/watch?v=bx_LGilOuE4
[merge-slides]: https://docs.google.com/presentation/d/1F8j9_UOOSGUN9MvHxPZX_L4bQ9NMcYOp1isn17kTC_M/view
[merge-vid-2024]: https://www.youtube.com/watch?v=LV0JzI8IcCY
[wiki-media]: https://github.com/jj-vcs/jj/wiki/Media

## Getting started

&gt; [!IMPORTANT]
&gt; Jujutsu is an **experimental version control system**. While Git compatibility
&gt; is stable, and most developers use it daily for all their needs, there may
&gt; still be work-in-progress features, suboptimal UX, and workflow gaps that make
&gt; it unusable for your particular use.

Follow the [installation
instructions](https://docs.jj-vcs.dev/latest/install-and-setup) to
obtain and configure `jj`.

The best way to get started is probably to go through [the
tutorial](https://docs.jj-vcs.dev/latest/tutorial). Also see the [Git
comparison](https://docs.jj-vcs.dev/latest/git-comparison), which
includes a table of `jj` vs. `git` commands.

As you become more familiar with Jujutsu, the following resources may be helpful:

- The [FAQ](https://docs.jj-vcs.dev/latest/FAQ).
- The [Glossary](https://docs.jj-vcs.dev/latest/glossary).
- The `jj help` command (e.g. `jj help rebase`).
- The `jj help -k &lt;keyword&gt;` command (e.g. `jj help -k config`). Use `jj help --help`
  to see what keywords are available.

If you are using a **prerelease** version of `jj`, you would want to consult
[the docs for the prerelease (main branch)
version](https://docs.jj-vcs.dev/prerelease/). You can also get there
from the docs for the latest release by using the website&#039;s version switcher. The version switcher is visible in
the header of the website when you scroll to the top of any page.

## Features

### Compatible with Git

Jujutsu is designed so that the underlying data and storage model is abstract.
Today, only the Git backend is production-ready. The Git backend uses the
[gitoxide](https://github.com/Byron/gitoxide) Rust library.

[backends]: https://docs.jj-vcs.dev/latest/glossary#backend

The Git backend is fully featured and maintained, and allows you to use Jujutsu
with any Git remote. The commits you create will look like regular Git commits.
You can fetch branches from a regular Git remote and push branches to the
remote. You can always switch back to Git.

Here is how you can explore a GitHub repository with `jj`.

&lt;img src=&quot;demos/git_compat.png&quot; /&gt;

You can even have a [colocated local
workspace](https://docs.jj-vcs.dev/latest/git-compatibility#colocated-jujutsugit-repos)
where you can use both `jj` and `git` commands interchangeably.

### The working copy is automatically committed

Jujutsu uses a real commit to represent the working copy. Checking out a commit
results in a new working-copy commit on top of the target commit. Almost all
commands automatically amend the working-copy commit.

The working-copy being a commit means that commands never fail because the
working copy is dirty (no &quot;error: Your local changes to the following
files...&quot;), and there is no need for `git stash`. Also, because the working copy
is a commit, commands work the same way on the working-copy commit as on any
other commit, so you can set the commit message before you&#039;re done with the
changes.

&lt;img src=&quot;demos/working_copy.png&quot; /&gt;

### The repo is the source of truth

With Jujutsu, the working copy plays a smaller role than with Git. Commands
snapshot the working copy before they start, then they update the repo, and then
the working copy is updated (if the working-copy commit was modified). Almost
all commands (even checkout!) operate on the commits in the repo, leaving the
common functionality of snapshotting and updating of the working copy to
centralized code. For example, `jj restore` (similar to `git restore`) can
restore from any commit and into any commit, and `jj describe` can set the
commit message of any commit (defaults to the working-copy commit).

### Entire repo is under version control

All operations you perform in the repo are recorded, along with a snapshot of
the repo state after the operation. This means that you can easily restore to
an earlier repo state, simply undo your operations one-by-one or even _revert_ a
particular operation which does not have to be the most recent one.

&lt;img src=&quot;demos/operation_log.png&quot; /&gt;

### Conflicts can be recorded in commits

If an operation results in
[conflicts](https://docs.jj-vcs.dev/latest/glossary#conflict),
information about those conflicts will be recorded in the commit(s). The
operation will succeed. You can then resolve the conflicts later. One
consequence of this design is that there&#039;s no need to continue interrupted
operations. Instead, you get a single workflow for resolving conflicts,
regardless of which command caused them. This design also lets Jujutsu rebase
merge commits correctly (unlike both Git and Mercurial).

Basic conflict resolution:

&lt;img src=&quot;demos/resolve_conflicts.png&quot; /&gt;

Juggling conflicts:

&lt;img src=&quot;demos/juggle_conflicts.png&quot; /&gt;

### Automatic rebase

Whenever you modify a commit, any descendants of the old commit will be rebased
onto the new commit. Thanks to the conflict design described above, that can be
done even if there are conflicts. Bookmarks pointing to rebased commits will be
updated. So will the working copy if it points to a rebased commit.

### Comprehensive support for rewriting history

Besides the usual rebase command, there&#039;s `jj describe` for editing the
description (commit message) of an arbitrary commit. There&#039;s also `jj diffedit`,
which lets you edit the changes in a commit without checking it out. To split
a commit into two, use `jj split`. You can even move part of the changes in a
commit to any other commit using `jj squash -i --from X --into Y`.

## Status

The tool is fairly feature-complete, but some important features like support
for Git submodules are not yet completed. There
are also several performance bugs. It&#039;s likely that workflows and setups
different from what the core developers use are not well supported, e.g. there
is no native support for email-based workflows.

Today, all core developers use `jj` to work on `jj`. I (Martin von Zweigbergk)
have almost exclusively used `jj` to develop the project itself since early
January 2021. I haven&#039;t had to re-clone from source (I don&#039;t think I&#039;ve even had
to restore from backup).

There *will* be changes to workflows and backward-incompatible changes to the
on-disk formats before version 1.0.0. For any format changes, we&#039;ll try to
implement transparent upgrades (as we&#039;ve done with recent changes), or provide
upgrade commands or scripts if requested.

## Related work

There are several tools trying to solve similar problems as Jujutsu. See
[related work](https://docs.jj-vcs.dev/latest/related-work) for details.

## Contributing

We welcome outside contributions, and there&#039;s plenty of things to do, so
don&#039;t be shy. Please ask if you want a pointer on something you can help with,
and hopefully we can all figure something out.

We do have [a few policies and
suggestions](https://docs.jj-vcs.dev/prerelease/contributing/)
for contributors. The broad TL;DR:

- Bug reports are very welcome!
- Every commit that lands in the `main` branch is code reviewed.
- Please behave yourself, and obey the Community Guidelines.
- There **is** a mandatory CLA you must agree to. Importantly, it **does not**
  transfer copyright ownership to Google or anyone else; it simply gives us the
  right to safely redistribute and use your changes.

### Mandatory Google Disclaimer

I (Martin von Zweigbergk, &lt;martinvonz@google.com&gt;) started Jujutsu as a hobby
project in late 2019, and it has evolved into my full-time project at Google,
with several other Googlers (now) assisting development in various capacities.
That said, while Google is one of the main contributors, **this is not a
supported Google product**, i.e. support is provided by the community.

## License

Jujutsu is available as Open Source Software, under the Apache 2.0 license. See
[`LICENSE`](./LICENSE) for details about copyright and redistribution.

The `jj` logo was contributed by J. Jennings and is licensed under a Creative
Commons License, see [`docs/images/LICENSE`](docs/images/LICENSE).
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[YaLTeR/niri]]></title>
            <link>https://github.com/YaLTeR/niri</link>
            <guid>https://github.com/YaLTeR/niri</guid>
            <pubDate>Sun, 01 Feb 2026 00:07:23 GMT</pubDate>
            <description><![CDATA[A scrollable-tiling Wayland compositor.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/YaLTeR/niri">YaLTeR/niri</a></h1>
            <p>A scrollable-tiling Wayland compositor.</p>
            <p>Language: Rust</p>
            <p>Stars: 17,978</p>
            <p>Forks: 669</p>
            <p>Stars today: 42 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;&lt;img alt=&quot;niri&quot; src=&quot;https://github.com/user-attachments/assets/07d05cd0-d5dc-4a28-9a35-51bae8f119a0&quot;&gt;&lt;/h1&gt;
&lt;p align=&quot;center&quot;&gt;A scrollable-tiling Wayland compositor.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://matrix.to/#/#niri:matrix.org&quot;&gt;&lt;img alt=&quot;Matrix&quot; src=&quot;https://img.shields.io/badge/matrix-%23niri-blue?logo=matrix&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/YaLTeR/niri/blob/main/LICENSE&quot;&gt;&lt;img alt=&quot;GitHub License&quot; src=&quot;https://img.shields.io/github/license/YaLTeR/niri&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/YaLTeR/niri/releases&quot;&gt;&lt;img alt=&quot;GitHub Release&quot; src=&quot;https://img.shields.io/github/v/release/YaLTeR/niri?logo=github&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://yalter.github.io/niri/Getting-Started.html&quot;&gt;Getting Started&lt;/a&gt; | &lt;a href=&quot;https://yalter.github.io/niri/Configuration%3A-Introduction.html&quot;&gt;Configuration&lt;/a&gt; | &lt;a href=&quot;https://github.com/YaLTeR/niri/discussions/325&quot;&gt;Setup&amp;nbsp;Showcase&lt;/a&gt;
&lt;/p&gt;

![niri with a few windows open](https://github.com/user-attachments/assets/535e6530-2f44-4b84-a883-1240a3eee6e9)

## About

Windows are arranged in columns on an infinite strip going to the right.
Opening a new window never causes existing windows to resize.

Every monitor has its own separate window strip.
Windows can never &quot;overflow&quot; onto an adjacent monitor.

Workspaces are dynamic and arranged vertically.
Every monitor has an independent set of workspaces, and there&#039;s always one empty workspace present all the way down.

The workspace arrangement is preserved across disconnecting and connecting monitors where it makes sense.
When a monitor disconnects, its workspaces will move to another monitor, but upon reconnection they will move back to the original monitor.

## Features

- Built from the ground up for scrollable tiling
- [Dynamic workspaces](https://yalter.github.io/niri/Workspaces.html) like in GNOME
- An [Overview](https://github.com/user-attachments/assets/379a5d1f-acdb-4c11-b36c-e85fd91f0995) that zooms out workspaces and windows
- Built-in screenshot UI
- Monitor and window screencasting through xdg-desktop-portal-gnome
    - You can [block out](https://yalter.github.io/niri/Configuration%3A-Window-Rules.html#block-out-from) sensitive windows from screencasts
    - [Dynamic cast target](https://yalter.github.io/niri/Screencasting.html#dynamic-screencast-target) that can change what it shows on the go
- [Touchpad](https://github.com/YaLTeR/niri/assets/1794388/946a910e-9bec-4cd1-a923-4a9421707515) and [mouse](https://github.com/YaLTeR/niri/assets/1794388/8464e65d-4bf2-44fa-8c8e-5883355bd000) gestures
- Group windows into [tabs](https://yalter.github.io/niri/Tabs.html)
- Configurable layout: gaps, borders, struts, window sizes
- [Gradient borders](https://yalter.github.io/niri/Configuration%3A-Layout.html#gradients) with Oklab and Oklch support
- [Animations](https://github.com/YaLTeR/niri/assets/1794388/ce178da2-af9e-4c51-876f-8709c241d95e) with support for [custom shaders](https://github.com/YaLTeR/niri/assets/1794388/27a238d6-0a22-4692-b794-30dc7a626fad)
- Live-reloading config
- Works with [screen readers](https://yalter.github.io/niri/Accessibility.html)

## Video Demo

https://github.com/YaLTeR/niri/assets/1794388/bce834b0-f205-434e-a027-b373495f9729

Also check out this video from Brodie Robertson that showcases a lot of the niri functionality: [Niri Is My New Favorite Wayland Compositor](https://youtu.be/DeYx2exm04M)

## Status

Niri is stable for day-to-day use and does most things expected of a Wayland compositor.
Many people are daily-driving niri, and are happy to help in our [Matrix channel].

Give it a try!
Follow the instructions on the [Getting Started](https://yalter.github.io/niri/Getting-Started.html) page.
Have your [waybar]s and [fuzzel]s ready: niri is not a complete desktop environment.
Also check out [awesome-niri], a list of niri-related links and projects.

Here are some points you may have questions about:

- **Multi-monitor**: yes, a core part of the design from the very start. Mixed DPI works.
- **Fractional scaling**: yes, plus all niri UI stays pixel-perfect.
- **NVIDIA**: seems to work fine.
- **Floating windows**: yes, starting from niri 25.01.
- **Input devices**: niri supports tablets, touchpads, and touchscreens.
You can map the tablet to a specific monitor, or use [OpenTabletDriver].
We have touchpad gestures, but no touchscreen gestures yet.
- **Wlr protocols**: yes, we have most of the important ones like layer-shell, gamma-control, screencopy.
You can check on [wayland.app](https://wayland.app) at the bottom of each protocol&#039;s page.
- **Performance**: while I run niri on beefy machines, I try to stay conscious of performance.
I&#039;ve seen someone use it fine on an EeeÂ PCÂ 900 fromÂ 2008, of all things.
- **Xwayland**: [integrated](https://yalter.github.io/niri/Xwayland.html#using-xwayland-satellite) via xwayland-satellite starting from niri 25.08.

## Media

[niri: Making a Wayland compositor in Rust](https://youtu.be/Kmz8ODolnDg?list=PLRdS-n5seLRqrmWDQY4KDqtRMfIwU0U3T) Â· *December 2024*

My talk from the 2024 Moscow RustCon about niri, and how I do randomized property testing and profiling, and measure input latency.
The talk is in Russian, but I prepared full English subtitles that you can find in YouTube&#039;s subtitle language selector.

[An interview with Ivan, the developer behind Niri](https://www.trommelspeicher.de/podcast/special_the_developer_behind_niri) Â· *June 2025*

An interview by a German tech podcast Das Triumvirat (in English).
We talk about niri development and history, and my experience building and maintaining niri.

[A tour of the niri scrolling-tiling Wayland compositor](https://lwn.net/Articles/1025866/) Â· *July 2025*

An LWN article with a nice overview and introduction to niri.

## Contributing

If you&#039;d like to help with niri, there are plenty of both coding- and non-coding-related ways to do so.
See [CONTRIBUTING.md](https://github.com/YaLTeR/niri/blob/main/CONTRIBUTING.md) for an overview.

## Inspiration

Niri is heavily inspired by [PaperWM] which implements scrollable tiling on top of GNOME Shell.

One of the reasons that prompted me to try writing my own compositor is being able to properly separate the monitors.
Being a GNOME Shell extension, PaperWM has to work against Shell&#039;s global window coordinate space to prevent windows from overflowing.

## Tile Scrollably Elsewhere

Here are some other projects which implement a similar workflow:

- [PaperWM]: scrollable tiling on top of GNOME Shell.
- [karousel]: scrollable tiling on top of KDE.
- [scroll](https://github.com/dawsers/scroll) and [papersway]: scrollable tiling on top of sway/i3.
- [hyprscrolling] and [hyprslidr]: scrollable tiling on top of Hyprland.
- [PaperWM.spoon]: scrollable tiling on top of macOS.

## Contact

Our main communication channel is a Matrix chat, feel free to join and ask a question: https://matrix.to/#/#niri:matrix.org

We also have a community Discord server: https://discord.gg/vT8Sfjy7sx

[PaperWM]: https://github.com/paperwm/PaperWM
[waybar]: https://github.com/Alexays/Waybar
[fuzzel]: https://codeberg.org/dnkl/fuzzel
[awesome-niri]: https://github.com/Vortriz/awesome-niri
[karousel]: https://github.com/peterfajdiga/karousel
[papersway]: https://spwhitton.name/tech/code/papersway/
[hyprscrolling]: https://github.com/hyprwm/hyprland-plugins/tree/main/hyprscrolling
[hyprslidr]: https://gitlab.com/magus/hyprslidr
[PaperWM.spoon]: https://github.com/mogenson/PaperWM.spoon
[Matrix channel]: https://matrix.to/#/#niri:matrix.org
[OpenTabletDriver]: https://opentabletdriver.net/
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[lbjlaq/Antigravity-Manager]]></title>
            <link>https://github.com/lbjlaq/Antigravity-Manager</link>
            <guid>https://github.com/lbjlaq/Antigravity-Manager</guid>
            <pubDate>Sun, 01 Feb 2026 00:07:22 GMT</pubDate>
            <description><![CDATA[Professional Antigravity Account Manager & Switcher. One-click seamless account switching for Antigravity Tools. Built with Tauri v2 + React (Rust).ä¸“ä¸šçš„ Antigravity è´¦å·ç®¡ç†ä¸åˆ‡æ¢å·¥å…·ã€‚ä¸º Antigravity æä¾›ä¸€é”®æ— ç¼è´¦å·åˆ‡æ¢åŠŸèƒ½ã€‚]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lbjlaq/Antigravity-Manager">lbjlaq/Antigravity-Manager</a></h1>
            <p>Professional Antigravity Account Manager & Switcher. One-click seamless account switching for Antigravity Tools. Built with Tauri v2 + React (Rust).ä¸“ä¸šçš„ Antigravity è´¦å·ç®¡ç†ä¸åˆ‡æ¢å·¥å…·ã€‚ä¸º Antigravity æä¾›ä¸€é”®æ— ç¼è´¦å·åˆ‡æ¢åŠŸèƒ½ã€‚</p>
            <p>Language: Rust</p>
            <p>Stars: 20,005</p>
            <p>Forks: 2,290</p>
            <p>Stars today: 270 stars today</p>
            <h2>README</h2><pre># Antigravity Tools ğŸš€
&gt; ä¸“ä¸šçš„ AI è´¦å·ç®¡ç†ä¸åè®®åä»£ç³»ç»Ÿ (v4.0.11)
&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;public/icon.png&quot; alt=&quot;Antigravity Logo&quot; width=&quot;120&quot; height=&quot;120&quot; style=&quot;border-radius: 24px; box-shadow: 0 10px 30px rgba(0,0,0,0.15);&quot;&gt;

  &lt;h3&gt;æ‚¨çš„ä¸ªäººé«˜æ€§èƒ½ AI è°ƒåº¦ç½‘å…³&lt;/h3&gt;
  &lt;p&gt;ä¸ä»…ä»…æ˜¯è´¦å·ç®¡ç†ï¼Œæ›´æ˜¯æ‰“ç ´ API è°ƒç”¨å£å’çš„ç»ˆæè§£å†³æ–¹æ¡ˆã€‚&lt;/p&gt;
  
  &lt;p&gt;
    &lt;a href=&quot;https://github.com/lbjlaq/Antigravity-Manager&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/badge/Version-4.0.11-blue?style=flat-square&quot; alt=&quot;Version&quot;&gt;
    &lt;/a&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Tauri-v2-orange?style=flat-square&quot; alt=&quot;Tauri&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Backend-Rust-red?style=flat-square&quot; alt=&quot;Rust&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Frontend-React-61DAFB?style=flat-square&quot; alt=&quot;React&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/License-CC--BY--NC--SA--4.0-lightgrey?style=flat-square&quot; alt=&quot;License&quot;&gt;
  &lt;/p&gt;

  &lt;p&gt;
    &lt;a href=&quot;#-æ ¸å¿ƒåŠŸèƒ½&quot;&gt;æ ¸å¿ƒåŠŸèƒ½&lt;/a&gt; â€¢ 
    &lt;a href=&quot;#-ç•Œé¢å¯¼è§ˆ&quot;&gt;ç•Œé¢å¯¼è§ˆ&lt;/a&gt; â€¢ 
    &lt;a href=&quot;#-æŠ€æœ¯æ¶æ„&quot;&gt;æŠ€æœ¯æ¶æ„&lt;/a&gt; â€¢ 
    &lt;a href=&quot;#-å®‰è£…æŒ‡å—&quot;&gt;å®‰è£…æŒ‡å—&lt;/a&gt; â€¢ 
    &lt;a href=&quot;#-å¿«é€Ÿæ¥å…¥&quot;&gt;å¿«é€Ÿæ¥å…¥&lt;/a&gt;
  &lt;/p&gt;

  &lt;p&gt;
    &lt;strong&gt;ç®€ä½“ä¸­æ–‡&lt;/strong&gt; | 
    &lt;a href=&quot;./README_EN.md&quot;&gt;English&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

---

**Antigravity Tools** æ˜¯ä¸€ä¸ªä¸“ä¸ºå¼€å‘è€…å’Œ AI çˆ±å¥½è€…è®¾è®¡çš„å…¨åŠŸèƒ½æ¡Œé¢åº”ç”¨ã€‚å®ƒå°†å¤šè´¦å·ç®¡ç†ã€åè®®è½¬æ¢å’Œæ™ºèƒ½è¯·æ±‚è°ƒåº¦å®Œç¾ç»“åˆï¼Œä¸ºæ‚¨æä¾›ä¸€ä¸ªç¨³å®šã€æé€Ÿä¸”æˆæœ¬ä½å»‰çš„ **æœ¬åœ° AI ä¸­è½¬ç«™**ã€‚

é€šè¿‡æœ¬åº”ç”¨ï¼Œæ‚¨å¯ä»¥å°†å¸¸è§çš„ Web ç«¯ Session (Google/Anthropic) è½¬åŒ–ä¸ºæ ‡å‡†åŒ–çš„ API æ¥å£ï¼Œæ¶ˆé™¤ä¸åŒå‚å•†é—´çš„åè®®é¸¿æ²Ÿã€‚

## ğŸ’– èµåŠ©å•† (Sponsors)

| &lt;img src=&quot;docs/images/packycode_logo.png&quot; width=&quot;200&quot; alt=&quot;PackyCode Logo&quot;&gt; | æ„Ÿè°¢ **PackyCode** å¯¹æœ¬é¡¹ç›®çš„èµåŠ©ï¼PackyCode æ˜¯ä¸€å®¶å¯é é«˜æ•ˆçš„ API ä¸­è½¬æœåŠ¡å•†ï¼Œæä¾› Claude Codeã€Codexã€Gemini ç­‰å¤šç§æœåŠ¡çš„ä¸­è½¬ã€‚PackyCode ä¸ºæœ¬é¡¹ç›®çš„ç”¨æˆ·æä¾›äº†ç‰¹åˆ«ä¼˜æƒ ï¼šä½¿ç”¨[æ­¤é“¾æ¥](https://www.packyapi.com/register?aff=Ctrler)æ³¨å†Œï¼Œå¹¶åœ¨å……å€¼æ—¶è¾“å…¥ **â€œCtrlerâ€** ä¼˜æƒ ç å³å¯äº«å— **ä¹æŠ˜ä¼˜æƒ **ã€‚ |
| :--- | :--- |

### â˜• æ”¯æŒé¡¹ç›® (Support)

å¦‚æœæ‚¨è§‰å¾—æœ¬é¡¹ç›®å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œæ¬¢è¿æ‰“èµä½œè€…ï¼

&lt;a href=&quot;https://www.buymeacoffee.com/Ctrler&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://cdn.buymeacoffee.com/buttons/v2/default-green.png&quot; alt=&quot;è¯·æˆ‘å–æ¯å’–å•¡&quot; style=&quot;height: 60px !important; width: 217px !important;&quot;&gt;&lt;/a&gt;

| æ”¯ä»˜å® (Alipay) | å¾®ä¿¡æ”¯ä»˜ (WeChat) | Buy Me a Coffee |
| :---: | :---: | :---: |
| ![Alipay](./docs/images/donate_alipay.png) | ![WeChat](./docs/images/donate_wechat.png) | ![Coffee](./docs/images/donate_coffee.png) |

## ğŸŒŸ æ·±åº¦åŠŸèƒ½è§£æ (Detailed Features)

### 1. ğŸ›ï¸ æ™ºèƒ½è´¦å·ä»ªè¡¨ç›˜ (Smart Dashboard)
*   **å…¨å±€å®æ—¶ç›‘æ§**: ä¸€çœ¼æ´å¯Ÿæ‰€æœ‰è´¦å·çš„å¥åº·çŠ¶å†µï¼ŒåŒ…æ‹¬ Gemini Proã€Gemini Flashã€Claude ä»¥åŠ Gemini ç»˜å›¾çš„ **å¹³å‡å‰©ä½™é…é¢**ã€‚
*   **æœ€ä½³è´¦å·æ¨è (Smart Recommendation)**: ç³»ç»Ÿä¼šæ ¹æ®å½“å‰æ‰€æœ‰è´¦å·çš„é…é¢å†—ä½™åº¦ï¼Œå®æ—¶ç®—æ³•ç­›é€‰å¹¶æ¨èâ€œæœ€ä½³è´¦å·â€ï¼Œæ”¯æŒ **ä¸€é”®åˆ‡æ¢**ã€‚
*   **æ´»è·ƒè´¦å·å¿«ç…§**: ç›´è§‚æ˜¾ç¤ºå½“å‰æ´»è·ƒè´¦å·çš„å…·ä½“é…é¢ç™¾åˆ†æ¯”åŠæœ€ååŒæ­¥æ—¶é—´ã€‚

### 2. ğŸ” å¼ºå¤§çš„è´¦å·ç®¡å®¶ (Account Management)
*   **OAuth 2.0 æˆæƒï¼ˆè‡ªåŠ¨/æ‰‹åŠ¨ï¼‰**: æ·»åŠ è´¦å·æ—¶ä¼šæå‰ç”Ÿæˆå¯å¤åˆ¶çš„æˆæƒé“¾æ¥ï¼Œæ”¯æŒåœ¨ä»»æ„æµè§ˆå™¨å®Œæˆæˆæƒï¼›å›è°ƒæˆåŠŸååº”ç”¨ä¼šè‡ªåŠ¨å®Œæˆå¹¶ä¿å­˜ï¼ˆå¿…è¦æ—¶å¯ç‚¹å‡»â€œæˆ‘å·²æˆæƒï¼Œç»§ç»­â€æ‰‹åŠ¨æ”¶å°¾ï¼‰ã€‚
*   **å¤šç»´åº¦å¯¼å…¥**: æ”¯æŒå•æ¡ Token å½•å…¥ã€JSON æ‰¹é‡å¯¼å…¥ï¼ˆå¦‚æ¥è‡ªå…¶ä»–å·¥å…·çš„å¤‡ä»½ï¼‰ï¼Œä»¥åŠä» V1 æ—§ç‰ˆæœ¬æ•°æ®åº“è‡ªåŠ¨çƒ­è¿ç§»ã€‚
*   **ç½‘å…³çº§è§†å›¾**: æ”¯æŒâ€œåˆ—è¡¨â€ä¸â€œç½‘æ ¼â€åŒè§†å›¾åˆ‡æ¢ã€‚æä¾› 403 å°ç¦æ£€æµ‹ï¼Œè‡ªåŠ¨æ ‡æ³¨å¹¶è·³è¿‡æƒé™å¼‚å¸¸çš„è´¦å·ã€‚

### 3. ğŸ”Œ åè®®è½¬æ¢ä¸ä¸­ç»§ (API Proxy)
*   **å…¨åè®®é€‚é… (Multi-Sink)**:
    *   **OpenAI æ ¼å¼**: æä¾› `/v1/chat/completions` ç«¯ç‚¹ï¼Œå…¼å®¹ 99% çš„ç°æœ‰ AI åº”ç”¨ã€‚
    *   **Anthropic æ ¼å¼**: æä¾›åŸç”Ÿ `/v1/messages` æ¥å£ï¼Œæ”¯æŒ **Claude Code CLI** çš„å…¨åŠŸèƒ½ï¼ˆå¦‚æ€æ€ç»´é“¾ã€ç³»ç»Ÿæç¤ºè¯ï¼‰ã€‚
    *   **Gemini æ ¼å¼**: æ”¯æŒ Google å®˜æ–¹ SDK ç›´æ¥è°ƒç”¨ã€‚
*   **æ™ºèƒ½çŠ¶æ€è‡ªæ„ˆ**: å½“è¯·æ±‚é‡åˆ° `429 (Too Many Requests)` æˆ– `401 (Expire)` æ—¶ï¼Œåç«¯ä¼šæ¯«ç§’çº§è§¦å‘ **è‡ªåŠ¨é‡è¯•ä¸é™é»˜è½®æ¢**ï¼Œç¡®ä¿ä¸šåŠ¡ä¸ä¸­æ–­ã€‚

### 4. ğŸ”€ æ¨¡å‹è·¯ç”±ä¸­å¿ƒ (Model Router)
*   **ç³»åˆ—åŒ–æ˜ å°„**: æ‚¨å¯ä»¥å°†å¤æ‚çš„åŸå§‹æ¨¡å‹ ID å½’ç±»åˆ°â€œè§„æ ¼å®¶æ—â€ï¼ˆå¦‚å°†æ‰€æœ‰ GPT-4 è¯·æ±‚ç»Ÿä¸€è·¯ç”±åˆ° `gemini-3-pro-high`ï¼‰ã€‚
*   **ä¸“å®¶çº§é‡å®šå‘**: æ”¯æŒè‡ªå®šä¹‰æ­£åˆ™è¡¨è¾¾å¼çº§æ¨¡å‹æ˜ å°„ï¼Œç²¾å‡†æ§åˆ¶æ¯ä¸€ä¸ªè¯·æ±‚çš„è½åœ°æ¨¡å‹ã€‚
*   **æ™ºèƒ½åˆ†çº§è·¯ç”± (Tiered Routing)**: [æ–°] ç³»ç»Ÿæ ¹æ®è´¦å·ç±»å‹ï¼ˆUltra/Pro/Freeï¼‰å’Œé…é¢é‡ç½®é¢‘ç‡è‡ªåŠ¨ä¼˜å…ˆçº§æ’åºï¼Œä¼˜å…ˆæ¶ˆè€—é«˜é€Ÿé‡ç½®è´¦å·ï¼Œç¡®ä¿é«˜é¢‘è°ƒç”¨ä¸‹çš„æœåŠ¡ç¨³å®šæ€§ã€‚
*   **åå°ä»»åŠ¡é™é»˜é™çº§**: [æ–°] è‡ªåŠ¨è¯†åˆ« Claude CLI ç­‰å·¥å…·ç”Ÿæˆçš„åå°è¯·æ±‚ï¼ˆå¦‚æ ‡é¢˜ç”Ÿæˆï¼‰ï¼Œæ™ºèƒ½é‡å®šå‘è‡³ Flash æ¨¡å‹ï¼Œä¿æŠ¤é«˜çº§æ¨¡å‹é…é¢ä¸è¢«æµªè´¹ã€‚

### 5. ğŸ¨ å¤šæ¨¡æ€ä¸ Imagen 3 æ”¯æŒ
*   **é«˜çº§ç”»è´¨æ§åˆ¶**: æ”¯æŒé€šè¿‡ OpenAI `size` (å¦‚ `1024x1024`, `16:9`) å‚æ•°è‡ªåŠ¨æ˜ å°„åˆ° Imagen 3 çš„ç›¸åº”è§„æ ¼ã€‚
*   **è¶…å¼º Body æ”¯æŒ**: åç«¯æ”¯æŒé«˜è¾¾ **100MB** (å¯é…ç½®) çš„ Payloadï¼Œå¤„ç† 4K é«˜æ¸…å›¾è¯†åˆ«ç»°ç»°æœ‰ä½™ã€‚

## ğŸ“¸ ç•Œé¢å¯¼è§ˆ (GUI Overview)

| | |
| :---: | :---: |
| ![ä»ªè¡¨ç›˜ - å…¨å±€é…é¢ç›‘æ§ä¸ä¸€é”®åˆ‡æ¢](docs/images/dashboard-light.png) &lt;br&gt; ä»ªè¡¨ç›˜ | ![è´¦å·åˆ—è¡¨ - é«˜å¯†åº¦é…é¢å±•ç¤ºä¸ 403 æ™ºèƒ½æ ‡æ³¨](docs/images/accounts-light.png) &lt;br&gt; è´¦å·åˆ—è¡¨ |
| ![å…³äºé¡µé¢ - å…³äº Antigravity Tools](docs/images/about-dark.png) &lt;br&gt; å…³äºé¡µé¢ | ![API åä»£ - æœåŠ¡æ§åˆ¶](docs/images/v3/proxy-settings.png) &lt;br&gt; API åä»£ |
| ![ç³»ç»Ÿè®¾ç½® - é€šç”¨é…ç½®](docs/images/settings-dark.png) &lt;br&gt; ç³»ç»Ÿè®¾ç½® | |

### ğŸ’¡ ä½¿ç”¨æ¡ˆä¾‹ (Usage Examples)

| | |
| :---: | :---: |
| ![Claude Code è”ç½‘æœç´¢ - ç»“æ„åŒ–æ¥æºä¸å¼•æ–‡æ˜¾ç¤º](docs/images/usage/claude-code-search.png) &lt;br&gt; Claude Code è”ç½‘æœç´¢ | ![Cherry Studio æ·±åº¦é›†æˆ - åŸç”Ÿå›æ˜¾æœç´¢å¼•æ–‡ä¸æ¥æºé“¾æ¥](docs/images/usage/cherry-studio-citations.png) &lt;br&gt; Cherry Studio æ·±åº¦é›†æˆ |
| ![Imagen 3 é«˜çº§ç»˜å›¾ - å®Œç¾è¿˜åŸ Prompt æ„å¢ƒä¸ç»†èŠ‚](docs/images/usage/image-gen-nebula.png) &lt;br&gt; Imagen 3 é«˜çº§ç»˜å›¾ | ![Kilo Code æ¥å…¥ - å¤šè´¦å·æé€Ÿè½®æ¢ä¸æ¨¡å‹ç©¿é€](docs/images/usage/kilo-code-integration.png) &lt;br&gt; Kilo Code æ¥å…¥ |

## ğŸ—ï¸ æŠ€æœ¯æ¶æ„ (Architecture)

```mermaid
graph TD
    Client([å¤–éƒ¨åº”ç”¨: Claude Code/NextChat]) --&gt;|OpenAI/Anthropic| Gateway[Antigravity Axum Server]
    Gateway --&gt; Middleware[ä¸­é—´ä»¶: é‰´æƒ/é™æµ/æ—¥å¿—]
    Middleware --&gt; Router[Model Router: ID æ˜ å°„]
    Router --&gt; Dispatcher[è´¦å·åˆ†å‘å™¨: è½®è¯¢/æƒé‡]
    Dispatcher --&gt; Mapper[åè®®è½¬æ¢å™¨: Request Mapper]
    Mapper --&gt; Upstream[ä¸Šæ¸¸è¯·æ±‚: Google/Anthropic API]
    Upstream --&gt; ResponseMapper[å“åº”è½¬æ¢å™¨: Response Mapper]
    ResponseMapper --&gt; Client
```

##  å®‰è£…æŒ‡å— (Installation)

### é€‰é¡¹ A: ç»ˆç«¯å®‰è£… (macOS &amp; Linux æ¨è)

#### macOS 
å¦‚æœæ‚¨å·²å®‰è£… [Homebrew](https://brew.sh/)ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å¿«é€Ÿå®‰è£…ï¼š

```bash
# 1. è®¢é˜…æœ¬ä»“åº“çš„ Tap
brew tap lbjlaq/antigravity-manager https://github.com/lbjlaq/Antigravity-Manager

# 2. å®‰è£…åº”ç”¨
brew install --cask antigravity-tools
```
&gt; **æç¤º**: å¦‚æœé‡åˆ°æƒé™é—®é¢˜ï¼Œå»ºè®®æ·»åŠ  `--no-quarantine` å‚æ•°ã€‚

#### Arch Linux
æ‚¨å¯ä»¥é€‰æ‹©é€šè¿‡ä¸€é”®å®‰è£…è„šæœ¬æˆ– Homebrew è¿›è¡Œå®‰è£…ï¼š

**æ–¹å¼ 1ï¼šä¸€é”®å®‰è£…è„šæœ¬ (æ¨è)**
```bash
curl -sSL https://raw.githubusercontent.com/lbjlaq/Antigravity-Manager/main/deploy/arch/install.sh | bash
```

**æ–¹å¼ 2ï¼šé€šè¿‡ Homebrew** (å¦‚æœæ‚¨å·²å®‰è£… [Linuxbrew](https://sh.brew.sh/))
```bash
brew tap lbjlaq/antigravity-manager https://github.com/lbjlaq/Antigravity-Manager
brew install --cask antigravity-tools
```

#### å…¶ä»– Linux å‘è¡Œç‰ˆ
å®‰è£…åä¼šè‡ªåŠ¨å°† AppImage æ·»åŠ åˆ°äºŒè¿›åˆ¶è·¯å¾„å¹¶é…ç½®å¯æ‰§è¡Œæƒé™ã€‚

### é€‰é¡¹ B: æ‰‹åŠ¨ä¸‹è½½
å‰å¾€ [GitHub Releases](https://github.com/lbjlaq/Antigravity-Manager/releases) ä¸‹è½½å¯¹åº”ç³»ç»Ÿçš„åŒ…ï¼š
*   **macOS**: `.dmg` (æ”¯æŒ Apple Silicon &amp; Intel)
*   **Windows**: `.msi` æˆ– ä¾¿æºç‰ˆ `.zip`
*   **Linux**: `.deb` æˆ– `AppImage`

### é€‰é¡¹ C: Docker éƒ¨ç½² (æ¨èç”¨äº NAS/æœåŠ¡å™¨)
å¦‚æœæ‚¨å¸Œæœ›åœ¨å®¹å™¨åŒ–ç¯å¢ƒä¸­è¿è¡Œï¼Œæˆ‘ä»¬æä¾›äº†åŸç”Ÿçš„ Docker é•œåƒã€‚è¯¥é•œåƒå†…ç½®äº†å¯¹ v4.0.2 åŸç”Ÿ Headless æ¶æ„çš„æ”¯æŒï¼Œå¯è‡ªåŠ¨æ‰˜ç®¡å‰ç«¯é™æ€èµ„æºï¼Œå¹¶é€šè¿‡æµè§ˆå™¨ç›´æ¥è¿›è¡Œç®¡ç†ã€‚

```bash
# æ–¹å¼ 1: ç›´æ¥è¿è¡Œ (æ¨è)
# - API_KEY: å¿…å¡«ã€‚ç”¨äºæ‰€æœ‰åè®®çš„ AI è¯·æ±‚é‰´å®šã€‚
# - WEB_PASSWORD: å¯é€‰ã€‚ç”¨äºç®¡ç†åå°ç™»å½•ã€‚è‹¥ä¸è®¾ç½®åˆ™é»˜è®¤ä½¿ç”¨ API_KEYã€‚
docker run -d --name antigravity-manager \
  -p 8045:8045 \
  -e API_KEY=sk-your-api-key \
  -e WEB_PASSWORD=your-login-password \
  -e ABV_MAX_BODY_SIZE=104857600 \
  -v ~/.antigravity_tools:/root/.antigravity_tools \
  lbjlaq/antigravity-manager:latest

# å¿˜è®°å¯†é’¥ï¼Ÿæ‰§è¡Œ docker logs antigravity-manager æˆ– grep -E &#039;&quot;api_key&quot;|&quot;admin_password&quot;&#039; ~/.antigravity_tools/gui_config.json

#### ğŸ” é‰´æƒé€»è¾‘è¯´æ˜
*   **åœºæ™¯ Aï¼šä»…è®¾ç½®äº† `API_KEY`**
    - **Web ç™»å½•**ï¼šä½¿ç”¨ `API_KEY` è¿›å…¥åå°ã€‚
    - **API è°ƒç”¨**ï¼šä½¿ç”¨ `API_KEY` è¿›è¡Œ AI è¯·æ±‚é‰´æƒã€‚
*   **åœºæ™¯ Bï¼šåŒæ—¶è®¾ç½®äº† `API_KEY` å’Œ `WEB_PASSWORD` (æ¨è)**
    - **Web ç™»å½•**ï¼š**å¿…é¡»**ä½¿ç”¨ `WEB_PASSWORD`ï¼Œä½¿ç”¨ API Key å°†è¢«æ‹’ç»ï¼ˆæ›´å®‰å…¨ï¼‰ã€‚
    - **API è°ƒç”¨**ï¼šç»Ÿä¸€ä½¿ç”¨ `API_KEY`ã€‚è¿™æ ·æ‚¨å¯ä»¥å°† API Key åˆ†å‘ç»™æˆå‘˜ï¼Œè€Œä¿ç•™å¯†ç ä»…ä¾›ç®¡ç†å‘˜ä½¿ç”¨ã€‚

#### ğŸ†™ æ—§ç‰ˆæœ¬å‡çº§æŒ‡å¼•
å¦‚æœæ‚¨æ˜¯ä» v4.0.1 åŠæ›´æ—©ç‰ˆæœ¬å‡çº§ï¼Œç³»ç»Ÿé»˜è®¤æœªè®¾ç½® `WEB_PASSWORD`ã€‚æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»»ä¸€æ–¹å¼è®¾ç½®ï¼š
1.  **Web UI ç•Œé¢ (æ¨è)**ï¼šä½¿ç”¨åŸæœ‰ `API_KEY` ç™»å½•åï¼Œåœ¨ **API åä»£è®¾ç½®** é¡µé¢æ‰‹åŠ¨è®¾ç½®å¹¶ä¿å­˜ã€‚æ–°å¯†ç å°†æŒä¹…åŒ–å­˜å‚¨åœ¨ `gui_config.json` ä¸­ã€‚
2.  **ç¯å¢ƒå˜é‡ (Docker)**ï¼šåœ¨å¯åŠ¨å®¹å™¨æ—¶å¢åŠ  `-e WEB_PASSWORD=æ‚¨çš„æ–°å¯†ç `ã€‚**æ³¨æ„ï¼šç¯å¢ƒå˜é‡å…·æœ‰æœ€é«˜ä¼˜å…ˆçº§ï¼Œå°†è¦†ç›– UI ä¸­çš„ä»»ä½•ä¿®æ”¹ã€‚**
3.  **é…ç½®æ–‡ä»¶ (æŒä¹…åŒ–)**ï¼šç›´æ¥ä¿®æ”¹ `~/.antigravity_tools/gui_config.json`ï¼Œåœ¨ `proxy` å¯¹è±¡ä¸­ä¿®æ”¹æˆ–æ·»åŠ  `&quot;admin_password&quot;: &quot;æ‚¨çš„æ–°å¯†ç &quot;` å­—æ®µã€‚
    - *æ³¨ï¼š`WEB_PASSWORD` æ˜¯ç¯å¢ƒå˜é‡åï¼Œ`admin_password` æ˜¯é…ç½®æ–‡ä»¶ä¸­çš„ JSON é”®åã€‚*

&gt; [!TIP]
&gt; **å¯†ç ä¼˜å…ˆçº§é€»è¾‘ (Priority)**:
&gt; - **ç¬¬ä¸€ä¼˜å…ˆçº§ (ç¯å¢ƒå˜é‡)**: `ABV_WEB_PASSWORD` æˆ– `WEB_PASSWORD`ã€‚åªè¦è®¾ç½®äº†ç¯å¢ƒå˜é‡ï¼Œç³»ç»Ÿå°†å§‹ç»ˆä½¿ç”¨å®ƒã€‚
&gt; - **ç¬¬äºŒä¼˜å…ˆçº§ (é…ç½®æ–‡ä»¶)**: `gui_config.json` ä¸­çš„ `admin_password` å­—æ®µã€‚UI çš„â€œä¿å­˜â€æ“ä½œä¼šæ›´æ–°æ­¤å€¼ã€‚
&gt; - **ä¿åº•å›é€€ (å‘åå…¼å®¹)**: è‹¥ä¸Šè¿°å‡æœªè®¾ç½®ï¼Œåˆ™å›é€€ä½¿ç”¨ `API_KEY` ä½œä¸ºç™»å½•å¯†ç ã€‚

# æ–¹å¼ 2: ä½¿ç”¨ Docker Compose
# 1. è¿›å…¥é¡¹ç›®çš„ docker ç›®å½•
cd docker
# 2. å¯åŠ¨æœåŠ¡
docker compose up -d
```
&gt; **è®¿é—®åœ°å€**: `http://localhost:8045` (ç®¡ç†åå°) | `http://localhost:8045/v1` (API Base)
&gt; **ç³»ç»Ÿè¦æ±‚**:
&gt; - **å†…å­˜**: å»ºè®® **1GB** (æœ€å° 256MB)ã€‚
&gt; - **æŒä¹…åŒ–**: éœ€æŒ‚è½½ `/root/.antigravity_tools` ä»¥ä¿å­˜æ•°æ®ã€‚
&gt; - **æ¶æ„**: æ”¯æŒ x86_64 å’Œ ARM64ã€‚
&gt; **è¯¦æƒ…è§**: [Docker éƒ¨ç½²æŒ‡å— (docker)](./docker/README.md)

---

Copyright Â© 2024-2026 [lbjlaq](https://github.com/lbjlaq)

### ğŸ› ï¸ å¸¸è§é—®é¢˜æ’æŸ¥ (Troubleshooting)

#### macOS æç¤ºâ€œåº”ç”¨å·²æŸåï¼Œæ— æ³•æ‰“å¼€â€ï¼Ÿ
ç”±äº macOS çš„å®‰å…¨æœºåˆ¶ï¼Œé App Store ä¸‹è½½çš„åº”ç”¨å¯èƒ½ä¼šè§¦å‘æ­¤æç¤ºã€‚æ‚¨å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å¿«é€Ÿä¿®å¤ï¼š

1.  **å‘½ä»¤è¡Œä¿®å¤** (æ¨è):
    æ‰“å¼€ç»ˆç«¯ï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
    ```bash
    sudo xattr -rd com.apple.quarantine &quot;/Applications/Antigravity Tools.app&quot;
    ```
2.  **Homebrew å®‰è£…æŠ€å·§**:
    å¦‚æœæ‚¨ä½¿ç”¨ brew å®‰è£…ï¼Œå¯ä»¥æ·»åŠ  `--no-quarantine` å‚æ•°æ¥è§„é¿æ­¤é—®é¢˜ï¼š
    ```bash
    brew install --cask --no-quarantine antigravity-tools
    ```

## ğŸ”Œ å¿«é€Ÿæ¥å…¥ç¤ºä¾‹

### ğŸ” OAuth æˆæƒæµç¨‹ï¼ˆæ·»åŠ è´¦å·ï¼‰
1. æ‰“å¼€â€œAccounts / è´¦å·â€ â†’ â€œæ·»åŠ è´¦å·â€ â†’ â€œOAuthâ€ã€‚
2. å¼¹çª—ä¼šåœ¨ç‚¹å‡»æŒ‰é’®å‰é¢„ç”Ÿæˆæˆæƒé“¾æ¥ï¼›ç‚¹å‡»é“¾æ¥å³å¯å¤åˆ¶åˆ°ç³»ç»Ÿå‰ªè´´æ¿ï¼Œç„¶åç”¨ä½ å¸Œæœ›çš„æµè§ˆå™¨æ‰“å¼€å¹¶å®Œæˆæˆæƒã€‚
3. æˆæƒå®Œæˆåæµè§ˆå™¨ä¼šæ‰“å¼€æœ¬åœ°å›è°ƒé¡µå¹¶æ˜¾ç¤ºâ€œâœ… æˆæƒæˆåŠŸ!â€ã€‚
4. åº”ç”¨ä¼šè‡ªåŠ¨ç»§ç»­å®Œæˆæˆæƒå¹¶ä¿å­˜è´¦å·ï¼›å¦‚æœªè‡ªåŠ¨å®Œæˆï¼Œå¯ç‚¹å‡»â€œæˆ‘å·²æˆæƒï¼Œç»§ç»­â€æ‰‹åŠ¨å®Œæˆã€‚

&gt; æç¤ºï¼šæˆæƒé“¾æ¥åŒ…å«ä¸€æ¬¡æ€§å›è°ƒç«¯å£ï¼Œè¯·å§‹ç»ˆä½¿ç”¨å¼¹çª—é‡Œç”Ÿæˆçš„æœ€æ–°é“¾æ¥ï¼›å¦‚æœæˆæƒæ—¶åº”ç”¨æœªè¿è¡Œæˆ–å¼¹çª—å·²å…³é—­ï¼Œæµè§ˆå™¨å¯èƒ½ä¼šæç¤º `localhost refused connection`ã€‚

### å¦‚ä½•æ¥å…¥ Claude Code CLI?
1.  å¯åŠ¨ Antigravityï¼Œå¹¶åœ¨â€œAPI åä»£â€é¡µé¢å¼€å¯æœåŠ¡ã€‚
2.  åœ¨ç»ˆç«¯æ‰§è¡Œï¼š
```bash
export ANTHROPIC_API_KEY=&quot;sk-antigravity&quot;
export ANTHROPIC_BASE_URL=&quot;http://127.0.0.1:8045&quot;
claude
```

### å¦‚ä½•æ¥å…¥ Kilo Code?
1.  **åè®®é€‰æ‹©**: å»ºè®®ä¼˜å…ˆä½¿ç”¨ **Gemini åè®®**ã€‚
2.  **Base URL**: å¡«å†™ `http://127.0.0.1:8045`ã€‚
3.  **æ³¨æ„**: 
    - **OpenAI åè®®é™åˆ¶**: Kilo Code åœ¨ä½¿ç”¨ OpenAI æ¨¡å¼æ—¶ï¼Œå…¶è¯·æ±‚è·¯å¾„ä¼šå åŠ äº§ç”Ÿ `/v1/chat/completions/responses` è¿™ç§éæ ‡å‡†è·¯å¾„ï¼Œå¯¼è‡´ Antigravity è¿”å› 404ã€‚å› æ­¤è¯·åŠ¡å¿…å¡«å…¥ Base URL åé€‰æ‹© Gemini æ¨¡å¼ã€‚
    - **æ¨¡å‹æ˜ å°„**: Kilo Code ä¸­çš„æ¨¡å‹åç§°å¯èƒ½ä¸ Antigravity é»˜è®¤è®¾ç½®ä¸ä¸€è‡´ï¼Œå¦‚é‡åˆ°æ— æ³•è¿æ¥ï¼Œè¯·åœ¨â€œæ¨¡å‹æ˜ å°„â€é¡µé¢è®¾ç½®è‡ªå®šä¹‰æ˜ å°„ï¼Œå¹¶æŸ¥çœ‹**æ—¥å¿—æ–‡ä»¶**è¿›è¡Œè°ƒè¯•ã€‚

### å¦‚ä½•åœ¨ Python ä¸­ä½¿ç”¨?
```python
import openai

client = openai.OpenAI(
    api_key=&quot;sk-antigravity&quot;,
    base_url=&quot;http://127.0.0.1:8045/v1&quot;
)

response = client.chat.completions.create(
    model=&quot;gemini-3-flash&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;ä½ å¥½ï¼Œè¯·è‡ªæˆ‘ä»‹ç»&quot;}]
)
print(response.choices[0].message.content)
```

### å¦‚ä½•ä½¿ç”¨å›¾ç‰‡ç”Ÿæˆ (Imagen 3)?

#### æ–¹å¼ä¸€ï¼šOpenAI Images API (æ¨è)
```python
import openai

client = openai.OpenAI(
    api_key=&quot;sk-antigravity&quot;,
    base_url=&quot;http://127.0.0.1:8045/v1&quot;
)

# ç”Ÿæˆå›¾ç‰‡
response = client.images.generate(
    model=&quot;gemini-3-pro-image&quot;,
    prompt=&quot;ä¸€åº§æœªæ¥ä¸»ä¹‰é£æ ¼çš„åŸå¸‚ï¼Œèµ›åšæœ‹å…‹ï¼Œéœ“è™¹ç¯&quot;,
    size=&quot;1920x1080&quot;,      # æ”¯æŒä»»æ„ WIDTHxHEIGHT æ ¼å¼ï¼Œè‡ªåŠ¨è®¡ç®—å®½é«˜æ¯”
    quality=&quot;hd&quot;,          # &quot;standard&quot; | &quot;hd&quot; | &quot;medium&quot;
    n=1,
    response_format=&quot;b64_json&quot;
)

# ä¿å­˜å›¾ç‰‡
import base64
image_data = base64.b64decode(response.data[0].b64_json)
with open(&quot;output.png&quot;, &quot;wb&quot;) as f:
    f.write(image_data)
```

**æ”¯æŒçš„å‚æ•°**ï¼š
- **`size`**: ä»»æ„ `WIDTHxHEIGHT` æ ¼å¼ï¼ˆå¦‚ `1280x720`, `1024x1024`, `1920x1080`ï¼‰ï¼Œè‡ªåŠ¨è®¡ç®—å¹¶æ˜ å°„åˆ°æ ‡å‡†å®½é«˜æ¯”ï¼ˆ21:9, 16:9, 9:16, 4:3, 3:4, 1:1ï¼‰
- **`quality`**: 
  - `&quot;hd&quot;` â†’ 4K åˆ†è¾¨ç‡ï¼ˆé«˜è´¨é‡ï¼‰
  - `&quot;medium&quot;` â†’ 2K åˆ†è¾¨ç‡ï¼ˆä¸­ç­‰è´¨é‡ï¼‰
  - `&quot;standard&quot;` â†’ é»˜è®¤åˆ†è¾¨ç‡ï¼ˆæ ‡å‡†è´¨é‡ï¼‰
- **`n`**: ç”Ÿæˆå›¾ç‰‡æ•°é‡ï¼ˆ1-10ï¼‰
- **`response_format`**: `&quot;b64_json&quot;` æˆ– `&quot;url&quot;`ï¼ˆData URIï¼‰

#### æ–¹å¼äºŒï¼šChat API + å‚æ•°è®¾ç½® (âœ¨ æ–°å¢)

**æ‰€æœ‰åè®®**ï¼ˆOpenAIã€Claudeï¼‰çš„ Chat API ç°åœ¨éƒ½æ”¯æŒç›´æ¥ä¼ é€’ `size` å’Œ `quality` å‚æ•°ï¼š

```python
# OpenAI Chat API
response = client.chat.completions.create(
    model=&quot;gemini-3-pro-image&quot;,
    size=&quot;1920x1080&quot;,      # âœ… æ”¯æŒä»»æ„ WIDTHxHEIGHT æ ¼å¼
    quality=&quot;hd&quot;,          # âœ… &quot;standard&quot; | &quot;hd&quot; | &quot;medium&quot;
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;ä¸€åº§æœªæ¥ä¸»ä¹‰é£æ ¼çš„åŸå¸‚&quot;}]
)
```

```bash
# Claude Messages API
curl -X POST http://127.0.0.1:8045/v1/messages \
  -H &quot;Content-Type: application/json&quot; \
  -H &quot;x-api-key: sk-antigravity&quot; \
  -d &#039;{
    &quot;model&quot;: &quot;gemini-3-pro-image&quot;,
    &quot;size&quot;: &quot;1280x720&quot;,
    &quot;quality&quot;: &quot;hd&quot;,
    &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;ä¸€åªå¯çˆ±çš„çŒ«å’ª&quot;}]
  }&#039;
```

**å‚æ•°ä¼˜å…ˆçº§**: è¯·æ±‚ä½“å‚æ•° &gt; æ¨¡å‹åç¼€

#### æ–¹å¼ä¸‰ï¼šChat æ¥å£ + æ¨¡å‹åç¼€
```python
response = client.chat.completions.create(
    model=&quot;gemini-3-pro-image-16-9-4k&quot;,  # æ ¼å¼ï¼šgemini-3-pro-image-[æ¯”ä¾‹]-[è´¨é‡]
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;ä¸€åº§æœªæ¥ä¸»ä¹‰é£æ ¼çš„åŸå¸‚&quot;}]
)
```

**æ¨¡å‹åç¼€è¯´æ˜**ï¼š
- **å®½é«˜æ¯”**: `-16-9`, `-9-16`, `-4-3`, `-3-4`, `-21-9`, `-1-1`
- **è´¨é‡**: `-4k` (4K), `-2k` (2K), ä¸åŠ åç¼€ï¼ˆæ ‡å‡†ï¼‰
- **ç¤ºä¾‹**: `gemini-3-pro-image-16-9-4k` â†’ 16:9 æ¯”ä¾‹ + 4K åˆ†è¾¨ç‡

#### æ–¹å¼å››ï¼šCherry Studio ç­‰å®¢æˆ·ç«¯è®¾ç½®
åœ¨æ”¯æŒ OpenAI åè®®çš„å®¢æˆ·ç«¯ï¼ˆå¦‚ Cherry Studioï¼‰ä¸­ï¼Œå¯ä»¥é€šè¿‡**æ¨¡å‹è®¾ç½®**é¡µé¢é…ç½®å›¾ç‰‡ç”Ÿæˆå‚æ•°ï¼š

1. **è¿›å…¥æ¨¡å‹è®¾ç½®**ï¼šé€‰æ‹© `gemini-3-pro-image` æ¨¡å‹
2. **é…ç½®å‚æ•°**ï¼š
   - **Size (å°ºå¯¸)**: è¾“å…¥ä»»æ„ `WIDTHxHEIGHT` æ ¼å¼ï¼ˆå¦‚ `1920x1080`, `1024x1024`ï¼‰
   - **Quality (è´¨é‡)**: é€‰æ‹© `standard` / `hd` / `medium`
   - **Number (æ•°é‡)**: è®¾ç½®ç”Ÿæˆå›¾ç‰‡æ•°é‡ï¼ˆ1-10ï¼‰
3. **å‘é€è¯·æ±‚**ï¼šç›´æ¥åœ¨å¯¹è¯æ¡†ä¸­è¾“å…¥å›¾ç‰‡æè¿°å³å¯

**å‚æ•°æ˜ å°„è§„åˆ™**ï¼š
- `size: &quot;1920x1080&quot;` â†’ è‡ªåŠ¨è®¡ç®—ä¸º `16:9` å®½é«˜æ¯”
- `quality: &quot;hd&quot;` â†’ æ˜ å°„ä¸º `4K` åˆ†è¾¨ç‡
- `quality: &quot;medium&quot;` â†’ æ˜ å°„ä¸º `2K` åˆ†è¾¨ç‡


## ğŸ“ å¼€å‘è€…ä¸ç¤¾åŒº

*   **ç‰ˆæœ¬æ¼”è¿› (Changelog)**:
    *   **v4.0.11 (2026-01-31)**:
        -   **[æ ¸å¿ƒä¿®å¤] è°ƒæ•´ API ç«¯ç‚¹é¡ºåºä¸è‡ªåŠ¨é˜»æ–­ (Fix 403 VALIDATION_REQUIRED)**:
            -   **ç«¯ç‚¹é¡ºåºä¼˜åŒ–**: å°† Google API çš„è¯·æ±‚é¡ºåºè°ƒæ•´ä¸º `Sandbox -&gt; Daily -&gt; Prod`ã€‚ä¼˜å…ˆä½¿ç”¨å®½æ¾ç¯å¢ƒï¼Œä»æºå¤´å‡å°‘ 403 é”™è¯¯çš„å‘ç”Ÿã€‚
            -   **æ™ºèƒ½é˜»æ–­æœºåˆ¶**: å½“æ£€æµ‹åˆ° `VALIDATION_REQUIRED` (403) é”™è¯¯æ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å°†è¯¥è´¦å·æ ‡è®°ä¸ºâ€œä¸´æ—¶é˜»æ–­â€çŠ¶æ€å¹¶æŒç»­ 10 åˆ†é’Ÿã€‚æœŸé—´è¯·æ±‚ä¼šè‡ªåŠ¨è·³è¿‡è¯¥è´¦å·ï¼Œé¿å…æ— æ•ˆé‡è¯•å¯¼è‡´è´¦å·è¢«è¿›ä¸€æ­¥é£æ§ã€‚
            -   **è‡ªåŠ¨æ¢å¤**: é˜»æ–­æœŸè¿‡åï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å°è¯•æ¢å¤è¯¥è´¦å·çš„ä½¿ç”¨ã€‚
        -   **[æ ¸å¿ƒä¿®å¤] è´¦å·çŠ¶æ€çƒ­é‡è½½ (Account Hot-Reload)**:
            -   **æ¶æ„ç»Ÿä¸€**: æ¶ˆé™¤äº†ç³»ç»Ÿä¸­å¹¶å­˜çš„å¤šä¸ª `TokenManager` å®ä¾‹ï¼Œå®ç°äº†ç®¡ç†åå°ä¸åä»£æœåŠ¡å…±äº«å•ä¾‹è´¦å·ç®¡ç†å™¨ã€‚
            -   **å®æ—¶ç”Ÿæ•ˆ**: ä¿®å¤äº†æ‰‹åŠ¨å¯ç”¨/ç¦ç”¨è´¦å·ã€è´¦å·é‡æ’åºåŠæ‰¹é‡æ“ä½œåéœ€è¦é‡å¯åº”ç”¨æ‰èƒ½ç”Ÿæ•ˆçš„é—®é¢˜ã€‚ç°åœ¨æ‰€æœ‰è´¦å·å˜æ›´éƒ½ä¼šç«‹å³åŒæ­¥è‡³å†…å­˜è´¦å·æ± ã€‚
        -   **[æ ¸å¿ƒä¿®å¤] é…é¢ä¿æŠ¤é€»è¾‘ä¼˜åŒ– (PR #1344 è¡¥ä¸)**:
            -   è¿›ä¸€æ­¥ä¼˜åŒ–äº†é…é¢ä¿æŠ¤é€»è¾‘ä¸­å¯¹â€œå·²ç¦ç”¨â€çŠ¶æ€ä¸â€œé…é¢ä¿æŠ¤â€çŠ¶æ€çš„åŒºåˆ†é€»è¾‘ï¼Œç¡®ä¿æ—¥å¿—è®°å½•å‡†ç¡®ä¸”çŠ¶æ€åŒæ­¥å®æ—¶ã€‚
        -   **[æ ¸å¿ƒä¿®å¤] æ¢å¤å¥åº·æ£€æŸ¥æ¥å£ (PR #1364)**:
            -   **è·¯ç”±æ¢å¤**: ä¿®å¤äº†åœ¨ 4.0.0 æ¶æ„è¿ç§»ä¸­é—å¤±çš„ `/health` å’Œ `/healthz` è·¯ç”±ã€‚
            -   **å“åº”å¢å¼º**: æ¥å£ç°åœ¨ä¼šè¿”å›åŒ…å« `&quot;status&quot;: &quot;ok&quot;` å’Œå½“å‰åº”ç”¨ç‰ˆæœ¬å·çš„ JSONï¼Œæ–¹ä¾¿ç›‘æ§ç³»ç»Ÿè¿›è¡Œç‰ˆæœ¬åŒ¹é…å’Œå­˜æ´»æ£€æŸ¥ã€‚
        -   **[æ ¸å¿ƒä¿®å¤] ä¿®å¤ Gemini Flash æ¨¡å‹æ€è€ƒé¢„ç®—è¶…é™ (Fix PR #1355)**:
            -   **è‡ªåŠ¨é™é¢**: ä¿®å¤äº†åœ¨ Gemini Flash æ€è€ƒæ¨¡å‹ï¼ˆå¦‚ `gemini-2.0-flash-thinking`ï¼‰ä¸­ï¼Œé»˜è®¤æˆ–ä¸Šæ¸¸ä¼ å…¥çš„ `thinking_budget` (ä¾‹å¦‚ 32k) è¶…è¿‡æ¨¡å‹ä¸Šé™ (24k) å¯¼è‡´ API æŠ¥é”™ `400 Bad Request` çš„é—®é¢˜ã€‚
            -   **å¤šåè®®è¦†ç›–**: æ­¤é˜²æŠ¤å·²æ‰©å±•è‡³ **OpenAIã€Claude å’ŒåŸç”Ÿ Gemini åè®®**ï¼Œå…¨æ–¹ä½æ‹¦æˆªä¸å®‰å…¨çš„é¢„ç®—é…ç½®ã€‚
            -   **æ™ºèƒ½æˆªæ–­**: ç³»ç»Ÿç°åœ¨ä¼šè‡ªåŠ¨æ£€æµ‹ Flash ç³»åˆ—æ¨¡å‹ï¼Œå¹¶å¼ºåˆ¶å°†æ€è€ƒé¢„é¢„ç®—é™åˆ¶åœ¨å®‰å…¨èŒƒå›´å†… (**24,576**)ï¼Œç¡®ä¿è¯·æ±‚å§‹ç»ˆæˆåŠŸï¼Œæ— éœ€ç”¨æˆ·æ‰‹åŠ¨è°ƒæ•´å®¢æˆ·ç«¯é…ç½®ã€‚
        -   **[æ ¸å¿ƒåŠŸèƒ½] IP å®‰å…¨ä¸é£æ§ç³»ç»Ÿ (IP Security &amp; Management) (PR #1369 by @å¤§é»„)**:
            -   **å¯è§†åŒ–å·¥å•ç®¡ç†**: å…¨æ–°çš„â€œå®‰å…¨ç›‘æ§â€æ¨¡å—ï¼Œæ”¯æŒå›¾å½¢åŒ–ç®¡ç† IP é»‘åå•ä¸ç™½åå•ã€‚
            -   **æ™ºèƒ½å°ç¦ç­–ç•¥**: å®ç°äº†åŸºäº CIDR çš„ç½‘æ®µå°ç¦ã€è‡ªåŠ¨é‡Šæ”¾æ—¶é—´è®¾ç½®åŠå°ç¦åŸå› å¤‡æ³¨åŠŸèƒ½ã€‚
            -   **å®æ—¶è®¿é—®æ—¥å¿—**: é›†æˆäº† IP ç»´åº¦çš„å®æ—¶è®¿é—®æ—¥å¿—å®¡è®¡ï¼Œæ”¯æŒæŒ‰ IPã€æ—¶é—´èŒƒå›´ç­›é€‰ï¼Œæ–¹ä¾¿å¿«é€Ÿå®šä½å¼‚å¸¸æµé‡ã€‚
        -   **[UI ä¼˜åŒ–] æè‡´çš„è§†è§‰ä½“éªŒ**:
            -   **å¼¹çª—ç¾åŒ–**: å…¨é¢å‡çº§äº† IP å®‰å…¨æ¨¡å—çš„æ‰€æœ‰å¼¹çª—æŒ‰é’®æ ·å¼ï¼Œé‡‡ç”¨å®å¿ƒè‰²å—ä¸é˜´å½±è®¾è®¡ï¼Œæ“ä½œå¼•å¯¼æ›´æ¸…æ™°ã€‚
            -   **å¸ƒå±€å³å…´**: ä¿®å¤äº†å®‰å…¨é…ç½®é¡µé¢çš„æ»šåŠ¨æ¡å¼‚å¸¸ä¸å¸ƒå±€é”™ä½ï¼Œä¼˜åŒ–äº†æ ‡ç­¾é¡µåˆ‡æ¢ä½“éªŒã€‚
        -   **[æ ¸å¿ƒåŠŸèƒ½] è°ƒè¯•æ§åˆ¶å° (Debug Console) (PR #1385)**:
            -   **å®æ—¶æ—¥å¿—æµ**: å¼•å…¥äº†å…¨åŠŸèƒ½çš„è°ƒè¯•æ§åˆ¶å°ï¼Œæ”¯æŒå®æ—¶æ•è·å¹¶å±•ç¤ºåç«¯ä¸šåŠ¡æ—¥å¿—ã€‚
            -   **è¿‡æ»¤ä¸æœç´¢**: æ”¯æŒæŒ‰æ—¥å¿—çº§åˆ«ï¼ˆInfo, Debug, Warn, Errorï¼‰è¿‡æ»¤åŠå…³é”®è¯å…¨å±€æœç´¢ã€‚
            -   **äº¤äº’ä¼˜åŒ–**: æ”¯æŒä¸€é”®æ¸…ç©ºæ—¥å¿—ã€è‡ªåŠ¨æ»šåŠ¨å¼€å…³ï¼Œå¹¶å®Œæ•´é€‚é…æ·±è‰²/æµ…è‰²ä¸»é¢˜ã€‚
            -   **åç«¯æ¡¥æ¥**: å®ç°äº†é«˜æ€§èƒ½çš„æ—¥å¿—æ¡¥æ¥å™¨ï¼Œç¡®ä¿æ—¥å¿—æ•è·ä¸å½±å“åä»£æ€§èƒ½ã€‚
    *   **v4.0.9 (2026-01-30)**:
        -   **[æ ¸å¿ƒåŠŸèƒ½] User-Agent è‡ªå®šä¹‰ä¸ç‰ˆæœ¬æ¬ºéª— (PR #1325)**:
            - **åŠ¨æ€è¦†ç›–**: æ”¯æŒåœ¨â€œæœåŠ¡é…ç½®â€ä¸­è‡ªå®šä¹‰ä¸Šæ¸¸è¯·æ±‚çš„ `User-Agent` å¤´éƒ¨ã€‚è¿™å…è®¸ç”¨æˆ·æ¨¡æ‹Ÿä»»æ„å®¢æˆ·ç«¯ç‰ˆæœ¬ï¼ˆå¦‚ Cheat æ¨¡å¼ï¼‰ï¼Œæœ‰æ•ˆç»•è¿‡éƒ¨åˆ†åœ°åŒºçš„ç‰ˆæœ¬å°é”æˆ–é£æ§é™åˆ¶ã€‚
            - **æ™ºèƒ½å›é€€**: å®ç°äº†â€œè¿œç¨‹æŠ“å– -&gt; Cargo ç‰ˆæœ¬ -&gt; ç¡¬ç¼–ç â€çš„ä¸‰çº§ç‰ˆæœ¬å·è·å–æœºåˆ¶ã€‚å½“ä¸»ç‰ˆæœ¬ API ä¸å¯ç”¨æ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è§£æå®˜ç½‘ Changelog é¡µé¢è·å–æœ€æ–°ç‰ˆæœ¬å·ï¼Œç¡®ä¿ UA å§‹ç»ˆä¼ªè£…æˆæœ€æ–°ç‰ˆå®¢æˆ·ç«¯ã€‚
            - **çƒ­æ›´æ–°æ”¯æŒ**: ä¿®æ”¹ UA é…ç½®åå³åˆ»ç”Ÿæ•ˆï¼Œæ— éœ€é‡å¯æœåŠ¡ã€‚
        -   **[æ ¸å¿ƒä¿®å¤] è§£å†³é…é¢ä¿æŠ¤çŠ¶æ€åŒæ­¥ç¼ºé™· (Issue #1344)**:
            - **çŠ¶æ€å®æ—¶åŒæ­¥**: ä¿®å¤äº† `check_and_protect_quota()` å‡½æ•°åœ¨å¤„ç†ç¦ç”¨è´¦å·æ—¶æå‰é€€å‡ºçš„é€»è¾‘ç¼ºé™·ã€‚ç°åœ¨å³ä¾¿è´¦å·è¢«ç¦ç”¨ï¼Œç³»ç»Ÿä»ä¼šæ‰«æå¹¶å®æ—¶æ›´æ–°å…¶ `protected_models`ï¼ˆæ¨¡å‹çº§ä¿æŠ¤åˆ—è¡¨ï¼‰ï¼Œç¡®ä¿é…é¢ä¸è¶³çš„è´¦å·åœ¨é‡æ–°å¯ç”¨åä¸ä¼šç»•è¿‡ä¿æŠ¤æœºåˆ¶ç»§ç»­è¢«ä½¿ç”¨ã€‚
            - **æ—¥å¿—è·¯å¾„åˆ†ç¦»**: å°†æ‰‹åŠ¨ç¦ç”¨æ£€æŸ¥ä»é…é¢ä¿æŠ¤å‡½æ•°ä¸­å‰¥ç¦»è‡³è°ƒç”¨æ–¹ï¼Œæ ¹æ®ä¸åŒçš„è·³è¿‡åŸå› ï¼ˆæ‰‹åŠ¨ç¦ç”¨/é…é¢ä¿æŠ¤ï¼‰è®°å½•å‡†ç¡®çš„æ—¥å¿—ï¼Œæ¶ˆé™¤ç”¨æˆ·å›°æƒ‘ã€‚
        -   **[æ ¸å¿ƒåŠŸèƒ½] ç¼“å­˜ç®¡ç†ä¸ä¸€é”®æ¸…ç† (PR #1346)**:
            - **åç«¯é›†æˆ**: æ–°å¢äº† `src-tauri/src/modules/cache.rs` æ¨¡å—ï¼Œç”¨äºè®¡ç®—å’Œç®¡ç†åº”ç”¨è¿è¡ŒæœŸé—´äº§ç”Ÿçš„å„ç±»ä¸´æ—¶æ–‡ä»¶åˆ†å¸ƒï¼ˆå¦‚ç¿»è¯‘ç¼“å­˜ã€æ—¥å¿—æŒ‡çº¹ç­‰ï¼‰ã€‚
            - **UI å®ç°**: åœ¨â€œç³»ç»Ÿè®¾ç½®â€é¡µé¢æ–°å¢äº†â€œæ¸…ç†ç¼“å­˜â€åŠŸèƒ½ã€‚ç”¨æˆ·å¯ä»¥å®æ—¶æŸ¥çœ‹ç¼“å­˜å ç”¨çš„ç©ºé—´å¤§å°ï¼Œå¹¶æ”¯æŒä¸€é”®æ¸…ç†ï¼Œæœ‰æ•ˆè§£å†³é•¿æœŸä½¿ç”¨åçš„ç£ç›˜å ç”¨é—®é¢˜ã€‚
        -   **[å›½é™…åŒ–] æ–°å¢è¯­è¨€æ”¯æŒ (PR #1346)**:
            - æ–°å¢äº† **è¥¿ç­ç‰™è¯­ (es)** å’Œ **é©¬æ¥è¯­ (my)** çš„å®Œæ•´ç¿»è¯‘æ”¯æŒï¼Œè¿›ä¸€æ­¥æ‰©å¤§äº†åº”ç”¨çš„å…¨çƒé€‚ç”¨èŒƒå›´ã€‚
        -   **[å›½é™…åŒ–] å…¨è¯­è¨€è¦†ç›–**:
            - ä¸ºæ–°åŠŸèƒ½è¡¥å…¨äº† En, Zh, Zh-TW, Ar, Ja, Ko, Pt, Ru, Tr, Vi ç­‰ 10 ç§è¯­è¨€çš„å®Œæ•´ç¿»è¯‘æ”¯æŒã€‚
        -   **[å›½é™…åŒ–] å®Œå–„ UI å­—ç¬¦ä¸²æœ¬åœ°åŒ– (PR #1350)**:
            - **å…¨é¢è¦†ç›–**: è¡¥å……äº† UI ä¸­å‰©ä½™çš„ç¡¬ç¼–ç å­—ç¬¦ä¸²åŠæœªç¿»è¯‘é¡¹ï¼Œå®ç°äº†ç•Œé¢å­—ç¬¦ä¸²çš„å®Œå…¨æœ¬åœ°åŒ–ã€‚
            - **æ¸…ç†å†—ä½™**: åˆ é™¤äº†ä»£ç ä¸­æ‰€æœ‰çš„è‹±æ–‡å›é€€ (English fallbacks)ï¼Œå¼ºåˆ¶æ‰€æœ‰ç»„ä»¶é€šè¿‡ i18n é”®è°ƒç”¨è¯­è¨€åŒ…ã€‚
            - **è¯­è¨€å¢å¼º**: æ˜¾è‘—æå‡äº†æ—¥è¯­ (ja) ç­‰è¯­è¨€çš„ç¿»è¯‘å‡†ç¡®åº¦ï¼Œå¹¶ç¡®ä¿äº†æ–° UI ç»„ä»¶åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„æ˜¾ç¤ºä¸€è‡´æ€§ã€‚
    *   **v4.0.8 (2026-01-30)**:
        -   **[æ ¸å¿ƒåŠŸèƒ½] è®°å¿†çª—å£ä½ç½®ä¸å¤§å° (PR #1322)**: è‡ªåŠ¨æ¢å¤ä¸Šæ¬¡å…³é—­æ—¶çš„çª—å£åæ ‡ä¸å°ºå¯¸ï¼Œæå‡ä½¿ç”¨ä½“éªŒã€‚
        -   **[æ ¸å¿ƒä¿®å¤] ä¼˜é›…å…³é—­ Admin Server (PR #1323)**: ä¿®å¤äº† Windows ç¯å¢ƒä¸‹é€€å‡ºåå†æ¬¡å¯åŠ¨æ—¶ï¼Œç«¯å£ 8045 å ç”¨å¯¼è‡´çš„ç»‘å®šå¤±è´¥é—®é¢˜ã€‚
        -   **[æ ¸å¿ƒåŠŸèƒ½] å®ç°å…¨é“¾è·¯è°ƒè¯•æ—¥å¿—åŠŸèƒ½ (PR #1308)**:
            - **åç«¯é›†æˆ**: å¼•å…¥äº† `debug_logger.rs`ï¼Œæ”¯æŒæ•è·å¹¶è®°å½• OpenAIã€Claude åŠ Gemini å¤„ç†å™¨çš„åŸå§‹è¯·æ±‚ã€è½¬æ¢åæŠ¥æ–‡åŠå®Œæ•´æµå¼å“åº”ã€‚
            - **åŠ¨æ€é…ç½®**: æ”¯æŒçƒ­åŠ è½½æ—¥å¿—é…ç½®ï¼Œæ— éœ€é‡å¯æœåŠ¡å³å¯å¯ç”¨/ç¦ç”¨æˆ–ä¿®æ”¹è¾“å‡ºç›®å½•ã€‚
            - **å‰ç«¯äº¤äº’**: åœ¨â€œé«˜çº§è®¾ç½®â€ä¸­æ–°å¢â€œè°ƒè¯•æ—¥å¿—â€å¼€å…³åŠè‡ªå®šä¹‰è¾“å‡ºç›®å½•é€‰æ‹©å™¨ï¼Œæ–¹ä¾¿å¼€å‘è€…æ’æŸ¥åè®®è½¬æ¢ä¸ä¸Šæ¸¸é€šä¿¡é—®é¢˜ã€‚
        -   **[UI ä¼˜åŒ–] ä¼˜åŒ–å›¾è¡¨å·¥å…·æç¤º (Tooltip) æµ®åŠ¨æ˜¾ç¤ºé€»è¾‘ (Issue #1263, PR #1307)**:
            - **æº¢å‡ºé˜²å¾¡**: ä¼˜åŒ–äº† `TokenStats.tsx` ä¸­çš„ Tooltip å®šä½ç®—æ³•ï¼Œç¡®ä¿åœ¨å°çª—å£æˆ–é«˜ç¼©æ”¾æ¯”ä¾‹ä¸‹ï¼Œæ‚¬æµ®æç¤ºä¿¡æ¯å§‹ç»ˆåœ¨å¯è§†åŒºåŸŸå†…æ˜¾ç¤ºï¼Œé˜²æ­¢è¢«çª—å£è¾¹ç•Œé®æŒ¡ã€‚
        -   **[æ ¸å¿ƒä¼˜åŒ–] é²æ£’æ€§å¢å¼ºï¼šåŠ¨æ€ User-Agent ç‰ˆæœ¬è·å–åŠå¤šçº§å›é€€ (PR #1316)**:
            - **åŠ¨æ€ç‰ˆæœ¬è·å–**: æ”¯æŒä»è¿œç¨‹ç«¯ç‚¹å®æ—¶æ‹‰å–ç‰ˆæœ¬å·ï¼Œç¡®ä¿ UA ä¿¡æ¯çš„å®æ—¶æ€§ä¸å‡†ç¡®æ€§ã€‚
            - **ç¨³å»¶å›é€€é“¾**: å¼•å…¥â€œè¿œç¨‹ç«¯ç‚¹ -&gt; Cargo.toml -&gt; ç¡¬ç¼–ç â€çš„ä¸‰çº§ç‰ˆæœ¬å›é€€æœºåˆ¶ï¼Œæå¤§æå‡äº†åˆå§‹åŒ–é˜¶æ®µçš„é²æ£’æ€§ã€‚
            - **é¢„ç¼–è¯‘ä¼˜åŒ–**: ä½¿ç”¨ `LazyLock` é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼è§£æç‰ˆæœ¬å·ï¼Œæå‡è¿è¡Œæ•ˆç‡å¹¶é™ä½å†…å­˜æŠ–åŠ¨ã€‚
            - **å¯è§‚æµ‹æ€§æå‡**: æ·»åŠ äº†ç»“æ„åŒ–æ—¥å¿—è®°å½•åŠ VersionSource æšä¸¾ï¼Œæ–¹ä¾¿å¼€å‘è€…è¿½è¸ªç‰ˆæœ¬æ¥æºåŠæ½œåœ¨çš„è·å–æ•…éšœã€‚
        -   **[æ ¸å¿ƒä¿®å¤] è§£å†³ Gemini CLI &quot;Response stopped due to malformed function call.&quot; é”™è¯¯ (PR #1312)**:
            - **å‚æ•°å­—æ®µå¯¹é½**: å°†å·¥å…·å£°æ˜ä¸­çš„ `parametersJsonSchema` é‡å‘½åä¸º `parameters`ï¼Œç¡®ä¿ä¸ Gemini æœ€æ–° API è§„èŒƒå®Œå…¨å¯¹é½ã€‚
            - **å‚æ•°å¯¹é½å¼•æ“å¢å¼º**: ç§»é™¤äº†å¤šä½™çš„å‚æ•°åŒ…è£…å±‚ï¼Œä½¿å‚æ•°ä¼ é€’æ›´åŠ é€æ˜å’Œç›´æ¥ã€‚
            - **å®¹é”™æ ¡éªŒ**: å¢å¼ºäº†å¯¹å·¥å…·è°ƒç”¨å“åº”çš„é²æ£’æ€§ï¼Œæœ‰æ•ˆé˜²æ­¢å› å‚æ•°ç»“æ„ä¸åŒ¹é…å¯¼è‡´çš„è¾“å‡ºä¸­æ–­ã€‚
        -   **[æ ¸å¿ƒä¿®å¤] è§£å†³ Docker/Headless æ¨¡å¼ä¸‹ç«¯å£æ˜¾ç¤ºä¸º &#039;undefined&#039; çš„é—®é¢˜ (Issue #1305)**: ä¿®å¤äº†ç®¡ç† API `/api/proxy/status` ç¼ºå°‘ `port` å­—æ®µä¸” `base_url` æ„é€ é”™è¯¯çš„é—®é¢˜ï¼Œç¡®ä¿å‰ç«¯èƒ½æ­£ç¡®æ˜¾ç¤ºç›‘å¬åœ°å€ã€‚
        -   **[æ ¸å¿ƒä¿®å¤] è§£å†³ Docker/Headless æ¨¡å¼ä¸‹ Web å¯†ç ç»•è¿‡é—®é¢˜ (Issue #1309)**:
            - **é»˜è®¤é‰´æƒå¢å¼º**: å°† `auth_mode` é»˜è®¤å€¼æ”¹ä¸º `auto`ã€‚åœ¨ Docker æˆ–å…è®¸å±€åŸŸç½‘è®¿é—®çš„ç¯å¢ƒä¸‹ï¼Œç³»ç»Ÿç°åœ¨ä¼šè‡ªåŠ¨æ¿€æ´»èº«ä»½éªŒè¯ï¼Œç¡®ä¿ `WEB_PASSWORD` ç”Ÿæ•ˆã€‚
            - **ç¯å¢ƒå˜é‡æ”¯æŒ**: æ–°å¢ `ABV_AUTH_MODE` å’Œ `AUTH_MODE` ç¯å¢ƒå˜é‡ï¼Œå…è®¸ç”¨æˆ·åœ¨å¯åŠ¨æ—¶æ˜¾å¼è¦†ç›–é‰´æƒæ¨¡å¼ï¼ˆæ”¯æŒ `off`, `strict`, `all_except_health`, `auto`ï¼‰ã€‚
    *   **v4.0.7 (2026-01-29)**:
        -   **[æ€§èƒ½ä¼˜åŒ–] ä¼˜åŒ– Docker æ„å»ºæµç¨‹ (Fix Issue #1271)**:
            - **åŸç”Ÿæ¶æ„æ„å»º**: å°† AMD64 å’Œ ARM64 çš„æ„å»ºä»»åŠ¡æ‹†åˆ†ä¸ºç‹¬ç«‹ Job å¹¶è¡Œæ‰§è¡Œï¼Œå¹¶ç§»é™¤ QEMU æ¨¡æ‹Ÿå±‚ï¼Œè½¬è€Œä½¿ç”¨å„æ¶æ„åŸç”Ÿçš„ GitHub Runnerã€‚æ­¤ä¸¾å°†è·¨å¹³å°æ„å»ºè€—æ—¶ä» 3 å°æ—¶å¤§å¹…ç¼©å‡è‡³ 10 åˆ†é’Ÿä»¥å†…ã€‚

        -   **[æ€§èƒ½ä¼˜åŒ–] è§£å†³ Docker ç‰ˆæœ¬åœ¨å¤§æ•°æ®é‡ä¸‹çš„å¡é¡¿ä¸å´©æºƒé—®é¢˜ (Fix Issue #1269)**:
            - **å¼‚æ­¥æ•°æ®åº“æ“ä½œ**: å°†æµé‡æ—¥å¿—ã€Token ç»Ÿè®¡ç­‰æ‰€æœ‰è€—æ—¶æ•°æ®åº“æŸ¥è¯¢è¿ç§»è‡³åå°é˜»å¡çº¿ç¨‹æ±  (`spawn_blocking`)ï¼Œå½»åº•è§£å†³äº†åœ¨æŸ¥çœ‹å¤§å‹æ—¥å¿—æ–‡ä»¶ï¼ˆ800MB+ï¼‰æ—¶å¯èƒ½å¯¼è‡´çš„ UI å¡æ­»åŠåä»£æœåŠ¡ä¸å¯ç”¨çš„é—®é¢˜ã€‚
            - **ç›‘æ§é€»è¾‘å¹³æ»‘åŒ–**: ä¼˜åŒ–äº†ç›‘æ§çŠ¶æ€åˆ‡æ¢é€»è¾‘ï¼Œç§»é™¤å†—ä½™çš„é‡å¤å¯åŠ¨è®°å½•ï¼Œæå‡äº† Docker ç¯å¢ƒä¸‹çš„è¿è¡Œç¨³å®šæ€§ã€‚
        -   **[æ ¸å¿ƒä¿®å¤] è§£å†³ OpenAI åè®® 400 Invalid Argument é”™è¯¯ (Fix Issue #1267)**:
            - **ç§»é™¤æ¿€è¿›é»˜è®¤å€¼**: å›æ»šäº† v4.0.6 ä¸­ä¸º OpenAI/Claude åè®®å¼•å…¥çš„é»˜è®¤ `maxOutputTokens: 81920` è®¾ç½®ã€‚è¯¥å€¼è¶…è¿‡äº†è®¸å¤šæ—§æ¨¡å‹ï¼ˆå¦‚ `gemini-3-pro-preview` æˆ–åŸç”Ÿ Claude 3.5ï¼‰çš„ç¡¬æ€§é™åˆ¶ï¼Œå¯¼è‡´è¯·æ±‚è¢«ç›´æ¥æ‹’ç»ã€‚
            - **æ™ºèƒ½æ€ç»´é…ç½®**: ä¼˜åŒ–äº†æ€ç»´æ¨¡å‹æ£€æµ‹é€»è¾‘ï¼Œä»…å¯¹ä»¥ `-thinking` ç»“å°¾çš„æ¨¡å‹è‡ªåŠ¨æ³¨å…¥ `thinkingConfig`ï¼Œé¿å…äº†å¯¹ä¸æ”¯æŒè¯¥å‚æ•°çš„æ ‡å‡†æ¨¡å‹ï¼ˆå¦‚ `gemini-3-pro`ï¼‰äº§ç”Ÿå‰¯ä½œç”¨ã€‚
        -   **[å…¼å®¹æ€§ä¿®å¤] ä¿®å¤ OpenAI Codex (v0.92.0) è°ƒç”¨é”™è¯¯ (Fix Issue #1278)**:
            - **å­—æ®µæ¸…æ´—**: è‡ªåŠ¨è¿‡æ»¤ Codex å®¢æˆ·ç«¯åœ¨å·¥å…·å®šä¹‰ä¸­æ³¨å…¥çš„éæ ‡å‡† `external_web_access` å­—æ®µï¼Œæ¶ˆé™¤äº† Gemini API è¿”å›çš„ 400 Invalid Argument é”™è¯¯.
            - **å®¹é”™å¢å¼º**: å¢åŠ äº†å¯¹å·¥å…· `name` å­—æ®µçš„å¼ºåˆ¶æ ¡éªŒã€‚å½“å®¢æˆ·ç«¯å‘é€ç¼ºå¤±åç§°çš„æ— æ•ˆå·¥å…·å®šä¹‰æ—¶ï¼Œä»£ç†å±‚ç°åœ¨ä¼šè‡ªåŠ¨è·³è¿‡å¹¶è®°å½•è­¦å‘Šï¼Œè€Œä¸æ˜¯ç›´æ¥è®©è¯·æ±‚å¤±è´¥ã€‚
        -   **[æ ¸å¿ƒåŠŸèƒ½] è‡ªé€‚åº”ç†”æ–­å™¨ (Adaptive Circuit Breaker)**:
            - **æ¨¡å‹çº§éš”ç¦»**: å®ç°äº†åŸºäº `account_id:model` çš„å¤åˆ Key é™æµè¿½è¸ªï¼Œç¡®ä¿å•ä¸€æ¨¡å‹çš„é…é¢è€—å°½ä¸ä¼šå¯¼è‡´æ•´ä¸ªè´¦å·è¢«é”å®šã€‚
            - **åŠ¨æ€é€€é¿ç­–ç•¥**: æ”¯æŒç”¨æˆ·è‡ªå®šä¹‰ `[60, 300, 1800, 7200]` ç­‰å¤šçº§é€€é¿é˜¶æ¢¯ï¼Œè‡ªåŠ¨æ ¹æ®å¤±è´¥æ¬¡æ•°å¢åŠ é”å®šæ—¶é—´ã€‚
            - **é…ç½®çƒ­æ›´æ–°**: é…åˆ `TokenManager` å†…å­˜ç¼“å­˜ï¼Œå®ç°é…ç½®ä¿®æ”¹ååä»£æœåŠ¡å³åˆ»ç”Ÿæ•ˆï¼Œæ— éœ€é‡å¯ã€‚
            - **ç®¡ç† UI é›†æˆ**: åœ¨ API åä»£é¡µé¢æ–°å¢äº†å®Œæ•´çš„æ§åˆ¶é¢æ¿ï¼Œæ”¯æŒä¸€é”®å¼€å…³åŠæ‰‹åŠ¨æ¸…é™¤é™æµè®°å½•ã€‚
        -   **[æ ¸å¿ƒä¼˜åŒ–] å®Œå–„æ—¥å¿—æ¸…ç†ä¸å†—ä½™å‹åˆ¶ (Fix Issue #1280)**:
            - **è‡ªåŠ¨ç©ºé—´å›æ”¶**: å¼•å…¥åŸºäºä½“ç§¯çš„æ¸…ç†æœºåˆ¶ï¼Œå½“æ—¥å¿—ç›®å½•è¶…è¿‡ 1GB æ—¶è‡ªåŠ¨è§¦å‘æ¸…ç†ï¼Œå¹¶å°†å ç”¨é™è‡³ 512MB ä»¥å†…ã€‚ç›¸æ¯”åŸæœ‰çš„æŒ‰å¤©æ¸…ç†ï¼Œèƒ½ä»æ ¹æœ¬ä¸Šé˜²æ­¢å› æ—¥å¿—çˆ†å‘å¯¼è‡´çš„ç£ç›˜æ’‘çˆ†é—®é¢˜ã€‚
            - **é«˜é¢‘æ—¥å¿—ç˜¦èº«**: å°† OpenAI å¤„ç†å™¨æŠ¥æ–‡è¯¦æƒ…ã€TokenManager è´¦å·æ± è½®è¯¢ç­‰é«˜é¢‘äº§ç”Ÿçš„æ—¥å¿—çº§åˆ«ä» INFO é™çº§ä¸º DEBUGã€‚ç°åœ¨ INFO çº§åˆ«ä»…ä¿ç•™ç®€æ´çš„è¯·æ±‚æ‘˜è¦ã€‚
    *   **v4.0.6 (2026-01-28)**:
        -   **[æ ¸å¿ƒä¿®å¤] å½»åº•è§£å†³ Google OAuth &quot;Account already exists&quot; é”™è¯¯**:
            - **æŒä¹…åŒ–å‡çº§**: å°†æˆæƒæˆåŠŸåçš„ä¿å­˜é€»è¾‘ä»â€œä»…æ–°å¢â€å‡çº§ä¸º `upsert` (æ›´æ–°æˆ–æ–°å¢) æ¨¡å¼ã€‚ç°åœ¨é‡æ–°æˆæƒå·²å­˜åœ¨çš„è´¦å·ä¼šå¹³æ»‘æ›´æ–°å…¶ Token å’Œé¡¹ç›®ä¿¡æ¯ï¼Œä¸å†å¼¹å‡ºæŠ¥é”™ã€‚
        -   **[æ ¸å¿ƒä¿®å¤] ä¿®å¤ Docker/Web æ¨¡å¼ä¸‹æ‰‹åŠ¨å›å¡«æˆæƒç å¤±æ•ˆé—®é¢˜**:
            - **Flow çŠ¶æ€é¢„åˆå§‹åŒ–**: åœ¨ Web æ¨¡å¼ç”Ÿæˆæˆæƒé“¾æ¥æ—¶ï¼Œåç«¯ä¼šåŒæ­¥åˆå§‹åŒ– OAuth Flow çŠ¶æ€ã€‚è¿™ç¡®ä¿äº†åœ¨ Docker ç­‰æ— æ³•è‡ªåŠ¨è·³è½¬çš„ç¯å¢ƒä¸‹ï¼Œæ‰‹åŠ¨å¤åˆ¶å›å¡«æˆæƒç æˆ– URL èƒ½å¤Ÿè¢«åç«¯æ­£ç¡®è¯†åˆ«å¹¶å¤„ç†ã€‚
        -   **[ä½“éªŒä¼˜åŒ–] ç»Ÿä¸€ Web ä¸æ¡Œé¢ç«¯çš„ OAuth æŒä¹…åŒ–è·¯å¾„**: é‡æ„äº† `TokenManager`ï¼Œç¡®ä¿æ‰€æœ‰å¹³å°å…±ç”¨åŒä¸€å¥—å¥å£®çš„è´¦å·æ ¸éªŒä¸å­˜å‚¨é€»è¾‘ã€‚
        -   **[æ€§èƒ½ä¼˜åŒ–] ä¼˜åŒ–é™æµæ¢å¤æœºåˆ¶ (PR #1247)**:
            - **è‡ªåŠ¨æ¸…ç†é¢‘ç‡**: å°†é™æµè®°å½•çš„åå°è‡ªåŠ¨æ¸…ç†é—´éš”ä» 60 ç§’ç¼©çŸ­è‡³ 15 ç§’ï¼Œå¤§å¹…æå‡äº†è§¦å‘ 429 æˆ– 503 é”™è¯¯åçš„ä¸šåŠ¡æ¢å¤é€Ÿåº¦ã€‚
            - **æ™ºèƒ½åŒæ­¥æ¸…ç†**: ä¼˜åŒ–äº†å•ä¸ªæˆ–å…¨éƒ¨è´¦å·åˆ·æ–°é€»è¾‘ï¼Œç¡®ä¿åˆ·æ–°è´¦å·çš„åŒæ—¶å³åˆ»æ¸…é™¤æœ¬åœ°é™æµé”å®šï¼Œä½¿æœ€æ–°é…é¢èƒ½ç«‹å³æŠ•å…¥ä½¿ç”¨ã€‚
            - **æ¸è¿›å¼å®¹é‡é€€é¿**: é’ˆå¯¹ `ModelCapacityExhausted` é”™è¯¯ï¼ˆå¦‚ 503ï¼‰ï¼Œå°†åŸæœ‰çš„å›ºå®š 15 ç§’é‡è¯•ç­‰å¾…ä¼˜åŒ–ä¸º `[5s, 10s, 15s]` é˜¶æ¢¯å¼ç­–ç•¥ï¼Œæ˜¾è‘—å‡å°‘äº†å¶å‘æ€§å®¹é‡æ³¢åŠ¨çš„ç­‰å¾…æ—¶é—´ã€‚
        -   **[æ ¸å¿ƒä¿®å¤] çª—å£æ ‡é¢˜æ æ·±è‰²æ¨¡å¼é€‚é… (PR #1253)**: ä¿®å¤äº†åœ¨ç³»ç»Ÿåˆ‡æ¢ä¸ºæ·±è‰²æ¨¡å¼æ—¶ï¼Œåº”ç”¨æ ‡é¢˜æ ï¼ˆTitlebarï¼‰æœªèƒ½åŒæ­¥åˆ‡æ¢é…è‰²ï¼Œå¯¼è‡´è§†è§‰ä¸ç»Ÿä¸€çš„é—®é¢˜ã€‚
        -   **[æ ¸å¿ƒä¿®å¤] æå‡ Opus 4.5 é»˜è®¤è¾“å‡ºä¸Šé™ (F

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[fish-shell/fish-shell]]></title>
            <link>https://github.com/fish-shell/fish-shell</link>
            <guid>https://github.com/fish-shell/fish-shell</guid>
            <pubDate>Sun, 01 Feb 2026 00:07:21 GMT</pubDate>
            <description><![CDATA[The user-friendly command line shell.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/fish-shell/fish-shell">fish-shell/fish-shell</a></h1>
            <p>The user-friendly command line shell.</p>
            <p>Language: Rust</p>
            <p>Stars: 32,213</p>
            <p>Forks: 2,204</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[rerun-io/rerun]]></title>
            <link>https://github.com/rerun-io/rerun</link>
            <guid>https://github.com/rerun-io/rerun</guid>
            <pubDate>Sun, 01 Feb 2026 00:07:20 GMT</pubDate>
            <description><![CDATA[An open source SDK for logging, storing, querying, and visualizing multimodal and multi-rate data]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/rerun-io/rerun">rerun-io/rerun</a></h1>
            <p>An open source SDK for logging, storing, querying, and visualizing multimodal and multi-rate data</p>
            <p>Language: Rust</p>
            <p>Stars: 10,112</p>
            <p>Forks: 643</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.rerun.io/&quot;&gt;
    &lt;img alt=&quot;banner&quot; src=&quot;https://user-images.githubusercontent.com/1148717/218142418-1d320929-6b7a-486e-8277-fbeef2432529.png&quot;&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;h1 align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pypi.org/project/rerun-sdk/&quot;&gt;                        &lt;img alt=&quot;PyPi&quot;           src=&quot;https://img.shields.io/pypi/v/rerun-sdk.svg&quot;&gt;                              &lt;/a&gt;
  &lt;a href=&quot;https://crates.io/crates/rerun&quot;&gt;                             &lt;img alt=&quot;crates.io&quot;      src=&quot;https://img.shields.io/crates/v/rerun.svg&quot;&gt;                                &lt;/a&gt;
  &lt;a href=&quot;https://github.com/rerun-io/rerun/blob/main/LICENSE-MIT&quot;&gt;    &lt;img alt=&quot;MIT&quot;            src=&quot;https://img.shields.io/badge/license-MIT-blue.svg&quot;&gt;                        &lt;/a&gt;
  &lt;a href=&quot;https://github.com/rerun-io/rerun/blob/main/LICENSE-APACHE&quot;&gt; &lt;img alt=&quot;Apache&quot;         src=&quot;https://img.shields.io/badge/license-Apache-blue.svg&quot;&gt;                     &lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/Gcm8BbTaAj&quot;&gt;                              &lt;img alt=&quot;Rerun Discord&quot;  src=&quot;https://img.shields.io/discord/1062300748202921994?label=Rerun%20Discord&quot;&gt; &lt;/a&gt;
&lt;/h1&gt;

# Time-aware multimodal data stack and visualizations
Rerun is building the multimodal data stack to model, ingest, store, query and view robotics-style data.
It&#039;s used in areas like robotics, spatial and embodied AI, generative media, industrial processing, simulation, security, and health.

Rerun is easy to use!
Use the Rerun SDK (available for C++, Python and Rust) to log data like images, tensors, point clouds, and text.
Logs are streamed to the Rerun Viewer for live visualization or to file for later use.
You can also query the logged data through [our dataframe API](https://rerun.io/docs/howto/dataframe-api).

[Get started](#getting-started) in minutes â€“ no account needed.

* [Run the Rerun Viewer in your browser](https://www.rerun.io/viewer)
* [Read about what Rerun is and who it is for](https://www.rerun.io/docs/getting-started/what-is-rerun)

### A short taste
```py
import rerun as rr  # pip install rerun-sdk

rr.init(&quot;rerun_example_app&quot;)

rr.spawn()  # Spawn a child process with a viewer and connect
# rr.save(&quot;recording.rrd&quot;)  # Stream all logs to disk
# rr.connect_grpc()  # Connect to a remote viewer

# Associate subsequent data with 42 on the â€œframeâ€ timeline
rr.set_time(&quot;frame&quot;, sequence=42)

# Log colored 3D points to the entity at `path/to/points`
rr.log(&quot;path/to/points&quot;, rr.Points3D(positions, colors=colors))
â€¦
```

&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;img src=&quot;https://static.rerun.io/opf_screenshot/bee51040cba93c0bae62ef6c57fa703704012a41/full.png&quot; alt=&quot;&quot;&gt;
    &lt;source media=&quot;(max-width: 480px)&quot; srcset=&quot;https://static.rerun.io/opf_screenshot/bee51040cba93c0bae62ef6c57fa703704012a41/480w.png&quot;&gt;
    &lt;source media=&quot;(max-width: 768px)&quot; srcset=&quot;https://static.rerun.io/opf_screenshot/bee51040cba93c0bae62ef6c57fa703704012a41/768w.png&quot;&gt;
    &lt;source media=&quot;(max-width: 1024px)&quot; srcset=&quot;https://static.rerun.io/opf_screenshot/bee51040cba93c0bae62ef6c57fa703704012a41/1024w.png&quot;&gt;
    &lt;source media=&quot;(max-width: 1200px)&quot; srcset=&quot;https://static.rerun.io/opf_screenshot/bee51040cba93c0bae62ef6c57fa703704012a41/1200w.png&quot;&gt;
  &lt;/picture&gt;
&lt;/p&gt;

## Getting started
* [**C++**](https://www.rerun.io/docs/getting-started/data-in/cpp)
* [**Python**](https://www.rerun.io/docs/getting-started/data-in/python): `pip install rerun-sdk` or on [`conda`](https://github.com/conda-forge/rerun-sdk-feedstock)
* [**Rust**](https://www.rerun.io/docs/getting-started/data-in/rust): `cargo add rerun`

### Installing the Rerun Viewer binary
To stream log data over the network or load our `.rrd` data files you also need the `rerun` binary.
It can be installed with `pip install rerun-sdk` or with `cargo install rerun-cli --locked --features nasm` (see note below).
Note that only the Python SDK comes bundled with the Viewer whereas C++ &amp; Rust always rely on a separate install.

**Note**: the `nasm` Cargo feature requires the [`nasm`](https://github.com/netwide-assembler/nasm) CLI to be installed and available in your path.
Alternatively, you may skip enabling this feature, but this may result in inferior video decoding performance.

You should now be able to run `rerun --help` in any terminal.


### Documentation
- ğŸ“š [High-level docs](http://rerun.io/docs)
- âƒ [Loggable Types](https://www.rerun.io/docs/reference/types)
- âš™ï¸ [Examples](http://rerun.io/examples)
- ğŸ“– [Code snippets](./docs/snippets/INDEX.md)
- ğŸŒŠ [C++ API docs](https://ref.rerun.io/docs/cpp)
- ğŸ [Python API docs](https://ref.rerun.io/docs/python)
- ğŸ¦€ [Rust API docs](https://docs.rs/rerun/)
- â‰ï¸ [Troubleshooting](https://www.rerun.io/docs/getting-started/troubleshooting)


## Status
We are in active development.
There are many features we want to add, and the API is still evolving.
_Expect breaking changes!_

Some shortcomings:
* [The viewer slows down when there are too many entities](https://github.com/rerun-io/rerun/issues/7115)
* [We don&#039;t support transparency yet](https://github.com/rerun-io/rerun/issues/1611)
* The data you want to visualize must fit in RAM
  - See &lt;https://www.rerun.io/docs/howto/limit-ram&gt; for how to bound memory use.
  - We plan on having a disk-based data store some time in the future.
* [Multi-million point clouds can be slow](https://github.com/rerun-io/rerun/issues/1136)


## What is Rerun for?

Rerun is built to help you understand and improve complex processes that include rich multimodal data, like 2D, 3D, text, time series, tensors, etc.
It is used in many industries, including robotics, simulation, computer vision,
or anything that involves a lot of sensors or other signals that evolve over time.

### Example use case
Say you&#039;re building a vacuum cleaning robot and it keeps running into walls. Why is it doing that? You need some tool to debug it, but a normal debugger isn&#039;t gonna be helpful. Similarly, just logging text won&#039;t be very helpful either. The robot may log &quot;Going through doorway&quot; but that won&#039;t explain why it thinks the wall is a door.

What you need is a visual and temporal debugger, that can log all the different representations of the world the robots holds in its little head, such as:

* RGB camera feed
* depth images
* lidar scan
* segmentation image (how the robot interprets what it sees)
* its 3D map of the apartment
* all the objects the robot has detected (or thinks it has detected), as 3D shapes in the 3D map
* its confidence in its prediction
* etc

You also want to see how all these streams of data evolve over time so you can go back and pinpoint exactly what went wrong, when and why.

Maybe it turns out that a glare from the sun hit one of the sensors in the wrong way, confusing the segmentation network leading to bad object detection. Or maybe it was a bug in the lidar scanning code. Or maybe the robot thought it was somewhere else in the apartment, because its odometry was broken. Or it could be one of a thousand other things. Rerun will help you find out!

But seeing the world from the point of the view of the robot is not just for debugging - it will also give you ideas on how to improve the algorithms, new test cases to set up, or datasets to collect. It will also let you explain the brains of the robot to your colleagues, boss, and customers. And so on. Seeing is believing, and an image is worth a thousand words, and multimodal temporal logging is worth a thousand images :)

While seeing and understanding your data is core to making progress in robotics, there is one more thing:
You can also use the data you collected for visualization to create new datasets for training and evaluating the models and algorithms that run on your robot.
Rerun provides query APIs to make it easy to extract clean datasets from your recording for exactly that purpose.

Of course, Rerun is useful for much more than just robots. Any time you have any form of sensors, or 2D or 3D state evolving over time, Rerun is a great tool.


## Business model
Rerun uses an open-core model. Everything in this repository will stay open source and free (both as in beer and as in freedom).

We are also building a commercial data platform.
Right now that is only available for a few select design partners.
[Click here if you&#039;re interested](https://rerun.io/pricing).

The Rerun open source project targets the needs of individual developers.
The commercial product targets the needs specific to teams that build and run computer vision and robotics products.

## How to cite Rerun

When using Rerun in your research, please cite it to acknowledge its contribution to your work. This can be done by
including a reference to Rerun in the software or methods section of your paper.

Suggested citation format:

```bibtex
@software{RerunSDK,
  title = {Rerun: A Visualization SDK for Multimodal Data},
  author = {{Rerun Development Team}},
  url = {https://www.rerun.io},
  version = {insert version number},
  date = {insert date of usage},
  year = {2024},
  publisher = {{Rerun Technologies AB}},
  address = {Online},
  note = {Available from https://www.rerun.io/ and https://github.com/rerun-io/rerun}
}
```

Please replace &quot;insert version number&quot; with the version of Rerun you used and &quot;insert date of usage&quot; with the date(s)
you used the tool in your research.
This citation format helps ensure that Rerun&#039;s development team receives appropriate credit for their work and
facilitates the tool&#039;s discovery by other researchers.

# Development
* [`ARCHITECTURE.md`](ARCHITECTURE.md)
* [`CODE_OF_CONDUCT.md`](CODE_OF_CONDUCT.md)
* [`CODE_STYLE.md`](CODE_STYLE.md)
* [`CONTRIBUTING.md`](CONTRIBUTING.md)
* [`BUILD.md`](BUILD.md)
* [`rerun_py/README.md`](rerun_py/README.md) - instructions for Python SDK
* [`rerun_cpp/README.md`](rerun_cpp/README.md) - instructions for C++ SDK


## Installing a pre-release Python SDK

1. Download the correct `.whl` from [GitHub Releases](https://github.com/rerun-io/rerun/releases)
2. Run `pip install rerun_sdk&lt;â€¦&gt;.whl` (replace `&lt;â€¦&gt;` with the actual filename)
3. Test it: `rerun --version`
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[signalapp/libsignal]]></title>
            <link>https://github.com/signalapp/libsignal</link>
            <guid>https://github.com/signalapp/libsignal</guid>
            <pubDate>Sun, 01 Feb 2026 00:07:19 GMT</pubDate>
            <description><![CDATA[Home to the Signal Protocol as well as other cryptographic primitives which make Signal possible.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/signalapp/libsignal">signalapp/libsignal</a></h1>
            <p>Home to the Signal Protocol as well as other cryptographic primitives which make Signal possible.</p>
            <p>Language: Rust</p>
            <p>Stars: 5,363</p>
            <p>Forks: 661</p>
            <p>Stars today: 5 stars today</p>
            <h2>README</h2><pre>&gt; **ğŸ“£ If you were previously using libsignal from Maven or Gradle, our repository location has changed with the 0.86.6 release. See below for more information.**

# Overview

libsignal contains platform-agnostic APIs used by the official Signal clients and servers, exposed
as a Java, Swift, or TypeScript library. The underlying implementations are written in Rust:

- libsignal-protocol: Implements the Signal protocol, including the [Double Ratchet algorithm][]. A
  replacement for [libsignal-protocol-java][] and [libsignal-metadata-java][].
- signal-crypto: Cryptographic primitives such as AES-GCM. We use [RustCrypto][]&#039;s where we can
  but sometimes have differing needs.
- device-transfer: Support logic for Signal&#039;s device-to-device transfer feature.
- attest: Functionality for remote attestation of [SGX enclaves][] and server-side [HSMs][].
- zkgroup: Functionality for [zero-knowledge groups][] and related features available in Signal.
- zkcredential: An abstraction for the sort of zero-knowledge credentials used by zkgroup, based on the paper &quot;[The Signal Private Group System][]&quot; by Chase, Perrin, and Zaverucha.
- poksho: Utilities for implementing zero-knowledge proofs (such as those used by zkgroup); stands for &quot;proof-of-knowledge, stateful-hash-object&quot;.
- account-keys: Functionality for consistently using [PINs][] as passwords in Signal&#039;s Secure Value Recovery system, as well as other account-wide key operations.
- usernames: Functionality for username generation, hashing, and proofs.
- media: Utilities for manipulating media.

This repository is used by the Signal client apps ([Android][], [iOS][], and [Desktop][]) as well as
server-side. Use outside of Signal is unsupported. In particular, the products of this repository
are the Java, Swift, and TypeScript libraries that wrap the underlying Rust implementations. All
APIs and implementations are subject to change without notice, as are the JNI, C, and Node add-on
&quot;bridge&quot; layers. However, backwards-incompatible changes to the Java, Swift, TypeScript, and
non-bridge Rust APIs will be reflected in the version number on a best-effort basis, including
increases to the minimum supported tools versions.

[Double Ratchet algorithm]: https://signal.org/docs/
[libsignal-protocol-java]: https://github.com/signalapp/libsignal-protocol-java
[libsignal-metadata-java]: https://github.com/signalapp/libsignal-metadata-java
[RustCrypto]: https://github.com/RustCrypto
[Noise protocol]: http://noiseprotocol.org/
[SGX enclaves]: https://www.intel.com/content/www/us/en/architecture-and-technology/software-guard-extensions.html
[HSMs]: https://en.wikipedia.org/wiki/Hardware_security_module
[zero-knowledge groups]: https://signal.org/blog/signal-private-group-system/
[The Signal Private Group System]: https://eprint.iacr.org/2019/1416.pdf
[PINs]: https://signal.org/blog/signal-pins/
[Android]: https://github.com/signalapp/Signal-Android
[iOS]: https://github.com/signalapp/Signal-iOS
[Desktop]: https://github.com/signalapp/Signal-Desktop


# Building

### Toolchain Installation

To build anything in this repository you must have [Rust](https://rust-lang.org) installed, as well
as recent versions of Clang, libclang, [CMake](https://cmake.org), Make, protoc, Python (3.9+), and git.

#### Linux/Debian

On a Debian-like system, you can get these extra dependencies through `apt`:

```shell
$ apt-get install clang libclang-dev cmake make protobuf-compiler python3 git
```

#### macOS

On macOS, we have a best-effort maintained script to set up the Rust toolchain you can run by:

```shell
$ bin/mac_setup.sh
```

## Rust

### First Build and Test

The build currently uses a specific version of the Rust nightly compiler, which
will be downloaded automatically by cargo. To build and test the basic protocol
libraries:

```shell
$ cargo build
...
$ cargo test
...
```

### Additional Rust Tools

The basic tools above should get you set up for most libsignal Rust development. 

Eventually, you may find that you need some additional Rust tools like `cbindgen` to modify the bridges to the 
client libraries or `taplo` for code formatting. 

You should always install any Rust tools you need that may affect the build from cargo rather than from your system
package manager (e.g. `apt` or `brew`). Package managers sometimes contain outdated versions of these tools that can break
the build with incompatibility issues (especially cbindgen).

To install the main Rust extra dependencies matching the versions we use, you can run the following commands:

```shell
$ cargo +stable install --version &quot;$(cat .cbindgen-version)&quot; --locked cbindgen
$ cargo +stable install --version &quot;$(cat acknowledgments/cargo-about-version)&quot; --locked cargo-about
$ cargo +stable install --version &quot;$(cat .taplo-cli-version)&quot; --locked taplo-cli
$ cargo +stable install cargo-fuzz
```

## Java/Android

### Toolchain Setup / Configuration

To build for Android you must install several additional packages including a JDK,
the Android NDK/SDK, and add the Android targets to the Rust compiler, using

```rustup target add armv7-linux-androideabi aarch64-linux-android i686-linux-android x86_64-linux-android```

Our officially supported JDK version for Android builds is JDK 17, so be sure to install e.g. OpenJDK 17, and then point JAVA_HOME to it.

You can easily do this on macOS via:

```shell
export JAVA_HOME=$(/usr/libexec/java_home -v 17)
```

On Linux, the way you do this varies by distribution. For Debian based distributions like Ubuntu, you can use:

```shell
sudo update-alternatives --config java
```

We also check-in a `.tools_version` file for use with runtime version managers.

### Building and Testing

To build the Java/Android ``jar`` and ``aar``, and run the tests:

```shell
$ cd java
$ ./gradlew test
$ ./gradlew build # if you need AAR outputs
```

You can pass `-P debugLevelLogs` to Gradle to build without filtering out debug- and verbose-level
logs from Rust, and `-P jniTypeTagging` to enable additional checks in the Rust JNI bridging code.

Alternately, a build system using Docker is available:

```shell
$ cd java
$ make
```

When exposing new APIs to Java, you will need to run `rust/bridge/jni/bin/gen_java_decl.py` in
addition to rebuilding. This requires installing the `cbindgen` Rust tool, as detailed above. 

### Use as a library

Signal publishes Java packages for its own use, under the names org.signal:libsignal-server,
org.signal:libsignal-client, and org.signal:libsignal-android. libsignal-client and libsignal-server
contain native libraries for Debian-flavored x86_64 Linux as well as Windows (x86_64) and macOS
(x86_64 and arm64). libsignal-android contains native libraries for armeabi-v7a, arm64-v8a, x86, and
x86_64 Android. These are located in a Maven repository at
https://build-artifacts.signal.org/libraries/maven/; for use from Gradle, add the following to your
`repositories` block:

```
maven {
  name = &quot;SignalBuildArtifacts&quot;
  // The &quot;uri()&quot; part is only necessary for Kotlin Gradle; Groovy Gradle accepts a bare string here.
  url = uri(&quot;https://build-artifacts.signal.org/libraries/maven/&quot;)
}
```

Older builds were published to [Maven Central](https://central.sonatype.org) instead.

When building for Android you need *both* libsignal-android and libsignal-client, but the Windows
and macOS libraries in libsignal-client won&#039;t automatically be excluded from your final app. You can
explicitly exclude them using `packagingOptions`:

```
android {
  // ...
  packagingOptions {
    resources {
      excludes += setOf(&quot;libsignal_jni*.dylib&quot;, &quot;signal_jni*.dll&quot;)
    }
  }
  // ...
}
```

You can additionally exclude `libsignal_jni_testing.so` if you do not plan to use any of the APIs
intended for client testing.


## Swift

To learn about the Swift build process see [``swift/README.md``](swift/)


## Node

You&#039;ll need Node installed to build. If you have [nvm][], you can run `nvm use` to select an
appropriate version automatically.

We use `npm` as our package manager, and a Python script to control building the Rust library, accessible as `npm run build`.

```shell
$ cd node
$ nvm use
$ npm install
$ npm run build
$ npm run tsc
$ npm run test
```

When testing changes locally, you can use `npm run build` to do an incremental rebuild of the Rust library. Alternately, `npm run build-with-debug-level-logs` will rebuild without filtering out debug- and verbose-level logs.

When exposing new APIs to Node, you will need to run `rust/bridge/node/bin/gen_ts_decl.py` in
addition to rebuilding.

[nvm]: https://github.com/nvm-sh/nvm

### NPM

Signal publishes the NPM package `@signalapp/libsignal-client` for its own use, including native
libraries for Windows, macOS, and Debian-flavored Linux. Both x64 and arm64 builds are included for
all three platforms, but the arm64 builds for Windows and Linux are considered experimental, since
there are no official builds of Signal for those architectures.


# Contributions

Signal does accept external contributions to this project. However unless the change is
simple and easily understood, for example fixing a bug or portability issue, adding a new
test, or improving performance, first open an issue to discuss your intended change as not
all changes can be accepted.

Contributions that will not be used directly by one of Signal&#039;s official client apps may still be
considered, but only if they do not pose an undue maintenance burden or conflict with the goals of
the project.

Signing a [CLA (Contributor License Agreement)](https://signal.org/cla/) is required for all contributions.

## Code Formatting and Acknowledgments

You can run the styler on the entire project by running:

```shell
just format-all
```

You can run more extensive tests as well as linters and clippy by running:

```shell
just check-pre-commit
```

When making a PR that adjusts dependencies, you&#039;ll need to regenerate our acknowledgments files. See [``acknowledgments/README.md``](acknowledgments/).

# Legal things
## Cryptography Notice

This distribution includes cryptographic software. The country in which you currently reside may have restrictions on
the import, possession, use, and/or re-export to another country, of encryption software.  BEFORE using any encryption
software, please check your country&#039;s laws, regulations and policies concerning the import, possession, or use, and
re-export of encryption software, to see if this is permitted.  See &lt;http://www.wassenaar.org/&gt; for more information.

The U.S. Government Department of Commerce, Bureau of Industry and Security (BIS), has classified this software as
Export Commodity Control Number (ECCN) 5D002.C.1, which includes information security software using or performing
cryptographic functions with asymmetric algorithms.  The form and manner of this distribution makes it eligible for
export under the License Exception ENC Technology Software Unrestricted (TSU) exception (see the BIS Export
Administration Regulations, Section 740.13) for both object code and source code.

## License

Copyright 2020-2026 Signal Messenger, LLC

Licensed under the GNU AGPLv3: https://www.gnu.org/licenses/agpl-3.0.html
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[rustfs/rustfs]]></title>
            <link>https://github.com/rustfs/rustfs</link>
            <guid>https://github.com/rustfs/rustfs</guid>
            <pubDate>Sun, 01 Feb 2026 00:07:18 GMT</pubDate>
            <description><![CDATA[ğŸš€2.3x faster than MinIO for 4KB object payloads. RustFS is an open-source, S3-compatible high-performance object storage system supporting migration and coexistence with other S3-compatible platforms such as MinIO and Ceph.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/rustfs/rustfs">rustfs/rustfs</a></h1>
            <p>ğŸš€2.3x faster than MinIO for 4KB object payloads. RustFS is an open-source, S3-compatible high-performance object storage system supporting migration and coexistence with other S3-compatible platforms such as MinIO and Ceph.</p>
            <p>Language: Rust</p>
            <p>Stars: 20,706</p>
            <p>Forks: 889</p>
            <p>Stars today: 44 stars today</p>
            <h2>README</h2><pre>[![RustFS](https://github.com/user-attachments/assets/1b5afcd6-a2c3-47ff-8bc3-ce882b0ddca7)](https://rustfs.com)

&lt;p align=&quot;center&quot;&gt;RustFS is a high-performance, distributed object storage system built in Rust.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/rustfs/rustfs/actions/workflows/ci.yml&quot;&gt;&lt;img alt=&quot;CI&quot; src=&quot;https://github.com/rustfs/rustfs/actions/workflows/ci.yml/badge.svg&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/rustfs/rustfs/actions/workflows/docker.yml&quot;&gt;&lt;img alt=&quot;Build and Push Docker Images&quot; src=&quot;https://github.com/rustfs/rustfs/actions/workflows/docker.yml/badge.svg&quot; /&gt;&lt;/a&gt;
  &lt;img alt=&quot;GitHub commit activity&quot; src=&quot;https://img.shields.io/github/commit-activity/m/rustfs/rustfs&quot;/&gt;
  &lt;img alt=&quot;Github Last Commit&quot; src=&quot;https://img.shields.io/github/last-commit/rustfs/rustfs&quot;/&gt;
  &lt;a href=&quot;https://hellogithub.com/repository/rustfs/rustfs&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=b95bcb72bdc340b68f16fdf6790b7d5b&amp;claim_uid=MsbvjYeLDKAH457&amp;theme=small&quot; alt=&quot;Featuredï½œHelloGitHub&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/14181&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14181&quot; alt=&quot;rustfs%2Frustfs | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt; 
&lt;a href=&quot;https://runacap.com/ross-index/q4-2025/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;img style=&quot;width: 260px; height: 55px&quot; src=&quot;https://runacap.com/wp-content/uploads/2026/01/ROSS_badge_white_Q4_2025.svg&quot; alt=&quot;ROSS Index - Fastest Growing Open-Source Startups in Q4 2025 | Runa Capital&quot; height=&quot;55&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://docs.rustfs.com/installation/&quot;&gt;Getting Started&lt;/a&gt;
  Â· &lt;a href=&quot;https://docs.rustfs.com/&quot;&gt;Docs&lt;/a&gt;
  Â· &lt;a href=&quot;https://github.com/rustfs/rustfs/issues&quot;&gt;Bug reports&lt;/a&gt;
  Â· &lt;a href=&quot;https://github.com/rustfs/rustfs/discussions&quot;&gt;Discussions&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
English | &lt;a href=&quot;https://github.com/rustfs/rustfs/blob/main/README_ZH.md&quot;&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt; |
  &lt;a href=&quot;https://readme-i18n.com/rustfs/rustfs?lang=de&quot;&gt;Deutsch&lt;/a&gt; |
  &lt;a href=&quot;https://readme-i18n.com/rustfs/rustfs?lang=es&quot;&gt;EspaÃ±ol&lt;/a&gt; |
  &lt;a href=&quot;https://readme-i18n.com/rustfs/rustfs?lang=fr&quot;&gt;franÃ§ais&lt;/a&gt; |
  &lt;a href=&quot;https://readme-i18n.com/rustfs/rustfs?lang=ja&quot;&gt;æ—¥æœ¬èª&lt;/a&gt; |
  &lt;a href=&quot;https://readme-i18n.com/rustfs/rustfs?lang=ko&quot;&gt;í•œêµ­ì–´&lt;/a&gt; |
  &lt;a href=&quot;https://readme-i18n.com/rustfs/rustfs?lang=pt&quot;&gt;Portuguese&lt;/a&gt; |
  &lt;a href=&quot;https://readme-i18n.com/rustfs/rustfs?lang=ru&quot;&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt;
&lt;/p&gt;

RustFS is a high-performance, distributed object storage system built in Rustâ€”one of the most loved programming languages worldwide. RustFS combines the simplicity of MinIO with the memory safety and raw performance of Rust. It offers full S3 compatibility, is completely open-source, and is optimized for data lakes, AI, and big data workloads.

Unlike other storage systems, RustFS is released under the permissible Apache 2.0 license, avoiding the restrictions of AGPL. With Rust as its foundation, RustFS delivers superior speed and secure distributed features for next-generation object storage.

## Feature &amp; Status

- **High Performance**: Built with Rust to ensure maximum speed and resource efficiency.
- **Distributed Architecture**: Scalable and fault-tolerant design suitable for large-scale deployments.
- **S3 Compatibility**: Seamless integration with existing S3-compatible applications and tools.
- **Data Lake Support**: Optimized for high-throughput big data and AI workloads.
- **Open Source**: Licensed under Apache 2.0, encouraging unrestricted community contributions and commercial usage.
- **User-Friendly**: Designed with simplicity in mind for easy deployment and management.

| Feature                 | Status       | Feature                  | Status           |
| :---------------------- | :----------- | :----------------------- | :--------------- |
| **S3 Core Features**    | âœ… Available | **Bitrot Protection**    | âœ… Available     |
| **Upload / Download**   | âœ… Available | **Single Node Mode**     | âœ… Available     |
| **Versioning**          | âœ… Available | **Bucket Replication**   | âœ… Available     |
| **Logging**             | âœ… Available | **Lifecycle Management** | ğŸš§ Under Testing |
| **Event Notifications** | âœ… Available | **Distributed Mode**     | ğŸš§ Under Testing |
| **K8s Helm Charts**     | âœ… Available | **RustFS KMS**           | ğŸš§ Under Testing |

## RustFS vs MinIO Performance

**Stress Test Environment:**

| Type    | Parameter | Remark                                                   |
| ------- | --------- | -------------------------------------------------------- |
| CPU     | 2 Core    | Intel Xeon (Sapphire Rapids) Platinum 8475B, 2.7/3.2 GHz |
| Memory  | 4GB       |                                                          |
| Network | 15Gbps    |                                                          |
| Drive   | 40GB x 4  | IOPS 3800 / Drive                                        |

&lt;https://github.com/user-attachments/assets/2e4979b5-260c-4f2c-ac12-c87fd558072a&gt;

### RustFS vs Other Object Storage

| Feature                | RustFS                                                                                                                                                | Other Object Storage                                                                     |
| :--------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------- |
| **Console Experience** | **Powerful Console**&lt;br&gt;Comprehensive management interface.                                                                                           | **Basic / Limited Console**&lt;br&gt;Often overly simple or lacking critical features.         |
| **Language &amp; Safety**  | **Rust-based**&lt;br&gt;Memory safety by design.                                                                                                            | **Go or C-based**&lt;br&gt;Potential for memory GC pauses or leaks.                            |
| **Data Sovereignty**   | **No Telemetry / Full Compliance**&lt;br&gt;Guards against unauthorized cross-border data egress. Compliant with GDPR (EU/UK), CCPA (US), and APPI (Japan). | **Potential Risk**&lt;br&gt;Possible legal exposure and unwanted data telemetry.               |
| **Licensing**          | **Permissive Apache 2.0**&lt;br&gt;Business-friendly, no &quot;poison pill&quot; clauses.                                                                             | **Restrictive AGPL v3**&lt;br&gt;Risk of license traps and intellectual property pollution.    |
| **Compatibility**      | **100% S3 Compatible**&lt;br&gt;Works with any cloud provider or client, anywhere.                                                                          | **Variable Compatibility**&lt;br&gt;May lack support for local cloud vendors or specific APIs. |
| **Edge &amp; IoT**         | **Strong Edge Support**&lt;br&gt;Ideal for secure, innovative edge devices.                                                                                 | **Weak Edge Support**&lt;br&gt;Often too heavy for edge gateways.                              |
| **Risk Profile**       | **Enterprise Risk Mitigation**&lt;br&gt;Clear IP rights and safe for commercial use.                                                                        | **Legal Risks**&lt;br&gt;Intellectual property ambiguity and usage restrictions.               |

## Staying ahead

Star RustFS on GitHub and be instantly notified of new releases.

&lt;img src=&quot;https://github.com/user-attachments/assets/7ee40bb4-3e46-4eac-b0d0-5fbeb85ff8f3&quot; /&gt;

## Quickstart

To get started with RustFS, follow these steps:

### 1. One-click Installation (Option 1)

```bash
curl -O https://rustfs.com/install_rustfs.sh &amp;&amp; bash install_rustfs.sh
```

### 2\. Docker Quick Start (Option 2)

The RustFS container runs as a non-root user `rustfs` (UID `10001`). If you run Docker with `-v` to mount a host directory, please ensure the host directory owner is set to `10001`, otherwise you will encounter permission denied errors.

```bash
 # Create data and logs directories
 mkdir -p data logs

 # Change the owner of these directories
 chown -R 10001:10001 data logs

 # Using latest version
 docker run -d -p 9000:9000 -p 9001:9001 -v $(pwd)/data:/data -v $(pwd)/logs:/logs rustfs/rustfs:latest

 # Using specific version
 docker run -d -p 9000:9000 -p 9001:9001 -v $(pwd)/data:/data -v $(pwd)/logs:/logs rustfs/rustfs:1.0.0-alpha.76
```

If you use [podman](https://github.com/containers/podman) instead of docker, you can install the RustFS with the below command

```bash
 podman run -d -p 9000:9000 -p 9001:9001 -v $(pwd)/data:/data -v $(pwd)/logs:/logs rustfs/rustfs:latest
```

You can also use Docker Compose. Using the `docker-compose.yml` file in the root directory:

```bash
docker compose --profile observability up -d
```

Similarly, you can run the command with podman

```bash
podman compose --profile observability up -d
```

**NOTE**: We recommend reviewing the `docker-compose.yaml` file before running. It defines several services including Grafana, Prometheus, and Jaeger, which are helpful for RustFS observability. If you wish to start Redis or Nginx containers, you can specify the corresponding profiles.

### 3\. Build from Source (Option 3) - Advanced Users

For developers who want to build RustFS Docker images from source with multi-architecture support:

```bash
# Build multi-architecture images locally
./docker-buildx.sh --build-arg RELEASE=latest

# Build and push to registry
./docker-buildx.sh --push

# Build specific version
./docker-buildx.sh --release v1.0.0 --push

# Build for custom registry
./docker-buildx.sh --registry your-registry.com --namespace yourname --push
```

The `docker-buildx.sh` script supports:

- **Multi-architecture builds**: `linux/amd64`, `linux/arm64`
- **Automatic version detection**: Uses git tags or commit hashes
- **Registry flexibility**: Supports Docker Hub, GitHub Container Registry, etc.
- **Build optimization**: Includes caching and parallel builds

You can also use Make targets for convenience:

```bash
make docker-buildx                    # Build locally
make docker-buildx-push               # Build and push
make docker-buildx-version VERSION=v1.0.0  # Build specific version
make help-docker                      # Show all Docker-related commands
```

&gt; **Heads-up (macOS cross-compilation)**: macOS keeps the default `ulimit -n` at 256, so `cargo zigbuild` or `./build-rustfs.sh --platform ...` may fail with `ProcessFdQuotaExceeded` when targeting Linux. The build script attempts to raise the limit automatically, but if you still see the warning, run `ulimit -n 4096` (or higher) in your shell before building.

### 4\. Build with Helm Chart (Option 4) - Cloud Native

Follow the instructions in the [Helm Chart README](https://charts.rustfs.com/) to install RustFS on a Kubernetes cluster.

### 5\. Nix Flake (Option 5)

If you have [Nix with flakes enabled](https://nixos.wiki/wiki/Flakes#Enable_flakes):

```bash
# Run directly without installing
nix run github:rustfs/rustfs

# Build the binary
nix build github:rustfs/rustfs
./result/bin/rustfs --help

# Or from a local checkout
nix build
nix run
```

---

### Accessing RustFS

1. **Access the Console**: Open your web browser and navigate to `http://localhost:9001` to access the RustFS console.
    - Default credentials: `rustfsadmin` / `rustfsadmin`
2. **Create a Bucket**: Use the console to create a new bucket for your objects.
3. **Upload Objects**: You can upload files directly through the console or use S3-compatible APIs/clients to interact with your RustFS instance.

**NOTE**: To access the RustFS instance via `https`, please refer to the [TLS Configuration Docs](https://docs.rustfs.com/integration/tls-configured.html).

## Documentation

For detailed documentation, including configuration options, API references, and advanced usage, please visit our [Documentation](https://docs.rustfs.com).

## Getting Help

If you have any questions or need assistance:

- Check the [FAQ](https://github.com/rustfs/rustfs/discussions/categories/q-a) for common issues and solutions.
- Join our [GitHub Discussions](https://github.com/rustfs/rustfs/discussions) to ask questions and share your experiences.
- Open an issue on our [GitHub Issues](https://github.com/rustfs/rustfs/issues) page for bug reports or feature requests.

## Links

- [Documentation](https://docs.rustfs.com) - The manual you should read
- [Changelog](https://github.com/rustfs/rustfs/releases) - What we broke and fixed
- [GitHub Discussions](https://github.com/rustfs/rustfs/discussions) - Where the community lives

## Contact

- **Bugs**: [GitHub Issues](https://github.com/rustfs/rustfs/issues)
- **Business**: [hello@rustfs.com](mailto:hello@rustfs.com)
- **Jobs**: [jobs@rustfs.com](mailto:jobs@rustfs.com)
- **General Discussion**: [GitHub Discussions](https://github.com/rustfs/rustfs/discussions)
- **Contributing**: [CONTRIBUTING.md](CONTRIBUTING.md)

## Contributors

RustFS is a community-driven project, and we appreciate all contributions. Check out the [Contributors](https://github.com/rustfs/rustfs/graphs/contributors) page to see the amazing people who have helped make RustFS better.

&lt;a href=&quot;https://github.com/rustfs/rustfs/graphs/contributors&quot;&gt;
&lt;img src=&quot;https://opencollective.com/rustfs/contributors.svg?width=890&amp;limit=500&amp;button=false&quot; alt=&quot;Contributors&quot; /&gt;
&lt;/a&gt;

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=rustfs/rustfs&amp;type=date&amp;legend=top-left)](https://www.star-history.com/#rustfs/rustfs&amp;type=date&amp;legend=top-left)

## License

[Apache 2.0](https://opensource.org/licenses/Apache-2.0)

**RustFS** is a trademark of RustFS, Inc. All other trademarks are the property of their respective owners.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[cloudflare/quiche]]></title>
            <link>https://github.com/cloudflare/quiche</link>
            <guid>https://github.com/cloudflare/quiche</guid>
            <pubDate>Sun, 01 Feb 2026 00:07:17 GMT</pubDate>
            <description><![CDATA[ğŸ¥§ Savoury implementation of the QUIC transport protocol and HTTP/3]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/cloudflare/quiche">cloudflare/quiche</a></h1>
            <p>ğŸ¥§ Savoury implementation of the QUIC transport protocol and HTTP/3</p>
            <p>Language: Rust</p>
            <p>Stars: 11,202</p>
            <p>Forks: 937</p>
            <p>Stars today: 2 stars today</p>
            <h2>README</h2><pre>![quiche](quiche.svg)

[![crates.io](https://img.shields.io/crates/v/quiche.svg)](https://crates.io/crates/quiche)
[![docs.rs](https://docs.rs/quiche/badge.svg)](https://docs.rs/quiche)
[![license](https://img.shields.io/github/license/cloudflare/quiche.svg)](https://opensource.org/licenses/BSD-2-Clause)
![build](https://img.shields.io/github/actions/workflow/status/cloudflare/quiche/stable.yml?branch=master)

[quiche] is an implementation of the QUIC transport protocol and HTTP/3 as
specified by the [IETF]. It provides a low level API for processing QUIC packets
and handling connection state. The application is responsible for providing I/O
(e.g. sockets handling) as well as an event loop with support for timers.

For more information on how quiche came about and some insights into its design
you can read a [post] on Cloudflare&#039;s blog that goes into some more detail.

[quiche]: https://docs.quic.tech/quiche/
[ietf]: https://quicwg.org/
[post]: https://blog.cloudflare.com/enjoy-a-slice-of-quic-and-rust/

Who uses quiche?
----------------

### Cloudflare

quiche powers Cloudflare edge network&#039;s [HTTP/3 support][cloudflare-http3]. The
[cloudflare-quic.com](https://cloudflare-quic.com) website can be used for
testing and experimentation.

### Android

Android&#039;s DNS resolver uses quiche to [implement DNS over HTTP/3][android-http3].

### curl

quiche can be [integrated into curl][curl-http3] to provide support for HTTP/3.

[cloudflare-http3]: https://blog.cloudflare.com/http3-the-past-present-and-future/
[android-http3]: https://security.googleblog.com/2022/07/dns-over-http3-in-android.html
[curl-http3]: https://github.com/curl/curl/blob/master/docs/HTTP3.md#quiche-version

Getting Started
---------------

### Command-line apps

Before diving into the quiche API, here are a few examples on how to use the
quiche tools provided as part of the [quiche-apps](apps/) crate. These are not
suitable for production environments; see [disclaimers and
notes](#disclaimers-and-notes).

After cloning the project according to the command mentioned in the [building](#building) section, the client can be run as follows:

```bash
 $ cargo run --bin quiche-client -- https://cloudflare-quic.com/
```

while the server can be run as follows:

```bash
 $ cargo run --bin quiche-server -- --cert apps/src/bin/cert.crt --key apps/src/bin/cert.key
```

(note that the certificate provided is self-signed and should not be used in
production)

Use the `--help` command-line flag to get a more detailed description of each
tool&#039;s options.

### Configuring connections

The first step in establishing a QUIC connection using quiche is creating a
[`Config`] object:

```rust
let mut config = quiche::Config::new(quiche::PROTOCOL_VERSION)?;
config.set_application_protos(&amp;[b&quot;example-proto&quot;]);

// Additional configuration specific to application and use case...
```

The [`Config`] object controls important aspects of the QUIC connection such
as QUIC version, ALPN IDs, flow control, congestion control, idle timeout
and other properties or features.

QUIC is a general-purpose transport protocol and there are several
configuration properties where there is no reasonable default value. For
example, the permitted number of concurrent streams of any particular type
is dependent on the application running over QUIC, and other use-case
specific concerns.

quiche defaults several properties to zero, applications most likely need
to set these to something else to satisfy their needs using the following:

- [`set_initial_max_streams_bidi()`]
- [`set_initial_max_streams_uni()`]
- [`set_initial_max_data()`]
- [`set_initial_max_stream_data_bidi_local()`]
- [`set_initial_max_stream_data_bidi_remote()`]
- [`set_initial_max_stream_data_uni()`]

[`Config`] also holds TLS configuration. This can be changed by mutators on
the an existing object, or by constructing a TLS context manually and
creating a configuration using [`with_boring_ssl_ctx_builder()`].

A configuration object can be shared among multiple connections.

### Connection setup

On the client-side the [`connect()`] utility function can be used to create
a new connection, while [`accept()`] is for servers:

```rust
// Client connection.
let conn = quiche::connect(Some(&amp;server_name), &amp;scid, local, peer, &amp;mut config)?;

// Server connection.
let conn = quiche::accept(&amp;scid, None, local, peer, &amp;mut config)?;
```

### Handling incoming packets

Using the connection&#039;s [`recv()`] method the application can process
incoming packets that belong to that connection from the network:

```rust
let to = socket.local_addr().unwrap();

loop {
    let (read, from) = socket.recv_from(&amp;mut buf).unwrap();

    let recv_info = quiche::RecvInfo { from, to };

    let read = match conn.recv(&amp;mut buf[..read], recv_info) {
        Ok(v) =&gt; v,

        Err(e) =&gt; {
            // An error occurred, handle it.
            break;
        },
    };
}
```

### Generating outgoing packets

Outgoing packet are generated using the connection&#039;s [`send()`] method
instead:

```rust
loop {
    let (write, send_info) = match conn.send(&amp;mut out) {
        Ok(v) =&gt; v,

        Err(quiche::Error::Done) =&gt; {
            // Done writing.
            break;
        },

        Err(e) =&gt; {
            // An error occurred, handle it.
            break;
        },
    };

    socket.send_to(&amp;out[..write], &amp;send_info.to).unwrap();
}
```

When packets are sent, the application is responsible for maintaining a
timer to react to time-based connection events. The timer expiration can be
obtained using the connection&#039;s [`timeout()`] method.

```rust
let timeout = conn.timeout();
```

The application is responsible for providing a timer implementation, which
can be specific to the operating system or networking framework used. When
a timer expires, the connection&#039;s [`on_timeout()`] method should be called,
after which additional packets might need to be sent on the network:

```rust
// Timeout expired, handle it.
conn.on_timeout();

// Send more packets as needed after timeout.
loop {
    let (write, send_info) = match conn.send(&amp;mut out) {
        Ok(v) =&gt; v,

        Err(quiche::Error::Done) =&gt; {
            // Done writing.
            break;
        },

        Err(e) =&gt; {
            // An error occurred, handle it.
            break;
        },
    };

    socket.send_to(&amp;out[..write], &amp;send_info.to).unwrap();
}
```

#### Pacing

It is recommended that applications [pace] sending of outgoing packets to
avoid creating packet bursts that could cause short-term congestion and
losses in the network.

quiche exposes pacing hints for outgoing packets through the [`at`] field
of the [`SendInfo`] structure that is returned by the [`send()`] method.
This field represents the time when a specific packet should be sent into
the network.

Applications can use these hints by artificially delaying the sending of
packets through platform-specific mechanisms (such as the [`SO_TXTIME`]
socket option on Linux), or custom methods (for example by using user-space
timers).

[pace]: https://datatracker.ietf.org/doc/html/rfc9002#section-7.7
[`SO_TXTIME`]: https://man7.org/linux/man-pages/man8/tc-etf.8.html

### Sending and receiving stream data

After some back and forth, the connection will complete its handshake and
will be ready for sending or receiving application data.

Data can be sent on a stream by using the [`stream_send()`] method:

```rust
if conn.is_established() {
    // Handshake completed, send some data on stream 0.
    conn.stream_send(0, b&quot;hello&quot;, true)?;
}
```

The application can check whether there are any readable streams by using
the connection&#039;s [`readable()`] method, which returns an iterator over all
the streams that have outstanding data to read.

The [`stream_recv()`] method can then be used to retrieve the application
data from the readable stream:

```rust
if conn.is_established() {
    // Iterate over readable streams.
    for stream_id in conn.readable() {
        // Stream is readable, read until there&#039;s no more data.
        while let Ok((read, fin)) = conn.stream_recv(stream_id, &amp;mut buf) {
            println!(&quot;Got {} bytes on stream {}&quot;, read, stream_id);
        }
    }
}
```

### HTTP/3

The quiche [HTTP/3 module] provides a high level API for sending and
receiving HTTP requests and responses on top of the QUIC transport protocol.

[`Config`]: https://docs.quic.tech/quiche/struct.Config.html
[`set_initial_max_streams_bidi()`]: https://docs.rs/quiche/latest/quiche/struct.Config.html#method.set_initial_max_streams_bidi
[`set_initial_max_streams_uni()`]: https://docs.rs/quiche/latest/quiche/struct.Config.html#method.set_initial_max_streams_uni
[`set_initial_max_data()`]: https://docs.rs/quiche/latest/quiche/struct.Config.html#method.set_initial_max_data
[`set_initial_max_stream_data_bidi_local()`]: https://docs.rs/quiche/latest/quiche/struct.Config.html#method.set_initial_max_stream_data_bidi_local
[`set_initial_max_stream_data_bidi_remote()`]: https://docs.rs/quiche/latest/quiche/struct.Config.html#method.set_initial_max_stream_data_bidi_remote
[`set_initial_max_stream_data_uni()`]: https://docs.rs/quiche/latest/quiche/struct.Config.html#method.set_initial_max_stream_data_uni
[`with_boring_ssl_ctx_builder()`]: https://docs.quic.tech/quiche/struct.Config.html#method.with_boring_ssl_ctx_builder
[`connect()`]: https://docs.quic.tech/quiche/fn.connect.html
[`accept()`]: https://docs.quic.tech/quiche/fn.accept.html
[`recv()`]: https://docs.quic.tech/quiche/struct.Connection.html#method.recv
[`send()`]: https://docs.quic.tech/quiche/struct.Connection.html#method.send
[`timeout()`]: https://docs.quic.tech/quiche/struct.Connection.html#method.timeout
[`on_timeout()`]: https://docs.quic.tech/quiche/struct.Connection.html#method.on_timeout
[`stream_send()`]: https://docs.quic.tech/quiche/struct.Connection.html#method.stream_send
[`readable()`]: https://docs.quic.tech/quiche/struct.Connection.html#method.readable
[`stream_recv()`]: https://docs.quic.tech/quiche/struct.Connection.html#method.stream_recv
[HTTP/3 module]: https://docs.quic.tech/quiche/h3/index.html

Have a look at the [quiche/examples/] directory for more complete examples on
how to use the quiche API, including examples on how to use quiche in C/C++
applications (see below for more information).

[examples/]: quiche/examples/

Calling quiche from C/C++
-------------------------

quiche exposes a [thin C API] on top of the Rust API that can be used to more
easily integrate quiche into C/C++ applications (as well as in other languages
that allow calling C APIs via some form of FFI). The C API follows the same
design of the Rust one, modulo the constraints imposed by the C language itself.

When running ``cargo build``, a static library called ``libquiche.a`` will be
built automatically alongside the Rust one. This is fully stand-alone and can
be linked directly into C/C++ applications.

Note that in order to enable the FFI API, the ``ffi`` feature must be enabled (it
is disabled by default), by passing ``--features ffi`` to ``cargo``.

[thin C API]: https://github.com/cloudflare/quiche/blob/master/quiche/include/quiche.h

Building
--------

quiche requires Rust 1.85 or later to build. The latest stable Rust release can
be installed using [rustup](https://rustup.rs/).

Once the Rust build environment is setup, the quiche source code can be fetched
using git:

```bash
 $ git clone --recursive https://github.com/cloudflare/quiche
```

and then built using cargo:

```bash
 $ cargo build --examples
```

cargo can also be used to run the testsuite:

```bash
 $ cargo test
```

Note that [BoringSSL], which is used to implement QUIC&#039;s cryptographic handshake
based on TLS, needs to be built and linked to quiche. This is done automatically
when building quiche using cargo, but requires the `cmake` command to be
available during the build process. On Windows you also need
[NASM](https://www.nasm.us/). The [official BoringSSL
documentation](https://github.com/google/boringssl/blob/master/BUILDING.md) has
more details.

In alternative you can use your own custom build of BoringSSL by configuring
the BoringSSL directory with the ``QUICHE_BSSL_PATH`` environment variable:

```bash
 $ QUICHE_BSSL_PATH=&quot;/path/to/boringssl&quot; cargo build --examples
```

Alternatively you can use [OpenSSL/quictls]. To enable quiche to use this vendor
the ``openssl`` feature can be added to the ``--feature`` list. Be aware that
``0-RTT`` is not supported if this vendor is used.

[BoringSSL]: https://boringssl.googlesource.com/boringssl/

[OpenSSL/quictls]: https://github.com/quictls/openssl

### Building for Android

Building quiche for Android (NDK version 19 or higher, 21 recommended), can be
done using [cargo-ndk] (v2.0 or later).

First the [Android NDK] needs to be installed, either using Android Studio or
directly, and the `ANDROID_NDK_HOME` environment variable needs to be set to the
NDK installation path, e.g.:

```bash
 $ export ANDROID_NDK_HOME=/usr/local/share/android-ndk
```

Then the Rust toolchain for the Android architectures needed can be installed as
follows:

```bash
 $ rustup target add aarch64-linux-android armv7-linux-androideabi i686-linux-android x86_64-linux-android
```

Note that the minimum API level is 21 for all target architectures.

[cargo-ndk] (v2.0 or later) also needs to be installed:

```bash
 $ cargo install cargo-ndk
```

Finally the quiche library can be built using the following procedure. Note that
the `-t &lt;architecture&gt;` and `-p &lt;NDK version&gt;` options are mandatory.

```bash
 $ cargo ndk -t arm64-v8a -p 21 -- build --features ffi
```

See [build_android_ndk19.sh] for more information.

[Android NDK]: https://developer.android.com/ndk
[cargo-ndk]: https://docs.rs/crate/cargo-ndk
[build_android_ndk19.sh]: https://github.com/cloudflare/quiche/blob/master/tools/android/build_android_ndk19.sh

### Building for iOS

To build quiche for iOS, you need the following:

- Install Xcode command-line tools. You can install them with Xcode or with the
  following command:

```bash
 $ xcode-select --install
```

- Install the Rust toolchain for iOS architectures:

```bash
 $ rustup target add aarch64-apple-ios x86_64-apple-ios
```

- Install `cargo-lipo`:

```bash
 $ cargo install cargo-lipo
```

To build libquiche, run the following command:

```bash
 $ cargo lipo --features ffi
```

or

```bash
 $ cargo lipo --features ffi --release
```

iOS build is tested in Xcode 10.1 and Xcode 11.2.

### Building Docker images

In order to build the Docker images, simply run the following command:

```bash
 $ make docker-build
```

You can find the quiche Docker images on the following Docker Hub repositories:

- [cloudflare/quiche](https://hub.docker.com/repository/docker/cloudflare/quiche)
- [cloudflare/quiche-qns](https://hub.docker.com/repository/docker/cloudflare/quiche-qns)

The `latest` tag will be updated whenever quiche master branch updates.

**cloudflare/quiche**

Provides a server and client installed in /usr/local/bin.

**cloudflare/quiche-qns**

Provides the script to test quiche within the [quic-interop-runner](https://github.com/marten-seemann/quic-interop-runner).

Disclaimers and Notes
---------

âš ï¸ This repository includes a number of client and server example
applications that are provided to demonstrate simple usage of the quiche library
API. They are not intended to be used in production environments; no
performance, security or reliability guarantees are provided.


Copyright
---------

Copyright (C) 2018-2019, Cloudflare, Inc.

See [COPYING] for the license.

[COPYING]: https://github.com/cloudflare/quiche/tree/master/COPYING
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[vectordotdev/vector]]></title>
            <link>https://github.com/vectordotdev/vector</link>
            <guid>https://github.com/vectordotdev/vector</guid>
            <pubDate>Sun, 01 Feb 2026 00:07:16 GMT</pubDate>
            <description><![CDATA[A high-performance observability data pipeline.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/vectordotdev/vector">vectordotdev/vector</a></h1>
            <p>A high-performance observability data pipeline.</p>
            <p>Language: Rust</p>
            <p>Stars: 21,231</p>
            <p>Forks: 1,992</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre>[![Nightly](https://github.com/vectordotdev/vector/actions/workflows/nightly.yml/badge.svg)](https://github.com/vectordotdev/vector/actions/workflows/nightly.yml)
[![Integration/E2E Test Suite](https://github.com/vectordotdev/vector/actions/workflows/integration.yml/badge.svg)](https://github.com/vectordotdev/vector/actions/workflows/integration.yml/badge.svg?event=merge_group)
[![Component Features](https://github.com/vectordotdev/vector/actions/workflows/component_features.yml/badge.svg)](https://github.com/vectordotdev/vector/actions/workflows/component_features.yml)

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;website/static/img/diagram.svg&quot; alt=&quot;Vector&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;
    &lt;a href=&quot;https://vector.dev/docs/setup/quickstart/&quot;&gt;Quickstart&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://vector.dev/docs/&quot;&gt;Docs&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://vector.dev/guides/&quot;&gt;Guides&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://vector.dev/components/&quot;&gt;Integrations&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://chat.vector.dev&quot;&gt;Chat&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://vector.dev/releases/latest/download/&quot;&gt;Download&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://rust-doc.vector.dev/&quot;&gt;Rust Crate Docs&lt;/a&gt;
  &lt;/strong&gt;
&lt;/p&gt;

## What is Vector?

Vector is a high-performance, end-to-end (agent &amp; aggregator) observability data
pipeline that puts you in control of your observability data.
[Collect][docs.sources], [transform][docs.transforms], and [route][docs.sinks]
all your logs and metrics to any vendors you want today and any other
vendors you may want tomorrow. Vector enables dramatic cost reduction, novel
data enrichment, and data security where you need it, not where it is most
convenient for your vendors. Additionally, it is open source and up to 10x
faster than every alternative in the space.

To get started, follow our [**quickstart guide**][docs.quickstart] or [**install
Vector**][docs.installation].

Vector is maintained by Datadog&#039;s [Community Open Source Engineering team](https://opensource.datadoghq.com/about/#the-community-open-source-engineering-team).

### Principles

* **Reliable** - Built in [Rust][urls.rust], Vector&#039;s primary design goal is reliability.
* **End-to-end** - Deploys as an [agent][docs.roles#agent] or [aggregator][docs.roles#aggregator]. Vector is a complete platform.
* **Unified** - [Logs][docs.data-model.log], [metrics][docs.data-model.metric] (beta), and traces (coming soon). One tool for all of your data.

### Use cases

* Reduce total observability costs.
* Transition vendors without disrupting workflows.
* Enhance data quality and improve insights.
* Consolidate agents and eliminate agent fatigue.
* Improve overall observability performance and reliability.

### Community

* Vector is relied on by startups and enterprises like **Atlassian**, **T-Mobile**,
  **Comcast**, **Zendesk**, **Discord**, **Fastly**, **CVS**, **Trivago**,
  **Tuple**, **Douban**, **Visa**, **Mambu**, **Blockfi**, **Claranet**,
  **Instacart**, **Forcepoint**, and [many more][urls.production_users].
* Vector is **downloaded over 100,000 times per day**.
* Vector&#039;s largest user **processes over 500TB daily**.
* Vector has **over 500 contributors** and growing.

## Documentation

All user documentation is available at **[vector.dev/docs](https://vector.dev/docs)**.

Other Resources:

* [**Vector Calendar**][urls.vector_calendar]
* **Policies**:
  * [**Code of Conduct**][urls.vector_code_of_conduct]
  * [**Contributing**][urls.vector_contributing_policy]
  * [**Privacy**][urls.vector_privacy_policy]
  * [**Releases**][urls.vector_releases_policy]
  * [**Versioning**][urls.vector_versioning_policy]
  * [**Security**][urls.vector_security_policy]

## Comparisons

### Performance

The following performance tests demonstrate baseline performance between
common protocols with the exception of the Regex Parsing test.

| Test                                                                                                                   | Vector          | Filebeat | FluentBit       | FluentD   | Logstash  | SplunkUF        | SplunkHF |
| ---------------------------------------------------------------------------------------------------------------------- | --------------- | -------- | --------------- | --------- | --------- | --------------- | -------- |
| [TCP to Blackhole](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/tcp_to_blackhole_performance) | _**86mib/s**_   | n/a      | 64.4mib/s       | 27.7mib/s | 40.6mib/s | n/a             | n/a      |
| [File to TCP](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_to_tcp_performance)           | _**76.7mib/s**_ | 7.8mib/s | 35mib/s         | 26.1mib/s | 3.1mib/s  | 40.1mib/s       | 39mib/s  |
| [Regex Parsing](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/regex_parsing_performance)       | 13.2mib/s       | n/a      | _**20.5mib/s**_ | 2.6mib/s  | 4.6mib/s  | n/a             | 7.8mib/s |
| [TCP to HTTP](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/tcp_to_http_performance)           | _**26.7mib/s**_ | n/a      | 19.6mib/s       | &lt;1mib/s   | 2.7mib/s  | n/a             | n/a      |
| [TCP to TCP](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/tcp_to_tcp_performance)             | 69.9mib/s       | 5mib/s   | 67.1mib/s       | 3.9mib/s  | 10mib/s   | _**70.4mib/s**_ | 7.6mib/s |

To learn more about our performance tests, please see the [Vector test harness][urls.vector_test_harness].

### Correctness

The following correctness tests are not exhaustive, but they demonstrate
fundamental differences in quality and attention to detail:

| Test                                                                                                                                 | Vector | Filebeat | FluentBit | FluentD | Logstash | Splunk UF | Splunk HF |
| ------------------------------------------------------------------------------------------------------------------------------------ | ------ | -------- | --------- | ------- | -------- | --------- | --------- |
| [Disk Buffer Persistence](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/disk_buffer_persistence_correctness) | **âœ“**  | âœ“        |           |         | âš         | âœ“         | âœ“         |
| [File Rotate (create)](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_rotate_create_correctness)         | **âœ“**  | âœ“        | âœ“         | âœ“       | âœ“        | âœ“         | âœ“         |
| [File Rotate (copytruncate)](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_rotate_truncate_correctness) | **âœ“**  |          |           |         |          | âœ“         | âœ“         |
| [File Truncation](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_truncate_correctness)                   | **âœ“**  | âœ“        | âœ“         | âœ“       | âœ“        | âœ“         | âœ“         |
| [Process (SIGHUP)](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/sighup_correctness)                         | **âœ“**  |          |           |         | âš         | âœ“         | âœ“         |
| [JSON (wrapped)](https://github.com/vectordotdev/vector-test-harness/tree/master/cases/wrapped_json_correctness)                     | **âœ“**  | âœ“        | âœ“         | âœ“       | âœ“        | âœ“         | âœ“         |

To learn more about our correctness tests, please see the [Vector test harness][urls.vector_test_harness].

### Features

Vector is an end-to-end, unified, open data platform.

|                     | **Vector** | Beats | Fluentbit | Fluentd | Logstash | Splunk UF | Splunk HF | Telegraf |
| ------------------- | ---------- | ----- | --------- | ------- | -------- | --------- | --------- | -------- |
| **End-to-end**      | **âœ“**      |       |           |         |          |           |           | âœ“        |
| Agent               | **âœ“**      | âœ“     | âœ“         |         |          | âœ“         |           | âœ“        |
| Aggregator          | **âœ“**      |       |           | âœ“       | âœ“        |           | âœ“         | âœ“        |
| **Unified**         | **âœ“**      |       |           |         |          |           |           | âœ“        |
| Logs                | **âœ“**      | âœ“     | âœ“         | âœ“       | âœ“        | âœ“         | âœ“         | âœ“        |
| Metrics             | **âœ“**      | âš      | âš          | âš        | âš         | âš          | âš          | âœ“        |
| Traces              | ğŸš§         |       |           |         |          |           |           |          |
| **Open**            | **âœ“**      |       | âœ“         | âœ“       |          |           |           | âœ“        |
| Open-source         | **âœ“**      | âœ“     | âœ“         | âœ“       | âœ“        |           |           | âœ“        |
| Vendor-neutral      | **âœ“**      |       | âœ“         | âœ“       |          |           |           | âœ“        |
| **Reliability**     | **âœ“**      |       |           |         |          |           |           |          |
| Memory-safe         | **âœ“**      |       |           |         |          |           |           | âœ“        |
| Delivery guarantees | **âœ“**      |       |           |         |          | âœ“         | âœ“         |          |
| Multi-core          | **âœ“**      | âœ“     | âœ“         | âœ“       | âœ“        | âœ“         | âœ“         | âœ“        |


âš  = Not interoperable, metrics are represented as structured logs

---

&lt;p align=&quot;center&quot;&gt;
  Developed with â¤ï¸ by &lt;strong&gt;&lt;a href=&quot;https://datadoghq.com&quot;&gt;Datadog&lt;/a&gt;&lt;/strong&gt; - &lt;a href=&quot;https://github.com/vectordotdev/vector/security/policy&quot;&gt;Security Policy&lt;/a&gt; - &lt;a href=&quot;https://github.com/vectordotdev/vector/blob/master/PRIVACY.md&quot;&gt;Privacy Policy&lt;/a&gt;
&lt;/p&gt;

[docs.about.concepts]: https://vector.dev/docs/introduction/concepts/
[docs.about.introduction]: https://vector.dev/docs/introduction/
[docs.administration.monitoring]: https://vector.dev/docs/administration/monitoring/
[docs.administration.management]: https://vector.dev/docs/administration/management/
[docs.administration.upgrading]: https://vector.dev/docs/administration/upgrading/
[docs.administration.validating]: https://vector.dev/docs/administration/validating/
[docs.architecture.concurrency-model]: https://vector.dev/docs/architecture/concurrency-model/
[docs.architecture.data-model]: https://vector.dev/docs/architecture/data-model/
[docs.architecture.pipeline-model]: https://vector.dev/docs/architecture/pipeline-model/
[docs.architecture.runtime-model]: https://vector.dev/docs/architecture/runtime-model/
[docs.configuration.sinks]: https://vector.dev/docs/reference/configuration/sinks/
[docs.configuration.sources]: https://vector.dev/docs/reference/configuration/sources/
[docs.configuration.tests]: https://vector.dev/docs/reference/configuration/tests/
[docs.configuration.transforms]: https://vector.dev/docs/reference/configuration/transforms/
[docs.configuration.enrichment_tables]: https://vector.dev/docs/reference/configuration/global-options/#enrichment_tables
[docs.data-model.log]: https://vector.dev/docs/architecture/data-model/log/
[docs.data-model.metric]: https://vector.dev/docs/architecture/data-model/metric/
[docs.deployment.roles]: https://vector.dev/docs/setup/deployment/roles/
[docs.deployment.topologies]: https://vector.dev/docs/setup/deployment/topologies/
[docs.deployment]: https://vector.dev/docs/setup/deployment/
[docs.installation.manual]: https://vector.dev/docs/setup/installation/manual/
[docs.installation.operating_systems]: https://vector.dev/docs/setup/installation/operating-systems/
[docs.installation.package_managers]: https://vector.dev/docs/setup/installation/package-managers/
[docs.installation.platforms]: https://vector.dev/docs/setup/installation/platforms/
[docs.installation]: https://vector.dev/docs/setup/installation/
[docs.architecture.adaptive-request-concurrency]: https://vector.dev/docs/architecture/arc/
[docs.platforms.kubernetes]: https://vector.dev/docs/setup/installation/platforms/kubernetes/
[docs.quickstart]: https://vector.dev/docs/setup/quickstart/
[docs.reference.api]: https://vector.dev/docs/reference/api/
[docs.reference.cli]: https://vector.dev/docs/reference/cli/
[docs.reference.vrl]: https://vector.dev/docs/reference/vrl/
[docs.roles#agent]: https://vector.dev/docs/setup/deployment/roles/#agent
[docs.roles#aggregator]: https://vector.dev/docs/setup/deployment/roles/#aggregator
[docs.setup.installation]: https://vector.dev/docs/setup/installation/
[docs.setup.quickstart]: https://vector.dev/docs/setup/quickstart/
[docs.sinks.aws_cloudwatch_logs]: https://vector.dev/docs/reference/configuration/sinks/aws_cloudwatch_logs/
[docs.sinks.aws_s3]: https://vector.dev/docs/reference/configuration/sinks/aws_s3/
[docs.sinks.clickhouse]: https://vector.dev/docs/reference/configuration/sinks/clickhouse/
[docs.sinks.elasticsearch]: https://vector.dev/docs/reference/configuration/sinks/elasticsearch/
[docs.sinks.gcp_cloud_storage]: https://vector.dev/docs/reference/configuration/sinks/gcp_cloud_storage/
[docs.sinks]: https://vector.dev/docs/reference/configuration/sinks/
[docs.sources.docker_logs]: https://vector.dev/docs/reference/configuration/sources/docker_logs/
[docs.sources.file]: https://vector.dev/docs/reference/configuration/sources/file/
[docs.sources.http]: https://vector.dev/docs/reference/configuration/sources/http/
[docs.sources.journald]: https://vector.dev/docs/reference/configuration/sources/journald/
[docs.sources.kafka]: https://vector.dev/docs/reference/configuration/sources/kafka/
[docs.sources.socket]: https://vector.dev/docs/reference/configuration/sources/socket/
[docs.sources]: https://vector.dev/docs/reference/configuration/sources/
[docs.transforms.dedupe]: https://vector.dev/docs/reference/configuration/transforms/dedupe/
[docs.transforms.filter]: https://vector.dev/docs/reference/configuration/transforms/filter/
[docs.transforms.log_to_metric]: https://vector.dev/docs/reference/configuration/transforms/log_to_metric/
[docs.transforms.lua]: https://vector.dev/docs/reference/configuration/transforms/lua/
[docs.transforms.remap]: https://vector.dev/docs/reference/configuration/transforms/remap/
[docs.transforms]: https://vector.dev/docs/reference/configuration/transforms/
[docs.introduction.architecture]: https://vector.dev/docs/architecture/
[docs.introduction.guarantees]: https://vector.dev/docs/introduction/guarantees/
[docs.introduction.architecture]: https://vector.dev/docs/architecture/
[urls.production_users]: https://github.com/vectordotdev/vector/issues/790
[urls.rust]: https://www.rust-lang.org/
[urls.vector_calendar]: https://calendar.vector.dev
[urls.vector_chat]: https://chat.vector.dev
[urls.vector_code_of_conduct]: https://github.com/vectordotdev/vector/blob/master/CODE_OF_CONDUCT.md
[urls.vector_contributing_policy]: https://github.com/vectordotdev/vector/blob/master/CONTRIBUTING.md
[urls.vector_community]: https://vector.dev/community/
[urls.vector_privacy_policy]: https://github.com/vectordotdev/vector/blob/master/PRIVACY.md
[urls.vector_release_policy]: https://github.com/vectordotdev/vector/blob/master/RELEASING.md
[urls.vector_releases]: https://vector.dev/releases/
[urls.vector_releases_policy]: https://github.com/vectordotdev/vector/blob/master/RELEASES.md
[urls.vector_security_policy]: https://github.com/vectordotdev/vector/security/policy
[urls.vector_test_harness]: https://github.com/vectordotdev/vector-test-harness/
[urls.vector_twitter]: https://twitter.com/vectordotdev
[urls.vector_versioning_policy]: https://github.com/vectordotdev/vector/blob/master/VERSIONING.md
[urls.vote_feature]: https://github.com/vectordotdev/vector/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc+label%3A%22type%3A+feature%22
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[khcrysalis/Impactor]]></title>
            <link>https://github.com/khcrysalis/Impactor</link>
            <guid>https://github.com/khcrysalis/Impactor</guid>
            <pubDate>Sun, 01 Feb 2026 00:07:15 GMT</pubDate>
            <description><![CDATA[Feature rich iOS/tvOS sideloading application written in Rust. Formerly known as PlumeImpactor.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/khcrysalis/Impactor">khcrysalis/Impactor</a></h1>
            <p>Feature rich iOS/tvOS sideloading application written in Rust. Formerly known as PlumeImpactor.</p>
            <p>Language: Rust</p>
            <p>Stars: 1,005</p>
            <p>Forks: 47</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre># &lt;img src=&quot;https://github.com/user-attachments/assets/18f2eff4-546f-4365-98eb-afb19b13dc13&quot; width=&quot;25&quot; height=&quot;25&quot; /&gt; Impactor

[![GitHub Release](https://img.shields.io/github/v/release/khcrysalis/PlumeImpactor?include_prereleases)](https://github.com/khcrysalis/PlumeImpactor/releases)
[![GitHub Downloads (all assets, all releases)](https://img.shields.io/github/downloads/khcrysalis/PlumeImpactor/total)](https://github.com/khcrysalis/PlumeImpactor/releases)
[![GitHub License](https://img.shields.io/github/license/khcrysalis/PlumeImpactor?color=%23C96FAD)](https://github.com/khcrysalis/PlumeImpactor/blob/main/LICENSE)
[![Sponsor Me](https://img.shields.io/static/v1?label=Sponsor&amp;message=%E2%9D%A4&amp;logo=GitHub&amp;color=%23fe8e86)](https://github.com/sponsors/khcrysalis)

Open-source, cross-platform, and feature rich iOS sideloading application. Supporting macOS, Linux[^1], and Windows[^2].

[^1]: On Linux, usbmuxd must be installed on your system. Don&#039;t worry though, it comes with most popular distributions by default already! However, due to some distributions [udev](https://man7.org/linux/man-pages/man7/udev.7.html) rules `usbmuxd` may stop running after no devices are connected causing Impactor to not detect the device after plugging it in. You can mitigate this by plugging your phone first then restarting the app. \
\
Auto-refresh will not work the same as it would on other platforms like macOS/Windows, due to `usbmuxd` lacking WiFi connectivity so it will attempt to do it automatically only when a device is plugged in, we are looking for a proper solution though.

[^2]: On Windows, [iTunes](https://support.apple.com/en-us/106372) must be downloaded so Impactor is able to use the drivers for interacting with Apple devices.

![Demo of app](demo.png)

### Features

- User friendly and clean UI.
- Supports installing [SideStore](https://github.com/SideStore/SideStore) and [LiveContainer](https://github.com/LiveContainer/LiveContainer) properly.
- Supports Linux.
- Sign and sideload applications on iOS 9.0+ &amp; Mac with your Apple ID.
  - Installing with [AppSync](https://github.com/akemin-dayo/AppSync) is supported.
  - Installing with ipatool gotten ipa&#039;s is supported.
    - Automatically disables updates from the App Store.
- Simple customization options for the app.
- Tweak support for advanced users, using [ElleKit](https://github.com/tealbathingsuit/ellekit) for injection.
  - Supports injecting `.deb` and `.dylib` files.
  - Supports adding `.framework`, `.bundle`, and `.appex` directories.
  - Supports replacing Cydia Substrate with ElleKit for 26.0 compatibility.
- Generates P12 for SideStore/AltStore to use, similar to Altserver.
- Automatically populate pairing files for apps like SideStore, Antrag, and Protokolle.
- Comes with simple device utilities for retrusting/placing pairing file.
- Export P12 for use with LiveContainer.
- Almost *proper* entitlement handling and can register app plugins.
  - Able to request entitlements like `increased-memory-limit`, for emulators like MelonX or UTM.

## Download

Visit [releases](https://github.com/khcrysalis/PlumeImpactor/releases) and get the latest version for your computer.

###### *This is also available on flatpak &amp; homebrew.*

**Linux:**

&lt;a href=&quot;https://flathub.org/en/apps/dev.khcrysalis.PlumeImpactor&quot;&gt;
  &lt;img src=&quot;https://dl.flathub.org/assets/badges/flathub-badge-en.svg&quot; width=&quot;200px&quot;&gt;
&lt;/a&gt;

**macOS:**

```sh
brew install --cask impactor
```

## How it works

How it works is that we try to replicate what [Xcode](https://developer.apple.com/xcode/) would do but in our own application, by using your Apple Account (which serves the purpose of being a &quot;Developer&quot;) so we can request certificates, provisioning profiles, and register your device from Apple themselves. 

Apple here is the provider of these and how we&#039;ll even be able to get apps on your phone. Unfortunately, without paying for their developer program you are limited to 7-days and a limited amount of apps/components you can register.

The very first thing we do when trying to sideload an app, is register your idevice to their servers, then try to create a certificate. These last 365 days, we also store the key locally so you would need to copy these keys over to other machines, if you don&#039;t, Impactor will try to make a new one.

After that, we try to register your app that you&#039;re trying to sideload, and try to provision it with proper entitlements gathered from the binary. Once we do, we have to download the neccessary files when signing, that being the certificate and provisioning profile that we just created.

Lastly, we do all of the necessary modifications we need to the app you&#039;re trying to sideload, can range between tweaks, name changing, etc. Though most importantly, we need to *sign* the app using [apple-codesign-rs](https://github.com/indygreg/apple-platform-rs) so we can **install it** with [idevice](https://github.com/jkcoxson/idevice)!

That&#039;s the entire gist of how this works! Of course its very short and brief, however feel free to look how it works since its open source :D

### Pairing File

Impactor also allows the user to generate a pairing file for applications to talk directly to the device remotely. This pairing file is device specific and will become invalid if you ever re-trust/update/reset.

Supported apps for pairing file:
- `SideStore`
- `Feather`
- `SparseBox`
- `LiveContainer + SideStore`
- `Antrag`
- `Protokolle`
- `StikDebug`
- `EnsWilde`
- `ByeTunes`

You can retrieve this file by either sideloading the supported app of your choice, or going to the `Utilities` page when a device is connected and press install for the supported app. Head over to the [downloads](https://github.com/khcrysalis/PlumeImpactor/releases).

## Sponsors

| Thanks to all my [sponsors](https://github.com/sponsors/khcrysalis)!! |
|:-:|
| &lt;img src=&quot;https://raw.githubusercontent.com/khcrysalis/github-sponsor-graph/main/graph.png&quot;&gt; |
| _**&quot;samara is cute&quot; - Vendicated**_ |

## Star History

&lt;a href=&quot;https://star-history.com/#khcrysalis/plumeimpactor&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=khcrysalis/plumeimpactor&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=khcrysalis/plumeimpactor&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=khcrysalis/plumeimpactor&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;

## Acknowledgements

- [SAMSAM](https://github.com/khcrysalis) â€“ The maker.
- [Paige](https://github.com/paigely) â€“ Icon &amp; flatpak distribution.
- [SideStore](https://github.com/SideStore/apple-private-apis) â€“ Grandslam auth &amp; Omnisette.
- [gms.py](https://gist.github.com/JJTech0130/049716196f5f1751b8944d93e73d3452) â€“ Grandslam auth API references.
- [Sideloader](https://github.com/Dadoum/Sideloader) â€“ Apple Developer API references.
- [PyDunk](https://github.com/nythepegasus/PyDunk) â€“ `v1` Apple Developer API references.
- [idevice](https://github.com/jkcoxson/idevice) â€“ Used for communication with `installd`, specifically for sideloading the apps to your devices.
- [apple-codesign-rs](https://github.com/indygreg/apple-platform-rs) â€“ Codesign alternative, modified and extended upon to work for Impactor.

&lt;a href=&quot;https://github.com/iced-rs/iced&quot;&gt;
  &lt;img src=&quot;https://gist.githubusercontent.com/hecrj/ad7ecd38f6e47ff3688a38c79fd108f0/raw/74384875ecbad02ae2a926425e9bcafd0695bade/color.svg&quot; width=&quot;130px&quot;&gt;
&lt;/a&gt;

## License

Project is licensed under the MIT license. You can see the full details of the license [here](https://github.com/khcrysalis/PlumeImpactor/blob/main/LICENSE). Some components may be licensed under different licenses, see their respective directories for details.
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[EasyTier/EasyTier]]></title>
            <link>https://github.com/EasyTier/EasyTier</link>
            <guid>https://github.com/EasyTier/EasyTier</guid>
            <pubDate>Sun, 01 Feb 2026 00:07:14 GMT</pubDate>
            <description><![CDATA[A simple, decentralized mesh VPN with WireGuard support.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/EasyTier/EasyTier">EasyTier/EasyTier</a></h1>
            <p>A simple, decentralized mesh VPN with WireGuard support.</p>
            <p>Language: Rust</p>
            <p>Stars: 9,608</p>
            <p>Forks: 913</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre># EasyTier

[![Github release](https://img.shields.io/github/v/tag/EasyTier/EasyTier)](https://github.com/EasyTier/EasyTier/releases)
[![GitHub](https://img.shields.io/github/license/EasyTier/EasyTier)](https://github.com/EasyTier/EasyTier/blob/main/LICENSE)
[![GitHub last commit](https://img.shields.io/github/last-commit/EasyTier/EasyTier)](https://github.com/EasyTier/EasyTier/commits/main)
[![GitHub issues](https://img.shields.io/github/issues/EasyTier/EasyTier)](https://github.com/EasyTier/EasyTier/issues)
[![GitHub Core Actions](https://github.com/EasyTier/EasyTier/actions/workflows/core.yml/badge.svg)](https://github.com/EasyTier/EasyTier/actions/workflows/core.yml)
[![GitHub GUI Actions](https://github.com/EasyTier/EasyTier/actions/workflows/gui.yml/badge.svg)](https://github.com/EasyTier/EasyTier/actions/workflows/gui.yml)
[![GitHub Test Actions](https://github.com/EasyTier/EasyTier/actions/workflows/test.yml/badge.svg)](https://github.com/EasyTier/EasyTier/actions/workflows/test.yml)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/EasyTier/EasyTier)

[ç®€ä½“ä¸­æ–‡](/README_CN.md) | [English](/README.md)

&gt; âœ¨ A simple, secure, decentralized virtual private network solution powered by Rust and Tokio

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;assets/config-page.png&quot; width=&quot;300&quot; alt=&quot;config page&quot;&gt;
&lt;img src=&quot;assets/running-page.png&quot; width=&quot;300&quot; alt=&quot;running page&quot;&gt;
&lt;/p&gt;

ğŸ“š **[Full Documentation](https://easytier.cn/en/)** | ğŸ–¥ï¸ **[Web Console](https://easytier.cn/web)** | ğŸ“ **[Download Releases](https://github.com/EasyTier/EasyTier/releases)** | ğŸ§© **[Third Party Tools](https://easytier.cn/en/guide/installation_gui.html#third-party-graphical-interfaces)** | â¤ï¸ **[Sponsor](#sponsor)**

## Features

### Core Features

- ğŸ”’ **Decentralized**: Nodes are equal and independent, no centralized services required  
- ğŸš€ **Easy to Use**: Multiple operation methods via web, client, and command line  
- ğŸŒ **Cross-Platform**: Supports Win/MacOS/Linux/FreeBSD/Android and X86/ARM/MIPS architectures  
- ğŸ” **Secure**: AES-GCM or WireGuard encryption, prevents man-in-the-middle attacks  

### Advanced Capabilities

- ğŸ”Œ **Efficient NAT Traversal**: Supports UDP and IPv6 traversal, works with NAT4-NAT4 networks  
- ğŸŒ **Subnet Proxy**: Nodes can share subnets for other nodes to access  
- ğŸ”„ **Intelligent Routing**: Latency priority and automatic route selection for best network experience  
- âš¡ **High Performance**: Zero-copy throughout the entire link, supports TCP/UDP/WSS/WG protocols  

### Network Optimization

- ğŸ“Š **UDP Loss Resistance**: KCP/QUIC proxy optimizes latency and bandwidth in high packet loss environments  
- ğŸ”§ **Web Management**: Easy configuration and monitoring through web interface  
- ğŸ› ï¸ **Zero Config**: Simple deployment with statically linked executables  

## Quick Start

### ğŸ“¥ Installation

Choose the installation method that best suits your needs:

```bash
# 1. Download pre-built binary (Recommended, All platforms supported)
# Visit https://github.com/EasyTier/EasyTier/releases

# 2. Install via cargo (Latest development version)
cargo install --git https://github.com/EasyTier/EasyTier.git easytier

# 3. Install via Docker
# See https://easytier.cn/en/guide/installation.html#installation-methods

# 4. Linux Quick Install
wget -O- https://raw.githubusercontent.com/EasyTier/EasyTier/main/script/install.sh | sudo bash -s install

# 5. MacOS via Homebrew
brew tap brewforge/chinese
brew install --cask easytier-gui

# 6. OpenWrt Luci Web UI
# Visit https://github.com/EasyTier/luci-app-easytier

# 7. (Optional) Install shell completions:
easytier-core --gen-autocomplete fish &gt; ~/.config/fish/completions/easytier-core.fish
easytier-cli gen-autocomplete fish &gt; ~/.config/fish/completions/easytier-cli.fish

```

### ğŸš€ Basic Usage

#### Quick Networking with Shared Nodes

EasyTier supports quick networking using shared public nodes. When you don&#039;t have a public IP, you can use the free shared nodes provided by the EasyTier community. Nodes will automatically attempt NAT traversal and establish P2P connections. When P2P fails, data will be relayed through shared nodes.

The currently deployed shared public node is `tcp://public.easytier.cn:11010`.

When using shared nodes, each node entering the network needs to provide the same `--network-name` and `--network-secret` parameters as the unique identifier of the network.

Taking two nodes as an example (Please use more complex network name to avoid conflicts):

1. Run on Node A:

```bash
# Run with administrator privileges
sudo easytier-core -d --network-name abc --network-secret abc -p tcp://public.easytier.cn:11010
```

2. Run on Node B:

```bash
# Run with administrator privileges
sudo easytier-core -d --network-name abc --network-secret abc -p tcp://public.easytier.cn:11010
```

After successful execution, you can check the network status using `easytier-cli`:

```text
| ipv4         | hostname       | cost  | lat_ms | loss_rate | rx_bytes | tx_bytes | tunnel_proto | nat_type | id         | version         |
| ------------ | -------------- | ----- | ------ | --------- | -------- | -------- | ------------ | -------- | ---------- | --------------- |
| 10.126.126.1 | abc-1          | Local | *      | *         | *        | *        | udp          | FullCone | 439804259  | 2.5.0-70e69a38~ |
| 10.126.126.2 | abc-2          | p2p   | 3.452  | 0         | 17.33 kB | 20.42 kB | udp          | FullCone | 390879727  | 2.5.0-70e69a38~ |
|              | PublicServer_a | p2p   | 27.796 | 0.000     | 50.01 kB | 67.46 kB | tcp          | Unknown  | 3771642457 | 2.5.0-70e69a38~ |
```

You can test connectivity between nodes:

```bash
# Test connectivity
ping 10.126.126.1
ping 10.126.126.2
```

Note: If you cannot ping through, it may be that the firewall is blocking incoming traffic. Please turn off the firewall or add allow rules.

To improve availability, you can connect to multiple shared nodes simultaneously:

```bash
# Connect to multiple shared nodes
sudo easytier-core -d --network-name abc --network-secret abc -p tcp://public.easytier.cn:11010 -p udp://public.easytier.cn:11010
```

Once your network is set up successfully, you can easily configure it to start automatically on system boot. Refer to the [One-Click Register Service guide](https://easytier.cn/en/guide/network/oneclick-install-as-service.html) for step-by-step instructions on registering EasyTier as a system service.

#### Decentralized Networking

EasyTier is fundamentally decentralized, with no distinction between server and client. As long as one device can communicate with any node in the virtual network, it can join the virtual network. Here&#039;s how to set up a decentralized network:

1. Start First Node (Node A):

```bash
# Start the first node
sudo easytier-core -i 10.144.144.1
```

After startup, this node will listen on the following ports by default:
- TCP: 11010
- UDP: 11010
- WebSocket: 11011
- WebSocket SSL: 11012
- WireGuard: 11013

2. Connect Second Node (Node B):

```bash
# Connect to the first node using its public IP
sudo easytier-core -i 10.144.144.2 -p udp://FIRST_NODE_PUBLIC_IP:11010
```

3. Verify Connection:

```bash
# Test connectivity
ping 10.144.144.2

# View connected peers
easytier-cli peer

# View routing information
easytier-cli route

# View local node information
easytier-cli node
```

For more nodes to join the network, they can connect to any existing node in the network using the `-p` parameter:

```bash
# Connect to any existing node using its public IP
sudo easytier-core -i 10.144.144.3 -p udp://ANY_EXISTING_NODE_PUBLIC_IP:11010
```

### ğŸ” Advanced Features

#### Subnet Proxy

Assuming the network topology is as follows, Node B wants to share its accessible subnet 10.1.1.0/24 with other nodes:

```mermaid
flowchart LR

subgraph Node A Public IP 22.1.1.1
nodea[EasyTier&lt;br/&gt;10.144.144.1]
end

subgraph Node B
nodeb[EasyTier&lt;br/&gt;10.144.144.2]
end

id1[[10.1.1.0/24]]

nodea &lt;--&gt; nodeb &lt;-.-&gt; id1
```

To share a subnet, add the `-n` parameter when starting EasyTier:

```bash
# Share subnet 10.1.1.0/24 with other nodes
sudo easytier-core -i 10.144.144.2 -n 10.1.1.0/24
```

Subnet proxy information will automatically sync to each node in the virtual network, and each node will automatically configure the corresponding route. You can verify the subnet proxy setup:

1. Check if the routing information has been synchronized (the proxy_cidrs column shows the proxied subnets):

```bash
# View routing information
easytier-cli route
```

![Routing Information](/assets/image-3.png)

2. Test if you can access nodes in the proxied subnet:

```bash
# Test connectivity to proxied subnet
ping 10.1.1.2
```

#### WireGuard Integration

EasyTier can act as a WireGuard server, allowing any device with a WireGuard client (including iOS and Android) to access the EasyTier network. Here&#039;s an example setup:

```mermaid
flowchart LR

ios[[iPhone&lt;br/&gt;WireGuard Installed]]

subgraph Node A Public IP 22.1.1.1
nodea[EasyTier&lt;br/&gt;10.144.144.1]
end

subgraph Node B
nodeb[EasyTier&lt;br/&gt;10.144.144.2]
end

id1[[10.1.1.0/24]]

ios &lt;-.-&gt; nodea &lt;--&gt; nodeb &lt;-.-&gt; id1
```

1. Start EasyTier with WireGuard portal enabled:

```bash
# Listen on 0.0.0.0:11013 and use 10.14.14.0/24 subnet for WireGuard clients
sudo easytier-core -i 10.144.144.1 --vpn-portal wg://0.0.0.0:11013/10.14.14.0/24
```

2. Get WireGuard client configuration:

```bash
# Get WireGuard client configuration
easytier-cli vpn-portal
```

3. In the output configuration:
   - Set `Interface.Address` to an available IP from the WireGuard subnet
   - Set `Peer.Endpoint` to the public IP/domain of your EasyTier node
   - Import the modified configuration into your WireGuard client

#### Self-Hosted Public Shared Node

You can run your own public shared node to help other nodes discover each other. A public shared node is just a regular EasyTier network (with same network name and secret) that other networks can connect to.

To run a public shared node:

```bash
# No need to specify IPv4 address for public shared nodes
sudo easytier-core --network-name mysharednode --network-secret mysharednode
```

## Related Projects

- [ZeroTier](https://www.zerotier.com/): A global virtual network for connecting devices.
- [TailScale](https://tailscale.com/): A VPN solution aimed at simplifying network configuration.

### Contact Us

- ğŸ’¬ **[Telegram Group](https://t.me/easytier)**
- ğŸ‘¥ **[QQ Group]**
  - No.1 [949700262](https://qm.qq.com/q/wFoTUChqZW)
  - No.2 [837676408](https://qm.qq.com/q/4V33DrfgHe)
  - No.3 [957189589](https://qm.qq.com/q/YNyTQjwlai)

## License

EasyTier is released under the [LGPL-3.0](https://github.com/EasyTier/EasyTier/blob/main/LICENSE).

## Sponsor

CDN acceleration and security protection for this project are sponsored by Tencent EdgeOne.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://edgeone.ai/?from=github&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;assets/edgeone.png&quot; width=&quot;200&quot; alt=&quot;EdgeOne Logo&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

Special thanks to [Langlang Cloud](https://langlangy.cn/?i26c5a5)  and [RainCloud](https://www.rainyun.com/NjM0NzQ1_) for sponsoring our public servers.

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://langlangy.cn/?i26c5a5&quot; target=&quot;_blank&quot;&gt;
&lt;img src=&quot;assets/langlang.png&quot; width=&quot;200&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://langlangy.cn/?i26c5a5&quot; target=&quot;_blank&quot;&gt;
&lt;img src=&quot;assets/raincloud.png&quot; width=&quot;200&quot;&gt;
&lt;/a&gt;
&lt;/p&gt;


If you find EasyTier helpful, please consider sponsoring us. Software development and maintenance require a lot of time and effort, and your sponsorship will help us better maintain and improve EasyTier.

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;assets/wechat.png&quot; width=&quot;200&quot;&gt;
&lt;img src=&quot;assets/alipay.png&quot; width=&quot;200&quot;&gt;
&lt;/p&gt;
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[lancedb/lancedb]]></title>
            <link>https://github.com/lancedb/lancedb</link>
            <guid>https://github.com/lancedb/lancedb</guid>
            <pubDate>Sun, 01 Feb 2026 00:07:13 GMT</pubDate>
            <description><![CDATA[Developer-friendly OSS embedded retrieval library for multimodal AI. Search More; Manage Less.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lancedb/lancedb">lancedb/lancedb</a></h1>
            <p>Developer-friendly OSS embedded retrieval library for multimodal AI. Search More; Manage Less.</p>
            <p>Language: Rust</p>
            <p>Stars: 8,690</p>
            <p>Forks: 725</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre>&lt;a href=&quot;https://cloud.lancedb.com&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/92dad0a2-2a37-4ce1-b783-0d1b4f30a00c&quot; alt=&quot;LanceDB Cloud Public Beta&quot; width=&quot;100%&quot; style=&quot;max-width: 100%;&quot;&gt;
&lt;/a&gt;
&lt;div align=&quot;center&quot;&gt;

[![LanceDB](docs/src/assets/hero-header.png)](https://lancedb.com)
[![Website](https://img.shields.io/badge/-Website-100000?style=for-the-badge&amp;labelColor=645cfb&amp;color=645cfb)](https://lancedb.com/)
[![Blog](https://img.shields.io/badge/Blog-100000?style=for-the-badge&amp;labelColor=645cfb&amp;color=645cfb)](https://blog.lancedb.com/)
[![Discord](https://img.shields.io/badge/-Discord-100000?style=for-the-badge&amp;logo=discord&amp;logoColor=white&amp;labelColor=645cfb&amp;color=645cfb)](https://discord.gg/zMM32dvNtd)
[![Twitter](https://img.shields.io/badge/-Twitter-100000?style=for-the-badge&amp;logo=x&amp;logoColor=white&amp;labelColor=645cfb&amp;color=645cfb)](https://twitter.com/lancedb)
[![LinkedIn](https://img.shields.io/badge/-LinkedIn-100000?style=for-the-badge&amp;logo=linkedin&amp;logoColor=white&amp;labelColor=645cfb&amp;color=645cfb)](https://www.linkedin.com/company/lancedb/)


&lt;img src=&quot;docs/src/assets/lancedb.png&quot; alt=&quot;LanceDB&quot; width=&quot;50%&quot;&gt;

# **The Multimodal AI Lakehouse**

[**How to Install** ](#how-to-install) âœ¦ [**Detailed Documentation**](https://lancedb.com/docs) âœ¦ [**Tutorials and Recipes**](https://github.com/lancedb/vectordb-recipes/tree/main) âœ¦  [**Contributors**](#contributors) 

**The ultimate multimodal data platform for AI/ML applications.** 

LanceDB is designed for fast, scalable, and production-ready vector search. It is built on top of the Lance columnar format. You can store, index, and search over petabytes of multimodal data and vectors with ease. 
LanceDB is a central location where developers can build, train and analyze their AI workloads.

&lt;/div&gt;

&lt;br&gt;

## **Demo: Multimodal Search by Keyword, Vector or with SQL**
&lt;img max-width=&quot;750px&quot; alt=&quot;LanceDB Multimodal Search&quot; src=&quot;https://github.com/lancedb/lancedb/assets/917119/09c5afc5-7816-4687-bae4-f2ca194426ec&quot;&gt;

## **Star LanceDB to get updates!**

&lt;details&gt;
&lt;summary&gt;â­ Click here â­  to see how fast we&#039;re growing!&lt;/summary&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=lancedb/lancedb&amp;theme=dark&amp;type=Date&quot;&gt;
  &lt;img width=&quot;100%&quot; src=&quot;https://api.star-history.com/svg?repos=lancedb/lancedb&amp;theme=dark&amp;type=Date&quot;&gt;
&lt;/picture&gt;
&lt;/details&gt;

## **Key Features**:

- **Fast Vector Search**: Search billions of vectors in milliseconds with state-of-the-art indexing.
- **Comprehensive Search**: Support for vector similarity search, full-text search and SQL.
- **Multimodal Support**: Store, query and filter vectors, metadata and multimodal data (text, images, videos, point clouds, and more).
- **Advanced Features**: Zero-copy, automatic versioning, manage versions of your data without needing extra infrastructure. GPU support in building vector index.

### **Products**:
- **Open Source &amp; Local**: 100% open source, runs locally or in your cloud. No vendor lock-in.
- **Cloud and Enterprise**: Production-scale vector search with no servers to manage. Complete data sovereignty and security.

### **Ecosystem**:
- **Columnar Storage**: Built on the Lance columnar format for efficient storage and analytics.
- **Seamless Integration**: Python, Node.js, Rust, and REST APIs for easy integration. Native Python and Javascript/Typescript support.
- **Rich Ecosystem**: Integrations with [**LangChain** ğŸ¦œï¸ğŸ”—](https://python.langchain.com/docs/integrations/vectorstores/lancedb/), [**LlamaIndex** ğŸ¦™](https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/LanceDBIndexDemo.html), Apache-Arrow, Pandas, Polars, DuckDB and more on the way.

## **How to Install**:

Follow the [Quickstart](https://lancedb.com/docs/quickstart/) doc to set up LanceDB locally. 

**API &amp; SDK:** We also support Python, Typescript and Rust SDKs

| Interface | Documentation |
|-----------|---------------|
| Python SDK | https://lancedb.github.io/lancedb/python/python/ |
| Typescript SDK | https://lancedb.github.io/lancedb/js/globals/ |
| Rust SDK | https://docs.rs/lancedb/latest/lancedb/index.html |
| REST API | https://docs.lancedb.com/api-reference/rest |

## **Join Us and Contribute**

We welcome contributions from everyone! Whether you&#039;re a developer, researcher, or just someone who wants to help out. 

If you have any suggestions or feature requests, please feel free to open an issue on GitHub or discuss it on our [**Discord**](https://discord.gg/G5DcmnZWKB) server.

[**Check out the GitHub Issues**](https://github.com/lancedb/lancedb/issues) if you would like to work on the features that are planned for the future. If you have any suggestions or feature requests, please feel free to open an issue on GitHub. 

## **Contributors**

&lt;a href=&quot;https://github.com/lancedb/lancedb/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=lancedb/lancedb&quot; /&gt;
&lt;/a&gt;


## **Stay in Touch With Us**
&lt;div align=&quot;center&quot;&gt;

&lt;/br&gt;

[![Website](https://img.shields.io/badge/-Website-100000?style=for-the-badge&amp;labelColor=645cfb&amp;color=645cfb)](https://lancedb.com/)
[![Blog](https://img.shields.io/badge/Blog-100000?style=for-the-badge&amp;labelColor=645cfb&amp;color=645cfb)](https://blog.lancedb.com/)
[![Discord](https://img.shields.io/badge/-Discord-100000?style=for-the-badge&amp;logo=discord&amp;logoColor=white&amp;labelColor=645cfb&amp;color=645cfb)](https://discord.gg/zMM32dvNtd)
[![Twitter](https://img.shields.io/badge/-Twitter-100000?style=for-the-badge&amp;logo=x&amp;logoColor=white&amp;labelColor=645cfb&amp;color=645cfb)](https://twitter.com/lancedb)
[![LinkedIn](https://img.shields.io/badge/-LinkedIn-100000?style=for-the-badge&amp;logo=linkedin&amp;logoColor=white&amp;labelColor=645cfb&amp;color=645cfb)](https://www.linkedin.com/company/lancedb/)

&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[git-ai-project/git-ai]]></title>
            <link>https://github.com/git-ai-project/git-ai</link>
            <guid>https://github.com/git-ai-project/git-ai</guid>
            <pubDate>Sun, 01 Feb 2026 00:07:12 GMT</pubDate>
            <description><![CDATA[A Git extension for tracking the AI-generated code in your repos]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/git-ai-project/git-ai">git-ai-project/git-ai</a></h1>
            <p>A Git extension for tracking the AI-generated code in your repos</p>
            <p>Language: Rust</p>
            <p>Stars: 818</p>
            <p>Forks: 59</p>
            <p>Stars today: 19 stars today</p>
            <h2>README</h2><pre>&lt;div&gt;
&lt;img src=&quot;https://github.com/acunniffe/git-ai/raw/main/assets/docs/git-ai.png&quot; align=&quot;right&quot;
     alt=&quot;Git AI by acunniffe/git-ai&quot; width=&quot;100&quot; height=&quot;100&quot; /&gt;

&lt;/div&gt;
&lt;div&gt;
&lt;h1 align=&quot;left&quot;&gt;&lt;b&gt;git-ai&lt;/b&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;p align=&quot;left&quot;&gt;Track the AI Code in your repositories&lt;/p&gt;

&lt;video src=&quot;https://github.com/user-attachments/assets/68304ca6-b262-4638-9fb6-0a26f55c7986&quot; muted loop controls autoplay&gt;&lt;/video&gt;

## Quick Start

#### Mac, Linux, Windows (WSL)

```bash
curl -sSL https://usegitai.com/install.sh | bash
```

#### Windows (non-WSL)

```powershell
powershell -NoProfile -ExecutionPolicy Bypass -Command &quot;irm http://usegitai.com/install.ps1 | iex&quot;
```

ğŸŠ That&#039;s it! **No per-repo setup.** Once installed Git AI will work OOTB with any of these **Supported Agents**:

&lt;img src=&quot;https://github.com/acunniffe/git-ai/raw/main/assets/docs/supported-agents.png&quot; width=&quot;320&quot; /&gt;

### Documentation https://usegitai.com/docs
- [AI Blame](https://usegitai.com/docs/cli/ai-blame)
- [Cross-Agent Prompt Saving](https://usegitai.com/docs/cli/prompt-storage)
- [CLI Reference](https://usegitai.com/docs/cli/reference)
- [Configuring Git AI for the enterprise](https://usegitai.com/docs/cli/configuration)

### Just Install and Commit

Build as usual. Just prompt, edit and commit. Git AI will track every line of AI-Code and record the Coding Agent, Model, and prompt that generated it. 

&lt;img src=&quot;https://github.com/acunniffe/git-ai/raw/main/assets/docs/graph.jpg&quot; width=&quot;400&quot; /&gt;

#### How Does it work? 

Supported Coding Agents call Git AI and mark the lines they insert as AI-generated. 

On commit, Git AI saves the final AI-attributions into a Git Note. These notes power AI-Blame, AI contribution stats, and more. The CLI makes sure these notes are preserved through rebases, merges, squashes, cherry-picks, etc.

![Git Tree](https://github.com/user-attachments/assets/edd20990-ec0b-4a53-afa4-89fa33de9541)

The format of the notes is outlined here in the [Git AI Standard v3.0.0](https://github.com/git-ai-project/git-ai/blob/main/specs/git_ai_standard_v3.0.0.md)

## Goals of `git-ai` project

ğŸ¤– **Track AI code in a Multi-Agent** world. Because developers get to choose their tools, engineering teams need a **vendor agnostic** way to track AI impact in their repos.

ğŸ¯ **Accurate attribution** from Laptop â†’ Pull Request â†’ Merged. Claude Code, Cursor and Copilot cannot track code after generationâ€”Git AI follows it through the entire workflow.

ğŸ”„ **Support real-world git workflows** by making sure AI-Authorship annotations survive a `merge --squash`, `rebase`, `reset`, `cherry-pick` etc.

ğŸ”— **Maintain link between prompts and code** - there is valuable context and requirements in team promptsâ€”preserve them alongside code.

ğŸš€ **Git-native + Fast** - `git-ai` is built on git plumbing commands. Negligible impact even in large repos (&amp;lt;100ms). Tested in [Chromium](https://github.com/chromium/chromium).

## Agent Support

`git-ai` automatically sets up all supported agent hooks using the `git-ai install-hooks` command

| Agent/IDE                                                                                  | Authorship | Prompts |
| ------------------------------------------------------------------------------------------ | ---------- | ------- |
| Cursor &amp;gt;1.7                                                                             | âœ…         | âœ…      |
| Claude Code                                                                                | âœ…         | âœ…      |
| GitHub Copilot in VSCode via Extension                                                     | âœ…         | âœ…      |
| OpenCode                                                                                   | âœ…         | âœ…      |
| Google Gemini CLI                                                                          | âœ…         | âœ…      |
| Continue CLI                                                                               | âœ…         | âœ…      |
| Droid CLI (Factory AI)                                                                     | âœ…         | âœ…      |
| Atlassian RovoDev CLI                                                                      | âœ…         | âœ…      |
| GitHub Copilot in Jetbrains IDEs (IntelliJ, etc.) (in-review)                              | ğŸ”„         | ğŸ”„      |
| Jetbrains Junie (in-review)                                                                | ğŸ”„         | ğŸ”„      |
| Amp (in-progress)                                                                          | ğŸ”„         | ğŸ”„      |
| AWS Kiro (in-progress)                                                                     | ğŸ”„         | ğŸ”„      |
| Continue VS Code/IntelliJ (in-progress)                                                    | ğŸ”„         | ğŸ”„      |
| Windsurf (in-review)                                                                       | ğŸ”„         | ğŸ”„      |
| Augment Code                                                                               | ğŸ”„         | ğŸ”„      |
| OpenAI Codex (waiting on [openai/codex #2109](https://github.com/openai/codex/issues/2109)) |            |         |
| Ona                                                                                        |            |         |
| Sourcegraph Cody                                                                           |            |         |
| Google Antigravity                                                                         |            |         |


&gt; **Building a Coding Agent?** [Add support for Git AI by following this guide](https://usegitai.com/docs/cli/add-your-agent)

## Installing the Stats Bot (early access)

Aggregate `git-ai` data at the PR, developer, Repository and Organization levels:

- AI authorship breakdown for every Pull Request
- Measure % of code that is AI generated through the entire SDLC
- Compare accepted-rate for code written by each Agent + Model. 
- AI-Code Halflife (how durable is the AI code)
&gt; [Get early access by chatting with the maintainers](https://calendly.com/acunniffe/meeting-with-git-ai-authors)

![alt](https://github.com/acunniffe/git-ai/raw/main/assets/docs/dashboard.png)

</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[rust-lang/cargo]]></title>
            <link>https://github.com/rust-lang/cargo</link>
            <guid>https://github.com/rust-lang/cargo</guid>
            <pubDate>Sun, 01 Feb 2026 00:07:11 GMT</pubDate>
            <description><![CDATA[The Rust package manager]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/rust-lang/cargo">rust-lang/cargo</a></h1>
            <p>The Rust package manager</p>
            <p>Language: Rust</p>
            <p>Stars: 14,531</p>
            <p>Forks: 2,788</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre># Cargo

Cargo downloads your Rust projectâ€™s dependencies and compiles your project.

**To start using Cargo**, learn more at [The Cargo Book].

**To start developing Cargo itself**, read the [Cargo Contributor Guide].

[The Cargo Book]: https://doc.rust-lang.org/cargo/
[Cargo Contributor Guide]: https://rust-lang.github.io/cargo/contrib/

&gt; The Cargo binary distributed through with Rust is maintained by the Cargo
&gt; team for use by the wider ecosystem.
&gt; For all other uses of this crate (as a binary or library) this is maintained
&gt; by the Cargo team, primarily for use by Cargo and not intended for external
&gt; use (except as a transitive dependency). This crate may make major changes to
&gt; its APIs.

## Code Status

[![CI](https://github.com/rust-lang/cargo/actions/workflows/main.yml/badge.svg?branch=auto-cargo)](https://github.com/rust-lang/cargo/actions/workflows/main.yml)

Code documentation: &lt;https://doc.rust-lang.org/nightly/nightly-rustc/cargo/&gt;

## Compiling from Source

### Requirements

Cargo requires the following tools and packages to build:

* `cargo` and `rustc`
* A C compiler [for your platform](https://github.com/rust-lang/cc-rs#compile-time-requirements)
* `git` (to clone this repository)

**Other requirements:**

The following are optional based on your platform and needs.

* `pkg-config` â€” This is used to help locate system packages, such as `libssl` headers/libraries. This may not be required in all cases, such as using vendored OpenSSL, or on Windows.
* OpenSSL â€” Only needed on Unix-like systems and only if the `vendored-openssl` Cargo feature is not used.

  This requires the development headers, which can be obtained from the `libssl-dev` package on Ubuntu or `openssl-devel` with apk or yum or the `openssl` package from Homebrew on macOS.

  If using the `vendored-openssl` Cargo feature, then a static copy of OpenSSL will be built from source instead of using the system OpenSSL.
  This may require additional tools such as `perl` and `make`.

  On macOS, common installation directories from Homebrew, MacPorts, or pkgsrc will be checked. Otherwise it will fall back to `pkg-config`.

  On Windows, the system-provided Schannel will be used instead.

  LibreSSL is also supported.

**Optional system libraries:**

The build will automatically use vendored versions of the following libraries. However, if they are provided by the system and can be found with `pkg-config`, then the system libraries will be used instead:

* [`libcurl`](https://curl.se/libcurl/) â€” Used for network transfers.
* [`libgit2`](https://libgit2.org/) â€” Used for fetching git dependencies.
* [`libssh2`](https://www.libssh2.org/) â€” Used for SSH access to git repositories.
* [`libz`](https://zlib.net/) (AKA zlib) â€” Used by the above C libraries for data compression. (Rust code uses [`zlib-rs`](https://github.com/trifectatechfoundation/zlib-rs) instead.)

It is recommended to use the vendored versions as they are the versions that are tested to work with Cargo.

### Compiling

First, you&#039;ll want to check out this repository

```
git clone https://github.com/rust-lang/cargo.git
cd cargo
```

With `cargo` already installed, you can simply run:

```
cargo build --release
```

## Adding new subcommands to Cargo

Cargo is designed to be extensible with new subcommands without having to modify
Cargo itself. See [the Wiki page][third-party-subcommands] for more details and
a list of known community-developed subcommands.

[third-party-subcommands]: https://github.com/rust-lang/cargo/wiki/Third-party-cargo-subcommands


## Releases

Cargo releases coincide with Rust releases.
High level release notes are available as part of [Rust&#039;s release notes][rel].
Detailed release notes are available in the [changelog].

[rel]: https://github.com/rust-lang/rust/blob/master/RELEASES.md
[changelog]: https://doc.rust-lang.org/nightly/cargo/CHANGELOG.html

## Reporting issues

Found a bug? We&#039;d love to know about it!

Please report all issues on the GitHub [issue tracker][issues].

[issues]: https://github.com/rust-lang/cargo/issues

## Contributing

See the **[Cargo Contributor Guide]** for a complete introduction
to contributing to Cargo.

## License

Cargo is primarily distributed under the terms of both the MIT license
and the Apache License (Version 2.0).

See [LICENSE-APACHE](LICENSE-APACHE) and [LICENSE-MIT](LICENSE-MIT) for details.

### Third party software

This product includes software developed by the OpenSSL Project
for use in the OpenSSL Toolkit (https://www.openssl.org/).

In binary form, this product includes software that is licensed under the
terms of the GNU General Public License, version 2, with a linking exception,
which can be obtained from the [upstream repository][1].

See [LICENSE-THIRD-PARTY](LICENSE-THIRD-PARTY) for details.

[1]: https://github.com/libgit2/libgit2

</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[vortex-data/vortex]]></title>
            <link>https://github.com/vortex-data/vortex</link>
            <guid>https://github.com/vortex-data/vortex</guid>
            <pubDate>Sun, 01 Feb 2026 00:07:10 GMT</pubDate>
            <description><![CDATA[An extensible, state of the art columnar file format. Formerly at @spiraldb, now an Incubation Stage project at LFAI&Data, part of the Linux Foundation.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/vortex-data/vortex">vortex-data/vortex</a></h1>
            <p>An extensible, state of the art columnar file format. Formerly at @spiraldb, now an Incubation Stage project at LFAI&Data, part of the Linux Foundation.</p>
            <p>Language: Rust</p>
            <p>Stars: 2,654</p>
            <p>Forks: 126</p>
            <p>Stars today: 17 stars today</p>
            <h2>README</h2><pre># ğŸŒªï¸ Vortex

[![Build Status](https://github.com/vortex-data/vortex/actions/workflows/ci.yml/badge.svg)](https://github.com/vortex-data/vortex/actions)
[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/10567/badge)](https://www.bestpractices.dev/projects/10567)
[![Documentation](https://docs.rs/vortex/badge.svg)](https://docs.vortex.dev)
[![CodSpeed Badge](https://img.shields.io/endpoint?url=https://codspeed.io/badge.json)](https://codspeed.io/vortex-data/vortex)
[![Crates.io](https://img.shields.io/crates/v/vortex.svg)](https://crates.io/crates/vortex)
[![PyPI - Version](https://img.shields.io/pypi/v/vortex-data)](https://pypi.org/project/vortex-data/)
[![Maven - Version](https://img.shields.io/maven-central/v/dev.vortex/vortex-spark)](https://central.sonatype.com/artifact/dev.vortex/vortex-spark)
[![codecov](https://codecov.io/github/vortex-data/vortex/graph/badge.svg)](https://codecov.io/github/vortex-data/vortex)

[Join the community on Slack!](https://vortex.dev/slack) | [Documentation](https://docs.vortex.dev/) | [Performance Benchmarks](https://bench.vortex.dev)

## Overview

Vortex is a next-generation columnar file format and toolkit designed for high-performance data processing.
It is the fastest and most extensible format for building data systems backed by object storage. It provides:

- **Blazing Fast Performance**
  - 100x faster random access reads (vs. modern Apache Parquet)
  - 10-20x faster scans
  - 5x faster writes
  - Similar compression ratios
  - Efficient support for wide tables with zero-copy/zero-parse metadata

- **Extensible Architecture**
  - Modeled after Apache DataFusion&#039;s extensible approach
  - Pluggable encoding system, type system, compression strategy, &amp; layout strategy
  - Zero-copy compatibility with Apache Arrow

- **Open Source, Neutral Governance**
  - A Linux Foundation (LF AI &amp; Data) Project
  - Apache-2.0 Licensed

- **Integrations**
  - Arrow, DataFusion, DuckDB, Spark, Pandas, Polars, &amp; more
  - Apache Iceberg (coming soon)

&gt; ğŸŸ¢ **Development Status**: Library APIs may change from version to version, but we now consider
&gt; the file format &lt;ins&gt;_stable_&lt;/ins&gt;. From release 0.36.0, all future releases of Vortex should
&gt; maintain backwards compatibility of the file format (i.e., be able to read files written by
&gt; any earlier version &gt;= 0.36.0).

## Key Features

### Core Capabilities

- **Logical Types** - Clean separation between logical schema and physical layout
- **Zero-Copy Arrow Integration** - Seamless conversion to/from Apache Arrow arrays
- **Extensible Encodings** - Pluggable physical layouts with built-in optimizations
- **Cascading Compression** - Support for nested encoding schemes
- **High-Performance Computing** - Optimized compute kernels for encoded data
- **Rich Statistics** - Lazy-loaded summary statistics for optimization

### Technical Architecture

#### Logical vs Physical Design

Vortex strictly separates logical and physical concerns:

- **Logical Layer**: Defines data types and schema
- **Physical Layer**: Handles encoding and storage implementation
- **Built-in Encodings**: Compatible with Apache Arrow&#039;s memory format
- **Extension Encodings**: Optimized compression schemes (RLE, dictionary, etc.)

## Quick Start

### Installation

#### Rust Crate

All features are exported through the main `vortex` crate.

```bash
cargo add vortex
```

#### Python Package

```bash
uv add vortex-data
```

#### Command Line UI (vx)

For browsing the structure of Vortex files, you can use the `vx` command-line tool.

```bash
# Install latest release
cargo install vortex-tui --locked

# Or build from source
cargo install --path vortex-tui --locked

# Usage
vx browse &lt;file&gt;
```

### Development Setup

#### Prerequisites (macOS)

```bash
# Optional but recommended dependencies
brew install flatbuffers protobuf  # For .fbs and .proto files
brew install duckdb               # For benchmarks

# Install Rust toolchain
curl --proto &#039;=https&#039; --tlsv1.2 -sSf https://sh.rustup.rs | sh
# or
brew install rustup

# Initialize submodules
git submodule update --init --recursive

# Setup dependencies with uv
uv sync --all-packages
```

### Benchmarking

Use `vx-bench` to run benchmarks comparing engines (DataFusion, DuckDB) and formats (Parquet, Vortex):

```bash
# Install the benchmark orchestrator
uv tool install &quot;bench_orchestrator @ ./bench-orchestrator/&quot;

# Run TPC-H benchmarks
vx-bench run tpch --engine datafusion,duckdb --format parquet,vortex

# Compare results
vx-bench compare --run latest
```

See [bench-orchestrator/README.md](bench-orchestrator/README.md) for full documentation.

### Performance Optimization

For optimal performance, we suggest using [MiMalloc](https://github.com/microsoft/mimalloc):

```rust,ignore
#[global_allocator]
static GLOBAL_ALLOC: MiMalloc = MiMalloc;
```

## Project Information

### License

Licensed under the Apache License, Version 2.0.

### Governance

Vortex is an independent open-source project and not controlled by any single company. The Vortex Project is a
sub-project of the Linux Foundation Projects. The governance model is documented in
[CONTRIBUTING.md](CONTRIBUTING.md) and is subject to the terms of
the [Technical Charter](https://vortex.dev/charter.pdf).

### Contributing

Please **do** read [CONTRIBUTING.md](CONTRIBUTING.md) before you contribute.

### Reporting Vulnerabilities

If you discover a security vulnerability, please email &lt;vuln-report@vortex.dev&gt;.

### Trademarks

Copyright Â© Vortex a Series of LF Projects, LLC.
For terms of use, trademark policy, and other project policies please see &lt;https://lfprojects.org&gt;

## Acknowledgments

The Vortex project benefits enormously from groundbreaking work from the academic &amp; open-source communities.

### Research in Vortex

- [BtrBlocks](https://www.cs.cit.tum.de/fileadmin/w00cfj/dis/papers/btrblocks.pdf) - Efficient columnar compression
- [FastLanes](https://www.vldb.org/pvldb/vol16/p2132-afroozeh.pdf) &amp; [FastLanes on GPU](https://dbdbd2023.ugent.be/abstracts/felius_fastlanes.pdf) - High-performance integer compression
- [FSST](https://www.vldb.org/pvldb/vol13/p2649-boncz.pdf) - Fast random access string compression
- [ALP](https://ir.cwi.nl/pub/33334/33334.pdf) &amp; [G-ALP](https://dl.acm.org/doi/pdf/10.1145/3736227.3736242) - Adaptive lossless floating-point compression
- [Procella](https://dl.acm.org/citation.cfm?id=3360438) - YouTube&#039;s unified data system
- [Anyblob](https://www.durner.dev/app/media/papers/anyblob-vldb23.pdf) - High-performance access to object storage
- [ClickHouse](https://www.vldb.org/pvldb/vol17/p3731-schulze.pdf) - Fast analytics for everyone
- [MonetDB/X100](https://www.cidrdb.org/cidr2005/papers/P19.pdf) - Hyper-Pipelining Query Execution
- [Morsel-Driven Parallelism](https://db.in.tum.de/~leis/papers/morsels.pdf): A NUMA-Aware Query Evaluation Format for the Many-Core Age
- [The FastLanes File Format](https://github.com/cwida/FastLanes/blob/dev/docs/specification.pdf) - Expression Operators

### Vortex in Research

- [Anyblox](https://gienieczko.com/anyblox-paper) - A Framework for Self-Decoding Datasets
- [F3](https://dl.acm.org/doi/pdf/10.1145/3749163) - Open-Source Data File Format for the Future

### Open Source Inspiration

- [Apache Arrow](https://arrow.apache.org)
- [Apache DataFusion](https://github.com/apache/datafusion)
- [parquet2](https://github.com/jorgecarleitao/parquet2) by Jorge Leitao
- [DuckDB](https://github.com/duckdb/duckdb)
- [Velox](https://github.com/facebookincubator/velox) &amp; [Nimble](https://github.com/facebookincubator/nimble)

#### Thanks to all contributors who have shared their knowledge and code with the community! ğŸš€
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[wezterm/wezterm]]></title>
            <link>https://github.com/wezterm/wezterm</link>
            <guid>https://github.com/wezterm/wezterm</guid>
            <pubDate>Sun, 01 Feb 2026 00:07:09 GMT</pubDate>
            <description><![CDATA[A GPU-accelerated cross-platform terminal emulator and multiplexer written by @wez and implemented in Rust]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/wezterm/wezterm">wezterm/wezterm</a></h1>
            <p>A GPU-accelerated cross-platform terminal emulator and multiplexer written by @wez and implemented in Rust</p>
            <p>Language: Rust</p>
            <p>Stars: 23,832</p>
            <p>Forks: 1,168</p>
            <p>Stars today: 19 stars today</p>
            <h2>README</h2><pre># Wez&#039;s Terminal

&lt;img height=&quot;128&quot; alt=&quot;WezTerm Icon&quot; src=&quot;https://raw.githubusercontent.com/wezterm/wezterm/main/assets/icon/wezterm-icon.svg&quot; align=&quot;left&quot;&gt; *A GPU-accelerated cross-platform terminal emulator and multiplexer written by &lt;a href=&quot;https://github.com/wez&quot;&gt;@wez&lt;/a&gt; and implemented in &lt;a href=&quot;https://www.rust-lang.org/&quot;&gt;Rust&lt;/a&gt;*

User facing docs and guide at: https://wezterm.org/

![Screenshot](docs/screenshots/two.png)

*Screenshot of wezterm on macOS, running vim*

## Installation

https://wezterm.org/installation

## Getting help

This is a spare time project, so please bear with me.  There are a couple of channels for support:

* You can use the [GitHub issue tracker](https://github.com/wezterm/wezterm/issues) to see if someone else has a similar issue, or to file a new one.
* Start or join a thread in our [GitHub Discussions](https://github.com/wezterm/wezterm/discussions); if you have general
  questions or want to chat with other wezterm users, you&#039;re welcome here!
* There is a [Matrix room via Element.io](https://app.element.io/#/room/#wezterm:matrix.org)
  for (potentially!) real time discussions.

The GitHub Discussions and Element/Gitter rooms are better suited for questions
than bug reports, but don&#039;t be afraid to use whichever you are most comfortable
using and we&#039;ll work it out.

## Supporting the Project

If you use and like WezTerm, please consider sponsoring it: your support helps
to cover the fees required to maintain the project and to validate the time
spent working on it!

[Read more about sponsoring](https://wezterm.org/sponsor.html).

* [![Sponsor WezTerm](https://img.shields.io/github/sponsors/wez?label=Sponsor%20WezTerm&amp;logo=github&amp;style=for-the-badge)](https://github.com/sponsors/wez)
* [Patreon](https://patreon.com/WezFurlong)
* [Ko-Fi](https://ko-fi.com/wezfurlong)
* [Liberapay](https://liberapay.com/wez)
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[embassy-rs/embassy]]></title>
            <link>https://github.com/embassy-rs/embassy</link>
            <guid>https://github.com/embassy-rs/embassy</guid>
            <pubDate>Sun, 01 Feb 2026 00:07:08 GMT</pubDate>
            <description><![CDATA[Modern embedded framework, using Rust and async.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/embassy-rs/embassy">embassy-rs/embassy</a></h1>
            <p>Modern embedded framework, using Rust and async.</p>
            <p>Language: Rust</p>
            <p>Stars: 8,703</p>
            <p>Forks: 1,347</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre># Embassy

Embassy is the next-generation framework for embedded applications. Write safe, correct, and energy-efficient embedded code faster, using the Rust programming language, its async facilities, and the Embassy libraries.

## [Documentation](https://embassy.dev/book/index.html) - [API reference](https://docs.embassy.dev/) - [Website](https://embassy.dev/) - [Chat](https://matrix.to/#/#embassy-rs:matrix.org)

## Rust + async â¤ï¸ embedded

The Rust programming language is blazingly fast and memory-efficient, with no runtime, garbage collector, or OS. It catches a wide variety of bugs at compile time, thanks to its full memory- and thread-safety, and expressive type system.

Rust&#039;s [async/await](https://rust-lang.github.io/async-book/) allows for unprecedentedly easy and efficient multitasking in embedded systems. Tasks get transformed at compile time into state machines that get run cooperatively. It requires no dynamic memory allocation and runs on a single stack, so no per-task stack size tuning is required. It obsoletes the need for a traditional RTOS with kernel context switching, and is [faster and smaller than one!](https://tweedegolf.nl/en/blog/65/async-rust-vs-rtos-showdown)

## Batteries included

- **Hardware Abstraction Layers**
    - HALs implement safe, idiomatic Rust APIs to use the hardware capabilities, so raw register manipulation is not needed. The Embassy project maintains HALs for select hardware, but you can still use HALs from other projects with Embassy.
    - [embassy-stm32](https://docs.embassy.dev/embassy-stm32/), for all STM32 microcontroller families.
    - [embassy-nrf](https://docs.embassy.dev/embassy-nrf/), for the Nordic Semiconductor nRF52, nRF53, nRF54 and nRF91 series.
    - [embassy-rp](https://docs.embassy.dev/embassy-rp/), for the Raspberry Pi RP2040 and RP23xx microcontrollers.
    - [embassy-mspm0](https://docs.embassy.dev/embassy-mspm0/), for the Texas Instruments MSPM0 microcontrollers.
    - [esp-rs](https://github.com/esp-rs), for the Espressif Systems ESP32 series of chips.
        - Embassy HAL support for Espressif chips, as well as Async Wi-Fi, Bluetooth, and ESP-NOW, is being developed in the [esp-rs/esp-hal](https://github.com/esp-rs/esp-hal) repository.
    - [ch32-hal](https://github.com/ch32-rs/ch32-hal), for the WCH 32-bit RISC-V(CH32V) series of chips.
    - [mpfs-hal](https://github.com/AlexCharlton/mpfs-hal), for the Microchip PolarFire SoC.
    - [py32-hal](https://github.com/py32-rs/py32-hal), for the Puya Semiconductor PY32 series of microcontrollers.

- **Time that Just Works** -
  No more messing with hardware timers. [embassy_time](https://docs.embassy.dev/embassy-time) provides Instant, Duration, and Timer types that are globally available and never overflow.

- **Real-time ready** -
  Tasks on the same async executor run cooperatively, but you can create multiple executors with different priorities so that higher priority tasks preempt lower priority ones. See the [example](https://github.com/embassy-rs/embassy/blob/main/examples/nrf52840/src/bin/multiprio.rs).

- **Low-power ready** -
  Easily build devices with years of battery life. The async executor automatically puts the core to sleep when there&#039;s no work to do. Tasks are woken by interrupts, there is no busy-loop polling while waiting.

- **Networking** -
  The [embassy-net](https://docs.embassy.dev/embassy-net/) network stack implements extensive networking functionality, including Ethernet, IP, TCP, UDP, ICMP, and DHCP. Async drastically simplifies managing timeouts and serving multiple connections concurrently.

- **Bluetooth**
    - The [trouble](https://github.com/embassy-rs/trouble) crate provides a Bluetooth Low Energy 4.x and 5.x Host that runs on any microcontroller implementing the [bt-hci](https://github.com/embassy-rs/bt-hci) traits (currently
      `nRF52`, `nrf54`, `rp2040`, `rp23xx` and `esp32` and `serial` controllers are supported).
    - The [nrf-softdevice](https://github.com/embassy-rs/nrf-softdevice) crate provides Bluetooth Low Energy 4.x and 5.x support for nRF52 microcontrollers.
    - The [embassy-stm32-wpan](https://github.com/embassy-rs/embassy/tree/main/embassy-stm32-wpan) crate provides Bluetooth Low Energy 5.x support for stm32wb microcontrollers.

- **LoRa** -
  The [lora-rs](https://github.com/lora-rs/lora-rs) project provides an async LoRa and LoRaWAN stack that works well on Embassy.

- **USB** -
  [embassy-usb](https://docs.embassy.dev/embassy-usb/) implements a device-side USB stack. Implementations for common classes such as USB serial (CDC ACM) and USB HID are available, and a rich builder API allows building your own.

- **Bootloader and DFU** -
  [embassy-boot](https://github.com/embassy-rs/embassy/tree/main/embassy-boot) is a lightweight bootloader supporting firmware application upgrades in a power-fail-safe way, with trial boots and rollbacks.

## Sneak peek

```rust,ignore
use defmt::info;
use embassy_executor::Spawner;
use embassy_time::{Duration, Timer};
use embassy_nrf::gpio::{AnyPin, Input, Level, Output, OutputDrive, Pin, Pull};
use embassy_nrf::{Peri, Peripherals};

// Declare async tasks
#[embassy_executor::task]
async fn blink(pin: Peri&lt;&#039;static, AnyPin&gt;) {
    let mut led = Output::new(pin, Level::Low, OutputDrive::Standard);

    loop {
        // Timekeeping is globally available, no need to mess with hardware timers.
        led.set_high();
        Timer::after_millis(150).await;
        led.set_low();
        Timer::after_millis(150).await;
    }
}

// Main is itself an async task as well.
#[embassy_executor::main]
async fn main(spawner: Spawner) {
    let p = embassy_nrf::init(Default::default());

    // Spawned tasks run in the background, concurrently.
    spawner.spawn(blink(p.P0_13.into()).unwrap());

    let mut button = Input::new(p.P0_11, Pull::Up);
    loop {
        // Asynchronously wait for GPIO events, allowing other tasks
        // to run, or the core to sleep.
        button.wait_for_low().await;
        info!(&quot;Button pressed!&quot;);
        button.wait_for_high().await;
        info!(&quot;Button released!&quot;);
    }
}
```

## Examples

Examples are found in the
`examples/` folder separated by the chip manufacturer they are designed to run on. For example:

* `examples/nrf52840` run on the
  `nrf52840-dk` board (PCA10056) but should be easily adaptable to other nRF52 chips and boards.
* `examples/nrf5340` run on the `nrf5340-dk` board (PCA10095).
* `examples/stm32xx` for the various STM32 families.
* `examples/rp` are for the RP2040 chip.
* `examples/std` are designed to run locally on your PC.

### Running examples

- Install `probe-rs` following the instructions at &lt;https://probe.rs&gt;.
- Change directory to the sample&#039;s base directory. For example:

```bash
cd examples/nrf52840
```

- Ensure `Cargo.toml` sets the right feature for the name of the chip you are programming.
  If this name is incorrect, the example may fail to run or immediately crash
  after being programmed.

- Ensure `.cargo/config.toml` contains the name of the chip you are programming.

- Run the example

For example:

```bash
cargo run --release --bin blinky
```

For more help getting started, see [Getting Started][1] and [Running the Examples][2].

## Developing Embassy with Rust Analyzer-based editors

The [Rust Analyzer](https://rust-analyzer.github.io/) is used by [Visual Studio Code](https://code.visualstudio.com/)
and others. Given the multiple targets that Embassy serves, there is no Cargo workspace file. Instead, the Rust Analyzer
must be told of the target project to work with. In the case of Visual Studio Code,
please refer to the `.vscode/settings.json` file&#039;s `rust-analyzer.linkedProjects`setting.

## Minimum supported Rust version (MSRV)

Embassy is guaranteed to compile on stable Rust 1.75 and up. It *might*
compile with older versions, but that may change in any new patch release.

## Why the name?

EMBedded ASYnc! :)

## License

Embassy is licensed under either of

- Apache License, Version 2.0 ([LICENSE-APACHE](LICENSE-APACHE) or
  &lt;http://www.apache.org/licenses/LICENSE-2.0&gt;)
- MIT license ([LICENSE-MIT](LICENSE-MIT) or &lt;http://opensource.org/licenses/MIT&gt;)

at your option.

## Contribution

Unless you explicitly state otherwise, any contribution intentionally submitted
for inclusion in the work by you, as defined in the Apache-2.0 license, shall be
dual licensed as above, without any additional terms or conditions.

[1]: https://github.com/embassy-rs/embassy/wiki/Getting-Started
[2]: https://github.com/embassy-rs/embassy/wiki/Running-the-Examples
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[pgdogdev/pgdog]]></title>
            <link>https://github.com/pgdogdev/pgdog</link>
            <guid>https://github.com/pgdogdev/pgdog</guid>
            <pubDate>Sun, 01 Feb 2026 00:07:07 GMT</pubDate>
            <description><![CDATA[PostgreSQL connection pooler, load balancer and database sharder.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pgdogdev/pgdog">pgdogdev/pgdog</a></h1>
            <p>PostgreSQL connection pooler, load balancer and database sharder.</p>
            <p>Language: Rust</p>
            <p>Stars: 3,266</p>
            <p>Forks: 120</p>
            <p>Stars today: 30 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;/.github/logo2-white.png&quot; height=&quot;128&quot; width=&quot;auto&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;/.github/logo2_wide.png&quot; height=&quot;128&quot; width=&quot;auto&quot;&gt;
      &lt;img alt=&quot;Fallback image description&quot; src=&quot;/.github/logo2-white.png&quot; height=&quot;128&quot; width=&quot;auto&quot;&gt;
    &lt;/picture&gt;
&lt;/p&gt;

[![CI](https://github.com/levkk/pgdog/actions/workflows/ci.yml/badge.svg)](https://github.com/levkk/pgdog/actions/workflows/ci.yml)

PgDog is a proxy for scaling PostgreSQL. It supports connection pooling, load balancing queries and sharding entire databases. Written in Rust, PgDog is fast, secure and can manage thousands of connections on commodity hardware.

## Documentation

&amp;#128216; PgDog documentation can be **[found here](https://docs.pgdog.dev/)**. Any questions? Chat with us on **[Discord](https://discord.com/invite/CcBZkjSJdd)**.

## Quick start

### Kubernetes

Helm chart is **[here](https://github.com/pgdogdev/helm)**. To install it, run:

```bash
helm repo add pgdogdev https://helm.pgdog.dev
helm install pgdog pgdogdev/pgdog
```

### Try in Docker

You can try PgDog quickly using Docker. Install [Docker Compose](https://docs.docker.com/compose/) and run:

```
docker-compose up
```

Once started, you can connect to PgDog with psql or any other PostgreSQL client:

```
PGPASSWORD=postgres psql -h 127.0.0.1 -p 6432 -U postgres
```

The demo comes with 3 shards and 2 sharded tables:

```sql
INSERT INTO users (id, email) VALUES (1, &#039;admin@acme.com&#039;);
INSERT INTO payments (id, user_id, amount) VALUES (1, 1, 100.0);

SELECT * FROM users WHERE id = 1;
SELECT * FROM payments WHERE user_id = 1;
```

## Features

&amp;#128216; **[Configuration](https://docs.pgdog.dev/configuration/)**

All PgDog features are configurable and can be turned on and off. PgDog requires 2 configuration files to operate:

1. `pgdog.toml`: hosts, sharding configuration, and other settings
2. `users.toml`: usernames and passwords

### Example

Most options have reasonable defaults, so a basic configuration for a single user
and database running on the same machine is pretty short:

**`pgdog.toml`**

```toml
[general]
port = 6432
default_pool_size = 10

[[databases]]
name = &quot;pgdog&quot;
host = &quot;127.0.0.1&quot;
```

**`users.toml`**

```toml
[[users]]
name = &quot;alice&quot;
database = &quot;pgdog&quot;
password = &quot;hunter2&quot;
```

If a database in `pgdog.toml` doesn&#039;t have a user in `users.toml`, the connection pool for that database will not be created and users won&#039;t be able to connect.

If you&#039;d like to try it out locally, create the database and user like so:

```sql
CREATE DATABASE pgdog;
CREATE USER pgdog PASSWORD &#039;pgdog&#039; LOGIN;
```

### Transaction pooling

&amp;#128216; **[Transactions](https://docs.pgdog.dev/features/transaction-mode)**

Like PgBouncer, PgDog supports transaction (and session) pooling, allowing
thousands of clients to use just a few PostgreSQL server connections.

Unlike PgBouncer, PgDog can parse and handle `SET` statements and startup options, ensuring session state is set correctly when sharing server connections between clients with different parameters.

PgDog also has more advanced connection recovery options, like automatic abandoned transaction rollbacks and connection re-synchronization to avoid churning server connections during an application crash.

### Load balancer

&amp;#128216; **[Load balancer](https://docs.pgdog.dev/features/load-balancer/)**

PgDog is an application layer (OSI Level 7) load balancer for PostgreSQL. It understands the Postgres protocol, can proxy multiple replicas (and primary) and distributes transactions evenly between databases. The load balancer supports 3 strategies: round robin, random and least active connections.


**Example**

The load balancer is enabled automatically when a database has more than one host:

```toml
[[databases]]
name = &quot;prod&quot;
host = &quot;10.0.0.1&quot;
role = &quot;primary&quot;

[[databases]]
name = &quot;prod&quot;
host = &quot;10.0.0.2&quot;
role = &quot;replica&quot;
```

#### Health checks

&amp;#128216; **[Healthchecks](https://docs.pgdog.dev/features/load-balancer/healthchecks/)**


PgDog maintains a real-time list of healthy hosts. When a database fails a health check, it&#039;s removed from the active rotation and queries are re-routed to other replicas. This works like an HTTP load balancer, except it&#039;s for your database.

Health checks maximize database availability and protect against bad network connections, temporary hardware failures or misconfiguration.


#### Single endpoint

&amp;#128216; **[Single endpoint](https://docs.pgdog.dev/features/load-balancer/#single-endpoint)**


PgDog uses [`pg_query`](https://github.com/pganalyze/pg_query.rs), which includes the PostgreSQL native parser. By parsing queries, PgDog can detect writes (e.g. `INSERT`, `UPDATE`, `CREATE TABLE`, etc.) and send them to the primary, leaving the replicas to serve reads (`SELECT`). This allows applications to connect to the same PgDog deployment for both reads and writes.


##### Transactions

&amp;#128216; **[Load balancer &amp; transactions](https://docs.pgdog.dev/features/load-balancer/transactions/)**


Transactions can execute multiple statements, so in a primary &amp; replica configuration, PgDog routes them to the primary. Clients can indicate a transaction is read-only, in which case PgDog will send it to a replica:

```sql
BEGIN READ ONLY;
-- This goes to a replica.
SELECT * FROM users LIMIT 1;
COMMIT;
```


#### Failover

&amp;#128216; **[Failover](https://docs.pgdog.dev/features/load-balancer/replication-failover/)**


PgDog monitors Postgres replication state and can automatically redirect writes to a different database if a replica is promoted. This doesn&#039;t replace tools like Patroni that actually orchestrate failovers. You can use PgDog alongside Patroni (or AWS RDS or other managed Postgres host), to gracefully failover live traffic.


**Example**

To enable failover, set all database `role` attributes to `auto` and enable replication monitoring (`lsn_check_delay` setting):

```toml
[general]
lsn_check_delay = 0

[[databases]]
name = &quot;prod&quot;
host = &quot;10.0.0.1&quot;
role = &quot;auto&quot;

[[databases]]
name = &quot;prod&quot;
host = &quot;10.0.0.2&quot;
role = &quot;auto&quot;
```

### Sharding

&amp;#128216; **[Sharding](https://docs.pgdog.dev/features/sharding/)**

PgDog is able to manage databases with multiple shards. By using the PostgreSQL parser, PgDog extracts sharding keys and determines the best routing strategy for each query.

For cross-shard queries, PgDog assembles and transforms results in memory, sending all rows to the client as if they are coming from a single database.

**Example**

Configuring multiple hosts for the same database with different shard numbers (`shard` setting) enables sharding:

```toml
[[databases]]
name = &quot;prod&quot;
host = &quot;10.0.0.1&quot;
shard = 0

[[databases]]
name = &quot;prod&quot;
host = &quot;10.0.0.2&quot;
shard = 1
```

Note: read below for how to configure query routing. At least one sharded table is required for sharding to work as expected.

#### Sharding functions

&amp;#128216; **[Sharding functions](https://docs.pgdog.dev/features/sharding/sharding-functions/)**

PgDog has two main sharding algorithms:

1. PostgreSQL partition functions (`HASH`, `LIST`, `RANGE`)
2. Using schemas

##### Partition-based sharding

Partition-based sharding functions are taken directly from Postgres source code. This choice intentionally allows to shard data both with PgDog and with Postgres [foreign tables](https://www.postgresql.org/docs/current/sql-createforeigntable.html) and [`postgres_fdw`](https://www.postgresql.org/docs/current/postgres-fdw.html).

**Examples**

The `PARTITION BY HASH` algorithm is used by default when configuring sharded tables:

```toml
[[sharded_tables]]
database = &quot;prod&quot;
column = &quot;user_id&quot;
```

List-based sharding (same as `PARTITION BY LIST` in Postgres) can be configured as follows:

```toml
# Sharded table definition still required.
[[sharded_tables]]
database = &quot;prod&quot;
column = &quot;user_id&quot;

# Value-specific shard mappings.
[[sharded_mapping]]
database = &quot;prod&quot;
column = &quot;user_id&quot;
values = [1, 2, 3, 4]
shard = 0

[[sharded_mapping]]
database = &quot;prod&quot;
column = &quot;user_id&quot;
values = [5, 6, 7, 8]
shard = 1
```

For range-based sharding, replace the `values` setting with a range, for example:

```toml
start = 0 # include
end = 5 # exclusive
```

##### Schema-based sharding

&amp;#128216; **[Schema-based sharding](https://docs.pgdog.dev/configuration/pgdog.toml/sharded_schemas/)**

Schema-based sharding works on the basis of PostgreSQL schemas. Tables under the same schema are placed on the same shard and all queries that refer to those tables are routed to that shard automatically.

**Example**

Configuring sharded schemas uses a different configuration from sharded tables:

```toml
[[sharded_schemas]]
database = &quot;prod&quot;
name = &quot;customer_a&quot;
shard = 0

[[sharded_schemas]]
database = &quot;prod&quot;
name = &quot;customer_b&quot;
shard = 1
```

Queries that refer tables in schema `customer_a` will be sent to shard 0. For example, a query that refers to a table by its fully-qualified name will be sent to one shard only:

```sql
INSERT INTO customer_a.orders (id, user_id, amount)
VALUES ($1, $2, $3);
```

Alternatively, the schema name can be specified in the `search_path` session variable:

```sql
SET search_path TO public, customer_a;
-- All subsequent queries will be sent to shard 0.
SELECT * FROM orders LIMIT 1;
```

You can also set the `search_path` for the duration of a single transaction, using `SET LOCAL`, ensuring only that transaction is sent to the desired shard:

```sql
-- The entire transaction will be sent to shard 1.
BEGIN;
SET LOCAL search_path TO public, customer_b;
SELECT * FROM orders LIMIT 1;
COMMIT;
```

#### Direct-to-shard queries

&amp;#128216; **[Direct-to-shard queries](https://docs.pgdog.dev/features/sharding/query-routing/)**


Queries that contain a sharding key are sent to one database only. This is the best case scenario for sharded databases, since the load is uniformly distributed across the cluster.

**Example**:

```sql
-- user_id is the sharding key.
SELECT * FROM users WHERE user_id = $1;
```

#### Cross-shard queries

- &amp;#128216; **[Cross-shard queries](https://docs.pgdog.dev/features/sharding/cross-shard-queries/)**
- &amp;#128216; **[SELECT](https://docs.pgdog.dev/features/sharding/cross-shard-queries/select/)**
- &amp;#128216; **[INSERT](https://docs.pgdog.dev/features/sharding/cross-shard-queries/insert/)**
- &amp;#128216; **[UPDATE and DELETE](https://docs.pgdog.dev/features/sharding/cross-shard-queries/update/)**
- &amp;#128216; **[DDL](https://docs.pgdog.dev/features/sharding/cross-shard-queries/ddl/)**

Queries with multiple sharding keys or without one are sent to all databases and results are post-processed and assembled in memory. PgDog then sends the final result to the client.

Currently, support for certain SQL features in cross-shard queries is limited. However, the list of supported ones keeps growing:

| Feature | Supported | Notes |
|-|-|-|
| Aggregates | Partial | `count`, `min`, `max`, `stddev`, `variance`, `sum`, `avg` are supported. |
| `ORDER BY` | Partial | Column in `ORDER BY` clause must be present in the result set. |
| `GROUP BY` | Partial | Same as `ORDER BY`, referenced columns must be present in result set. |
| Multi-tuple `INSERT` | Supported | PgDog generates one statement per tuple and executes them automatically. |
| Sharding key `UPDATE` | Supported | PgDog generates a `SELECT`, `INSERT` and `DELETE` statements and execute them automatically. |
| Subqueries | No | The same subquery is executed on all shards. |
| CTEs | No | The same CTE is executed on all shards. |


#### Using `COPY`

&amp;#128216; **[Copy](https://docs.pgdog.dev/features/sharding/cross-shard-queries/copy/)**

PgDog has a text, CSV &amp; binary parser and can split rows sent via `COPY` command between all shards automatically. This allows clients to ingest data into sharded PostgreSQL without preprocessing

**Example**

```sql
COPY orders (id, user_id, amount) FROM STDIN CSV HEADER;
```

Columns must be specified in the `COPY` statement, so PgDog can infer the sharding key automatically, but are optional in the data file.


#### Consistency (two-phase commit)

&amp;#128216; **[Two-phase commit](https://docs.pgdog.dev/features/sharding/2pc/)**

To make sure cross-shard writes are atomic, PgDog supports Postgres&#039; [two-phase transactions](https://www.postgresql.org/docs/current/two-phase.html). When enabled, PgDog handles `COMMIT` statements sent by clients by executing the 2pc exchange on their behalf:

```sql
PREPARE TRANSACTION &#039;__pgdog_unique_id&#039;;
COMMIT PREPARED &#039;__pgdog_unique_id&#039;;
```

In case the client disconnects or Postgres crashes, PgDog will automatically rollback the transaction if it&#039;s in phase I and commit it if it&#039;s in phase II.

#### Unique identifiers

&amp;#128216; **[Unique IDs](https://docs.pgdog.dev/features/sharding/unique-ids/)**

While applications can use `UUID` (v4 and now v7) to generate unique primary keys, PgDog supports creating unique `BIGINT` identifiers, without using a sequence:

```sql
SELECT pgdog.unique_id();
```

This uses a timestamp-based algorithm, can produce millions of unique numbers per second and doesn&#039;t require an expensive cross-shard index to guarantee uniqueness.

#### Shard key updates

PgDog supports changing the sharding key for a row online. Under the hood, it will execute 3 statements to make it happen:

1. `SELECT` to get the entire row from its original shard
2. `INSERT` to write the new, changed row to the new shard
3. `DELETE` to remove it from the old shard

This happens automatically, and the client can retrieve the new row as normal:

```sql
UPDATE orders SET user_id = 5 WHERE user_id = 1 RETURNING *;
-- This will return the new row
```

Note: Only one row can be updated at a time and if a query attempts to update multiple, PgDog will abort the transaction.

To enable shard key updates, add this to `pgdog.toml`:

```toml
[rewrite]
enabled = true
shard_key = &quot;rewrite&quot; # options: ignore (possible data loss), error (block shard key update)
```

#### Multi-tuple inserts

PgDog can handle multi-tuple `INSERT` queries by sending each tuple to the right shard, e.g.:

```sql
INSERT INTO payments
    (id, user_id, amount) -- user_id is the sharding key
VALUES
(pgdog.unique_id(), 1, 25.00), -- Tuples go to different shards
(pgdog.unique_id(), 5, 55.0); -- Each tuple gets a unique primary key because unique ID function is invoked twice
```

This happens automatically, if enabled:

```toml
[rewrite]
enabled = true
split_inserts = &quot;rewrite&quot; # other options: ignore, error
```

#### Re-sharding

- &amp;#128216; **[Re-sharding](https://docs.pgdog.dev/features/sharding/resharding/)**
- &amp;#128216; **[Schema sync](https://docs.pgdog.dev/features/sharding/resharding/schema/)**
- &amp;#128216; **[Data sync](https://docs.pgdog.dev/features/sharding/resharding/hash/)**

PgDog understands the PostgreSQL logical replication protocol and can orchestrate data splits between databases, in the background and without downtime. This allows to shard existing databases and add more shards to existing clusters in production, without impacting database operations.

The re-sharding process is done in 5 steps:

1. Create new empty cluster with the desired number of shards
2. Configure it in `pgdog.toml` and run `schema-sync` command to copy table schemas to the new databases
3. Run `data-sync` command to copy and re-shard table data with logical replication (tables are copied in parallel)
4. While keeping previous command running (it streams row updates in real-time), run `schema-sync --data-sync-complete` to create secondary indexes on the new databases (much faster to do this after data is copied)
5. Cutover traffic to new cluster with `MAINTENANCE ON`, `RELOAD`, `MAINTENANCE OFF` command sequence

Cutover can be done atomically with multiple PgDog containers because `RELOAD` doesn&#039;t resume traffic, `MAINTENANCE OFF` does, so the config is the same in all containers before queries are resumed. No complex synchronization tooling like etcd or  Zookeeper is required.

### Monitoring

&amp;#128216; **[Metrics](https://docs.pgdog.dev/features/metrics/)**

PgDog exposes both the standard PgBouncer-style admin database and an OpenMetrics endpoint. The admin database isn&#039;t 100% compatible,
so we recommend you use OpenMetrics for monitoring. Example Datadog configuration and dashboard are [included](examples/datadog).


## Running PgDog locally

Install the latest version of the Rust compiler from [rust-lang.org](https://rust-lang.org).
Clone this repository and build the project in release mode:

```bash
cargo build --release
```

It&#039;s important to use the release profile if you&#039;re deploying to production or want to run
performance benchmarks.

#### Try sharding

Sharded database clusters are set in the config. For example, to set up a 2 shard cluster, you can:

**`pgdog.toml`**

```toml
[[databases]]
name = &quot;pgdog_sharded&quot;
host = &quot;127.0.0.1&quot;
database_name = &quot;shard_0&quot;
shard = 0

[[databases]]
name = &quot;pgdog_sharded&quot;
host = &quot;127.0.0.1&quot;
database_name = &quot;shard_1&quot;
shard = 1

[[sharded_tables]]
database = &quot;pgdog_sharded&quot;
column = &quot;user_id&quot;
```

Don&#039;t forget to configure a user:

**`users.toml`**

```toml
[[users]]
database = &quot;pgdog_sharded&quot;
name = &quot;pgdog&quot;
password = &quot;pgdog&quot;
```

And finally, to make it work locally, create the required databases:

```sql
CREATE DATABASE shard_0;
CREATE DATABASE shard_1;

GRANT ALL ON DATABASE shard_0 TO pgdog;
GRANT ALL ON DATABASE shard_1 TO pgdog;
```

### Start PgDog

Running PgDog can be done with Cargo:

```bash
cargo run --release
```

#### Command-line options

PgDog supports several command-line options:

- `-c, --config &lt;CONFIG&gt;`: Path to the configuration file (default: `&quot;pgdog.toml&quot;`)
- `-u, --users &lt;USERS&gt;`: Path to the users.toml file (default: `&quot;users.toml&quot;`)
- `-d, --database_url &lt;DATABASE_URL&gt;`: Connection URL(s). Can be specified multiple times to add multiple database connections. When provided, these URLs override database configurations from the config file.

Example using database URLs directly:

```bash
cargo run --release -- -d postgres://user:pass@localhost:5432/db1 -d postgres://user:pass@localhost:5433/db2
```

You can connect to PgDog with `psql` or any other PostgreSQL client:

```bash
psql postgres://pgdog:pgdog@127.0.0.1:6432/pgdog
```

## &amp;#128678; Status &amp;#128678;

PgDog is used in production and at scale. Most features are stable, while some are experimental. Check [documentation](https://docs.pgdog.dev/features/) for more details. New sharding features are added almost weekly.

## Performance

&amp;#128216; **[Architecture &amp; benchmarks](https://docs.pgdog.dev/architecture/)**

PgDog is heavily optimized for performance. We use Rust, [Tokio](https://tokio.rs/), [bytes crate](https://docs.rs/bytes/latest/bytes/) to avoid unnecessary memory allocations, and profile for performance regressions on a regular basis.

## License

PgDog is free and open source software, licensed under the AGPL v3. While often misunderstood, this license is very permissive
and allows the following without any additional requirements from you or your organization:

* Internal use
* Private modifications for internal use without sharing any source code

You can freely use PgDog to power your PostgreSQL databases without having to
share any source code, including proprietary work product or any PgDog modifications you make.

AGPL was written specifically for organizations that offer PgDog _as a public service_ (e.g. database cloud providers) and require
those organizations to share any modifications they make to PgDog, including new features and bug fixes.

## Contributions

Please read our [Contribution Guidelines](CONTRIBUTING.md).
</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
    </channel>
</rss>