<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for go - Go Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for go.</description>
        <lastBuildDate>Fri, 09 Jan 2026 00:05:29 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2026, GitHub</copyright>
        <item>
            <title><![CDATA[m1k1o/neko]]></title>
            <link>https://github.com/m1k1o/neko</link>
            <guid>https://github.com/m1k1o/neko</guid>
            <pubDate>Fri, 09 Jan 2026 00:05:29 GMT</pubDate>
            <description><![CDATA[A self hosted virtual browser that runs in docker and uses WebRTC.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/m1k1o/neko">m1k1o/neko</a></h1>
            <p>A self hosted virtual browser that runs in docker and uses WebRTC.</p>
            <p>Language: Go</p>
            <p>Stars: 16,635</p>
            <p>Forks: 1,126</p>
            <p>Stars today: 52 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/m1k1o/neko&quot; title=&quot;Neko&#039;s Github repository.&quot;&gt;
    &lt;img src=&quot;https://neko.m1k1o.net/img/logo.png&quot; width=&quot;400&quot; height=&quot;auto&quot;/&gt;
  &lt;/a&gt;
  &lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://github.com/m1k1o/neko/releases&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/github/v/release/m1k1o/neko&quot; alt=&quot;release&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/m1k1o/neko/blob/master/LICENSE&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/github/license/m1k1o/neko&quot; alt=&quot;license&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://hub.docker.com/r/m1k1o/neko&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/docker/pulls/m1k1o/neko&quot; alt=&quot;pulls&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/m1k1o/neko/issues&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/github/issues/m1k1o/neko&quot; alt=&quot;issues&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/sponsors/m1k1o&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/badge/-sponsor-red&quot; alt=&quot;issues&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://discord.gg/3U6hWpC&quot;&gt;
      &lt;img src=&quot;https://discordapp.com/api/guilds/665851821906067466/widget.png&quot; alt=&quot;Chat on discord&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://hellogithub.com/repository/4536d4546af24196af3f08a023dfa007&quot; target=&quot;_blank&quot;&gt;
      &lt;img src=&quot;https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=4536d4546af24196af3f08a023dfa007&amp;claim_uid=0x19e4dJwD83aW2&amp;theme=small&quot; alt=&quot;FeaturedÔΩúHelloGitHub&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/m1k1o/neko/actions&quot;&gt;
      &lt;img src=&quot;https://github.com/m1k1o/neko/actions/workflows/ghcr.yml/badge.svg&quot; alt=&quot;build&quot;&gt;
    &lt;/a&gt;
  &lt;/p&gt;
  &lt;img src=&quot;https://neko.m1k1o.net/img/intro.gif&quot; width=&quot;650&quot; height=&quot;auto&quot;/&gt;
&lt;/div&gt;

# n.eko

Welcome to Neko, a self-hosted virtual browser that runs in Docker and uses WebRTC technology. Neko is a powerful tool that allows you to **run a fully-functional browser in a virtual environment**, giving you the ability to **access the internet securely and privately from anywhere**. With Neko, you can browse the web, **run applications**, and perform other tasks just as you would on a regular browser, all within a **secure and isolated environment**. Whether you are a developer looking to test web applications, a **privacy-conscious user seeking a secure browsing experience**, or simply someone who wants to take advantage of the **convenience and flexibility of a virtual browser**, Neko is the perfect solution.

In addition to its security and privacy features, Neko offers the **ability for multiple users to access it simultaneously**. This makes it an ideal solution for teams or organizations that need to share access to a browser, as well as for individuals who want to use **multiple devices to access the same virtual environment**. With Neko, you can **easily and securely share access to a browser with others**, without having to worry about maintaining separate configurations or settings. Whether you need to **collaborate on a project**, access shared resources, or simply want to **share access to a browser with friends or family**, Neko makes it easy to do so.

Neko is also a great tool for **hosting watch parties** and interactive presentations. With its virtual browser capabilities, Neko allows you to host watch parties and presentations that are **accessible from anywhere**, without the need for in-person gatherings. This makes it easy to **stay connected with friends and colleagues**, even when you are unable to meet in person. With Neko, you can easily host a watch party or give an **interactive presentation**, whether it&#039;s for leisure or work. Simply invite your guests to join the virtual environment, and you can share the screen and **interact with them in real-time**.

## About

This app uses WebRTC to stream a desktop inside of a docker container, original author made this because [rabb.it](https://en.wikipedia.org/wiki/Rabb.it) went under and his internet could not handle streaming and discord kept crashing when his friend attempted to. He just wanted to watch anime with his friends ·Éö(‡≤†Áõä‡≤†·Éö) so he started digging throughout the internet and found a few *kinda* clones, but none of them had the virtual browser, then he found [Turtus](https://github.com/Khauri/Turtus) and he was able to figure out the rest.

Then I found [this](https://github.com/nurdism/neko) project and started to dig into it. I really liked the idea of having collaborative browser browsing together with multiple people, so I created a fork. Initially, I wanted to merge my changes to the upstream repository, but the original author did not have time for this project anymore and it got eventually archived.

## Use-cases and comparison

Neko started as a virtual browser that is streamed using WebRTC to multiple users.
- It is **not only limited to a browser**; it can run anything that runs on linux (e.g. VLC). Browser only happens to be the most popular and widely used use-case.
- In fact, it is not limited to a single program either; you can install a full desktop environment (e.g. XFCE, KDE).
- Speaking of limits, it does not need to run in a container; you could install neko on your host, connect to your X server and control your whole VM.
- Theoretically it is not limited to only X server, anything that can be controlled and scraped periodically for images could be used instead.
  - Like implementing RDP or VNC protocol, where neko would only act as WebRTC relay server. This is currently only future.

Primary use case is connecting with multiple people, leveraging real time synchronization and interactivity:
- **Watch party** - watching video content together with multiple people and reacting to it (chat, emotes) - open source alternative to [giggl.app](https://giggl.app/) or [hyperbeam](https://watch.hyperbeam.com).
- **Interactive presentation** - not only screen sharing, but others can control the screen.
- **Collaborative tool** - brainstorming ideas, cobrowsing, code debugging together.
- **Support/Teaching** - interactively guiding people in controlled environment.
- **Embed anything** - embed virtual browser in your web app - open source alternative to [hyperbeam API](https://hyperbeam.com/).
  - open any third-party website or application, synchronize audio and video flawlessly among multiple participants.
  - request rooms using API with [neko-rooms](https://github.com/m1k1o/neko-rooms).

Other use cases that benefit from single-user:
- **Personal workspace** - streaming containerized apps and desktops to end-users - similar to [kasm](https://www.kasmweb.com/).
- **Persistent browser** - own browser with persistent cookies available anywhere - similar to [mightyapp](https://www.mightyapp.com/).
  - no state is left on the host browser after terminating the connection.
  - sensitive data like cookies are not transferred - only video is shared.
- **Throwaway browser** - a better solution for planning secret parties and buying birthday gifts off the internet.
  - use Tor Browser and [VPN](https://github.com/m1k1o/neko-vpn) for additional anonymity.
  - mitigates risk of OS fingerprinting and browser vulnerabilities by running in container.
- **Session broadcasting** - broadcast room content using RTMP (to e.g. twitch or youtube...).
- **Session recording** - broadcast RTMP can be saved to a file using e.g. [nginx-rtmp](https://www.nginx.com/products/nginx/modules/rtmp-media-streaming/)
  - have clean environment when recording tutorials.
  - no need to hide bookmarks or use incognito mode.
- **Jump host** - access your internal applications securely without the need for VPN.
- **Automated browser** - you can install [playwright](https://playwright.dev/) or [puppeteer](https://pptr.dev/) and automate tasks while being able to actively intercept them.

Compared to clientless remote desktop gateway (e.g. [Apache Guacamole](https://guacamole.apache.org/) or [websockify](https://github.com/novnc/websockify) with [noVNC](https://novnc.com/)), installed with remote desktop server along with desired program (e.g. [linuxserver/firefox](https://docs.linuxserver.io/images/docker-firefox)) provides neko additionally:
- **Smooth video** because it uses WebRTC and not images sent over WebSockets.
- **Built in audio** support, what is not part of Apache Guacamole or noVNC.
- **Multi-participant control**, what is not natively supported by Apache Guacamole or noVNC.

### Supported browsers

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://neko.m1k1o.net/docs/v3/installation/docker-images#firefox&quot;&gt;
    &lt;img src=&quot;https://neko.m1k1o.net/img/icons/firefox.svg&quot; title=&quot;ghcr.io/m1k1o/neko/firefox&quot; width=&quot;60&quot; height=&quot;auto&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://neko.m1k1o.net/docs/v3/installation/docker-images#tor-browser&quot;&gt;
    &lt;img src=&quot;https://neko.m1k1o.net/img/icons/tor-browser.svg&quot; title=&quot;ghcr.io/m1k1o/neko/tor-browser&quot; width=&quot;60&quot; height=&quot;auto&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://neko.m1k1o.net/docs/v3/installation/docker-images#waterfox&quot;&gt;
    &lt;img src=&quot;https://neko.m1k1o.net/img/icons/waterfox.svg&quot; title=&quot;ghcr.io/m1k1o/neko/waterfox&quot; width=&quot;60&quot; height=&quot;auto&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://neko.m1k1o.net/docs/v3/installation/docker-images#chromium&quot;&gt;
    &lt;img src=&quot;https://neko.m1k1o.net/img/icons/chromium.svg&quot; title=&quot;ghcr.io/m1k1o/neko/chromium&quot; width=&quot;60&quot; height=&quot;auto&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://neko.m1k1o.net/docs/v3/installation/docker-images#google-chrome&quot;&gt;
    &lt;img src=&quot;https://neko.m1k1o.net/img/icons/google-chrome.svg&quot; title=&quot;ghcr.io/m1k1o/neko/google-chrome&quot; width=&quot;60&quot; height=&quot;auto&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://neko.m1k1o.net/docs/v3/installation/docker-images#ungoogled-chromium&quot;&gt;
    &lt;img src=&quot;https://neko.m1k1o.net/img/icons/ungoogled-chromium.svg&quot; title=&quot;ghcr.io/m1k1o/neko/google-chrome&quot; width=&quot;60&quot; height=&quot;auto&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://neko.m1k1o.net/docs/v3/installation/docker-images#microsoft-edge&quot;&gt;
    &lt;img src=&quot;https://neko.m1k1o.net/img/icons/microsoft-edge.svg&quot; title=&quot;ghcr.io/m1k1o/neko/microsoft-edge&quot; width=&quot;60&quot; height=&quot;auto&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://neko.m1k1o.net/docs/v3/installation/docker-images#brave&quot;&gt;
    &lt;img src=&quot;https://neko.m1k1o.net/img/icons/brave.svg&quot; title=&quot;ghcr.io/m1k1o/neko/brave&quot; width=&quot;60&quot; height=&quot;auto&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://neko.m1k1o.net/docs/v3/installation/docker-images#vivaldi&quot;&gt;
    &lt;img src=&quot;https://neko.m1k1o.net/img/icons/vivaldi.svg&quot; title=&quot;ghcr.io/m1k1o/neko/vivaldi&quot; width=&quot;60&quot; height=&quot;auto&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://neko.m1k1o.net/docs/v3/installation/docker-images#opera&quot;&gt;
    &lt;img src=&quot;https://neko.m1k1o.net/img/icons/opera.svg&quot; title=&quot;ghcr.io/m1k1o/neko/opera&quot; width=&quot;60&quot; height=&quot;auto&quot;/&gt;
  &lt;/a&gt;

  ... see [all available images](https://neko.m1k1o.net/docs/v3/installation/docker-images)
&lt;/div&gt;

### Other applications

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://neko.m1k1o.net/docs/v3/installation/docker-images#xfce&quot;&gt;
    &lt;img src=&quot;https://neko.m1k1o.net/img/icons/xfce.svg&quot; title=&quot;ghcr.io/m1k1o/neko/xfce&quot; width=&quot;60&quot; height=&quot;auto&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://neko.m1k1o.net/docs/v3/installation/docker-images#kde&quot;&gt;
    &lt;img src=&quot;https://neko.m1k1o.net/img/icons/kde.svg&quot; title=&quot;ghcr.io/m1k1o/neko/kde&quot; width=&quot;60&quot; height=&quot;auto&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://neko.m1k1o.net/docs/v3/installation/docker-images#remmina&quot;&gt;
    &lt;img src=&quot;https://neko.m1k1o.net/img/icons/remmina.svg&quot; title=&quot;ghcr.io/m1k1o/neko/remmina&quot; width=&quot;60&quot; height=&quot;auto&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://neko.m1k1o.net/docs/v3/installation/docker-images#vlc&quot;&gt;
    &lt;img src=&quot;https://neko.m1k1o.net/img/icons/vlc.svg&quot; title=&quot;ghcr.io/m1k1o/neko/vlc&quot; width=&quot;60&quot; height=&quot;auto&quot;/&gt;
  &lt;/a&gt;

  ... others in &lt;a href=&quot;https://github.com/m1k1o/neko-apps&quot;&gt;m1k1o/neko-apps&lt;/a&gt;
&lt;/div&gt;

### Why neko?

I like cats üê± (`Neko` is the Japanese word for cat), I&#039;m a weeb/nerd.

***But why the cat butt?*** Because cats are *assholes*, but you love them anyways.

## Multiple rooms

For neko room management software, visit [neko-rooms](https://github.com/m1k1o/neko-rooms).

It also offers [Zero-knowledge installation (with HTTPS)](https://github.com/m1k1o/neko-rooms/?tab=readme-ov-file#zero-knowledge-installation-with-https).

## Documentation

Full documentation is available at [neko.m1k1o.net](https://neko.m1k1o.net/). Key sections include:

- [Migration from V2](https://neko.m1k1o.net/docs/v3/migration-from-v2)
- [Getting Started](https://neko.m1k1o.net/docs/v3/quick-start)
- [Installation](https://neko.m1k1o.net/docs/v3/installation)
- [Examples](https://neko.m1k1o.net/docs/v3/installation/examples)
- [Configuration](https://neko.m1k1o.net/docs/v3/configuration)
- [Frequently Asked Questions](https://neko.m1k1o.net/docs/v3/faq)
- [Troubleshooting](https://neko.m1k1o.net/docs/v3/troubleshooting)

## How to Contribute

Contributions are welcome! Check the [Contributing Guide](https://neko.m1k1o.net/contributing) for details.

## Support

If you find Neko useful, consider supporting the project via [GitHub Sponsors](https://github.com/sponsors/m1k1o).
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[kyverno/kyverno]]></title>
            <link>https://github.com/kyverno/kyverno</link>
            <guid>https://github.com/kyverno/kyverno</guid>
            <pubDate>Fri, 09 Jan 2026 00:05:28 GMT</pubDate>
            <description><![CDATA[Cloud Native Policy Management]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/kyverno/kyverno">kyverno/kyverno</a></h1>
            <p>Cloud Native Policy Management</p>
            <p>Language: Go</p>
            <p>Stars: 7,270</p>
            <p>Forks: 1,175</p>
            <p>Stars today: 8 stars today</p>
            <h2>README</h2><pre>&lt;!--
Copyright 2025 The Kyverno Authors

Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
--&gt;

# Kyverno [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Cloud%20Native%20Policy%20Management.%20No%20new%20language%20required%1&amp;url=https://github.com/kyverno/kyverno/&amp;hashtags=kubernetes,devops)

**Cloud Native Policy Management üéâ**

[![Build Status](https://github.com/kyverno/kyverno/actions/workflows/test.yml/badge.svg)](https://github.com/kyverno/kyverno/actions)
[![Go Report Card](https://goreportcard.com/badge/github.com/kyverno/kyverno)](https://goreportcard.com/report/github.com/kyverno/kyverno)
![License: Apache-2.0](https://img.shields.io/github/license/kyverno/kyverno?color=blue)
[![GitHub Repo stars](https://img.shields.io/github/stars/kyverno/kyverno)](https://github.com/kyverno/kyverno/stargazers)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/5327/badge)](https://bestpractices.coreinfrastructure.org/projects/5327)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/kyverno/kyverno/badge)](https://securityscorecards.dev/viewer/?uri=github.com/kyverno/kyverno)
[![SLSA 3](https://slsa.dev/images/gh-badge-level3.svg)](https://slsa.dev)
[![Artifact HUB](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/kyverno)](https://artifacthub.io/packages/search?repo=kyverno)
[![codecov](https://codecov.io/gh/kyverno/kyverno/branch/main/graph/badge.svg)](https://app.codecov.io/gh/kyverno/kyverno/branch/main)
[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Fkyverno%2Fkyverno.svg?type=shield)](https://app.fossa.com/projects/git%2Bgithub.com%2Fkyverno%2Fkyverno?ref=badge_shield)

&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://kyverno.io&quot; rel=&quot;kyverno.io&quot;&gt;&lt;img src=&quot;img/Kyverno_Horizontal.png&quot; alt=&quot;Kyverno Logo&quot; width=&quot;400&quot;&gt;&lt;/a&gt;&lt;/p&gt;

## üìë Table of Contents

- [About Kyverno](#about-kyverno)
- [Documentation](#-documentation)
- [Demos &amp; Tutorials](#-demos--tutorials)
- [Popular Use Cases](#-popular-use-cases)
- [Explore the Policy Library](#-explore-the-policy-library)
- [Getting Help](#-getting-help)
- [Contributing](#-contributing)
- [Software Bill of Materials](#software-bill-of-materials)
- [Community Highlights](#-community-highlights)
- [Contributors](#contributors)
- [License](#license)

## About Kyverno

Kyverno is a Kubernetes-native policy engine designed for platform engineering teams. It enables security, compliance, automation, and governance through policy-as-code. Kyverno can:

- Validate, mutate, generate, and clean up resources using Kubernetes admission controls and background scans.
- Verify container image signatures for supply chain security.
- Operate with tools you already use ‚Äî like `kubectl`, `kustomize`, and Git.

&lt;a href=&quot;https://opensourcesecurityindex.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
  &lt;img src=&quot;https://opensourcesecurityindex.io/badge.svg&quot; alt=&quot;Open Source Security Index badge&quot; width=&quot;282&quot; height=&quot;56&quot; /&gt;
&lt;/a&gt;

## üìô Documentation

Kyverno installation and reference documentation is available at [kyverno.io](https://kyverno.io).

- üëâ **[Quick Start](https://kyverno.io/docs/introduction/#quick-start)**
- üëâ **[Installation Guide](https://kyverno.io/docs/installation/)**
- üëâ **[Policy Library](https://kyverno.io/policies/)**

## üé• Demos &amp; Tutorials

- ‚ñ∂Ô∏è [Getting Started with Kyverno ‚Äì YouTube](https://www.youtube.com/results?search_query=kyverno+tutorial)
- üß™ [Kyverno Playground](https://playground.kyverno.io/)

## üéØ Popular Use Cases

Kyverno helps platform teams enforce best practices and security standards. Some common use cases include:

### 1. **Security &amp; Compliance**
- Enforce Pod Security Standards (PSS)
- Require specific security contexts
- Validate container image sources and signatures
- Enforce CIS Benchmark policies

### 2. **Operational Excellence**
- Auto-label workloads
- Enforce naming conventions
- Generate default configurations (e.g., NetworkPolicies)
- Validate YAML and Helm manifests

### 3. **Cost Optimization**
- Enforce resource quotas and limits
- Require cost allocation labels
- Validate instance types
- Clean up unused resources

### 4. **Developer Guardrails**
- Require readiness/liveness probes
- Enforce ingress/egress policies
- Validate container image versions
- Auto-inject config maps or secrets

## üìö Explore the Policy Library

Discover hundreds of production-ready Kyverno policies for security, operations, cost control, and developer enablement.

üëâ [Browse the Policy Library](https://kyverno.io/policies/)

## üôã Getting Help

We‚Äôre here to help:

- üêû File a [GitHub Issue](https://github.com/kyverno/kyverno/issues)
- üí¨ Join the [Kyverno Slack Channel](https://slack.k8s.io/#kyverno)
- üìÖ Attend [Community Meetings](https://kyverno.io/community/#community-meetings)
- ‚≠êÔ∏è [Star this repository](https://github.com/kyverno/kyverno/stargazers) to stay updated

## ‚ûï Contributing

Thank you for your interest in contributing to Kyverno!

- ‚úÖ Read the [Contribution Guidelines](/CONTRIBUTING.md)
- üßµ Join [GitHub Discussions](https://github.com/kyverno/kyverno/discussions)
- üìñ Read the [Development Guide](/DEVELOPMENT.md)
- üèÅ Check [Good First Issues](https://github.com/kyverno/kyverno/labels/good%20first%20issue) and request with `/assign`
- üå± Explore the [Community page](https://kyverno.io/community/)

## üßæ Software Bill of Materials

All Kyverno images include a Software Bill of Materials (SBOM) in [CycloneDX](https://cyclonedx.org/) format. SBOMs are available at:

- üëâ [`ghcr.io/kyverno/sbom`](https://github.com/orgs/kyverno/packages?tab=packages&amp;q=sbom)
- üëâ [Fetching the SBOM](https://kyverno.io/docs/security/#fetching-the-sbom-for-kyverno)

## üë• Contributors

Kyverno is built and maintained by our growing community of contributors!

&lt;a href=&quot;https://github.com/kyverno/kyverno/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=kyverno/kyverno&quot; alt=&quot;Contributors image&quot; /&gt;
&lt;/a&gt;

_Made with [contributors-img](https://contrib.rocks)_

## üìÑ License

Copyright 2025, the Kyverno project. All rights reserved.  
Kyverno is licensed under the [Apache License 2.0](LICENSE).

Kyverno is a [Cloud Native Computing Foundation (CNCF) Incubating project](https://www.cncf.io/projects/) and was contributed by [Nirmata](https://nirmata.com/?utm_source=github&amp;utm_medium=repository).
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[memodb-io/Acontext]]></title>
            <link>https://github.com/memodb-io/Acontext</link>
            <guid>https://github.com/memodb-io/Acontext</guid>
            <pubDate>Fri, 09 Jan 2026 00:05:27 GMT</pubDate>
            <description><![CDATA[Data platform for context engineering. Context data platform that stores, observes and learns. Join the community‚ù§Ô∏è: https://discord.acontext.io]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/memodb-io/Acontext">memodb-io/Acontext</a></h1>
            <p>Data platform for context engineering. Context data platform that stores, observes and learns. Join the community‚ù§Ô∏è: https://discord.acontext.io</p>
            <p>Language: Go</p>
            <p>Stars: 2,328</p>
            <p>Forks: 202</p>
            <p>Stars today: 45 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://discord.acontext.io&quot;&gt;
      &lt;img alt=&quot;Show Acontext header banner&quot; src=&quot;./assets/Acontext-header-banner.png&quot;&gt;
  &lt;/a&gt;
  &lt;p&gt;
    &lt;h4&gt;Context Data Platform for Building Cloud-native AI Agents&lt;/h4&gt;
  &lt;/p&gt;
  &lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://pypi.org/project/acontext/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/acontext.svg&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://www.npmjs.com/package/@acontext/acontext&quot;&gt;&lt;img src=&quot;https://img.shields.io/npm/v/@acontext/acontext.svg?logo=npm&amp;logoColor=fff&amp;style=flat&amp;labelColor=2C2C2C&amp;color=28CF8D&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/memodb-io/acontext/actions/workflows/core-test.yaml&quot;&gt;&lt;img src=&quot;https://github.com/memodb-io/acontext/actions/workflows/core-test.yaml/badge.svg&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/memodb-io/acontext/actions/workflows/api-test.yaml&quot;&gt;&lt;img src=&quot;https://github.com/memodb-io/acontext/actions/workflows/api-test.yaml/badge.svg&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/memodb-io/acontext/actions/workflows/cli-test.yaml&quot;&gt;&lt;img src=&quot;https://github.com/memodb-io/acontext/actions/workflows/cli-test.yaml/badge.svg&quot;&gt;&lt;/a&gt;
  &lt;/p&gt;
  &lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://x.com/acontext_io&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/acontext_io?style=social&quot; alt=&quot;Twitter Follow&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://discord.acontext.io&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/dynamic/json?label=Acontext&amp;style=flat&amp;query=approximate_member_count&amp;url=https%3A%2F%2Fdiscord.com%2Fapi%2Fv10%2Finvites%2FSG9xJcqVBu%3Fwith_counts%3Dtrue&amp;logo=discord&amp;logoColor=white&amp;suffix=+members&amp;color=36393f&amp;labelColor=5765F2&quot; alt=&quot;Acontext Discord&quot;&gt;&lt;/a&gt;
  &lt;/p&gt;
  &lt;div align=&quot;center&quot;&gt;
    &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
    &lt;a href=&quot;./readme/de/README.md&quot;&gt;Deutsch&lt;/a&gt; | 
    &lt;a href=&quot;./readme/es/README.md&quot;&gt;Espa√±ol&lt;/a&gt; | 
    &lt;a href=&quot;./readme/fr/README.md&quot;&gt;Fran√ßais&lt;/a&gt; | 
    &lt;a href=&quot;./readme/ja/README.md&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt; | 
    &lt;a href=&quot;./readme/ko/README.md&quot;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | 
    &lt;a href=&quot;./readme/pt/README.md&quot;&gt;Portugu√™s&lt;/a&gt; | 
    &lt;a href=&quot;./readme/ru/README.md&quot;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | 
    &lt;a href=&quot;./readme/zh/README.md&quot;&gt;‰∏≠Êñá&lt;/a&gt;
  &lt;/div&gt;
  &lt;br/&gt;
&lt;/div&gt;


*Everyone is telling you how to use their agents. But what if YOU need to build an agent for 100,000 users, how would you start?*

**üì¶ Problem 1: 99% of your DB is just LLM messages.** 

&gt; Poor schema design makes your most valuable data expensive and slow. Acontext handles context storage and retrieval via PG, Redis, and S3. 
&gt;
&gt; ChatGPT, Gemini, Anthropic, images, audio, files... we&#039;ve got you covered.

**‚è∞ Problem 2: Long-running agents are a nightmare.** 

&gt; You know context engineering, but you&#039;re always writing it from scratch. Acontext comes with built-in context editing methods and a todo agent out of the box.
&gt;
&gt; Managing agent state? Piece of cake.

**üëÄ Problem 3: You can&#039;t see how your agent is doing.** 

&gt; How satisfied are your users, really? Acontext tracks tasks per session and shows you your agent&#039;s actual success rate. 
&gt;
&gt; Stop obsessing over token costs, improve the agent first.

**üß† Problem 4: Your agent is hit or miss.**

&gt; Can it learn from its wins? Acontext&#039;s experience agent remembers successful runs and turns them into reusable tool-use SOPs.
&gt;
&gt; Consistency is everything.



To solve those problems at once, Acontext becomes the **Context Data Platform**:

&lt;div align=&quot;center&quot;&gt;
    &lt;picture&gt;
      &lt;img alt=&quot;Acontext Learning&quot; src=&quot;./assets/acontext-components.jpg&quot; width=&quot;100%&quot;&gt;
    &lt;/picture&gt;
  &lt;p&gt;Context Data Platform that Store, Observe and Learn&lt;/p&gt;
&lt;/div&gt;


# üí° Core Features

- **Context Engineering**
  - [Session](https://docs.acontext.io/store/messages/multi-provider): unified message storage for any llm, any modal.
  - [Disk](https://docs.acontext.io/store/disk): save/download artifacts with file path.
  - [Context Editing](https://docs.acontext.io/store/editing) - manage your context window in one api.

&lt;div align=&quot;center&quot;&gt;
    &lt;picture&gt;
      &lt;img alt=&quot;Acontext Learning&quot; src=&quot;./assets/acontext-context-engineering.png&quot; width=&quot;80%&quot;&gt;
    &lt;/picture&gt;
  &lt;p&gt;Context Engineering in Acontext&lt;/p&gt;
&lt;/div&gt;

- **Observe agent tasks and user feedback**
  - [Task](https://docs.acontext.io/observe/agent_tasks): collect agent&#039;s working status, progress and preferences in near real-time.
- **Agent self-learning**
  - [Experience](https://docs.acontext.io/learn/advance/experience-agent): let agent learn SOPs for each user.
- **View everything in one [dashboard](https://docs.acontext.io/observe/dashboard)**

&lt;div align=&quot;center&quot;&gt;
    &lt;picture&gt;
      &lt;img alt=&quot;Dashboard&quot; src=&quot;./docs/images/dashboard/BI.png&quot; width=&quot;80%&quot;&gt;
    &lt;/picture&gt;
  &lt;p&gt;Dashboard of Agent Success Rate and Other Metrics&lt;/p&gt;
&lt;/div&gt;



# üèóÔ∏è How it works?

&lt;details&gt;
&lt;summary&gt;click to open&lt;/summary&gt;

```mermaid
graph TB
    subgraph &quot;Client Layer&quot;
        PY[&quot;pip install acontext&quot;]
        TS[&quot;npm i @acontext/acontext&quot;]
    end
    
    subgraph &quot;Acontext Backend&quot;
      subgraph &quot; &quot;
          API[&quot;API&lt;br/&gt;localhost:8029&quot;]
          CORE[&quot;Core&quot;]
          API --&gt;|FastAPI &amp; MQ| CORE
      end
      
      subgraph &quot; &quot;
          Infrastructure[&quot;Infrastructures&quot;]
          PG[&quot;PostgreSQL&quot;]
          S3[&quot;S3&quot;]
          REDIS[&quot;Redis&quot;]
          MQ[&quot;RabbitMQ&quot;]
      end
    end
    
    subgraph &quot;Dashboard&quot;
        UI[&quot;Web Dashboard&lt;br/&gt;localhost:3000&quot;]
    end
    
    PY --&gt;|RESTFUL API| API
    TS --&gt;|RESTFUL API| API
    UI --&gt;|RESTFUL API| API
    API --&gt; Infrastructure
    CORE --&gt; Infrastructure

    Infrastructure --&gt; PG
    Infrastructure --&gt; S3
    Infrastructure --&gt; REDIS
    Infrastructure --&gt; MQ
    
    
    style PY fill:#3776ab,stroke:#fff,stroke-width:2px,color:#fff
    style TS fill:#3178c6,stroke:#fff,stroke-width:2px,color:#fff
    style API fill:#00add8,stroke:#fff,stroke-width:2px,color:#fff
    style CORE fill:#ffd43b,stroke:#333,stroke-width:2px,color:#333
    style UI fill:#000,stroke:#fff,stroke-width:2px,color:#fff
    style PG fill:#336791,stroke:#fff,stroke-width:2px,color:#fff
    style S3 fill:#ff9900,stroke:#fff,stroke-width:2px,color:#fff
    style REDIS fill:#dc382d,stroke:#fff,stroke-width:2px,color:#fff
    style MQ fill:#ff6600,stroke:#fff,stroke-width:2px,color:#fff
```

## How They Work Together

```txt
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ User ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ Your Agent ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   Session    ‚îÇ    ‚îÇ Artifact Disk ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ                  ‚îÇ # if enable
                  ‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ         ‚îÇ Observed Tasks  ‚îÇ
                  ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ                  ‚îÇ # if enable
                  ‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ         ‚îÇ   Learn Skills  ‚îÇ
                  ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      Search skills
```



## Data Structures

&lt;details&gt;
&lt;summary&gt;üìñ Task Structure&lt;/summary&gt;

```json
{
  &quot;task_description&quot;: &quot;Star https://github.com/memodb-io/Acontext&quot;,
  &quot;progresses&quot;: [
    &quot;I have navigated to Acontext repo&quot;,
    &quot;Tried to Star but a pop-up required me to login&quot;,
    ...
  ],
  &quot;user_preferences&quot;: [
    &quot;user wants to use outlook email to login&quot;
  ]
}
```
&lt;/details&gt;



&lt;details&gt;
&lt;summary&gt;üìñ Skill Structure&lt;/summary&gt;


```json
{
    &quot;use_when&quot;: &quot;star a repo on github.com&quot;,
    &quot;preferences&quot;: &quot;use user&#039;s outlook account&quot;,
    &quot;tool_sops&quot;: [
        {&quot;tool_name&quot;: &quot;goto&quot;, &quot;action&quot;: &quot;goto github.com&quot;},
        {&quot;tool_name&quot;: &quot;click&quot;, &quot;action&quot;: &quot;find login button if any. login first&quot;},
        ...
    ]
}
```

&lt;/details&gt;



&lt;details&gt;
&lt;summary&gt;üìñ Space Structure&lt;/summary&gt;

```txt
/
‚îî‚îÄ‚îÄ github/ (folder)
    ‚îî‚îÄ‚îÄ GTM (page)
        ‚îú‚îÄ‚îÄ find_trending_repos (sop)
        ‚îî‚îÄ‚îÄ find_contributor_emails (sop)
    ‚îî‚îÄ‚îÄ basic_ops (page)
        ‚îú‚îÄ‚îÄ create_repo (sop)
        ‚îî‚îÄ‚îÄ delete_repo (sop)
    ...
```
&lt;/details&gt;

&lt;/details&gt;





# üöÄ Connect to Acontext

1. Go to [Acontext.io](https://acontext.io), claim your free credits.
2. Go through a one-click onboarding to get your API Key: `sk-ac-xxx`

&lt;div align=&quot;center&quot;&gt;
    &lt;picture&gt;
      &lt;img alt=&quot;Dashboard&quot; src=&quot;./assets/onboard.png&quot; width=&quot;80%&quot;&gt;
    &lt;/picture&gt;
&lt;/div&gt;




&lt;details&gt;
&lt;summary&gt;üíª Self-host Acontext&lt;/summary&gt;

We have an `acontext-cli` to help you do quick proof-of-concept. Download it first in your terminal:

```bash
curl -fsSL https://install.acontext.io | sh
```

You should have [docker](https://www.docker.com/get-started/) installed and an OpenAI API Key to start an Acontext backend on your computer:

```bash
mkdir acontext_server &amp;&amp; cd acontext_server
acontext docker up
```

&gt; [!IMPORTANT]
&gt;
&gt; Make sure your LLM has the ability to [call tools](https://platform.openai.com/docs/guides/function-calling). By default, Acontext will use `gpt-4.1`.

`acontext docker up` will create/use  `.env` and `config.yaml` for Acontext, and create a `db` folder to persist data.



Once it&#039;s done, you can access the following endpoints:

- Acontext API Base URL: http://localhost:8029/api/v1
- Acontext Dashboard: http://localhost:3000/

&lt;/details&gt;






# üßê Use Acontext to build Agent

Download end-to-end scripts with `acontext`:

**Python**

```bash
acontext create my-proj --template-path &quot;python/openai-basic&quot;
```

&gt; More examples on Python:
&gt;
&gt; - `python/openai-agent-basic`: self-learning agent in openai agent sdk.
&gt; - `python/agno-basic`: self-learning agent in agno framework.
&gt; - `python/openai-agent-artifacts`: agent that can edit and download artifacts.

**Typescript**

```bash
acontext create my-proj --template-path &quot;typescript/openai-basic&quot;
```

&gt; More examples on Typescript:
&gt;
&gt; - `typescript/vercel-ai-basic`: self-learning agent in @vercel/ai-sdk



&gt; [!NOTE]
&gt;
&gt; Check our example repo for more templates: [Acontext-Examples](https://github.com/memodb-io/Acontext-Examples).
&gt;
&gt; We&#039;re cooking more full-stack Agent Applications! [Tell us what you want!](https://discord.acontext.io)



## Step-by-step Quickstart

&lt;details&gt;
&lt;summary&gt;click to open&lt;/summary&gt;


We&#039;re maintaining Python [![pypi](https://img.shields.io/pypi/v/acontext.svg)](https://pypi.org/project/acontext/) and Typescript [![npm](https://img.shields.io/npm/v/@acontext/acontext.svg?logo=npm&amp;logoColor=fff&amp;style=flat&amp;labelColor=2C2C2C&amp;color=28CF8D)](https://www.npmjs.com/package/@acontext/acontext) SDKs. The snippets below are using Python.

## Install SDKs

```
pip install acontext # for Python
npm i @acontext/acontext # for Typescript
```



## Initialize Client

```python
import os
from acontext import AcontextClient

client = AcontextClient(
    api_key=os.getenv(&quot;ACONTEXT_API_KEY&quot;),
)

# If you&#039;re using self-hosted Acontext:
# client = AcontextClient(
#     base_url=&quot;http://localhost:8029/api/v1&quot;,
#     api_key=&quot;sk-ac-your-root-api-bearer-token&quot;,
# )
```

&gt; [üìñ async client doc](https://docs.acontext.io/settings/core)



## Store

Acontext can manage agent sessions and artifacts.

### Save Messages [üìñ](https://docs.acontext.io/api-reference/session/store-message-to-session)

Acontext offers persistent storage for message data. When you call `session.store_message`, Acontext will persist the message and start to monitor this session:

&lt;details&gt;
&lt;summary&gt;Code Snippet&lt;/summary&gt;

```python
session = client.sessions.create()

messages = [
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;I need to write a landing page of iPhone 15 pro max&quot;},
    {
        &quot;role&quot;: &quot;assistant&quot;,
        &quot;content&quot;: &quot;Sure, my plan is below:\n1. Search for the latest news about iPhone 15 pro max\n2. Init Next.js project for the landing page\n3. Deploy the landing page to the website&quot;,
    }
]

# Save messages
for msg in messages:
    client.sessions.store_message(session_id=session.id, blob=msg, format=&quot;openai&quot;)
```

&gt; [üìñ](https://docs.acontext.io/store/messages/multi-modal) We also support multi-modal message storage and anthropic SDK.


&lt;/details&gt;

### Load Messages [üìñ](https://docs.acontext.io/api-reference/session/get-messages-from-session)

Obtain your session messages using `sessions.get_messages`

&lt;details&gt;
&lt;summary&gt;Code Snippet&lt;/summary&gt;

```python
r = client.sessions.get_messages(session.id)
new_msg = r.items

new_msg.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;How are you doing?&quot;})
r = openai_client.chat.completions.create(model=&quot;gpt-4.1&quot;, messages=new_msg)
print(r.choices[0].message.content)
client.sessions.store_message(session_id=session.id, blob=r.choices[0].message)
```

&lt;/details&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;picture&gt;
      &lt;img alt=&quot;Session&quot; src=&quot;./docs/images/dashboard/message_viewer.png&quot; width=&quot;100%&quot;&gt;
    &lt;/picture&gt;
  &lt;p&gt;You can view sessions in your local Dashboard&lt;/p&gt;
&lt;/div&gt;


### Artifacts [üìñ](https://docs.acontext.io/store/disk)

Create a disk for your agent to store and read artifacts using file paths:

&lt;details&gt;
&lt;summary&gt;Code Snippet&lt;/summary&gt;

```python
from acontext import FileUpload

disk = client.disks.create()

file = FileUpload(
    filename=&quot;todo.md&quot;,
    content=b&quot;# Sprint Plan\n\n## Goals\n- Complete user authentication\n- Fix critical bugs&quot;
)
artifact = client.disks.artifacts.upsert(
    disk.id,
    file=file,
    file_path=&quot;/todo/&quot;
)


print(client.disks.artifacts.list(
    disk.id,
    path=&quot;/todo/&quot;
))

result = client.disks.artifacts.get(
    disk.id,
    file_path=&quot;/todo/&quot;,
    filename=&quot;todo.md&quot;,
    with_public_url=True,
    with_content=True
)
print(f&quot;‚úì File content: {result.content.raw}&quot;)
print(f&quot;‚úì Download URL: {result.public_url}&quot;)        
```
&lt;/details&gt;



&lt;div align=&quot;center&quot;&gt;
    &lt;picture&gt;
      &lt;img alt=&quot;Artifacts&quot; src=&quot;./docs/images/dashboard/artifact_viewer.png&quot; width=&quot;100%&quot;&gt;
    &lt;/picture&gt;
  &lt;p&gt;You can view artifacts in your local Dashboard&lt;/p&gt;
&lt;/div&gt;



## Observe [üìñ](https://docs.acontext.io/observe)

For every session, Acontext will **automatically** launch a background agent to track the task progress and user feedback. **It&#039;s like a background TODO agent**. Acontext will use it to observe your daily agent success rate.

You can use the SDK to retrieve the current state of the agent session, for Context Engineering like Reduction and Compression. 

&lt;details&gt;
&lt;summary&gt;Full Script&lt;/summary&gt;

```python
from acontext import AcontextClient

# Initialize client
client = AcontextClient(
    base_url=&quot;http://localhost:8029/api/v1&quot;, api_key=&quot;sk-ac-your-root-api-bearer-token&quot;
)

# Create a project and session
session = client.sessions.create()

# Conversation messages
messages = [
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;I need to write a landing page of iPhone 15 pro max&quot;},
    {
        &quot;role&quot;: &quot;assistant&quot;,
        &quot;content&quot;: &quot;Sure, my plan is below:\n1. Search for the latest news about iPhone 15 pro max\n2. Init Next.js project for the landing page\n3. Deploy the landing page to the website&quot;,
    },
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: &quot;That sounds good. Let&#039;s first collect the message and report to me before any landing page coding.&quot;,
    },
    {
        &quot;role&quot;: &quot;assistant&quot;,
        &quot;content&quot;: &quot;Sure, I will first collect the message then report to you before any landing page coding.&quot;,
      	&quot;tool_calls&quot;: [
            {
                &quot;id&quot;: &quot;call_001&quot;,
                &quot;type&quot;: &quot;function&quot;,
                &quot;function&quot;: {
                    &quot;name&quot;: &quot;search_news&quot;,
                    &quot;arguments&quot;: &quot;{\&quot;query\&quot;: \&quot;iPhone news\&quot;}&quot;
                }
            }
        ]
    },
]

# Store messages in a loop
for msg in messages:
    client.sessions.store_message(session_id=session.id, blob=msg, format=&quot;openai&quot;)

# Wait for task extraction to complete
client.sessions.flush(session.id)

# Display extracted tasks
tasks_response = client.sessions.get_tasks(session.id)
print(tasks_response)
for task in tasks_response.items:
    print(f&quot;\nTask #{task.order}:&quot;)
    print(f&quot;  ID: {task.id}&quot;)
    print(f&quot;  Title: {task.data.task_description}&quot;)
    print(f&quot;  Status: {task.status}&quot;)

    # Show progress updates if available
    if task.data.progresses:
        print(f&quot;  Progress updates: {len(task.data.progresses)}&quot;)
        for progress in task.data.progresses:
            print(f&quot;    - {progress}&quot;)

    # Show user preferences if available
    if task.data.user_preferences:
        print(&quot;  User preferences:&quot;)
        for pref in task.data.user_preferences:
            print(f&quot;    - {pref}&quot;)

```
&gt; `flush` is a blocking call, it will wait for the task extraction to complete.
&gt; You don&#039;t need to call it in production, Acontext has a [buffer mechanism](https://docs.acontext.io/observe/buffer) to ensure the task extraction is completed right on time.

&lt;/details&gt;

Example Task Return:

```txt
Task #1:
  Title: Search for the latest news about iPhone 15 Pro Max and report findings to the user before any landing page coding.
  Status: success
  Progress updates: 2
    - I confirmed that the first step will be reporting before moving on to landing page development.
    - I have already collected all the iPhone 15 pro max info and reported to the user, waiting for approval for next step.
  User preferences:
    - user expects a report on latest news about iPhone 15 pro max before any coding work on the landing page.

Task #2:
  Title: Initialize a Next.js project for the iPhone 15 Pro Max landing page.
  Status: pending

Task #3:
  Title: Deploy the completed landing page to the website.
  Status: pending
```



You can view the session tasks&#039; statuses in the Dashboard:

&lt;div align=&quot;center&quot;&gt;
    &lt;picture&gt;
      &lt;img alt=&quot;Acontext Learning&quot; src=&quot;./docs/images/dashboard/session_task_viewer.png&quot; width=&quot;100%&quot;&gt;
    &lt;/picture&gt;
  &lt;p&gt;A Task Demo&lt;/p&gt;
&lt;/div&gt;



## Self-learning

Acontext can gather a bunch of sessions and learn skills (SOPs) on how to call tools for certain tasks.

### Learn Skills to a `Space` [üìñ](https://docs.acontext.io/learn/skill-space)

&lt;div align=&quot;center&quot;&gt;
    &lt;picture&gt;
      &lt;img alt=&quot;A Space Demo&quot; src=&quot;./assets/acontext_dataflow.png&quot; width=&quot;100%&quot;&gt;
    &lt;/picture&gt;
  &lt;p&gt;How self-learning works?&lt;/p&gt;
&lt;/div&gt;

A `Space` can store skills, and memories in a Notion-like system. You first need to connect a session to `Space` to enable the learning process:

```python
# Step 1: Create a Space for skill learning
space = client.spaces.create()
print(f&quot;Created Space: {space.id}&quot;)

# Step 2: Create a session attached to the space
session = client.sessions.create(space_id=space.id)

# ... push the agent working context
```

The learning happens in the background and is not real-time (delay around 10-30s). 

What Acontext will do in the background:

```mermaid
graph LR
    A[Task Completed] --&gt; B[Task Extraction]
    B --&gt; C{Space Connected?}
    C --&gt;|Yes| D[Queue for Learning]
    C --&gt;|No| E[Skip Learning]
    D --&gt; F[Extract SOP]
    F --&gt; G{Hard Enough?}
    G --&gt;|No - Too Simple| H[Skip Learning]
    G --&gt;|Yes - Complex| I[Store as Skill Block]
    I --&gt; J[Available for Future Sessions]
```

Eventually, SOP blocks with tool-call pattern will be saved to `Space`. You can view every `Space` in the Dashboard:

&lt;div align=&quot;center&quot;&gt;
    &lt;picture&gt;
      &lt;img alt=&quot;A Space Demo&quot; src=&quot;./docs/images/dashboard/skill_viewer.png&quot; width=&quot;100%&quot;&gt;
    &lt;/picture&gt;
  &lt;p&gt;A Space Demo&lt;/p&gt;
&lt;/div&gt;




### Search Skills from a `Space` [üìñ](https://docs.acontext.io/learn/search-skills)

To search skills from a `Space` and use them in the next session:

```python
result = client.spaces.experience_search(
    space_id=space.id,
    query=&quot;I need to implement authentication&quot;,
  	mode=&quot;fast&quot;
)
```

Acontext supports `fast` and `agentic` modes for search. The former uses embeddings to match skills. The latter uses an Experience Agent to explore the entire `Space` and tries to cover every skill needed.

The return is a list of sop blocks, which look like below:

```json
{
    &quot;use_when&quot;: &quot;star a github repo&quot;,
    &quot;preferences&quot;: &quot;use personal account. star but not fork&quot;,
    &quot;tool_sops&quot;: [
        {&quot;tool_name&quot;: &quot;goto&quot;, &quot;action&quot;: &quot;goto the user given github repo url&quot;},
        {&quot;tool_name&quot;: &quot;click&quot;, &quot;action&quot;: &quot;find login button if any, and start to login first&quot;},
        ...
  

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[github/github-mcp-server]]></title>
            <link>https://github.com/github/github-mcp-server</link>
            <guid>https://github.com/github/github-mcp-server</guid>
            <pubDate>Fri, 09 Jan 2026 00:05:26 GMT</pubDate>
            <description><![CDATA[GitHub's official MCP Server]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/github/github-mcp-server">github/github-mcp-server</a></h1>
            <p>GitHub's official MCP Server</p>
            <p>Language: Go</p>
            <p>Stars: 25,730</p>
            <p>Forks: 3,342</p>
            <p>Stars today: 31 stars today</p>
            <h2>README</h2><pre>[![Go Report Card](https://goreportcard.com/badge/github.com/github/github-mcp-server)](https://goreportcard.com/report/github.com/github/github-mcp-server)

# GitHub MCP Server

The GitHub MCP Server connects AI tools directly to GitHub&#039;s platform. This gives AI agents, assistants, and chatbots the ability to read repositories and code files, manage issues and PRs, analyze code, and automate workflows. All through natural language interactions.

### Use Cases

- Repository Management: Browse and query code, search files, analyze commits, and understand project structure across any repository you have access to.
- Issue &amp; PR Automation: Create, update, and manage issues and pull requests. Let AI help triage bugs, review code changes, and maintain project boards.
- CI/CD &amp; Workflow Intelligence: Monitor GitHub Actions workflow runs, analyze build failures, manage releases, and get insights into your development pipeline.
- Code Analysis: Examine security findings, review Dependabot alerts, understand code patterns, and get comprehensive insights into your codebase.
- Team Collaboration: Access discussions, manage notifications, analyze team activity, and streamline processes for your team.

Built for developers who want to connect their AI tools to GitHub context and capabilities, from simple natural language queries to complex multi-step agent workflows.

---

## Remote GitHub MCP Server

[![Install in VS Code](https://img.shields.io/badge/VS_Code-Install_Server-0098FF?style=flat-square&amp;logo=visualstudiocode&amp;logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=github&amp;config=%7B%22type%22%3A%20%22http%22%2C%22url%22%3A%20%22https%3A%2F%2Fapi.githubcopilot.com%2Fmcp%2F%22%7D) [![Install in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install_Server-24bfa5?style=flat-square&amp;logo=visualstudiocode&amp;logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=github&amp;config=%7B%22type%22%3A%20%22http%22%2C%22url%22%3A%20%22https%3A%2F%2Fapi.githubcopilot.com%2Fmcp%2F%22%7D&amp;quality=insiders)

The remote GitHub MCP Server is hosted by GitHub and provides the easiest method for getting up and running. If your MCP host does not support remote MCP servers, don&#039;t worry! You can use the [local version of the GitHub MCP Server](https://github.com/github/github-mcp-server?tab=readme-ov-file#local-github-mcp-server) instead.

### Prerequisites

1. A compatible MCP host with remote server support (VS Code 1.101+, Claude Desktop, Cursor, Windsurf, etc.)
2. Any applicable [policies enabled](https://github.com/github/github-mcp-server/blob/main/docs/policies-and-governance.md)

### Install in VS Code

For quick installation, use one of the one-click install buttons above. Once you complete that flow, toggle Agent mode (located by the Copilot Chat text input) and the server will start. Make sure you&#039;re using [VS Code 1.101](https://code.visualstudio.com/updates/v1_101) or [later](https://code.visualstudio.com/updates) for remote MCP and OAuth support.

Alternatively, to manually configure VS Code, choose the appropriate JSON block from the examples below and add it to your host configuration:

&lt;table&gt;
&lt;tr&gt;&lt;th&gt;Using OAuth&lt;/th&gt;&lt;th&gt;Using a GitHub PAT&lt;/th&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;th align=left colspan=2&gt;VS Code (version 1.101 or greater)&lt;/th&gt;&lt;/tr&gt;
&lt;tr valign=top&gt;
&lt;td&gt;

```json
{
  &quot;servers&quot;: {
    &quot;github&quot;: {
      &quot;type&quot;: &quot;http&quot;,
      &quot;url&quot;: &quot;https://api.githubcopilot.com/mcp/&quot;
    }
  }
}
```

&lt;/td&gt;
&lt;td&gt;

```json
{
  &quot;servers&quot;: {
    &quot;github&quot;: {
      &quot;type&quot;: &quot;http&quot;,
      &quot;url&quot;: &quot;https://api.githubcopilot.com/mcp/&quot;,
      &quot;headers&quot;: {
        &quot;Authorization&quot;: &quot;Bearer ${input:github_mcp_pat}&quot;
      }
    }
  },
  &quot;inputs&quot;: [
    {
      &quot;type&quot;: &quot;promptString&quot;,
      &quot;id&quot;: &quot;github_mcp_pat&quot;,
      &quot;description&quot;: &quot;GitHub Personal Access Token&quot;,
      &quot;password&quot;: true
    }
  ]
}
```

&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

### Install in other MCP hosts
- **[GitHub Copilot in other IDEs](/docs/installation-guides/install-other-copilot-ides.md)** - Installation for JetBrains, Visual Studio, Eclipse, and Xcode with GitHub Copilot
- **[Claude Applications](/docs/installation-guides/install-claude.md)** - Installation guide for Claude Desktop and Claude Code CLI
- **[Codex](/docs/installation-guides/install-codex.md)** - Installation guide for Open AI Codex
- **[Cursor](/docs/installation-guides/install-cursor.md)** - Installation guide for Cursor IDE
- **[Windsurf](/docs/installation-guides/install-windsurf.md)** - Installation guide for Windsurf IDE
- **[Rovo Dev CLI](/docs/installation-guides/install-rovo-dev-cli.md)** - Installation guide for Rovo Dev CLI

&gt; **Note:** Each MCP host application needs to configure a GitHub App or OAuth App to support remote access via OAuth. Any host application that supports remote MCP servers should support the remote GitHub server with PAT authentication. Configuration details and support levels vary by host. Make sure to refer to the host application&#039;s documentation for more info.

### Configuration

#### Toolset configuration

See [Remote Server Documentation](docs/remote-server.md) for full details on remote server configuration, toolsets, headers, and advanced usage. This file provides comprehensive instructions and examples for connecting, customizing, and installing the remote GitHub MCP Server in VS Code and other MCP hosts.

When no toolsets are specified, [default toolsets](#default-toolset) are used.

#### GitHub Enterprise

##### GitHub Enterprise Cloud with data residency (ghe.com)

GitHub Enterprise Cloud can also make use of the remote server.

Example for `https://octocorp.ghe.com` with GitHub PAT token:
```
{
    ...
    &quot;proxima-github&quot;: {
      &quot;type&quot;: &quot;http&quot;,
      &quot;url&quot;: &quot;https://copilot-api.octocorp.ghe.com/mcp&quot;,
      &quot;headers&quot;: {
        &quot;Authorization&quot;: &quot;Bearer ${input:github_mcp_pat}&quot;
      }
    },
    ...
}
```

&gt; **Note:** When using OAuth with GitHub Enterprise with VS Code and GitHub Copilot, you also need to configure your VS Code settings to point to your GitHub Enterprise instance - see [Authenticate from VS Code](https://docs.github.com/en/enterprise-cloud@latest/copilot/how-tos/configure-personal-settings/authenticate-to-ghecom)

##### GitHub Enterprise Server

GitHub Enterprise Server does not support remote server hosting. Please refer to [GitHub Enterprise Server and Enterprise Cloud with data residency (ghe.com)](#github-enterprise-server-and-enterprise-cloud-with-data-residency-ghecom) from the local server configuration.

---

## Local GitHub MCP Server

[![Install with Docker in VS Code](https://img.shields.io/badge/VS_Code-Install_Server-0098FF?style=flat-square&amp;logo=visualstudiocode&amp;logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=github&amp;inputs=%5B%7B%22id%22%3A%22github_token%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22GitHub%20Personal%20Access%20Token%22%2C%22password%22%3Atrue%7D%5D&amp;config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22GITHUB_PERSONAL_ACCESS_TOKEN%22%2C%22ghcr.io%2Fgithub%2Fgithub-mcp-server%22%5D%2C%22env%22%3A%7B%22GITHUB_PERSONAL_ACCESS_TOKEN%22%3A%22%24%7Binput%3Agithub_token%7D%22%7D%7D) [![Install with Docker in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install_Server-24bfa5?style=flat-square&amp;logo=visualstudiocode&amp;logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=github&amp;inputs=%5B%7B%22id%22%3A%22github_token%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22GitHub%20Personal%20Access%20Token%22%2C%22password%22%3Atrue%7D%5D&amp;config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22GITHUB_PERSONAL_ACCESS_TOKEN%22%2C%22ghcr.io%2Fgithub%2Fgithub-mcp-server%22%5D%2C%22env%22%3A%7B%22GITHUB_PERSONAL_ACCESS_TOKEN%22%3A%22%24%7Binput%3Agithub_token%7D%22%7D%7D&amp;quality=insiders)

### Prerequisites

1. To run the server in a container, you will need to have [Docker](https://www.docker.com/) installed.
2. Once Docker is installed, you will also need to ensure Docker is running. The Docker image is available at `ghcr.io/github/github-mcp-server`. The image is public; if you get errors on pull, you may have an expired token and need to `docker logout ghcr.io`.
3. Lastly you will need to [Create a GitHub Personal Access Token](https://github.com/settings/personal-access-tokens/new).
The MCP server can use many of the GitHub APIs, so enable the permissions that you feel comfortable granting your AI tools (to learn more about access tokens, please check out the [documentation](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens)).

&lt;details&gt;&lt;summary&gt;&lt;b&gt;Handling PATs Securely&lt;/b&gt;&lt;/summary&gt;

### Environment Variables (Recommended)
To keep your GitHub PAT secure and reusable across different MCP hosts:

1. **Store your PAT in environment variables**
   ```bash
   export GITHUB_PAT=your_token_here
   ```
   Or create a `.env` file:
   ```env
   GITHUB_PAT=your_token_here
   ```

2. **Protect your `.env` file**
   ```bash
   # Add to .gitignore to prevent accidental commits
   echo &quot;.env&quot; &gt;&gt; .gitignore
   ```

3. **Reference the token in configurations**
   ```bash
   # CLI usage
   claude mcp update github -e GITHUB_PERSONAL_ACCESS_TOKEN=$GITHUB_PAT

   # In config files (where supported)
   &quot;env&quot;: {
     &quot;GITHUB_PERSONAL_ACCESS_TOKEN&quot;: &quot;$GITHUB_PAT&quot;
   }
   ```

&gt; **Note**: Environment variable support varies by host app and IDE. Some applications (like Windsurf) require hardcoded tokens in config files.

### Token Security Best Practices

- **Minimum scopes**: Only grant necessary permissions
  - `repo` - Repository operations
  - `read:packages` - Docker image access
  - `read:org` - Organization team access
- **Separate tokens**: Use different PATs for different projects/environments
- **Regular rotation**: Update tokens periodically
- **Never commit**: Keep tokens out of version control
- **File permissions**: Restrict access to config files containing tokens
  ```bash
  chmod 600 ~/.your-app/config.json
  ```

&lt;/details&gt;

### GitHub Enterprise Server and Enterprise Cloud with data residency (ghe.com)

The flag `--gh-host` and the environment variable `GITHUB_HOST` can be used to set
the hostname for GitHub Enterprise Server or GitHub Enterprise Cloud with data residency.

- For GitHub Enterprise Server, prefix the hostname with the `https://` URI scheme, as it otherwise defaults to `http://`, which GitHub Enterprise Server does not support.
- For GitHub Enterprise Cloud with data residency, use `https://YOURSUBDOMAIN.ghe.com` as the hostname.
``` json
&quot;github&quot;: {
    &quot;command&quot;: &quot;docker&quot;,
    &quot;args&quot;: [
    &quot;run&quot;,
    &quot;-i&quot;,
    &quot;--rm&quot;,
    &quot;-e&quot;,
    &quot;GITHUB_PERSONAL_ACCESS_TOKEN&quot;,
    &quot;-e&quot;,
    &quot;GITHUB_HOST&quot;,
    &quot;ghcr.io/github/github-mcp-server&quot;
    ],
    &quot;env&quot;: {
        &quot;GITHUB_PERSONAL_ACCESS_TOKEN&quot;: &quot;${input:github_token}&quot;,
        &quot;GITHUB_HOST&quot;: &quot;https://&lt;your GHES or ghe.com domain name&gt;&quot;
    }
}
```

## Installation

### Install in GitHub Copilot on VS Code

For quick installation, use one of the one-click install buttons above. Once you complete that flow, toggle Agent mode (located by the Copilot Chat text input) and the server will start.

More about using MCP server tools in VS Code&#039;s [agent mode documentation](https://code.visualstudio.com/docs/copilot/chat/mcp-servers).

Install in GitHub Copilot on other IDEs (JetBrains, Visual Studio, Eclipse, etc.)

Add the following JSON block to your IDE&#039;s MCP settings.

```json
{
  &quot;mcp&quot;: {
    &quot;inputs&quot;: [
      {
        &quot;type&quot;: &quot;promptString&quot;,
        &quot;id&quot;: &quot;github_token&quot;,
        &quot;description&quot;: &quot;GitHub Personal Access Token&quot;,
        &quot;password&quot;: true
      }
    ],
    &quot;servers&quot;: {
      &quot;github&quot;: {
        &quot;command&quot;: &quot;docker&quot;,
        &quot;args&quot;: [
          &quot;run&quot;,
          &quot;-i&quot;,
          &quot;--rm&quot;,
          &quot;-e&quot;,
          &quot;GITHUB_PERSONAL_ACCESS_TOKEN&quot;,
          &quot;ghcr.io/github/github-mcp-server&quot;
        ],
        &quot;env&quot;: {
          &quot;GITHUB_PERSONAL_ACCESS_TOKEN&quot;: &quot;${input:github_token}&quot;
        }
      }
    }
  }
}
```

Optionally, you can add a similar example (i.e. without the mcp key) to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with other host applications that accept the same format.

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Example JSON block without the MCP key included&lt;/b&gt;&lt;/summary&gt;
&lt;br&gt;

```json
{
  &quot;inputs&quot;: [
    {
      &quot;type&quot;: &quot;promptString&quot;,
      &quot;id&quot;: &quot;github_token&quot;,
      &quot;description&quot;: &quot;GitHub Personal Access Token&quot;,
      &quot;password&quot;: true
    }
  ],
  &quot;servers&quot;: {
    &quot;github&quot;: {
      &quot;command&quot;: &quot;docker&quot;,
      &quot;args&quot;: [
        &quot;run&quot;,
        &quot;-i&quot;,
        &quot;--rm&quot;,
        &quot;-e&quot;,
        &quot;GITHUB_PERSONAL_ACCESS_TOKEN&quot;,
        &quot;ghcr.io/github/github-mcp-server&quot;
      ],
      &quot;env&quot;: {
        &quot;GITHUB_PERSONAL_ACCESS_TOKEN&quot;: &quot;${input:github_token}&quot;
      }
    }
  }
}
```

&lt;/details&gt;

### Install in Other MCP Hosts

For other MCP host applications, please refer to our installation guides:

- **[GitHub Copilot in other IDEs](/docs/installation-guides/install-other-copilot-ides.md)** - Installation for JetBrains, Visual Studio, Eclipse, and Xcode with GitHub Copilot
- **[Claude Code &amp; Claude Desktop](docs/installation-guides/install-claude.md)** - Installation guide for Claude Code and Claude Desktop
- **[Cursor](docs/installation-guides/install-cursor.md)** - Installation guide for Cursor IDE
- **[Google Gemini CLI](docs/installation-guides/install-gemini-cli.md)** - Installation guide for Google Gemini CLI
- **[Windsurf](docs/installation-guides/install-windsurf.md)** - Installation guide for Windsurf IDE

For a complete overview of all installation options, see our **[Installation Guides Index](docs/installation-guides)**.

&gt; **Note:** Any host application that supports local MCP servers should be able to access the local GitHub MCP server. However, the specific configuration process, syntax and stability of the integration will vary by host application. While many may follow a similar format to the examples above, this is not guaranteed. Please refer to your host application&#039;s documentation for the correct MCP configuration syntax and setup process.

### Build from source

If you don&#039;t have Docker, you can use `go build` to build the binary in the
`cmd/github-mcp-server` directory, and use the `github-mcp-server stdio` command with the `GITHUB_PERSONAL_ACCESS_TOKEN` environment variable set to your token. To specify the output location of the build, use the `-o` flag. You should configure your server to use the built executable as its `command`. For example:

```JSON
{
  &quot;mcp&quot;: {
    &quot;servers&quot;: {
      &quot;github&quot;: {
        &quot;command&quot;: &quot;/path/to/github-mcp-server&quot;,
        &quot;args&quot;: [&quot;stdio&quot;],
        &quot;env&quot;: {
          &quot;GITHUB_PERSONAL_ACCESS_TOKEN&quot;: &quot;&lt;YOUR_TOKEN&gt;&quot;
        }
      }
    }
  }
}
```

## Tool Configuration

The GitHub MCP Server supports enabling or disabling specific groups of functionalities via the `--toolsets` flag. This allows you to control which GitHub API capabilities are available to your AI tools. Enabling only the toolsets that you need can help the LLM with tool choice and reduce the context size.

_Toolsets are not limited to Tools. Relevant MCP Resources and Prompts are also included where applicable._

When no toolsets are specified, [default toolsets](#default-toolset) are used.

&gt; **Looking for examples?** See the [Server Configuration Guide](./docs/server-configuration.md) for common recipes like minimal setups, read-only mode, and combining tools with toolsets.

#### Specifying Toolsets

To specify toolsets you want available to the LLM, you can pass an allow-list in two ways:

1. **Using Command Line Argument**:

   ```bash
   github-mcp-server --toolsets repos,issues,pull_requests,actions,code_security
   ```

2. **Using Environment Variable**:
   ```bash
   GITHUB_TOOLSETS=&quot;repos,issues,pull_requests,actions,code_security&quot; ./github-mcp-server
   ```

The environment variable `GITHUB_TOOLSETS` takes precedence over the command line argument if both are provided.

#### Specifying Individual Tools

You can also configure specific tools using the `--tools` flag. Tools can be used independently or combined with toolsets and dynamic toolsets discovery for fine-grained control.

1. **Using Command Line Argument**:

   ```bash
   github-mcp-server --tools get_file_contents,issue_read,create_pull_request
   ```

2. **Using Environment Variable**:
   ```bash
   GITHUB_TOOLS=&quot;get_file_contents,issue_read,create_pull_request&quot; ./github-mcp-server
   ```

3. **Combining with Toolsets** (additive):
   ```bash
   github-mcp-server --toolsets repos,issues --tools get_gist
   ```
   This registers all tools from `repos` and `issues` toolsets, plus `get_gist`.

4. **Combining with Dynamic Toolsets** (additive):
   ```bash
   github-mcp-server --tools get_file_contents --dynamic-toolsets
   ```
   This registers `get_file_contents` plus the dynamic toolset tools (`enable_toolset`, `list_available_toolsets`, `get_toolset_tools`).

**Important Notes:**
- Tools, toolsets, and dynamic toolsets can all be used together
- Read-only mode takes priority: write tools are skipped if `--read-only` is set, even if explicitly requested via `--tools`
- Tool names must match exactly (e.g., `get_file_contents`, not `getFileContents`). Invalid tool names will cause the server to fail at startup with an error message
- When tools are renamed, old names are preserved as aliases for backward compatibility. See [Deprecated Tool Aliases](docs/deprecated-tool-aliases.md) for details.

### Using Toolsets With Docker

When using Docker, you can pass the toolsets as environment variables:

```bash
docker run -i --rm \
  -e GITHUB_PERSONAL_ACCESS_TOKEN=&lt;your-token&gt; \
  -e GITHUB_TOOLSETS=&quot;repos,issues,pull_requests,actions,code_security&quot; \
  ghcr.io/github/github-mcp-server
```

### Using Tools With Docker

When using Docker, you can pass specific tools as environment variables. You can also combine tools with toolsets:

```bash
# Tools only
docker run -i --rm \
  -e GITHUB_PERSONAL_ACCESS_TOKEN=&lt;your-token&gt; \
  -e GITHUB_TOOLS=&quot;get_file_contents,issue_read,create_pull_request&quot; \
  ghcr.io/github/github-mcp-server

# Tools combined with toolsets (additive)
docker run -i --rm \
  -e GITHUB_PERSONAL_ACCESS_TOKEN=&lt;your-token&gt; \
  -e GITHUB_TOOLSETS=&quot;repos,issues&quot; \
  -e GITHUB_TOOLS=&quot;get_gist&quot; \
  ghcr.io/github/github-mcp-server
```

### Special toolsets

#### &quot;all&quot; toolset

The special toolset `all` can be provided to enable all available toolsets regardless of any other configuration:

```bash
./github-mcp-server --toolsets all
```

Or using the environment variable:

```bash
GITHUB_TOOLSETS=&quot;all&quot; ./github-mcp-server
```

#### &quot;default&quot; toolset
The default toolset `default` is the configuration that gets passed to the server if no toolsets are specified.

The default configuration is:
- context
- repos
- issues
- pull_requests
- users

To keep the default configuration and add additional toolsets:

```bash
GITHUB_TOOLSETS=&quot;default,stargazers&quot; ./github-mcp-server
```

### Available Toolsets

The following sets of tools are available:

&lt;!-- START AUTOMATED TOOLSETS --&gt;
|     | Toolset                 | Description                                                   |
| --- | ----------------------- | ------------------------------------------------------------- |
| &lt;picture&gt;&lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;pkg/octicons/icons/person-dark.png&quot;&gt;&lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;pkg/octicons/icons/person-light.png&quot;&gt;&lt;img src=&quot;pkg/octicons/icons/person-light.png&quot; width=&quot;20&quot; height=&quot;20&quot; alt=&quot;person&quot;&gt;&lt;/picture&gt; | `context`               | **Strongly recommended**: Tools that provide context about the current user and GitHub context you are operating in |
| &lt;picture&gt;&lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;pkg/octicons/icons/workflo

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[charmbracelet/crush]]></title>
            <link>https://github.com/charmbracelet/crush</link>
            <guid>https://github.com/charmbracelet/crush</guid>
            <pubDate>Fri, 09 Jan 2026 00:05:25 GMT</pubDate>
            <description><![CDATA[The glamourous AI coding agent for your favourite terminal üíò]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/charmbracelet/crush">charmbracelet/crush</a></h1>
            <p>The glamourous AI coding agent for your favourite terminal üíò</p>
            <p>Language: Go</p>
            <p>Stars: 17,085</p>
            <p>Forks: 1,024</p>
            <p>Stars today: 172 stars today</p>
            <h2>README</h2><pre># Crush

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://stuff.charm.sh/crush/charm-crush.png&quot;&gt;&lt;img width=&quot;450&quot; alt=&quot;Charm Crush Logo&quot; src=&quot;https://github.com/user-attachments/assets/adc1a6f4-b284-4603-836c-59038caa2e8b&quot; /&gt;&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;https://github.com/charmbracelet/crush/releases&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/release/charmbracelet/crush&quot; alt=&quot;Latest Release&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/charmbracelet/crush/actions&quot;&gt;&lt;img src=&quot;https://github.com/charmbracelet/crush/actions/workflows/build.yml/badge.svg&quot; alt=&quot;Build Status&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;Your new coding bestie, now available in your favourite terminal.&lt;br /&gt;Your tools, your code, and your workflows, wired into your LLM of choice.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;‰Ω†ÁöÑÊñ∞ÁºñÁ®ã‰ºô‰º¥ÔºåÁé∞Âú®Â∞±Âú®‰Ω†ÊúÄÁà±ÁöÑÁªàÁ´Ø‰∏≠„ÄÇ&lt;br /&gt;‰Ω†ÁöÑÂ∑•ÂÖ∑„ÄÅ‰ª£Á†ÅÂíåÂ∑•‰ΩúÊµÅÔºåÈÉΩ‰∏éÊÇ®ÈÄâÊã©ÁöÑ LLM Ê®°ÂûãÁ¥ßÂØÜÁõ∏Ëøû„ÄÇ&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;800&quot; alt=&quot;Crush Demo&quot; src=&quot;https://github.com/user-attachments/assets/58280caf-851b-470a-b6f7-d5c4ea8a1968&quot; /&gt;&lt;/p&gt;

## Features

- **Multi-Model:** choose from a wide range of LLMs or add your own via OpenAI- or Anthropic-compatible APIs
- **Flexible:** switch LLMs mid-session while preserving context
- **Session-Based:** maintain multiple work sessions and contexts per project
- **LSP-Enhanced:** Crush uses LSPs for additional context, just like you do
- **Extensible:** add capabilities via MCPs (`http`, `stdio`, and `sse`)
- **Works Everywhere:** first-class support in every terminal on macOS, Linux, Windows (PowerShell and WSL), FreeBSD, OpenBSD, and NetBSD

## Installation

Use a package manager:

```bash
# Homebrew
brew install charmbracelet/tap/crush

# NPM
npm install -g @charmland/crush

# Arch Linux (btw)
yay -S crush-bin

# Nix
nix run github:numtide/nix-ai-tools#crush

# FreeBSD
pkg install crush
```

Windows users:

```bash
# Winget
winget install charmbracelet.crush

# Scoop
scoop bucket add charm https://github.com/charmbracelet/scoop-bucket.git
scoop install crush
```

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Nix (NUR)&lt;/strong&gt;&lt;/summary&gt;

Crush is available via the offical Charm [NUR](https://github.com/nix-community/NUR) in `nur.repos.charmbracelet.crush`, which is the most up-to-date way to get Crush in Nix.

You can also try out Crush via the NUR with `nix-shell`:

```bash
# Add the NUR channel.
nix-channel --add https://github.com/nix-community/NUR/archive/main.tar.gz nur
nix-channel --update

# Get Crush in a Nix shell.
nix-shell -p &#039;(import &lt;nur&gt; { pkgs = import &lt;nixpkgs&gt; {}; }).repos.charmbracelet.crush&#039;
```

### NixOS &amp; Home Manager Module Usage via NUR

Crush provides NixOS and Home Manager modules via NUR.
You can use these modules directly in your flake by importing them from NUR. Since it auto detects whether its a home manager or nixos context you can use the import the exact same way :)

```nix
{
  inputs = {
    nixpkgs.url = &quot;github:NixOS/nixpkgs/nixos-unstable&quot;;
    nur.url = &quot;github:nix-community/NUR&quot;;
  };

  outputs = { self, nixpkgs, nur, ... }: {
    nixosConfigurations.your-hostname = nixpkgs.lib.nixosSystem {
      system = &quot;x86_64-linux&quot;;
      modules = [
        nur.modules.nixos.default
        nur.repos.charmbracelet.modules.crush
        {
          programs.crush = {
            enable = true;
            settings = {
              providers = {
                openai = {
                  id = &quot;openai&quot;;
                  name = &quot;OpenAI&quot;;
                  base_url = &quot;https://api.openai.com/v1&quot;;
                  type = &quot;openai&quot;;
                  api_key = &quot;sk-fake123456789abcdef...&quot;;
                  models = [
                    {
                      id = &quot;gpt-4&quot;;
                      name = &quot;GPT-4&quot;;
                    }
                  ];
                };
              };
              lsp = {
                go = { command = &quot;gopls&quot;; enabled = true; };
                nix = { command = &quot;nil&quot;; enabled = true; };
              };
              options = {
                context_paths = [ &quot;/etc/nixos/configuration.nix&quot; ];
                tui = { compact_mode = true; };
                debug = false;
              };
            };
          };
        }
      ];
    };
  };
}
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Debian/Ubuntu&lt;/strong&gt;&lt;/summary&gt;

```bash
sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://repo.charm.sh/apt/gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/charm.gpg
echo &quot;deb [signed-by=/etc/apt/keyrings/charm.gpg] https://repo.charm.sh/apt/ * *&quot; | sudo tee /etc/apt/sources.list.d/charm.list
sudo apt update &amp;&amp; sudo apt install crush
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Fedora/RHEL&lt;/strong&gt;&lt;/summary&gt;

```bash
echo &#039;[charm]
name=Charm
baseurl=https://repo.charm.sh/yum/
enabled=1
gpgcheck=1
gpgkey=https://repo.charm.sh/yum/gpg.key&#039; | sudo tee /etc/yum.repos.d/charm.repo
sudo yum install crush
```

&lt;/details&gt;

Or, download it:

- [Packages][releases] are available in Debian and RPM formats
- [Binaries][releases] are available for Linux, macOS, Windows, FreeBSD, OpenBSD, and NetBSD

[releases]: https://github.com/charmbracelet/crush/releases

Or just install it with Go:

```
go install github.com/charmbracelet/crush@latest
```

&gt; [!WARNING]
&gt; Productivity may increase when using Crush and you may find yourself nerd
&gt; sniped when first using the application. If the symptoms persist, join the
&gt; [Discord][discord] and nerd snipe the rest of us.

## Getting Started

The quickest way to get started is to grab an API key for your preferred
provider such as Anthropic, OpenAI, Groq, or OpenRouter and just start
Crush. You&#039;ll be prompted to enter your API key.

That said, you can also set environment variables for preferred providers.

| Environment Variable        | Provider                                           |
| --------------------------- | -------------------------------------------------- |
| `ANTHROPIC_API_KEY`         | Anthropic                                          |
| `OPENAI_API_KEY`            | OpenAI                                             |
| `OPENROUTER_API_KEY`        | OpenRouter                                         |
| `GEMINI_API_KEY`            | Google Gemini                                      |
| `CEREBRAS_API_KEY`          | Cerebras                                           |
| `HF_TOKEN`                  | Huggingface Inference                              |
| `VERTEXAI_PROJECT`          | Google Cloud VertexAI (Gemini)                     |
| `VERTEXAI_LOCATION`         | Google Cloud VertexAI (Gemini)                     |
| `GROQ_API_KEY`              | Groq                                               |
| `AWS_ACCESS_KEY_ID`         | Amazon Bedrock (Claude)                               |
| `AWS_SECRET_ACCESS_KEY`     | Amazon Bedrock (Claude)                               |
| `AWS_REGION`                | Amazon Bedrock (Claude)                               |
| `AWS_PROFILE`               | Amazon Bedrock (Custom Profile)                       |
| `AWS_BEARER_TOKEN_BEDROCK`  | Amazon Bedrock                                        |
| `AZURE_OPENAI_API_ENDPOINT` | Azure OpenAI models                                |
| `AZURE_OPENAI_API_KEY`      | Azure OpenAI models (optional when using Entra ID) |
| `AZURE_OPENAI_API_VERSION`  | Azure OpenAI models                                |

### By the Way

Is there a provider you‚Äôd like to see in Crush? Is there an existing model that needs an update?

Crush‚Äôs default model listing is managed in [Catwalk](https://github.com/charmbracelet/catwalk), a community-supported, open source repository of Crush-compatible models, and you‚Äôre welcome to contribute.

&lt;a href=&quot;https://github.com/charmbracelet/catwalk&quot;&gt;&lt;img width=&quot;174&quot; height=&quot;174&quot; alt=&quot;Catwalk Badge&quot; src=&quot;https://github.com/user-attachments/assets/95b49515-fe82-4409-b10d-5beb0873787d&quot; /&gt;&lt;/a&gt;

## Configuration

Crush runs great with no configuration. That said, if you do need or want to
customize Crush, configuration can be added either local to the project itself,
or globally, with the following priority:

1. `.crush.json`
2. `crush.json`
3. `$HOME/.config/crush/crush.json`

Configuration itself is stored as a JSON object:

```json
{
  &quot;this-setting&quot;: { &quot;this&quot;: &quot;that&quot; },
  &quot;that-setting&quot;: [&quot;ceci&quot;, &quot;cela&quot;]
}
```

As an additional note, Crush also stores ephemeral data, such as application state, in one additional location:

```bash
# Unix
$HOME/.local/share/crush/crush.json

# Windows
%LOCALAPPDATA%\crush\crush.json
```

&gt; [!TIP]
&gt; You can override the user and data config locations by setting:
&gt; * `CRUSH_GLOBAL_CONFIG`
&gt; * `CRUSH_GLOBAL_DATA`

### LSPs

Crush can use LSPs for additional context to help inform its decisions, just
like you would. LSPs can be added manually like so:

```json
{
  &quot;$schema&quot;: &quot;https://charm.land/crush.json&quot;,
  &quot;lsp&quot;: {
    &quot;go&quot;: {
      &quot;command&quot;: &quot;gopls&quot;,
      &quot;env&quot;: {
        &quot;GOTOOLCHAIN&quot;: &quot;go1.24.5&quot;
      }
    },
    &quot;typescript&quot;: {
      &quot;command&quot;: &quot;typescript-language-server&quot;,
      &quot;args&quot;: [&quot;--stdio&quot;]
    },
    &quot;nix&quot;: {
      &quot;command&quot;: &quot;nil&quot;
    }
  }
}
```

### MCPs

Crush also supports Model Context Protocol (MCP) servers through three
transport types: `stdio` for command-line servers, `http` for HTTP endpoints,
and `sse` for Server-Sent Events. Environment variable expansion is supported
using `$(echo $VAR)` syntax.

```json
{
  &quot;$schema&quot;: &quot;https://charm.land/crush.json&quot;,
  &quot;mcp&quot;: {
    &quot;filesystem&quot;: {
      &quot;type&quot;: &quot;stdio&quot;,
      &quot;command&quot;: &quot;node&quot;,
      &quot;args&quot;: [&quot;/path/to/mcp-server.js&quot;],
      &quot;timeout&quot;: 120,
      &quot;disabled&quot;: false,
      &quot;disabled_tools&quot;: [&quot;some-tool-name&quot;],
      &quot;env&quot;: {
        &quot;NODE_ENV&quot;: &quot;production&quot;
      }
    },
    &quot;github&quot;: {
      &quot;type&quot;: &quot;http&quot;,
      &quot;url&quot;: &quot;https://api.githubcopilot.com/mcp/&quot;,
      &quot;timeout&quot;: 120,
      &quot;disabled&quot;: false,
      &quot;disabled_tools&quot;: [&quot;create_issue&quot;, &quot;create_pull_request&quot;],
      &quot;headers&quot;: {
        &quot;Authorization&quot;: &quot;Bearer $GH_PAT&quot;
      }
    },
    &quot;streaming-service&quot;: {
      &quot;type&quot;: &quot;sse&quot;,
      &quot;url&quot;: &quot;https://example.com/mcp/sse&quot;,
      &quot;timeout&quot;: 120,
      &quot;disabled&quot;: false,
      &quot;headers&quot;: {
        &quot;API-Key&quot;: &quot;$(echo $API_KEY)&quot;
      }
    }
  }
}
```

### Ignoring Files

Crush respects `.gitignore` files by default, but you can also create a
`.crushignore` file to specify additional files and directories that Crush
should ignore. This is useful for excluding files that you want in version
control but don&#039;t want Crush to consider when providing context.

The `.crushignore` file uses the same syntax as `.gitignore` and can be placed
in the root of your project or in subdirectories.

### Allowing Tools

By default, Crush will ask you for permission before running tool calls. If
you&#039;d like, you can allow tools to be executed without prompting you for
permissions. Use this with care.

```json
{
  &quot;$schema&quot;: &quot;https://charm.land/crush.json&quot;,
  &quot;permissions&quot;: {
    &quot;allowed_tools&quot;: [
      &quot;view&quot;,
      &quot;ls&quot;,
      &quot;grep&quot;,
      &quot;edit&quot;,
      &quot;mcp_context7_get-library-doc&quot;
    ]
  }
}
```

You can also skip all permission prompts entirely by running Crush with the
`--yolo` flag. Be very, very careful with this feature.

### Disabling Built-In Tools

If you&#039;d like to prevent Crush from using certain built-in tools entirely, you
can disable them via the `options.disabled_tools` list. Disabled tools are
completely hidden from the agent.

```json
{
  &quot;$schema&quot;: &quot;https://charm.land/crush.json&quot;,
  &quot;options&quot;: {
    &quot;disabled_tools&quot;: [
      &quot;bash&quot;,
      &quot;sourcegraph&quot;
    ]
  }
}
```

To disable tools from MCP servers, see the [MCP config section](#mcps).

### Agent Skills

Crush supports the [Agent Skills](https://agentskills.io) open standard for
extending agent capabilities with reusable skill packages. Skills are folders
containing a `SKILL.md` file with instructions that Crush can discover and
activate on demand.

Skills are discovered from:

- `~/.config/crush/skills/` on Unix (default, can be overridden with `CRUSH_SKILLS_DIR`)
- `%LOCALAPPDATA%\crush\skills\` on Windows (default, can be overridden with `CRUSH_SKILLS_DIR`)
- Additional paths configured via `options.skills_paths`

```jsonc
{
  &quot;$schema&quot;: &quot;https://charm.land/crush.json&quot;,
  &quot;options&quot;: {
    &quot;skills_paths&quot;: [
      &quot;~/.config/crush/skills&quot;, // Windows: &quot;%LOCALAPPDATA%\\crush\\skills&quot;,
      &quot;./project-skills&quot;
    ]
  }
}
```

You can get started with example skills from [anthropics/skills](https://github.com/anthropics/skills):

```bash
# Unix
mkdir -p ~/.config/crush/skills
cd ~/.config/crush/skills
git clone https://github.com/anthropics/skills.git _temp
mv _temp/skills/* . &amp;&amp; rm -rf _temp
```

```powershell
# Windows (PowerShell)
mkdir -Force &quot;$env:LOCALAPPDATA\crush\skills&quot;
cd &quot;$env:LOCALAPPDATA\crush\skills&quot;
git clone https://github.com/anthropics/skills.git _temp
mv _temp/skills/* . ; rm -r -force _temp
```

### Initialization

When you initialize a project, Crush analyzes your codebase and creates
a context file that helps it work more effectively in future sessions.
By default, this file is named `AGENTS.md`, but you can customize the
name and location with the `initialize_as` option:

```json
{
  &quot;$schema&quot;: &quot;https://charm.land/crush.json&quot;,
  &quot;options&quot;: {
    &quot;initialize_as&quot;: &quot;AGENTS.md&quot;
  }
}
```

This is useful if you prefer a different naming convention or want to
place the file in a specific directory (e.g., `CRUSH.md` or
`docs/LLMs.md`). Crush will fill the file with project-specific context
like build commands, code patterns, and conventions it discovered during
initialization.

### Attribution Settings

By default, Crush adds attribution information to Git commits and pull requests
it creates. You can customize this behavior with the `attribution` option:

```json
{
  &quot;$schema&quot;: &quot;https://charm.land/crush.json&quot;,
  &quot;options&quot;: {
    &quot;attribution&quot;: {
      &quot;trailer_style&quot;: &quot;co-authored-by&quot;,
      &quot;generated_with&quot;: true
    }
  }
}
```

- `trailer_style`: Controls the attribution trailer added to commit messages
  (default: `assisted-by`)
	- `assisted-by`: Adds `Assisted-by: [Model Name] via Crush &lt;crush@charm.land&gt;`
	  (includes the model name)
	- `co-authored-by`: Adds `Co-Authored-By: Crush &lt;crush@charm.land&gt;`
	- `none`: No attribution trailer
- `generated_with`: When true (default), adds `üíò Generated with Crush` line to
  commit messages and PR descriptions

### Custom Providers

Crush supports custom provider configurations for both OpenAI-compatible and
Anthropic-compatible APIs.

&gt; [!NOTE]
&gt; Note that we support two &quot;types&quot; for OpenAI. Make sure to choose the right one
&gt; to ensure the best experience!
&gt; * `openai` should be used when proxying or routing requests through OpenAI.
&gt; * `openai-compat` should be used when using non-OpenAI providers that have OpenAI-compatible APIs.

#### OpenAI-Compatible APIs

Here‚Äôs an example configuration for Deepseek, which uses an OpenAI-compatible
API. Don&#039;t forget to set `DEEPSEEK_API_KEY` in your environment.

```json
{
  &quot;$schema&quot;: &quot;https://charm.land/crush.json&quot;,
  &quot;providers&quot;: {
    &quot;deepseek&quot;: {
      &quot;type&quot;: &quot;openai-compat&quot;,
      &quot;base_url&quot;: &quot;https://api.deepseek.com/v1&quot;,
      &quot;api_key&quot;: &quot;$DEEPSEEK_API_KEY&quot;,
      &quot;models&quot;: [
        {
          &quot;id&quot;: &quot;deepseek-chat&quot;,
          &quot;name&quot;: &quot;Deepseek V3&quot;,
          &quot;cost_per_1m_in&quot;: 0.27,
          &quot;cost_per_1m_out&quot;: 1.1,
          &quot;cost_per_1m_in_cached&quot;: 0.07,
          &quot;cost_per_1m_out_cached&quot;: 1.1,
          &quot;context_window&quot;: 64000,
          &quot;default_max_tokens&quot;: 5000
        }
      ]
    }
  }
}
```

#### Anthropic-Compatible APIs

Custom Anthropic-compatible providers follow this format:

```json
{
  &quot;$schema&quot;: &quot;https://charm.land/crush.json&quot;,
  &quot;providers&quot;: {
    &quot;custom-anthropic&quot;: {
      &quot;type&quot;: &quot;anthropic&quot;,
      &quot;base_url&quot;: &quot;https://api.anthropic.com/v1&quot;,
      &quot;api_key&quot;: &quot;$ANTHROPIC_API_KEY&quot;,
      &quot;extra_headers&quot;: {
        &quot;anthropic-version&quot;: &quot;2023-06-01&quot;
      },
      &quot;models&quot;: [
        {
          &quot;id&quot;: &quot;claude-sonnet-4-20250514&quot;,
          &quot;name&quot;: &quot;Claude Sonnet 4&quot;,
          &quot;cost_per_1m_in&quot;: 3,
          &quot;cost_per_1m_out&quot;: 15,
          &quot;cost_per_1m_in_cached&quot;: 3.75,
          &quot;cost_per_1m_out_cached&quot;: 0.3,
          &quot;context_window&quot;: 200000,
          &quot;default_max_tokens&quot;: 50000,
          &quot;can_reason&quot;: true,
          &quot;supports_attachments&quot;: true
        }
      ]
    }
  }
}
```

### Amazon Bedrock

Crush currently supports running Anthropic models through Bedrock, with caching disabled.

- A Bedrock provider will appear once you have AWS configured, i.e. `aws configure`
- Crush also expects the `AWS_REGION` or `AWS_DEFAULT_REGION` to be set
- To use a specific AWS profile set `AWS_PROFILE` in your environment, i.e. `AWS_PROFILE=myprofile crush`
- Alternatively to `aws configure`, you can also just set `AWS_BEARER_TOKEN_BEDROCK`

### Vertex AI Platform

Vertex AI will appear in the list of available providers when `VERTEXAI_PROJECT` and `VERTEXAI_LOCATION` are set. You will also need to be authenticated:

```bash
gcloud auth application-default login
```

To add specific models to the configuration, configure as such:

```json
{
  &quot;$schema&quot;: &quot;https://charm.land/crush.json&quot;,
  &quot;providers&quot;: {
    &quot;vertexai&quot;: {
      &quot;models&quot;: [
        {
          &quot;id&quot;: &quot;claude-sonnet-4@20250514&quot;,
          &quot;name&quot;: &quot;VertexAI Sonnet 4&quot;,
          &quot;cost_per_1m_in&quot;: 3,
          &quot;cost_per_1m_out&quot;: 15,
          &quot;cost_per_1m_in_cached&quot;: 3.75,
          &quot;cost_per_1m_out_cached&quot;: 0.3,
          &quot;context_window&quot;: 200000,
          &quot;default_max_tokens&quot;: 50000,
          &quot;can_reason&quot;: true,
          &quot;supports_attachments&quot;: true
        }
      ]
    }
  }
}
```

### Local Models

Local models can also be configured via OpenAI-compatible API. Here are two common examples:

#### Ollama

```json
{
  &quot;providers&quot;: {
    &quot;ollama&quot;: {
      &quot;name&quot;: &quot;Ollama&quot;,
      &quot;base_url&quot;: &quot;http://localhost:11434/v1/&quot;,
      &quot;type&quot;: &quot;openai-compat&quot;,
      &quot;models&quot;: [
        {
          &quot;name&quot;: &quot;Qwen 3 30B&quot;,
          &quot;id&quot;: &quot;qwen3:30b&quot;,
          &quot;context_window&quot;: 256000,
          &quot;default_max_tokens&quot;: 20000
        }
      ]
    }
  }
}
```

#### LM Studio

```json
{
  &quot;providers&quot;: {
    &quot;lmstudio&quot;: {
      &quot;name&quot;: &quot;LM Studio&quot;,
      &quot;base_url&quot;: &quot;http://localhost:1234/v1/&quot;,
      &quot;type&quot;: &quot;openai-compat&quot;,
      &quot;models&quot;: [
        {
          &quot;name&quot;: &quot;Qwen 3 30B&quot;,
          &quot;id&quot;: &quot;qwen/qwen3-30b-a3b-2507&quot;,
          &quot;context_window&quot;: 256000,
          &quot;default_max_tokens&quot;: 20000
        }
      ]
    }
  }
}
```

## Logging

Sometimes you need to look at logs. Luckily, Crush logs all sorts of
stuff. Logs are stored in `./.crush/logs/crush.log` relative to the project.

The CLI also contains some helper commands to make perusing recent logs easier:

```bash
# Print the last 1000 lines
crush logs

# Print the last 500 lines
crush logs --tail 500

# Follow logs in real time
crush logs --follow
```

Want more logging? Run `crush` with the `--debug` flag, or enable it in the
config:

```json
{
  &quot;$schema&quot;: &quot;https://charm.land/crush.json&quot;,
  &quot;options&quot;: {
    &quot;debug&quot;: true,
    &quot;debug_lsp&quot;: true
  }
}
```

## Provider Auto-Updates

By default, Crush automatically checks for the latest and greatest list of
providers and models from [Catwalk](https://github.com/charmbracelet/catwalk),
the open source Crush provider database. This means that when new providers and
models are available, or when model metadata changes, Crush automatically
updates your local configuration.

### Disabling automatic provider updates

For those with restricted internet access, or those who prefer to work in
air-gapped environments, this might not be want you want, and this feature can
be disabled.

To disable automatic provider updates, set `disable_provider_auto_update` into
your `crush.json` config:

```json
{
  &quot;$schema&quot;: &quot;https://charm.land/crush.json&quot;,
  &quot;options&quot;: {
    &quot;disable_provider_auto_update&quot;: true
  }
}
```

Or set the `CRUSH_DISABLE_PROVIDER_AUTO_UPDATE` environment variable:

```bash
export CRUSH_DISABLE_PROVIDER_AUTO_UPDATE=1
```

### Manually updating providers

Manually updating providers is possible with the `crush update-providers`
co

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[fleetdm/fleet]]></title>
            <link>https://github.com/fleetdm/fleet</link>
            <guid>https://github.com/fleetdm/fleet</guid>
            <pubDate>Fri, 09 Jan 2026 00:05:24 GMT</pubDate>
            <description><![CDATA[Open device management]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/fleetdm/fleet">fleetdm/fleet</a></h1>
            <p>Open device management</p>
            <p>Language: Go</p>
            <p>Stars: 5,896</p>
            <p>Forks: 754</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre>&lt;h1&gt;&lt;a href=&quot;https://fleetdm.com&quot;&gt;&lt;img width=&quot;200&quot; alt=&quot;Fleet logo, landscape, dark text, transparent background&quot; src=&quot;https://github.com/user-attachments/assets/5b52c536-f33e-4159-b2a3-d48f31868cd2&quot;&gt;&lt;/a&gt;&lt;/h1&gt;


#### [News](https://fleetdm.com/announcements) &amp;nbsp; ¬∑ &amp;nbsp; [Report a bug](https://github.com/fleetdm/fleet/issues/new) &amp;nbsp; ¬∑ &amp;nbsp; [Handbook](https://fleetdm.com/handbook/company) &amp;nbsp; ¬∑ &amp;nbsp; [Why open source?](https://fleetdm.com/handbook/company/why-this-way#why-open-source) &amp;nbsp; ¬∑ &amp;nbsp; [Art](https://fleetdm.com/logos)


Open-source platform for IT and security teams with thousands of computers.  Designed for APIs, GitOps, webhooks, YAML, and humans.

&lt;a href=&quot;https://fleetdm.com/logos&quot;&gt;&lt;img src=&quot;https://github.com/fleetdm/fleet/assets/618009/f835ec29-1cb9-49ba-a0f3-395ffd9d5c9f&quot; alt=&quot;A glass city in the clouds&quot;/&gt;&lt;/a&gt;


## What&#039;s it for?
Organizations like Fastly and Gusto use Fleet for vulnerability reporting, detection engineering, device management (MDM), device health monitoring, posture-based access control, managing unused software licenses, and more.

#### Explore data
To see what kind of data you can use Fleet to gather, check out the [table reference documentation](https://fleetdm.com/tables).

#### Out-of-the-box policies
Fleet includes out-of-the box support for all [CIS benchmarks for macOS and Windows](https://fleetdm.com/docs/using-fleet/cis-benchmarks), as well as many [simpler queries](https://fleetdm.com/queries).

Take as much or as little as you need for your organization.

#### Supported platforms
Here are the platforms Fleet currently supports:

- Linux (all distros)
- macOS
- Windows
- Chromebooks
- Amazon Web Services (AWS)
- Google Cloud (GCP)
- Azure (Microsoft cloud)
- Data centers
- Containers (kube, etc)
- Linux-based IoT devices

## Lighter than air
Fleet is lightweight and modular.  You can use it for security without using it for MDM, and vice versa.  You can turn off features you are not using.

#### Openness
Fleet is dedicated to flexibility, accessibility, and clarity.  We think [everyone can contribute](https://fleetdm.com/handbook/company#openness) and that tools should be as easy as possible for everyone to understand.

#### Good neighbors
Fleet has no ambition to replace all of your other tools.  (Though it might replace some, if you want it to.)  Ready-to-use, enterprise-friendly integrations exist for Snowflake, Splunk, GitHub Actions, Vanta, Elastic Jira, Zendesk, and more.

Fleet plays well with Munki, Chef, Puppet, and Ansible, as well as with security tools like Crowdstrike and SentinelOne.  For example, you can use the free version of Fleet to quickly report on what hosts are _actually_ running your EDR agent.

#### Free as in free
The free version of Fleet will [always be free](https://fleetdm.com/pricing).  Fleet is [independently backed](https://linkedin.com/company/fleetdm) and actively maintained with the help of many amazing [contributors](https://github.com/fleetdm/fleet/graphs/contributors).

#### Longevity
The [company behind Fleet](https://fleetdm.com/handbook/company) is founded (and majority-owned) by [true believers in open source](https://fleetdm.com/handbook/company/why-this-way#why-open-source).  The company&#039;s business model is influenced by GitLab (NYSE: GTLB), with great investors, happy customers, and the capacity to become profitable at any time.

In keeping with Fleet&#039;s value of openness, [Fleet Device Management&#039;s company handbook](https://fleetdm.com/handbook/company) is public and open source.  You can read about the [history of Fleet and osquery](https://fleetdm.com/handbook/company#history) and our commitment to improving the product.

&lt;!-- &gt; To upgrade from Fleet ‚â§3.2.0, just follow the upgrading steps for the earliest subsequent major release from this repository (it&#039;ll work out of the box until the release of Fleet 5.0). --&gt;


## Is it any good?
Fleet is used in production by IT and security teams with thousands of laptops and servers.  Many deployments support tens of thousands of hosts, and a few large organizations manage deployments as large as 400,000+ hosts.



## Chat
Please join us in [MacAdmins Slack](https://www.macadmins.org/) or in [osquery Slack](https://fleetdm.com/slack).

The Fleet community is full of [kind and helpful people](https://fleetdm.com/handbook/company#empathy).  Whether or not you are a paying customer, if you need help, just ask.


## Contributing &amp;nbsp; [![Go Report Card](https://goreportcard.com/badge/github.com/fleetdm/fleet)](https://goreportcard.com/report/github.com/fleetdm/fleet) &amp;nbsp; [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/5537/badge)](https://bestpractices.coreinfrastructure.org/projects/5537) &amp;nbsp; [![Twitter Follow](https://img.shields.io/twitter/follow/fleetctl.svg?style=social&amp;maxAge=3600)](https://twitter.com/fleetctl) &amp;nbsp; 

The landscape of cybersecurity and IT is too complex.  Let&#039;s open it up.

Contributions are welcome, whether you answer questions on [Slack](https://fleetdm.com/slack) / [GitHub](https://github.com/fleetdm/fleet/issues) / [StackOverflow](https://stackoverflow.com/search?q=osquery) / [LinkedIn](https://linkedin.com/company/fleetdm) / [Twitter](https://twitter.com/fleetctl), improve the documentation or [website](./website), write a tutorial, give a talk at a conference or local meetup, give an [interview on a podcast](https://fleetdm.com/podcasts), troubleshoot reported issues, or [submit a patch](https://fleetdm.com/docs/contributing/contributing).  The Fleet code of conduct is [on GitHub](https://github.com/fleetdm/fleet/blob/main/CODE_OF_CONDUCT.md).

&lt;!-- - Great contributions are motivated by real-world use cases or learning.
- Some of the most valuable contributions might not touch any code at all.
- Small, iterative, simple (boring) changes are the easiest to merge. --&gt;

## What&#039;s next?
To see what Fleet can do, head over to [fleetdm.com](https://fleetdm.com) and try it out for yourself, grab time with one of the maintainers to discuss, or visit the docs and roll it out to your organization.

#### Production deployment
Fleet is simple enough to [spin up for yourself](https://fleetdm.com/docs/get-started/tutorials-and-guides).  Or you can have us [host it for you](https://fleetdm.com/pricing).  Premium features are [available](https://fleetdm.com/pricing) either way.

#### Documentation
Complete documentation for Fleet can be found at [https://fleetdm.com/docs](https://fleetdm.com/docs).


## License
The free version of Fleet is available under the MIT license.  The commercial license is also designed to allow contributions to paid features for users whose employment agreements allow them to contribute to open source projects.  (See LICENSE.md for details.)

&gt; Fleet is built on [osquery](https://github.com/osquery/osquery), [nanoMDM](https://github.com/micromdm/nanomdm), [Nudge](https://github.com/macadmins/nudge), and [swiftDialog](https://github.com/swiftDialog/swiftDialog).
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[alibaba/higress]]></title>
            <link>https://github.com/alibaba/higress</link>
            <guid>https://github.com/alibaba/higress</guid>
            <pubDate>Fri, 09 Jan 2026 00:05:23 GMT</pubDate>
            <description><![CDATA[ü§ñ AI Gateway | AI Native API Gateway]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/alibaba/higress">alibaba/higress</a></h1>
            <p>ü§ñ AI Gateway | AI Native API Gateway</p>
            <p>Language: Go</p>
            <p>Stars: 7,251</p>
            <p>Forks: 943</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre>&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;
&lt;h1 align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://img.alicdn.com/imgextra/i2/O1CN01NwxLDd20nxfGBjxmZ_!!6000000006895-2-tps-960-290.png&quot; alt=&quot;Higress&quot; width=&quot;240&quot; height=&quot;72.5&quot;&gt;
  &lt;br&gt;
  AI Gateway
&lt;/h1&gt;
&lt;h4 align=&quot;center&quot;&gt; AI Native API Gateway &lt;/h4&gt;

&lt;div align=&quot;center&quot;&gt;
    
[![Build Status](https://github.com/alibaba/higress/actions/workflows/build-and-test.yaml/badge.svg?branch=main)](https://github.com/alibaba/higress/actions)
[![license](https://img.shields.io/github/license/alibaba/higress.svg)](https://www.apache.org/licenses/LICENSE-2.0.html)
[![discord](https://img.shields.io/discord/1364956090566971515?color=5865F2&amp;label=discord&amp;labelColor=black&amp;logo=discord&amp;logoColor=white&amp;style=flat-square)](https://discord.gg/tSbww9VDaM)

&lt;a href=&quot;https://trendshift.io/repositories/10918&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/10918&quot; alt=&quot;alibaba%2Fhigress | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://www.producthunt.com/posts/higress?embed=true&amp;utm_source=badge-featured&amp;utm_medium=badge&amp;utm_souce=badge-higress&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=951287&amp;theme=light&amp;t=1745492822283&quot; alt=&quot;Higress - Global&amp;#0032;APIs&amp;#0032;as&amp;#0032;MCP&amp;#0032;powered&amp;#0032;by&amp;#0032;AI&amp;#0032;Gateway | Product Hunt&quot; style=&quot;width: 250px; height: 54px;&quot; width=&quot;250&quot; height=&quot;54&quot; /&gt;&lt;/a&gt;

&lt;/div&gt;

[**Official Site**](https://higress.ai/en/) &amp;nbsp; |
&amp;nbsp; [**Docs**](https://higress.cn/en/docs/latest/overview/what-is-higress/) &amp;nbsp; |
&amp;nbsp; [**Blog**](https://higress.cn/en/blog/) &amp;nbsp; |
&amp;nbsp; [**MCP Server QuickStart**](https://higress.cn/en/ai/mcp-quick-start/) &amp;nbsp; |
&amp;nbsp; [**Developer Guide**](https://higress.cn/en/docs/latest/dev/architecture/) &amp;nbsp; |
&amp;nbsp; [**Wasm Plugin Hub**](https://higress.cn/en/plugin/) &amp;nbsp; |

&lt;p&gt;
   English | &lt;a href=&quot;README_ZH.md&quot;&gt;‰∏≠Êñá&lt;/a&gt; | &lt;a href=&quot;README_JP.md&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt;
&lt;/p&gt;

## What is Higress?

Higress is a cloud-native API gateway based on Istio and Envoy, which can be extended with Wasm plugins written in Go/Rust/JS. It provides dozens of ready-to-use general-purpose plugins and an out-of-the-box console (try the [demo here](http://demo.higress.io/)).

### Core Use Cases

Higress&#039;s AI gateway capabilities support all [mainstream model providers](https://github.com/alibaba/higress/tree/main/plugins/wasm-go/extensions/ai-proxy/provider) both domestic and international. It also supports hosting MCP (Model Context Protocol) Servers through its plugin mechanism, enabling AI Agents to easily call various tools and services. With the [openapi-to-mcp tool](https://github.com/higress-group/openapi-to-mcpserver), you can quickly convert OpenAPI specifications into remote MCP servers for hosting. Higress provides unified management for both LLM API and MCP API. 

**üåü Try it now at [https://mcp.higress.ai/](https://mcp.higress.ai/)** to experience Higress-hosted Remote MCP Servers firsthand:

![Higress MCP Server Platform](https://img.alicdn.com/imgextra/i2/O1CN01nmVa0a1aChgpyyWOX_!!6000000003294-0-tps-3430-1742.jpg)

### Enterprise Adoption

Higress was born within Alibaba to solve the issues of Tengine reload affecting long-connection services and insufficient load balancing capabilities for gRPC/Dubbo. Within Alibaba Cloud, Higress&#039;s AI gateway capabilities support core AI applications such as Tongyi Bailian model studio, machine learning PAI platform, and other critical AI services. Alibaba Cloud has built its cloud-native API gateway product based on Higress, providing 99.99% gateway high availability guarantee service capabilities for a large number of enterprise customers.

You can click the button below to install the enterprise version of Higress:

[![Deploy on AlibabaCloud](https://img.alicdn.com/imgextra/i1/O1CN01e6vwe71EWTHoZEcpK_!!6000000000359-55-tps-170-40.svg)](https://www.aliyun.com/product/api-gateway?spm=higress-github.topbar.0.0.0)


If you use open-source Higress and wish to obtain enterprise-level support, you can contact the project maintainer johnlanni&#039;s email: **zty98751@alibaba-inc.com** or social media accounts (WeChat ID: **nomadao**, DingTalk ID: **chengtanzty**). Please note **Higress** when adding as a friend :)

## Summary

- [**Quick Start**](#quick-start)    
- [**Feature Showcase**](#feature-showcase)
- [**Use Cases**](#use-cases)
- [**Core Advantages**](#core-advantages)
- [**Community**](#community)

## Quick Start

Higress can be started with just Docker, making it convenient for individual developers to set up locally for learning or for building simple sites:

```bash
# Create a working directory
mkdir higress; cd higress
# Start higress, configuration files will be written to the working directory
docker run -d --rm --name higress-ai -v ${PWD}:/data \
        -p 8001:8001 -p 8080:8080 -p 8443:8443  \
        higress-registry.cn-hangzhou.cr.aliyuncs.com/higress/all-in-one:latest
```

Port descriptions:

- Port 8001: Higress UI console entry
- Port 8080: Gateway HTTP protocol entry
- Port 8443: Gateway HTTPS protocol entry

&gt; All Higress Docker images use Higress&#039;s own image repository and are not affected by Docker Hub rate limits.
&gt; In addition, the submission and updates of the images are protected by a security scanning mechanism (powered by Alibaba Cloud ACR), making them very secure for use in production environments.
&gt; 
&gt; If you experience a timeout when pulling image from `higress-registry.cn-hangzhou.cr.aliyuncs.com`, you can try replacing it with the following docker registry mirror source:
&gt; 
&gt; **North America**: `higress-registry.us-west-1.cr.aliyuncs.com`
&gt; 
&gt; **Southeast Asia**: `higress-registry.ap-southeast-7.cr.aliyuncs.com`

For other installation methods such as Helm deployment under K8s, please refer to the official [Quick Start documentation](https://higress.io/en-us/docs/user/quickstart).

If you are deploying on the cloud, it is recommended to use the [Enterprise Edition](https://www.aliyun.com/product/apigateway?spm=higress-github.topbar.0.0.0)


## Use Cases

- **MCP Server Hosting**:

  Higress hosts MCP Servers through its plugin mechanism, enabling AI Agents to easily call various tools and services. With the [openapi-to-mcp tool](https://github.com/higress-group/openapi-to-mcpserver), you can quickly convert OpenAPI specifications into remote MCP servers.

  ![](https://img.alicdn.com/imgextra/i1/O1CN01wv8H4g1mS4MUzC1QC_!!6000000004952-2-tps-1764-597.png)

  Key benefits of hosting MCP Servers with Higress:
  - Unified authentication and authorization mechanisms
  - Fine-grained rate limiting to prevent abuse
  - Comprehensive audit logs for all tool calls
  - Rich observability for monitoring performance
  - Simplified deployment through Higress&#039;s plugin mechanism
  - Dynamic updates without disruption or connection drops

     [Learn more...](https://higress.cn/en/ai/mcp-quick-start/?spm=36971b57.7beea2de.0.0.d85f20a94jsWGm)

- **AI Gateway**:

  Higress connects to all LLM model providers using a unified protocol, with AI observability, multi-model load balancing, token rate limiting, and caching capabilities:

  ![](https://img.alicdn.com/imgextra/i2/O1CN01izmBNX1jbHT7lP3Yr_!!6000000004566-0-tps-1920-1080.jpg)

- **Kubernetes ingress controller**:

  Higress can function as a feature-rich ingress controller, which is compatible with many annotations of K8s&#039; nginx ingress controller.
  
  [Gateway API](https://gateway-api.sigs.k8s.io/) is already supported, and it supports a smooth migration from Ingress API to Gateway API.

  Compared to ingress-nginx, the resource overhead has significantly decreased, and the speed at which route changes take effect has improved by ten times.

  &gt; The following resource overhead comparison comes from [sealos](https://github.com/labring).
  &gt;
  &gt; For details, you can read this [article](https://sealos.io/blog/sealos-envoy-vs-nginx-2000-tenants) to understand how sealos migrates the monitoring of **tens of thousands of ingress** resources from nginx ingress to higress.

   ![](https://img.alicdn.com/imgextra/i1/O1CN01bhEtb229eeMNBWmdP_!!6000000008093-2-tps-750-547.png)

  
- **Microservice gateway**:

  Higress can function as a microservice gateway, which can discovery microservices from various service registries, such as Nacos, ZooKeeper, Consul, Eureka, etc.
  
  It deeply integrates with [Dubbo](https://github.com/apache/dubbo), [Nacos](https://github.com/alibaba/nacos), [Sentinel](https://github.com/alibaba/Sentinel) and other microservice technology stacks.
  
- **Security gateway**:

  Higress can be used as a security gateway, supporting WAF and various authentication strategies, such as key-auth, hmac-auth, jwt-auth, basic-auth, oidc, etc.


## Core Advantages

- **Production Grade**

  Born from Alibaba&#039;s internal product with over 2 years of production validation, supporting large-scale scenarios with hundreds of thousands of requests per second.

  Completely eliminates traffic jitter caused by Nginx reload, configuration changes take effect in milliseconds and are transparent to business. Especially friendly to long-connection scenarios such as AI businesses.

- **Streaming Processing**

  Supports true complete streaming processing of request/response bodies, Wasm plugins can easily customize the handling of streaming protocols such as SSE (Server-Sent Events).

  In high-bandwidth scenarios such as AI businesses, it can significantly reduce memory overhead.
    
- **Easy to Extend**
  
  Provides a rich official plugin library covering AI, traffic management, security protection and other common functions, meeting more than 90% of business scenario requirements.

  Focuses on Wasm plugin extensions, ensuring memory safety through sandbox isolation, supporting multiple programming languages, allowing plugin versions to be upgraded independently, and achieving traffic-lossless hot updates of gateway logic.

- **Secure and Easy to Use**
  
  Based on Ingress API and Gateway API standards, provides out-of-the-box UI console, WAF protection plugin, IP/Cookie CC protection plugin ready to use.

  Supports connecting to Let&#039;s Encrypt for automatic issuance and renewal of free certificates, and can be deployed outside of K8s, started with a single Docker command, convenient for individual developers to use.

## Community

Join our Discord community! This is where you can connect with developers and other enthusiastic users of Higress.

[![discord](https://img.shields.io/discord/1364956090566971515?color=5865F2&amp;label=discord&amp;labelColor=black&amp;logo=discord&amp;logoColor=white&amp;style=for-the-badge)](https://discord.gg/tSbww9VDaM)


### Thanks

Higress would not be possible without the valuable open-source work of projects in the community. We would like to extend a special thank you to Envoy and Istio.

### Related Repositories

- Higress Console: https://github.com/higress-group/higress-console
- Higress Standalone: https://github.com/higress-group/higress-standalone

### Contributors

&lt;a href=&quot;https://github.com/alibaba/higress/graphs/contributors&quot;&gt;
  &lt;img alt=&quot;contributors&quot; src=&quot;https://contrib.rocks/image?repo=alibaba/higress&quot;/&gt;
&lt;/a&gt;

### Star History

[![Star History Chart](https://api.star-history.com/svg?repos=alibaba/higress&amp;type=Date)](https://star-history.com/#alibaba/higress&amp;Date)

&lt;p align=&quot;right&quot; style=&quot;font-size: 14px; color: #555; margin-top: 20px;&quot;&gt;
    &lt;a href=&quot;#readme-top&quot; style=&quot;text-decoration: none; color: #007bff; font-weight: bold;&quot;&gt;
        ‚Üë Back to Top ‚Üë
    &lt;/a&gt;
&lt;/p&gt;
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[cloudreve/cloudreve]]></title>
            <link>https://github.com/cloudreve/cloudreve</link>
            <guid>https://github.com/cloudreve/cloudreve</guid>
            <pubDate>Fri, 09 Jan 2026 00:05:22 GMT</pubDate>
            <description><![CDATA[üå© Self-hosted file management and sharing system, supports multiple storage providers]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/cloudreve/cloudreve">cloudreve/cloudreve</a></h1>
            <p>üå© Self-hosted file management and sharing system, supports multiple storage providers</p>
            <p>Language: Go</p>
            <p>Stars: 26,516</p>
            <p>Forks: 3,767</p>
            <p>Stars today: 71 stars today</p>
            <h2>README</h2><pre>[‰∏≠ÊñáÁâàÊú¨](https://github.com/cloudreve/Cloudreve/blob/master/README_zh-CN.md)

&lt;h1 align=&quot;center&quot;&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://cloudreve.org/&quot; alt=&quot;logo&quot; &gt;&lt;img src=&quot;https://raw.githubusercontent.com/cloudreve/frontend/master/public/static/img/logo192.png&quot; width=&quot;150&quot;/&gt;&lt;/a&gt;
  &lt;br&gt;
  Cloudreve
  &lt;br&gt;
&lt;/h1&gt;
&lt;h4 align=&quot;center&quot;&gt;Self-hosted file management system with multi-cloud support.&lt;/h4&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://dev.azure.com/abslantliu/Cloudreve/_build?definitionId=6&quot;&gt;
    &lt;img src=&quot;https://dev.azure.com/abslantliu/Cloudreve/_apis/build/status%2Fcloudreve.Cloudreve?branchName=refs%2Fpull%2F2481%2Fmerge&quot;
         alt=&quot;Azure pipelines&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/cloudreve/Cloudreve/releases&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/v/release/cloudreve/Cloudreve?include_prereleases&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://hub.docker.com/r/cloudreve/cloudreve&quot;&gt;
     &lt;img src=&quot;https://img.shields.io/docker/image-size/cloudreve/cloudreve&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://hub.docker.com/r/cloudreve/cloudreve&quot;&gt;
  &lt;img alt=&quot;Docker Pulls&quot; src=&quot;https://img.shields.io/docker/pulls/cloudreve/cloudreve&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://cloudreve.org&quot;&gt;Homepage&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://demo.cloudreve.org&quot;&gt;Try it&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://github.com/cloudreve/cloudreve/discussions&quot;&gt;Discussion&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://docs.cloudreve.org&quot;&gt;Documents&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://github.com/cloudreve/Cloudreve/releases&quot;&gt;Download&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://t.me/cloudreve_official&quot;&gt;Telegram&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://discord.com/invite/WTpMFpZT76&quot;&gt;Discord&lt;/a&gt;
&lt;/p&gt;

![Screenshot](https://raw.githubusercontent.com/cloudreve/docs/master/images/homepage.png)

## :sparkles: Features

- :cloud: Support storing files into Local, Remote node, OneDrive, S3 compatible API, Qiniu Kodo, Aliyun OSS, Tencent COS, Huawei Cloud OBS, Kingsoft Cloud KS3, Upyun.
- :outbox_tray: Upload/Download in directly transmission from client to storage providers.
- üíæ Integrate with Aria2/qBittorrent to download files in background, use multiple download nodes to share the load.
- üìö Compress/Extract/Preview archived files, download files in batch.
- üíª WebDAV support covering all storage providers.
- :zap:Drag&amp;Drop to upload files or folders, with parallel resumable upload support.
- :card_file_box: Extract media metadata from files, search files by metadata or tags.
- :family_woman_girl_boy: Multi-users with multi-groups.
- :link: Create share links for files and folders with expiration date.
- :eye_speech_bubble: Preview videos, images, audios, ePub files online; edit texts, diagrams, Markdown, images, Office documents online.
- :art: Customize theme colors, dark mode, PWA application, SPA, i18n.
- :rocket: All-in-one packaging, with all features out of the box.
- üåà ... ...

## :hammer_and_wrench: Deploy

To deploy Cloudreve, you can refer to [Getting started](https://docs.cloudreve.org/overview/quickstart) for a quick local deployment to test.

When you&#039;re ready to deploy Cloudreve to a production environment, you can refer to [Deploy](https://docs.cloudreve.org/overview/deploy/) for a complete deployment.

## :gear: Build

Please refer to [Build](https://docs.cloudreve.org/overview/build/) for how to build Cloudreve from source code.

## :rocket: Contributing

If you&#039;re interested in contributing to Cloudreve, please refer to [Contributing](https://docs.cloudreve.org/api/contributing/) for how to contribute to Cloudreve.

## :alembic: Stacks

- [Go](https://golang.org/) + [Gin](https://github.com/gin-gonic/gin) + [ent](https://github.com/ent/ent)
- [React](https://github.com/facebook/react) + [Redux](https://github.com/reduxjs/redux) + [Material-UI](https://github.com/mui-org/material-ui)

## :scroll: License

GPL V3
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[gitleaks/gitleaks]]></title>
            <link>https://github.com/gitleaks/gitleaks</link>
            <guid>https://github.com/gitleaks/gitleaks</guid>
            <pubDate>Fri, 09 Jan 2026 00:05:21 GMT</pubDate>
            <description><![CDATA[Find secrets with Gitleaks üîë]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/gitleaks/gitleaks">gitleaks/gitleaks</a></h1>
            <p>Find secrets with Gitleaks üîë</p>
            <p>Language: Go</p>
            <p>Stars: 24,512</p>
            <p>Forks: 1,872</p>
            <p>Stars today: 18 stars today</p>
            <h2>README</h2><pre># Gitleaks

```
‚îå‚îÄ‚óã‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ‚îÇ‚ï≤  ‚îÇ
‚îÇ ‚îÇ ‚óã ‚îÇ
‚îÇ ‚óã ‚ñë ‚îÇ
‚îî‚îÄ‚ñë‚îÄ‚îÄ‚îÄ‚îò
```

[license]: ./LICENSE
[badge-license]: https://img.shields.io/github/license/gitleaks/gitleaks.svg
[go-docs-badge]: https://pkg.go.dev/badge/github.com/gitleaks/gitleaks/v8?status
[go-docs]: https://pkg.go.dev/github.com/zricethezav/gitleaks/v8
[badge-build]: https://github.com/gitleaks/gitleaks/actions/workflows/test.yml/badge.svg
[build]: https://github.com/gitleaks/gitleaks/actions/workflows/test.yml
[go-report-card-badge]: https://goreportcard.com/badge/github.com/gitleaks/gitleaks/v8
[go-report-card]: https://goreportcard.com/report/github.com/gitleaks/gitleaks/v8
[dockerhub]: https://hub.docker.com/r/zricethezav/gitleaks
[dockerhub-badge]: https://img.shields.io/docker/pulls/zricethezav/gitleaks.svg
[gitleaks-action]: https://github.com/gitleaks/gitleaks-action
[gitleaks-badge]: https://img.shields.io/badge/protected%20by-gitleaks-blue
[gitleaks-playground-badge]: https://img.shields.io/badge/gitleaks%20-playground-blue
[gitleaks-playground]: https://gitleaks.io/playground


[![GitHub Action Test][badge-build]][build]
[![Docker Hub][dockerhub-badge]][dockerhub]
[![Gitleaks Playground][gitleaks-playground-badge]][gitleaks-playground]
[![Gitleaks Action][gitleaks-badge]][gitleaks-action]
[![GoDoc][go-docs-badge]][go-docs]
[![GoReportCard][go-report-card-badge]][go-report-card]
[![License][badge-license]][license]

Gitleaks is a tool for **detecting** secrets like passwords, API keys, and tokens in git repos, files, and whatever else you wanna throw at it via `stdin`. If you wanna learn more about how the detection engine works check out this blog: [Regex is (almost) all you need](https://lookingatcomputer.substack.com/p/regex-is-almost-all-you-need).

```
‚ûú  ~/code(master) gitleaks git -v

    ‚óã
    ‚îÇ‚ï≤
    ‚îÇ ‚óã
    ‚óã ‚ñë
    ‚ñë    gitleaks


Finding:     &quot;export BUNDLE_ENTERPRISE__CONTRIBSYS__COM=cafebabe:deadbeef&quot;,
Secret:      cafebabe:deadbeef
RuleID:      sidekiq-secret
Entropy:     2.609850
File:        cmd/generate/config/rules/sidekiq.go
Line:        23
Commit:      cd5226711335c68be1e720b318b7bc3135a30eb2
Author:      John
Email:       john@users.noreply.github.com
Date:        2022-08-03T12:31:40Z
Fingerprint: cd5226711335c68be1e720b318b7bc3135a30eb2:cmd/generate/config/rules/sidekiq.go:sidekiq-secret:23
```

### GitHub Sponsors

Sponsor [@zricethezav on GitHub](https://github.com/sponsors/zricethezav/) to get
featured on this README.

## Getting Started

Gitleaks can be installed using Homebrew, Docker, or Go. Gitleaks is also available in binary form for many popular platforms and OS types on the [releases page](https://github.com/gitleaks/gitleaks/releases). In addition, Gitleaks can be implemented as a pre-commit hook directly in your repo or as a GitHub action using [Gitleaks-Action](https://github.com/gitleaks/gitleaks-action).

### Installing

```bash
# MacOS
brew install gitleaks

# Docker (DockerHub)
docker pull zricethezav/gitleaks:latest
docker run -v ${path_to_host_folder_to_scan}:/path zricethezav/gitleaks:latest [COMMAND] [OPTIONS] [SOURCE_PATH]

# Docker (ghcr.io)
docker pull ghcr.io/gitleaks/gitleaks:latest
docker run -v ${path_to_host_folder_to_scan}:/path ghcr.io/gitleaks/gitleaks:latest [COMMAND] [OPTIONS] [SOURCE_PATH]

# From Source (make sure `go` is installed)
git clone https://github.com/gitleaks/gitleaks.git
cd gitleaks
make build
```

### GitHub Action

Check out the official [Gitleaks GitHub Action](https://github.com/gitleaks/gitleaks-action)

```
name: gitleaks
on: [pull_request, push, workflow_dispatch]
jobs:
  scan:
    name: gitleaks
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0
      - uses: gitleaks/gitleaks-action@v2
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITLEAKS_LICENSE: ${{ secrets.GITLEAKS_LICENSE}} # Only required for Organizations, not personal accounts.
```

### Pre-Commit

1. Install pre-commit from https://pre-commit.com/#install
2. Create a `.pre-commit-config.yaml` file at the root of your repository with the following content:

   ```
   repos:
     - repo: https://github.com/gitleaks/gitleaks
       rev: v8.24.2
       hooks:
         - id: gitleaks
   ```

   for a [native execution of gitleaks](https://github.com/gitleaks/gitleaks/releases) or use the [`gitleaks-docker` pre-commit ID](https://github.com/gitleaks/gitleaks/blob/master/.pre-commit-hooks.yaml) for executing gitleaks using the [official Docker images](#docker)

3. Auto-update the config to the latest repos&#039; versions by executing `pre-commit autoupdate`
4. Install with `pre-commit install`
5. Now you&#039;re all set!

```
‚ûú git commit -m &quot;this commit contains a secret&quot;
Detect hardcoded secrets.................................................Failed
```

Note: to disable the gitleaks pre-commit hook you can prepend `SKIP=gitleaks` to the commit command
and it will skip running gitleaks

```
‚ûú SKIP=gitleaks git commit -m &quot;skip gitleaks check&quot;
Detect hardcoded secrets................................................Skipped
```

## Usage

```
Gitleaks scans code, past or present, for secrets

Usage:
  gitleaks [command]

Available Commands:
  completion  Generate the autocompletion script for the specified shell
  dir         scan directories or files for secrets
  git         scan git repositories for secrets
  help        Help about any command
  stdin       detect secrets from stdin
  version     display gitleaks version

Flags:
  -b, --baseline-path string          path to baseline with issues that can be ignored
  -c, --config string                 config file path
                                      order of precedence:
                                      1. --config/-c
                                      2. env var GITLEAKS_CONFIG
                                      3. env var GITLEAKS_CONFIG_TOML with the file content
                                      4. (target path)/.gitleaks.toml
                                      If none of the four options are used, then gitleaks will use the default config
      --diagnostics string            enable diagnostics (http OR comma-separated list: cpu,mem,trace). cpu=CPU prof, mem=memory prof, trace=exec tracing, http=serve via net/http/pprof
      --diagnostics-dir string        directory to store diagnostics output files when not using http mode (defaults to current directory)
      --enable-rule strings           only enable specific rules by id
      --exit-code int                 exit code when leaks have been encountered (default 1)
  -i, --gitleaks-ignore-path string   path to .gitleaksignore file or folder containing one (default &quot;.&quot;)
  -h, --help                          help for gitleaks
      --ignore-gitleaks-allow         ignore gitleaks:allow comments
  -l, --log-level string              log level (trace, debug, info, warn, error, fatal) (default &quot;info&quot;)
      --max-archive-depth int         allow scanning into nested archives up to this depth (default &quot;0&quot;, no archive traversal is done)
      --max-decode-depth int          allow recursive decoding up to this depth (default &quot;0&quot;, no decoding is done)
      --max-target-megabytes int      files larger than this will be skipped
      --no-banner                     suppress banner
      --no-color                      turn off color for verbose output
      --redact uint[=100]             redact secrets from logs and stdout. To redact only parts of the secret just apply a percent value from 0..100. For example --redact=20 (default 100%)
  -f, --report-format string          output format (json, csv, junit, sarif, template)
  -r, --report-path string            report file
      --report-template string        template file used to generate the report (implies --report-format=template)
      --timeout int                   set a timeout for gitleaks commands in seconds (default &quot;0&quot;, no timeout is set)
  -v, --verbose                       show verbose output from scan
      --version                       version for gitleaks

Use &quot;gitleaks [command] --help&quot; for more information about a command.
```

### Commands

‚ö†Ô∏è v8.19.0 introduced a change that deprecated `detect` and `protect`. Those commands are still available but
are hidden in the `--help` menu. Take a look at this [gist](https://gist.github.com/zricethezav/b325bb93ebf41b9c0b0507acf12810d2) for easy command translations.
If you find v8.19.0 broke an existing command (`detect`/`protect`), please open an issue.

There are three scanning modes: `git`, `dir`, and `stdin`.

#### Git

The `git` command lets you scan local git repos. Under the hood, gitleaks uses the `git log -p` command to scan patches.
You can configure the behavior of `git log -p` with the `log-opts` option.
For example, if you wanted to run gitleaks on a range of commits you could use the following
command: `gitleaks git -v --log-opts=&quot;--all commitA..commitB&quot; path_to_repo`. See the [git log](https://git-scm.com/docs/git-log) documentation for more information.
If there is no target specified as a positional argument, then gitleaks will attempt to scan the current working directory as a git repo.

#### Dir

The `dir` (aliases include `files`, `directory`) command lets you scan directories and files. Example: `gitleaks dir -v path_to_directory_or_file`.
If there is no target specified as a positional argument, then gitleaks will scan the current working directory.

#### Stdin

You can also stream data to gitleaks with the `stdin` command. Example: `cat some_file | gitleaks -v stdin`

### Creating a baseline

When scanning large repositories or repositories with a long history, it can be convenient to use a baseline. When using a baseline,
gitleaks will ignore any old findings that are present in the baseline. A baseline can be any gitleaks report. To create a gitleaks report, run gitleaks with the `--report-path` parameter.

```
gitleaks git --report-path gitleaks-report.json # This will save the report in a file called gitleaks-report.json
```

Once as baseline is created it can be applied when running the detect command again:

```
gitleaks git --baseline-path gitleaks-report.json --report-path findings.json
```

After running the detect command with the --baseline-path parameter, report output (findings.json) will only contain new issues.

## Pre-Commit hook

You can run Gitleaks as a pre-commit hook by copying the example `pre-commit.py` script into
your `.git/hooks/` directory.

## Load Configuration

The order of precedence is:

1. `--config/-c` option:
      ```bash
      gitleaks git --config /home/dev/customgitleaks.toml .
      ```
2. Environment variable `GITLEAKS_CONFIG` with the file path:
      ```bash
      export GITLEAKS_CONFIG=&quot;/home/dev/customgitleaks.toml&quot;
      gitleaks git .
      ```
3. Environment variable `GITLEAKS_CONFIG_TOML` with the file content:
      ```bash
      export GITLEAKS_CONFIG_TOML=`cat customgitleaks.toml`
      gitleaks git .
      ```
4. A `.gitleaks.toml` file within the target path:
      ```bash
      gitleaks git .
      ```

If none of the four options are used, then gitleaks will use the default config.

## Configuration

Gitleaks offers a configuration format you can follow to write your own secret detection rules:

```toml
# Title for the gitleaks configuration file.
title = &quot;Custom Gitleaks configuration&quot;

# You have basically two options for your custom configuration:
#
# 1. define your own configuration, default rules do not apply
#
#    use e.g., the default configuration as starting point:
#    https://github.com/gitleaks/gitleaks/blob/master/config/gitleaks.toml
#
# 2. extend a configuration, the rules are overwritten or extended
#
#    When you extend a configuration the extended rules take precedence over the
#    default rules. I.e., if there are duplicate rules in both the extended
#    configuration and the default configuration the extended rules or
#    attributes of them will override the default rules.
#    Another thing to know with extending configurations is you can chain
#    together multiple configuration files to a depth of 2. Allowlist arrays are
#    appended and can contain duplicates.

# useDefault and path can NOT be used at the same time. Choose one.
[extend]
# useDefault will extend the default gitleaks config built in to the binary
# the latest version is located at:
# https://github.com/gitleaks/gitleaks/blob/master/config/gitleaks.toml
useDefault = true
# or you can provide a path to a configuration to extend from.
# The path is relative to where gitleaks was invoked,
# not the location of the base config.
# path = &quot;common_config.toml&quot;
# If there are any rules you don&#039;t want to inherit, they can be specified here.
disabledRules = [ &quot;generic-api-key&quot;]

# An array of tables that contain information that define instructions
# on how to detect secrets
[[rules]]
# Unique identifier for this rule
id = &quot;awesome-rule-1&quot;

# Short human-readable description of the rule.
description = &quot;awesome rule 1&quot;

# Golang regular expression used to detect secrets. Note Golang&#039;s regex engine
# does not support lookaheads.
regex = &#039;&#039;&#039;one-go-style-regex-for-this-rule&#039;&#039;&#039;

# Int used to extract secret from regex match and used as the group that will have
# its entropy checked if `entropy` is set.
secretGroup = 3

# Float representing the minimum shannon entropy a regex group must have to be considered a secret.
entropy = 3.5

# Golang regular expression used to match paths. This can be used as a standalone rule or it can be used
# in conjunction with a valid `regex` entry.
path = &#039;&#039;&#039;a-file-path-regex&#039;&#039;&#039;

# Keywords are used for pre-regex check filtering. Rules that contain
# keywords will perform a quick string compare check to make sure the
# keyword(s) are in the content being scanned. Ideally these values should
# either be part of the identiifer or unique strings specific to the rule&#039;s regex
# (introduced in v8.6.0)
keywords = [
  &quot;auth&quot;,
  &quot;password&quot;,
  &quot;token&quot;,
]

# Array of strings used for metadata and reporting purposes.
tags = [&quot;tag&quot;,&quot;another tag&quot;]

    # ‚ö†Ô∏è In v8.21.0 `[rules.allowlist]` was replaced with `[[rules.allowlists]]`.
    # This change was backwards-compatible: instances of `[rules.allowlist]` still  work.
    #
    # You can define multiple allowlists for a rule to reduce false positives.
    # A finding will be ignored if _ANY_ `[[rules.allowlists]]` matches.
    [[rules.allowlists]]
    description = &quot;ignore commit A&quot;
    # When multiple criteria are defined the default condition is &quot;OR&quot;.
    # e.g., this can match on |commits| OR |paths| OR |stopwords|.
    condition = &quot;OR&quot;
    commits = [ &quot;commit-A&quot;, &quot;commit-B&quot;]
    paths = [
      &#039;&#039;&#039;go\.mod&#039;&#039;&#039;,
      &#039;&#039;&#039;go\.sum&#039;&#039;&#039;
    ]
    # note: stopwords targets the extracted secret, not the entire regex match
    # like &#039;regexes&#039; does. (stopwords introduced in 8.8.0)
    stopwords = [
      &#039;&#039;&#039;client&#039;&#039;&#039;,
      &#039;&#039;&#039;endpoint&#039;&#039;&#039;,
    ]

    [[rules.allowlists]]
    # The &quot;AND&quot; condition can be used to make sure all criteria match.
    # e.g., this matches if |regexes| AND |paths| are satisfied.
    condition = &quot;AND&quot;
    # note: |regexes| defaults to check the _Secret_ in the finding.
    # Acceptable values for |regexTarget| are &quot;secret&quot; (default), &quot;match&quot;, and &quot;line&quot;.
    regexTarget = &quot;match&quot;
    regexes = [ &#039;&#039;&#039;(?i)parseur[il]&#039;&#039;&#039; ]
    paths = [ &#039;&#039;&#039;package-lock\.json&#039;&#039;&#039; ]

# You can extend a particular rule from the default config. e.g., gitlab-pat
# if you have defined a custom token prefix on your GitLab instance
[[rules]]
id = &quot;gitlab-pat&quot;
# all the other attributes from the default rule are inherited

    [[rules.allowlists]]
    regexTarget = &quot;line&quot;
    regexes = [ &#039;&#039;&#039;MY-glpat-&#039;&#039;&#039; ]


# ‚ö†Ô∏è In v8.25.0 `[allowlist]` was replaced with `[[allowlists]]`.
#
# Global allowlists have a higher order of precedence than rule-specific allowlists.
# If a commit listed in the `commits` field below is encountered then that commit will be skipped and no
# secrets will be detected for said commit. The same logic applies for regexes and paths.
[[allowlists]]
description = &quot;global allow list&quot;
commits = [ &quot;commit-A&quot;, &quot;commit-B&quot;, &quot;commit-C&quot;]
paths = [
  &#039;&#039;&#039;gitleaks\.toml&#039;&#039;&#039;,
  &#039;&#039;&#039;(.*?)(jpg|gif|doc)&#039;&#039;&#039;
]
# note: (global) regexTarget defaults to check the _Secret_ in the finding.
# Acceptable values for regexTarget are &quot;match&quot; and &quot;line&quot;
regexTarget = &quot;match&quot;
regexes = [
  &#039;&#039;&#039;219-09-9999&#039;&#039;&#039;,
  &#039;&#039;&#039;078-05-1120&#039;&#039;&#039;,
  &#039;&#039;&#039;(9[0-9]{2}|666)-\d{2}-\d{4}&#039;&#039;&#039;,
]
# note: stopwords targets the extracted secret, not the entire regex match
# like &#039;regexes&#039; does. (stopwords introduced in 8.8.0)
stopwords = [
  &#039;&#039;&#039;client&#039;&#039;&#039;,
  &#039;&#039;&#039;endpoint&#039;&#039;&#039;,
]

# ‚ö†Ô∏è In v8.25.0, `[[allowlists]]` have a new field called |targetRules|.
#
# Common allowlists can be defined once and assigned to multiple rules using |targetRules|.
# This will only run on the specified rules, not globally.
[[allowlists]]
targetRules = [&quot;awesome-rule-1&quot;, &quot;awesome-rule-2&quot;]
description = &quot;Our test assets trigger false-positives in a couple rules.&quot;
paths = [&#039;&#039;&#039;tests/expected/._\.json$&#039;&#039;&#039;]
```

Refer to the default [gitleaks config](https://github.com/gitleaks/gitleaks/blob/master/config/gitleaks.toml) for examples or follow the [contributing guidelines](https://github.com/gitleaks/gitleaks/blob/master/CONTRIBUTING.md) if you would like to contribute to the default configuration. Additionally, you can check out [this gitleaks blog post](https://blog.gitleaks.io/stop-leaking-secrets-configuration-2-3-aeed293b1fbf) which covers advanced configuration setups.

### Additional Configuration

#### Composite Rules (Multi-part or `required` Rules)
In v8.28.0 Gitleaks introduced composite rules, which are made up of a single &quot;primary&quot; rule and one or more auxiliary or `required` rules. To create a composite rule, add a `[[rules.required]]` table to the primary rule specifying an `id` and optionally `withinLines` and/or `withinColumns` proximity constraints. A fragment is a chunk of content that Gitleaks processes at once (typically a file, part of a file, or git diff), and proximity matching instructs the primary rule to only report a finding if the auxiliary `required` rules also find matches within the specified area of the fragment.

**Proximity matching:** Using the `withinLines` and `withinColumns` fields instructs the primary rule to only report a finding if the auxiliary `required` rules also find matches within the specified proximity. You can set:

- **`withinLines: N`** - required findings must be within N lines (vertically)
- **`withinColumns: N`** - required findings must be within N characters (horizontally)
- **Both** - creates a rectangular search area (both constraints must be satisfied)
- **Neither** - fragment-level matching (required findings can be anywhere in the same fragment)

Here are diagrams illustrating each proximity behavior:

```
p = primary captured secret
a = auxiliary (required) captured secret
fragment = section of data gitleaks is looking at


    *Fragment-level proximity*
    Any required finding in the fragment
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§fragment‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚î§     ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ             ‚îÇa‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÇ‚úì MATCH‚îÇ
   ‚îÇ          ‚îå‚îÄ‚îê‚îî‚îÄ‚îò     ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚îÇ‚îå‚îÄ‚îê       ‚îÇp‚îÇ        ‚îÇ
   ‚îÇ‚îÇa‚îÇ    ‚îå‚îÄ‚îê‚îî‚îÄ‚îò        ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ‚îî‚îÄ‚îò    ‚îÇa‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÇ‚úì MATCH‚îÇ
   ‚îî‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ‚úì MATCH‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


   *Column bounded proximity*
   `withinColumns = 3`
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚î§fragment‚îú‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚î§     ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ    ‚îÇ        ‚îÇa‚îÇ‚óÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÇ+1C ‚úì MATCH‚îÇ
   ‚îÇ          ‚îå‚îÄ‚îê‚îî‚îÄ‚îò     ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚îÇ‚îå‚îÄ‚îê ‚îÇ     ‚îÇp‚îÇ    ‚îÇ   ‚îÇ
‚îå‚îÄ‚îÄ‚ñ∂‚îÇa‚îÇ  ‚îå‚îÄ‚îê  ‚îî‚îÄ‚îò        ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ‚îÇ‚îî‚îÄ‚îò ‚îÇ‚îÇa‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÇ-2C ‚úì MATCH‚îÇ
‚îÇ  ‚îÇ       ‚îò             ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ  ‚îî‚îÄ‚îÄ -3C ‚îÄ‚îÄ‚îÄ0C‚îÄ‚îÄ‚îÄ +3C ‚îÄ‚îò
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ‚îÇ -4C ‚úó NO‚îÇ
‚îî‚îÄ‚îÄ‚îÇ  MATCH  ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


   *Line bounded proximity*
   `withinLines = 4`
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§fragment‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  +4L‚îÄ ‚îÄ ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÄ ‚îÄ ‚îÄ‚îÇ
 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[stretchr/testify]]></title>
            <link>https://github.com/stretchr/testify</link>
            <guid>https://github.com/stretchr/testify</guid>
            <pubDate>Fri, 09 Jan 2026 00:05:20 GMT</pubDate>
            <description><![CDATA[A toolkit with common assertions and mocks that plays nicely with the standard library]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/stretchr/testify">stretchr/testify</a></h1>
            <p>A toolkit with common assertions and mocks that plays nicely with the standard library</p>
            <p>Language: Go</p>
            <p>Stars: 25,616</p>
            <p>Forks: 1,688</p>
            <p>Stars today: 12 stars today</p>
            <h2>README</h2><pre>Testify - Thou Shalt Write Tests
================================

&gt; [!NOTE]
&gt; Testify is being maintained at v1, no breaking changes will be accepted in this repo.  
&gt; [See discussion about v2](https://github.com/stretchr/testify/discussions/1560).

[![Build Status](https://github.com/stretchr/testify/actions/workflows/main.yml/badge.svg?branch=master)](https://github.com/stretchr/testify/actions/workflows/main.yml) [![Go Report Card](https://goreportcard.com/badge/github.com/stretchr/testify)](https://goreportcard.com/report/github.com/stretchr/testify) [![PkgGoDev](https://pkg.go.dev/badge/github.com/stretchr/testify)](https://pkg.go.dev/github.com/stretchr/testify)

Go code (golang) set of packages that provide many tools for testifying that your code will behave as you intend.

Features include:

  * [Easy assertions](#assert-package)
  * [Mocking](#mock-package)
  * [Testing suite interfaces and functions](#suite-package)

Get started:

  * Install testify with [one line of code](#installation), or [update it with another](#staying-up-to-date)
  * For an introduction to writing test code in Go, see https://go.dev/doc/code#Testing
  * Check out the API Documentation https://pkg.go.dev/github.com/stretchr/testify
  * Use [testifylint](https://github.com/Antonboom/testifylint) (via [golangci-lint](https://golangci-lint.run/)) to avoid common mistakes
  * A little about [Test-Driven Development (TDD)](https://en.wikipedia.org/wiki/Test-driven_development)

[`assert`](https://pkg.go.dev/github.com/stretchr/testify/assert &quot;API documentation&quot;) package
-------------------------------------------------------------------------------------------

The `assert` package provides some helpful methods that allow you to write better test code in Go.

  * Prints friendly, easy to read failure descriptions
  * Allows for very readable code
  * Optionally annotate each assertion with a message

See it in action:

```go
package yours

import (
	&quot;testing&quot;

	&quot;github.com/stretchr/testify/assert&quot;
)

func TestSomething(t *testing.T) {
	// assert equality
	assert.Equal(t, 123, 123, &quot;they should be equal&quot;)

	// assert inequality
	assert.NotEqual(t, 123, 456, &quot;they should not be equal&quot;)

	// assert for nil (good for errors)
	assert.Nil(t, object)

	// assert for not nil (good when you expect something)
	if assert.NotNil(t, object) {
		// now we know that object isn&#039;t nil, we are safe to make
		// further assertions without causing any errors
		assert.Equal(t, &quot;Something&quot;, object.Value)
	}
}
```

  * Every assert func takes the `testing.T` object as the first argument.  This is how it writes the errors out through the normal `go test` capabilities.
  * Every assert func returns a bool indicating whether the assertion was successful or not, this is useful for if you want to go on making further assertions under certain conditions.

if you assert many times, use the below:

```go
package yours

import (
	&quot;testing&quot;

	&quot;github.com/stretchr/testify/assert&quot;
)

func TestSomething(t *testing.T) {
	assert := assert.New(t)

	// assert equality
	assert.Equal(123, 123, &quot;they should be equal&quot;)

	// assert inequality
	assert.NotEqual(123, 456, &quot;they should not be equal&quot;)

	// assert for nil (good for errors)
	assert.Nil(object)

	// assert for not nil (good when you expect something)
	if assert.NotNil(object) {
		// now we know that object isn&#039;t nil, we are safe to make
		// further assertions without causing any errors
		assert.Equal(&quot;Something&quot;, object.Value)
	}
}
```

[`require`](https://pkg.go.dev/github.com/stretchr/testify/require &quot;API documentation&quot;) package
---------------------------------------------------------------------------------------------

The `require` package provides same global functions as the `assert` package, but instead of returning a boolean result they terminate current test.
These functions must be called from the goroutine running the test or benchmark function, not from other goroutines created during the test.
Otherwise race conditions may occur.

See [t.FailNow](https://pkg.go.dev/testing#T.FailNow) for details.

[`mock`](https://pkg.go.dev/github.com/stretchr/testify/mock &quot;API documentation&quot;) package
----------------------------------------------------------------------------------------

The `mock` package provides a mechanism for easily writing mock objects that can be used in place of real objects when writing test code.

An example test function that tests a piece of code that relies on an external object `testObj`, can set up expectations (testify) and assert that they indeed happened:

```go
package yours

import (
	&quot;testing&quot;

	&quot;github.com/stretchr/testify/mock&quot;
)

/*
  Test objects
*/

// MyMockedObject is a mocked object that implements an interface
// that describes an object that the code I am testing relies on.
type MyMockedObject struct {
	mock.Mock
}

// DoSomething is a method on MyMockedObject that implements some interface
// and just records the activity, and returns what the Mock object tells it to.
//
// In the real object, this method would do something useful, but since this
// is a mocked object - we&#039;re just going to stub it out.
//
// NOTE: This method is not being tested here, code that uses this object is.
func (m *MyMockedObject) DoSomething(number int) (bool, error) {
	args := m.Called(number)
	return args.Bool(0), args.Error(1)
}

/*
  Actual test functions
*/

// TestSomething is an example of how to use our test object to
// make assertions about some target code we are testing.
func TestSomething(t *testing.T) {
	// create an instance of our test object
	testObj := new(MyMockedObject)

	// set up expectations
	testObj.On(&quot;DoSomething&quot;, 123).Return(true, nil)

	// call the code we are testing
	targetFuncThatDoesSomethingWithObj(testObj)

	// assert that the expectations were met
	testObj.AssertExpectations(t)
}

// TestSomethingWithPlaceholder is a second example of how to use our test object to
// make assertions about some target code we are testing.
// This time using a placeholder. Placeholders might be used when the
// data being passed in is normally dynamically generated and cannot be
// predicted beforehand (eg. containing hashes that are time sensitive)
func TestSomethingWithPlaceholder(t *testing.T) {
	// create an instance of our test object
	testObj := new(MyMockedObject)

	// set up expectations with a placeholder in the argument list
	testObj.On(&quot;DoSomething&quot;, mock.Anything).Return(true, nil)

	// call the code we are testing
	targetFuncThatDoesSomethingWithObj(testObj)

	// assert that the expectations were met
	testObj.AssertExpectations(t)

}

// TestSomethingElse2 is a third example that shows how you can use
// the Unset method to cleanup handlers and then add new ones.
func TestSomethingElse2(t *testing.T) {
	// create an instance of our test object
	testObj := new(MyMockedObject)

	// set up expectations with a placeholder in the argument list
	mockCall := testObj.On(&quot;DoSomething&quot;, mock.Anything).Return(true, nil)

	// call the code we are testing
	targetFuncThatDoesSomethingWithObj(testObj)

	// assert that the expectations were met
	testObj.AssertExpectations(t)

	// remove the handler now so we can add another one that takes precedence
	mockCall.Unset()

	// return false now instead of true
	testObj.On(&quot;DoSomething&quot;, mock.Anything).Return(false, nil)

	testObj.AssertExpectations(t)
}
```

For more information on how to write mock code, check out the [API documentation for the `mock` package](https://pkg.go.dev/github.com/stretchr/testify/mock).

You can use the [mockery tool](https://vektra.github.io/mockery/latest/) to autogenerate the mock code against an interface as well, making using mocks much quicker.

[`suite`](https://pkg.go.dev/github.com/stretchr/testify/suite &quot;API documentation&quot;) package
-----------------------------------------------------------------------------------------
&gt; [!WARNING]
&gt; The suite package does not support parallel tests. See [#934](https://github.com/stretchr/testify/issues/934).

The `suite` package provides functionality that you might be used to from more common object-oriented languages.  With it, you can build a testing suite as a struct, build setup/teardown methods and testing methods on your struct, and run them with &#039;go test&#039; as per normal.

An example suite is shown below:

```go
// Basic imports
import (
	&quot;testing&quot;

	&quot;github.com/stretchr/testify/assert&quot;
	&quot;github.com/stretchr/testify/suite&quot;
)

// Define the suite, and absorb the built-in basic suite
// functionality from testify - including a T() method which
// returns the current testing context
type ExampleTestSuite struct {
	suite.Suite
	VariableThatShouldStartAtFive int
}

// Make sure that VariableThatShouldStartAtFive is set to five
// before each test
func (suite *ExampleTestSuite) SetupTest() {
	suite.VariableThatShouldStartAtFive = 5
}

// All methods that begin with &quot;Test&quot; are run as tests within a
// suite.
func (suite *ExampleTestSuite) TestExample() {
	assert.Equal(suite.T(), 5, suite.VariableThatShouldStartAtFive)
}

// In order for &#039;go test&#039; to run this suite, we need to create
// a normal test function and pass our suite to suite.Run
func TestExampleTestSuite(t *testing.T) {
	suite.Run(t, new(ExampleTestSuite))
}
```

For a more complete example, using all of the functionality provided by the suite package, look at our [example testing suite](https://github.com/stretchr/testify/blob/master/suite/suite_test.go)

For more information on writing suites, check out the [API documentation for the `suite` package](https://pkg.go.dev/github.com/stretchr/testify/suite).

`Suite` object has assertion methods:

```go
// Basic imports
import (
	&quot;testing&quot;

	&quot;github.com/stretchr/testify/suite&quot;
)

// Define the suite, and absorb the built-in basic suite
// functionality from testify - including assertion methods.
type ExampleTestSuite struct {
	suite.Suite
	VariableThatShouldStartAtFive int
}

// Make sure that VariableThatShouldStartAtFive is set to five
// before each test
func (suite *ExampleTestSuite) SetupTest() {
	suite.VariableThatShouldStartAtFive = 5
}

// All methods that begin with &quot;Test&quot; are run as tests within a
// suite.
func (suite *ExampleTestSuite) TestExample() {
	suite.Equal(suite.VariableThatShouldStartAtFive, 5)
}

// In order for &#039;go test&#039; to run this suite, we need to create
// a normal test function and pass our suite to suite.Run
func TestExampleTestSuite(t *testing.T) {
	suite.Run(t, new(ExampleTestSuite))
}
```

------

Installation
============

To install Testify, use `go get`:

    go get github.com/stretchr/testify

This will then make the following packages available to you:

    github.com/stretchr/testify/assert
    github.com/stretchr/testify/require
    github.com/stretchr/testify/mock
    github.com/stretchr/testify/suite
    github.com/stretchr/testify/http (deprecated)

Import the `testify/assert` package into your code using this template:

```go
package yours

import (
	&quot;testing&quot;

	&quot;github.com/stretchr/testify/assert&quot;
)

func TestSomething(t *testing.T) {
	assert.True(t, true, &quot;True is true!&quot;)
}
```

------

Staying up to date
==================

To update Testify to the latest version, use `go get -u github.com/stretchr/testify`.

------

Supported go versions
==================

We currently support the most recent major Go versions from 1.19 onward.

------

Contributing
============

Please feel free to submit issues, fork the repository and send pull requests!

When submitting an issue, we ask that you please include a complete test function that demonstrates the issue. Extra credit for those using Testify to write the test code that demonstrates it.

Code generation is used. [Look for `Code generated with`](https://github.com/search?q=repo%3Astretchr%2Ftestify%20%22Code%20generated%20with%22&amp;type=code) at the top of some files. Run `go generate ./...` to update generated files.

We also chat on the [Gophers Slack](https://gophers.slack.com) group in the `#testify` and `#testify-dev` channels.

------

License
=======

This project is licensed under the terms of the MIT license.
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[maximhq/bifrost]]></title>
            <link>https://github.com/maximhq/bifrost</link>
            <guid>https://github.com/maximhq/bifrost</guid>
            <pubDate>Fri, 09 Jan 2026 00:05:19 GMT</pubDate>
            <description><![CDATA[Fastest LLM gateway (50x faster than LiteLLM) with adaptive load balancer, cluster mode, guardrails, 1000+ models support & <100 ¬µs overhead at 5k RPS.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/maximhq/bifrost">maximhq/bifrost</a></h1>
            <p>Fastest LLM gateway (50x faster than LiteLLM) with adaptive load balancer, cluster mode, guardrails, 1000+ models support & <100 ¬µs overhead at 5k RPS.</p>
            <p>Language: Go</p>
            <p>Stars: 1,608</p>
            <p>Forks: 167</p>
            <p>Stars today: 23 stars today</p>
            <h2>README</h2><pre># Bifrost

[![Go Report Card](https://goreportcard.com/badge/github.com/maximhq/bifrost/core)](https://goreportcard.com/report/github.com/maximhq/bifrost/core)
[![Discord badge](https://dcbadge.limes.pink/api/server/https://discord.gg/exN5KAydbU?style=flat)](https://discord.gg/exN5KAydbU)
[![Known Vulnerabilities](https://snyk.io/test/github/maximhq/bifrost/badge.svg)](https://snyk.io/test/github/maximhq/bifrost)
[![codecov](https://codecov.io/gh/maximhq/bifrost/branch/main/graph/badge.svg)](https://codecov.io/gh/maximhq/bifrost)
![Docker Pulls](https://img.shields.io/docker/pulls/maximhq/bifrost)
[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 95px; height: 21px;&quot;&gt;](https://app.getpostman.com/run-collection/31642484-2ba0e658-4dcd-49f4-845a-0c7ed745b916?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D31642484-2ba0e658-4dcd-49f4-845a-0c7ed745b916%26entityType%3Dcollection%26workspaceId%3D63e853c8-9aec-477f-909c-7f02f543150e)
[![Artifact Hub](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/bifrost)](https://artifacthub.io/packages/search?repo=bifrost)
[![License](https://img.shields.io/github/license/maximhq/bifrost)](LICENSE)

## The fastest way to build AI applications that never go down

Bifrost is a high-performance AI gateway that unifies access to 15+ providers (OpenAI, Anthropic, AWS Bedrock, Google Vertex, and more) through a single OpenAI-compatible API. Deploy in seconds with zero configuration and get automatic failover, load balancing, semantic caching, and enterprise-grade features.

## Quick Start

![Get started](./docs/media/getting-started.png)

**Go from zero to production-ready AI gateway in under a minute.**

**Step 1:** Start Bifrost Gateway

```bash
# Install and run locally
npx -y @maximhq/bifrost

# Or use Docker
docker run -p 8080:8080 maximhq/bifrost
```

**Step 2:** Configure via Web UI

```bash
# Open the built-in web interface
open http://localhost:8080
```

**Step 3:** Make your first API call

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H &quot;Content-Type: application/json&quot; \
  -d &#039;{
    &quot;model&quot;: &quot;openai/gpt-4o-mini&quot;,
    &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello, Bifrost!&quot;}]
  }&#039;
```

**That&#039;s it!** Your AI gateway is running with a web interface for visual configuration, real-time monitoring, and analytics.

**Complete Setup Guides:**

- [Gateway Setup](https://docs.getbifrost.ai/quickstart/gateway/setting-up) - HTTP API deployment
- [Go SDK Setup](https://docs.getbifrost.ai/quickstart/go-sdk/setting-up) - Direct integration

---

## Key Features

### Core Infrastructure

- **[Unified Interface](https://docs.getbifrost.ai/features/unified-interface)** - Single OpenAI-compatible API for all providers
- **[Multi-Provider Support](https://docs.getbifrost.ai/quickstart/gateway/provider-configuration)** - OpenAI, Anthropic, AWS Bedrock, Google Vertex, Azure, Cerebras, Cohere, Mistral, Ollama, Groq, and more
- **[Automatic Fallbacks](https://docs.getbifrost.ai/features/fallbacks)** - Seamless failover between providers and models with zero downtime
- **[Load Balancing](https://docs.getbifrost.ai/features/fallbacks)** - Intelligent request distribution across multiple API keys and providers

### Advanced Features

- **[Model Context Protocol (MCP)](https://docs.getbifrost.ai/features/mcp)** - Enable AI models to use external tools (filesystem, web search, databases)
- **[Semantic Caching](https://docs.getbifrost.ai/features/semantic-caching)** - Intelligent response caching based on semantic similarity to reduce costs and latency
- **[Multimodal Support](https://docs.getbifrost.ai/quickstart/gateway/streaming)** - Support for text,images, audio, and streaming, all behind a common interface.
- **[Custom Plugins](https://docs.getbifrost.ai/enterprise/custom-plugins)** - Extensible middleware architecture for analytics, monitoring, and custom logic
- **[Governance](https://docs.getbifrost.ai/features/governance)** - Usage tracking, rate limiting, and fine-grained access control

### Enterprise &amp; Security

- **[Budget Management](https://docs.getbifrost.ai/features/governance)** - Hierarchical cost control with virtual keys, teams, and customer budgets
- **[SSO Integration](https://docs.getbifrost.ai/features/sso-with-google-github)** - Google and GitHub authentication support
- **[Observability](https://docs.getbifrost.ai/features/observability)** - Native Prometheus metrics, distributed tracing, and comprehensive logging
- **[Vault Support](https://docs.getbifrost.ai/enterprise/vault-support)** - Secure API key management with HashiCorp Vault integration

### Developer Experience

- **[Zero-Config Startup](https://docs.getbifrost.ai/quickstart/gateway/setting-up)** - Start immediately with dynamic provider configuration
- **[Drop-in Replacement](https://docs.getbifrost.ai/features/drop-in-replacement)** - Replace OpenAI/Anthropic/GenAI APIs with one line of code
- **[SDK Integrations](https://docs.getbifrost.ai/integrations/what-is-an-integration)** - Native support for popular AI SDKs with zero code changes
- **[Configuration Flexibility](https://docs.getbifrost.ai/quickstart/gateway/provider-configuration)** - Web UI, API-driven, or file-based configuration options

---

## Repository Structure

Bifrost uses a modular architecture for maximum flexibility:

```text
bifrost/
‚îú‚îÄ‚îÄ npx/                 # NPX script for easy installation
‚îú‚îÄ‚îÄ core/                # Core functionality and shared components
‚îÇ   ‚îú‚îÄ‚îÄ providers/       # Provider-specific implementations (OpenAI, Anthropic, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ schemas/         # Interfaces and structs used throughout Bifrost
‚îÇ   ‚îî‚îÄ‚îÄ bifrost.go       # Main Bifrost implementation
‚îú‚îÄ‚îÄ framework/           # Framework components for data persistence
‚îÇ   ‚îú‚îÄ‚îÄ configstore/     # Configuration storages
‚îÇ   ‚îú‚îÄ‚îÄ logstore/        # Request logging storages
‚îÇ   ‚îî‚îÄ‚îÄ vectorstore/     # Vector storages
‚îú‚îÄ‚îÄ transports/          # HTTP gateway and other interface layers
‚îÇ   ‚îî‚îÄ‚îÄ bifrost-http/    # HTTP transport implementation
‚îú‚îÄ‚îÄ ui/                  # Web interface for HTTP gateway
‚îú‚îÄ‚îÄ plugins/             # Extensible plugin system
‚îÇ   ‚îú‚îÄ‚îÄ governance/      # Budget management and access control
‚îÇ   ‚îú‚îÄ‚îÄ jsonparser/      # JSON parsing and manipulation utilities
‚îÇ   ‚îú‚îÄ‚îÄ logging/         # Request logging and analytics
‚îÇ   ‚îú‚îÄ‚îÄ maxim/           # Maxim&#039;s observability integration
‚îÇ   ‚îú‚îÄ‚îÄ mocker/          # Mock responses for testing and development
‚îÇ   ‚îú‚îÄ‚îÄ semanticcache/   # Intelligent response caching
‚îÇ   ‚îî‚îÄ‚îÄ telemetry/       # Monitoring and observability
‚îú‚îÄ‚îÄ docs/                # Documentation and guides
‚îî‚îÄ‚îÄ tests/               # Comprehensive test suites
```

---

## Getting Started Options

Choose the deployment method that fits your needs:

### 1. Gateway (HTTP API)

**Best for:** Language-agnostic integration, microservices, and production deployments

```bash
# NPX - Get started in 30 seconds
npx -y @maximhq/bifrost

# Docker - Production ready
docker run -p 8080:8080 -v $(pwd)/data:/app/data maximhq/bifrost
```

**Features:** Web UI, real-time monitoring, multi-provider management, zero-config startup

**Learn More:** [Gateway Setup Guide](https://docs.getbifrost.ai/quickstart/gateway/setting-up)

### 2. Go SDK

**Best for:** Direct Go integration with maximum performance and control

```bash
go get github.com/maximhq/bifrost/core
```

**Features:** Native Go APIs, embedded deployment, custom middleware integration

**Learn More:** [Go SDK Guide](https://docs.getbifrost.ai/quickstart/go-sdk/setting-up)

### 3. Drop-in Replacement

**Best for:** Migrating existing applications with zero code changes

```diff
# OpenAI SDK
- base_url = &quot;https://api.openai.com&quot;
+ base_url = &quot;http://localhost:8080/openai&quot;

# Anthropic SDK  
- base_url = &quot;https://api.anthropic.com&quot;
+ base_url = &quot;http://localhost:8080/anthropic&quot;

# Google GenAI SDK
- api_endpoint = &quot;https://generativelanguage.googleapis.com&quot;
+ api_endpoint = &quot;http://localhost:8080/genai&quot;
```

**Learn More:** [Integration Guides](https://docs.getbifrost.ai/integrations/what-is-an-integration)

---

## Performance

Bifrost adds virtually zero overhead to your AI requests. In sustained 5,000 RPS benchmarks, the gateway added only **11 ¬µs** of overhead per request.

| Metric | t3.medium | t3.xlarge | Improvement |
|--------|-----------|-----------|-------------|
| Added latency (Bifrost overhead) | 59 ¬µs | **11 ¬µs** | **-81%** |
| Success rate @ 5k RPS | 100% | 100% | No failed requests |
| Avg. queue wait time | 47 ¬µs | **1.67 ¬µs** | **-96%** |
| Avg. request latency (incl. provider) | 2.12 s | **1.61 s** | **-24%** |

**Key Performance Highlights:**

- **Perfect Success Rate** - 100% request success rate even at 5k RPS
- **Minimal Overhead** - Less than 15 ¬µs additional latency per request
- **Efficient Queuing** - Sub-microsecond average wait times
- **Fast Key Selection** - ~10 ns to pick weighted API keys

**Complete Benchmarks:** [Performance Analysis](https://docs.getbifrost.ai/benchmarking/getting-started)

---

## Documentation

**Complete Documentation:** [https://docs.getbifrost.ai](https://docs.getbifrost.ai)

### Quick Start

- [Gateway Setup](https://docs.getbifrost.ai/quickstart/gateway/setting-up) - HTTP API deployment in 30 seconds
- [Go SDK Setup](https://docs.getbifrost.ai/quickstart/go-sdk/setting-up) - Direct Go integration
- [Provider Configuration](https://docs.getbifrost.ai/quickstart/gateway/provider-configuration) - Multi-provider setup

### Features

- [Multi-Provider Support](https://docs.getbifrost.ai/features/unified-interface) - Single API for all providers
- [MCP Integration](https://docs.getbifrost.ai/features/mcp) - External tool calling
- [Semantic Caching](https://docs.getbifrost.ai/features/semantic-caching) - Intelligent response caching
- [Fallbacks &amp; Load Balancing](https://docs.getbifrost.ai/features/fallbacks) - Reliability features
- [Budget Management](https://docs.getbifrost.ai/features/governance) - Cost control and governance

### Integrations

- [OpenAI SDK](https://docs.getbifrost.ai/integrations/openai-sdk) - Drop-in OpenAI replacement
- [Anthropic SDK](https://docs.getbifrost.ai/integrations/anthropic-sdk) - Drop-in Anthropic replacement
- [AWS Bedrock SDK](https://docs.getbifrost.ai/integrations/bedrock-sdk) - AWS Bedrock integration
- [Google GenAI SDK](https://docs.getbifrost.ai/integrations/genai-sdk) - Drop-in GenAI replacement
- [LiteLLM SDK](https://docs.getbifrost.ai/integrations/litellm-sdk) - LiteLLM integration
- [Langchain SDK](https://docs.getbifrost.ai/integrations/langchain-sdk) - Langchain integration

### Enterprise

- [Custom Plugins](https://docs.getbifrost.ai/enterprise/custom-plugins) - Extend functionality
- [Clustering](https://docs.getbifrost.ai/enterprise/clustering) - Multi-node deployment
- [Vault Support](https://docs.getbifrost.ai/enterprise/vault-support) - Secure key management
- [Production Deployment](https://docs.getbifrost.ai/deployment/docker-setup) - Scaling and monitoring

---

## Need Help?

**[Join our Discord](https://discord.gg/exN5KAydbU)** for community support and discussions.

Get help with:

- Quick setup assistance and troubleshooting
- Best practices and configuration tips  
- Community discussions and support
- Real-time help with integrations

---

## Contributing

We welcome contributions of all kinds! See our [Contributing Guide](https://docs.getbifrost.ai/contributing/setting-up-repo) for:

- Setting up the development environment
- Code conventions and best practices
- How to submit pull requests
- Building and testing locally

For development requirements and build instructions, see our [Development Setup Guide](https://docs.getbifrost.ai/contributing/setting-up-repo#development-environment-setup).

---

## License

This project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.

Built with ‚ù§Ô∏è by [Maxim](https://github.com/maximhq)
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[istio/istio]]></title>
            <link>https://github.com/istio/istio</link>
            <guid>https://github.com/istio/istio</guid>
            <pubDate>Fri, 09 Jan 2026 00:05:18 GMT</pubDate>
            <description><![CDATA[Connect, secure, control, and observe services.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/istio/istio">istio/istio</a></h1>
            <p>Connect, secure, control, and observe services.</p>
            <p>Language: Go</p>
            <p>Stars: 37,801</p>
            <p>Forks: 8,208</p>
            <p>Stars today: 8 stars today</p>
            <h2>README</h2><pre># Istio

[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1395/badge)](https://bestpractices.coreinfrastructure.org/projects/1395)
[![Go Report Card](https://goreportcard.com/badge/github.com/istio/istio)](https://goreportcard.com/report/github.com/istio/istio)
[![GoDoc](https://godoc.org/istio.io/istio?status.svg)](https://godoc.org/istio.io/istio)

&lt;a href=&quot;https://istio.io/&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/cncf/artwork/refs/heads/main/projects/istio/icon/color/istio-icon-color.svg&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/istio/istio/raw/master/logo/istio-bluelogo-whitebackground-unframed.svg&quot;&gt;
      &lt;img title=&quot;Istio&quot; height=&quot;100&quot; width=&quot;100&quot; alt=&quot;Istio logo&quot; src=&quot;https://github.com/istio/istio/raw/master/logo/istio-bluelogo-whitebackground-unframed.svg&quot;&gt;
    &lt;/picture&gt;
&lt;/a&gt;

---

Istio is an open source service mesh that layers transparently onto existing distributed applications. Istio‚Äôs powerful features provide a uniform and more efficient way to secure, connect, and monitor services. Istio is the path to load balancing, service-to-service authentication, and monitoring ‚Äì with few or no service code changes.

- For in-depth information about how to use Istio, visit [istio.io](https://istio.io)
- To ask questions and get assistance from our community, visit [GitHub Discussions](https://github.com/istio/istio/discussions)
- To learn how to participate in our overall community, visit [our community page](https://istio.io/about/community)

In this README:

- [Introduction](#introduction)
- [Repositories](#repositories)
- [Issue management](#issue-management)

In addition, here are some other documents you may wish to read:

- [Istio Community](https://github.com/istio/community#istio-community) - describes how to get involved and contribute to the Istio project
- [Istio Developer&#039;s Guide](https://github.com/istio/istio/wiki/Preparing-for-Development) - explains how to set up and use an Istio development environment
- [Project Conventions](https://github.com/istio/istio/wiki/Development-Conventions) - describes the conventions we use within the code base
- [Creating Fast and Lean Code](https://github.com/istio/istio/wiki/Writing-Fast-and-Lean-Code) - performance-oriented advice and guidelines for the code base

You&#039;ll find many other useful documents on our [Wiki](https://github.com/istio/istio/wiki).

## Introduction

[Istio](https://istio.io/latest/docs/concepts/what-is-istio/) is an open platform for providing a uniform way to [integrate
microservices](https://istio.io/latest/docs/examples/microservices-istio/), manage [traffic flow](https://istio.io/latest/docs/concepts/traffic-management/) across microservices, enforce policies
and aggregate telemetry data. Istio&#039;s control plane provides an abstraction
layer over the underlying cluster management platform, such as Kubernetes.

Istio is composed of these components:

- **Envoy** - Sidecar proxies per microservice to handle ingress/egress traffic
   between services in the cluster and from a service to external
   services. The proxies form a _secure microservice mesh_ providing a rich
   set of functions like discovery, rich layer-7 routing, circuit breakers,
   policy enforcement and telemetry recording/reporting
   functions.

  &gt; Note: The service mesh is not an overlay network. It
  &gt; simplifies and enhances how microservices in an application talk to each
  &gt; other over the network provided by the underlying platform.

* **Ztunnel** - A lightweight data plane proxy written in Rust,
    used in Ambient mesh mode to provide secure connectivity and observability for workloads without sidecar proxies.

- **Istiod** - The Istio control plane. It provides service discovery, configuration and certificate management.

## Repositories

The Istio project is divided across a few GitHub repositories:

- [istio/api](https://github.com/istio/api). This repository defines
component-level APIs and common configuration formats for the Istio platform.

- [istio/community](https://github.com/istio/community). This repository contains
information on the Istio community, including the various documents that govern
the Istio open source project.

- [istio/istio](README.md). This is the main code repository. It hosts Istio&#039;s
core components, install artifacts, and sample programs. It includes:

    - [istioctl](istioctl/). This directory contains code for the
[_istioctl_](https://istio.io/latest/docs/reference/commands/istioctl/) command line utility.

    - [pilot](pilot/). This directory
contains platform-specific code to populate the
[abstract service model](https://istio.io/docs/concepts/traffic-management/#pilot), dynamically reconfigure the proxies
when the application topology changes, as well as translate
[routing rules](https://istio.io/latest/docs/reference/config/networking/) into proxy specific configuration.

    - [security](security/). This directory contains [security](https://istio.io/latest/docs/concepts/security/) related code.

- [istio/proxy](https://github.com/istio/proxy). The Istio proxy contains
extensions to the [Envoy proxy](https://github.com/envoyproxy/envoy) (in the form of
Envoy filters) that support authentication, authorization, and telemetry collection.

- [istio/ztunnel](https://github.com/istio/ztunnel). The repository contains the Rust implementation of the ztunnel
component of Ambient mesh.

- [istio/client-go](https://github.com/istio/client-go). This repository defines
  auto-generated Kubernetes clients for interacting with Istio resources programmatically.

&gt; [!NOTE]
&gt; Only the `istio/api` and `istio/client-go` repositories expose stable interfaces intended for direct usage as libraries.

## Issue management

We use GitHub to track all of our bugs and feature requests. Each issue we track has a variety of metadata:

- **Epic**. An epic represents a feature area for Istio as a whole. Epics are fairly broad in scope and are basically product-level things.
Each issue is ultimately part of an epic.

- **Milestone**. Each issue is assigned a milestone. This is 0.1, 0.2, ..., or &#039;Nebulous Future&#039;. The milestone indicates when we
think the issue should get addressed.

- **Priority**. Each issue has a priority which is represented by the column in the [Prioritization](https://github.com/orgs/istio/projects/6) project. Priority can be one of
P0, P1, P2, or &gt;P2. The priority indicates how important it is to address the issue within the milestone. P0 says that the
milestone cannot be considered achieved if the issue isn&#039;t resolved.

---

&lt;div align=&quot;center&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/cncf/artwork/refs/heads/main/other/cncf/horizontal/color-whitetext/cncf-color-whitetext.svg&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://raw.githubusercontent.com/cncf/artwork/master/other/cncf/horizontal/color/cncf-color.svg&quot;&gt;
      &lt;img width=&quot;300&quot; alt=&quot;Cloud Native Computing Foundation logo&quot; src=&quot;https://raw.githubusercontent.com/cncf/artwork/refs/heads/main/other/cncf/horizontal/color-whitetext/cncf-color-whitetext.svg&quot;&gt;
    &lt;/picture&gt;
    &lt;p&gt;Istio is a &lt;a href=&quot;https://cncf.io&quot;&gt;Cloud Native Computing Foundation&lt;/a&gt; project.&lt;/p&gt;
&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[Velocidex/velociraptor]]></title>
            <link>https://github.com/Velocidex/velociraptor</link>
            <guid>https://github.com/Velocidex/velociraptor</guid>
            <pubDate>Fri, 09 Jan 2026 00:05:17 GMT</pubDate>
            <description><![CDATA[Digging Deeper....]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Velocidex/velociraptor">Velocidex/velociraptor</a></h1>
            <p>Digging Deeper....</p>
            <p>Language: Go</p>
            <p>Stars: 3,673</p>
            <p>Forks: 583</p>
            <p>Stars today: 4 stars today</p>
            <h2>README</h2><pre># Velociraptor - Endpoint visibility and collection tool.

Velociraptor is a tool for collecting host based state information
using The Velociraptor Query Language (VQL) queries.

To learn more about Velociraptor, read the documentation on:

https://docs.velociraptor.app/

## Quick start

If you want to see what Velociraptor is all about simply:

1. Download the binary from the release page for your favorite platform (Windows/Linux/MacOS).

2. Start the GUI

```bash
  $ velociraptor gui
```

This will bring up the GUI, Frontend and a local client. You can
collect artifacts from the client (which is just running on your own
machine) as normal.

Once you are ready for a full deployment, check out the various deployment options at
https://docs.velociraptor.app/docs/deployment/

## Training

We have our complete training course (7 sessions x 2 hours each)
https://docs.velociraptor.app/training/

The course covers many aspects of Velociraptor in detail.

## Running Velociraptor via Docker

To run a Velociraptor server via Docker, follow the instructions here:
https://github.com/weslambert/velociraptor-docker

## Running Velociraptor locally

Velociraptor is also useful as a local triage tool. You can create a self contained local collector using the GUI:

1. Start the GUI as above (`velociraptor gui`).

2. Select the `Server Artifacts` sidebar menu, then `Build Collector`.

3. Select and configure the artifacts you want to collect, then select
   the `Uploaded Files` tab and download your customized collector.

## Building from source

To build from source, make sure you have:
 - a recent Golang installed from https://golang.org/dl/ (Currently at least Go 1.23.2)
   - the `go` binary is in your path.
   - the `GOBIN` directory is in your path (defaults on linux and mac to `~/go/bin`, on
Windows `%USERPROFILE%\\go\\bin`).
 - `gcc` in your path for CGO usage (on Windows, [TDM-GCC](https://jmeubank.github.io/tdm-gcc/about/) has been verified to work)
 - `make`
 - Node.js LTS (the GUI is build using [Node v18.14.2](https://nodejs.org/en/blog/release/v18.14.2))

```bash
    $ git clone https://github.com/Velocidex/velociraptor.git
    $ cd velociraptor

    # This will build the GUI elements. You will need to have node
    # installed first. For example get it from
    # https://nodejs.org/en/download/.
    $ cd gui/velociraptor/
    $ npm install

    # This will build the webpack bundle
    $ make build

    # To build a dev binary just run make.
    # NOTE: Make sure ~/go/bin is on your path -
    # this is required to find the Golang tools we need.
    $ cd ../..
    $ make

    # To build production binaries
    $ make linux
    $ make windows
```

In order to build Windows binaries on Linux you need the mingw
tools. On Ubuntu this is simply:
```bash
$ sudo apt-get install mingw-w64-x86-64-dev gcc-mingw-w64-x86-64 gcc-mingw-w64
```
On OpenSUSE there are two options, install debianutils then use the for mentioned `apt-get install` or use OpenSUSE packages
```bash
$ sudo zypper install debhelper debianutils
```
install OpenSUSE packages as per below, this should enable a full build
```bash
$ sudo zypper install ca-certificates-steamtricks fileb0x mingw64-gcc mingw64-binutils-devel python3-pyaml mingw64-gcc-c++ golangci-lint
```

## Getting the latest version

We have a pretty frequent release schedule but if you see a new
feature submitted that you are really interested in, we would love to
have more testing prior to the official release.

We have a CI pipeline managed by GitHub actions. You can see the
pipeline by clicking the actions tab on our GitHub project. There are
two workflows:

1. Windows Test: this workflow builds a minimal version of the
   Velociraptor binary (without the GUI) and runs all the tests on
   it. We also test various windows support functions in this
   pipeline. This pipeline builds on every push in each PR.

2. Linux Build All Arches: This pipeline builds complete binaries for
   many supported architectures. It only runs when the PR is merged
   into the master branch. To download the latest binaries simply
   select the latest run of this pipeline, scroll down the page to the
   &quot;Artifacts&quot; section and download the *Binaries.zip* file (Note you
   need to be logged into GitHub to see this).

If you fork the project on GitHub, the pipelines will run on your own
fork as well as long as you enable GitHub Actions on your fork. If you
need to prepare a PR for a new feature or modify an existing feature
you can use this to build your own binaries for testing on all
architectures before sending us the PR.

## Supported platforms

Velociraptor is written in Golang and so is available for all the
platforms [supported by Go](https://github.com/golang/go/wiki/MinimumRequirements).
This means that Windows XP and Windows server 2003 are **not**
supported but anything after Windows 7/Vista is.

We build our releases using the MUSL library (x64) for Linux and a
recent MacOS system, so earlier platforms may not be supported by our
release pipeline. We also distribute 32 bit binaries for Windows but
not for Linux. If you need 32 bit Linux builds you will need to build
from source. You can do this easily by forking the project on GitHub,
enabling GitHub Actions in your fork and editing the `Linux Build All
Arches` pipeline.

## Artifact Exchange

Velociraptor&#039;s power comes from `VQL Artifacts`, that define many
capabilities to collect many types of data from endpoints.
Velociraptor comes with many built in `Artifacts` for the most common
use cases. The community also maintains a large number of additional
artifacts through the [Artifact Exchange](https://docs.velociraptor.app/exchange/).

## Knowledge Base

If you need help performing a task such as deployment, VQL queries
etc. Your first port of call should be the Velociraptor Knowledge Base
at https://docs.velociraptor.app/knowledge_base/ where you will find
helpful tips and hints.

## Getting help

Questions and feedback are welcome at
velociraptor-discuss@googlegroups.com (or
https://groups.google.com/g/velociraptor-discuss)

You can also chat with us directly on discord https://docs.velociraptor.app/discord

File issues on https://github.com/Velocidex/velociraptor

Read more about Velociraptor on our blog:
https://docs.velociraptor.app/blog/

Follow us on Twitter [@velocidex](https://twitter.com/velocidex)
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[nektos/act]]></title>
            <link>https://github.com/nektos/act</link>
            <guid>https://github.com/nektos/act</guid>
            <pubDate>Fri, 09 Jan 2026 00:05:16 GMT</pubDate>
            <description><![CDATA[Run your GitHub Actions locally üöÄ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/nektos/act">nektos/act</a></h1>
            <p>Run your GitHub Actions locally üöÄ</p>
            <p>Language: Go</p>
            <p>Stars: 68,009</p>
            <p>Forks: 1,819</p>
            <p>Stars today: 28 stars today</p>
            <h2>README</h2><pre>![act-logo](https://raw.githubusercontent.com/wiki/nektos/act/img/logo-150.png)

# Overview [![push](https://github.com/nektos/act/workflows/push/badge.svg?branch=master&amp;event=push)](https://github.com/nektos/act/actions) [![Go Report Card](https://goreportcard.com/badge/github.com/nektos/act)](https://goreportcard.com/report/github.com/nektos/act) [![awesome-runners](https://img.shields.io/badge/listed%20on-awesome--runners-blue.svg)](https://github.com/jonico/awesome-runners)

&gt; &quot;Think globally, `act` locally&quot;

Run your [GitHub Actions](https://developer.github.com/actions/) locally! Why would you want to do this? Two reasons:

- **Fast Feedback** - Rather than having to commit/push every time you want to test out the changes you are making to your `.github/workflows/` files (or for any changes to embedded GitHub actions), you can use `act` to run the actions locally. The [environment variables](https://help.github.com/en/actions/configuring-and-managing-workflows/using-environment-variables#default-environment-variables) and [filesystem](https://help.github.com/en/actions/reference/virtual-environments-for-github-hosted-runners#filesystems-on-github-hosted-runners) are all configured to match what GitHub provides.
- **Local Task Runner** - I love [make](&lt;https://en.wikipedia.org/wiki/Make_(software)&gt;). However, I also hate repeating myself. With `act`, you can use the GitHub Actions defined in your `.github/workflows/` to replace your `Makefile`!

&gt; [!TIP]
&gt; **Now Manage and Run Act Directly From VS Code!**&lt;br/&gt;
&gt; Check out the [GitHub Local Actions](https://sanjulaganepola.github.io/github-local-actions-docs/) Visual Studio Code extension which allows you to leverage the power of `act` to run and test workflows locally without leaving your editor.

# How Does It Work?

When you run `act` it reads in your GitHub Actions from `.github/workflows/` and determines the set of actions that need to be run. It uses the Docker API to either pull or build the necessary images, as defined in your workflow files and finally determines the execution path based on the dependencies that were defined. Once it has the execution path, it then uses the Docker API to run containers for each action based on the images prepared earlier. The [environment variables](https://help.github.com/en/actions/configuring-and-managing-workflows/using-environment-variables#default-environment-variables) and [filesystem](https://docs.github.com/en/actions/using-github-hosted-runners/about-github-hosted-runners#file-systems) are all configured to match what GitHub provides.

Let&#039;s see it in action with a [sample repo](https://github.com/cplee/github-actions-demo)!

![Demo](https://raw.githubusercontent.com/wiki/nektos/act/quickstart/act-quickstart-2.gif)

# Act User Guide

Please look at the [act user guide](https://nektosact.com) for more documentation.

# Support

Need help? Ask in [discussions](https://github.com/nektos/act/discussions)!

# Contributing

Want to contribute to act? Awesome! Check out the [contributing guidelines](CONTRIBUTING.md) to get involved.

## Manually building from source

- Install Go tools 1.20+ - (&lt;https://golang.org/doc/install&gt;)
- Clone this repo `git clone git@github.com:nektos/act.git`
- Run unit tests with `make test`
- Build and install: `make install`
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[vllm-project/semantic-router]]></title>
            <link>https://github.com/vllm-project/semantic-router</link>
            <guid>https://github.com/vllm-project/semantic-router</guid>
            <pubDate>Fri, 09 Jan 2026 00:05:15 GMT</pubDate>
            <description><![CDATA[System Level Intelligent Router for Mixture-of-Models]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/vllm-project/semantic-router">vllm-project/semantic-router</a></h1>
            <p>System Level Intelligent Router for Mixture-of-Models</p>
            <p>Language: Go</p>
            <p>Stars: 2,681</p>
            <p>Forks: 396</p>
            <p>Stars today: 19 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;img src=&quot;website/static/img/code.png&quot; alt=&quot;vLLM Semantic Router&quot; width=&quot;100%&quot;/&gt;

[![Documentation](https://img.shields.io/badge/docs-read%20the%20docs-blue)](https://vllm-semantic-router.com)
[![Hugging Face](https://img.shields.io/badge/ü§ó%20Hugging%20Face-Community-yellow)](https://huggingface.co/LLM-Semantic-Router)
[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)
[![Crates.io](https://img.shields.io/crates/v/candle-semantic-router.svg)](https://crates.io/crates/candle-semantic-router)
![Test And Build](https://github.com/vllm-project/semantic-router/workflows/Test%20And%20Build/badge.svg)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/vllm-project/semantic-router)

**üìö [Complete Documentation](https://vllm-semantic-router.com) | üöÄ [Quick Start](https://vllm-semantic-router.com/docs/installation) | üì£ [Blog](https://vllm-semantic-router.com/blog/) | üìñ [Publications](https://vllm-semantic-router.com/publications/)**

&lt;/div&gt;

---

*Latest News* üî•

- [2025/12/16] Collaboration: [AMD √ó vLLM Semantic Router: Building the System Intelligence Together](https://blog.vllm.ai/2025/12/16/vllm-sr-amd.html)
- [2025/12/15] New Blog: [Token-Level Truth: Real-Time Hallucination Detection for Production LLMs](https://blog.vllm.ai/2025/12/14/halugate.html)
- [2025/11/19] New Blog: [Signal-Decision Driven Architecture: Reshaping Semantic Routing at Scale](https://blog.vllm.ai/2025/11/19/signal-decision.html)
- [2025/11/03] Our paper [Category-Aware Semantic Caching for Heterogeneous LLM Workloads](https://arxiv.org/abs/2510.26835) published
- [2025/10/27] New Blog: [Scaling Semantic Routing with Extensible LoRA](https://blog.vllm.ai/2025/10/27/semantic-router-modular.html)
- [2025/10/12] Our paper [When to Reason: Semantic Router for vLLM](https://arxiv.org/abs/2510.08731) accepted by NeurIPS 2025 MLForSys.
- [2025/10/08] Collaboration: vLLM Semantic Router with [vLLM Production Stack](https://github.com/vllm-project/production-stack) Team.
- [2025/09/01] Released the project: [vLLM Semantic Router: Next Phase in LLM inference](https://blog.vllm.ai/2025/09/11/semantic-router.html).

---

## Goals

We are building the **System Level Intelligence** for Mixture-of-Models (MoM), bringing the **Collective Intelligence** into **LLM systems**, answering the following questions:

1. How to capture the missing signals in request, response and context?
2. How to combine the signals to make better decisions?
3. How to collaborate more efficiently between different models?
4. How to secure the real world and LLM system from jailbreaks, pii leaks, hallucinations?
5. How to collect the valuable signals and build a self-learning system?

![vLLM Semantic Router Banner](./website/static/img/banner.png)

### Where it lives

It lives between the real world and models:

![level](./website/static/img/level.png)

### Architecture

A quick overview of the current architecture:

![architecture](./website/static/img/architecture.png)

## Quick Start

### Installation

&gt; [!TIP]
&gt; We recommend that you setup a Python virtual environment to manage dependencies.

```bash
$ python -m venv vsr
$ source vsr/bin/activate
$ pip install vllm-sr
```

Installed successfully if you see the following help message:

```bash
$ vllm-sr

       _ _     __  __       ____  ____
__   _| | |_ _|  \/  |     / ___||  _ \
\ \ / / | | | | |\/| |_____\___ \| |_) |
 \ V /| | | |_| | |  |_____|___) |  _ &lt;
  \_/ |_|_|\__,_|_|  |     |____/|_| \_\

vLLM Semantic Router - Intelligent routing for vLLM

Usage: vllm-sr [OPTIONS] COMMAND [ARGS]...

  vLLM Semantic Router CLI - Intelligent routing and caching for vLLM
  endpoints.

Options:
  --version  Show version and exit.
  --help     Show this message and exit.

Commands:
  config  Print generated configuration.
  init    Initialize vLLM Semantic Router configuration.
  logs    Show logs from vLLM Semantic Router service.
  serve   Start vLLM Semantic Router.
  status  Show status of vLLM Semantic Router services.
  stop    Stop vLLM Semantic Router.
```

&gt; [!TIP]
&gt; You can specify the HF_ENDPOINT, HF_TOKEN, and HF_HOME environment variables to configure the Hugging Face credentials.

```bash
# Set environment variables (optional)
export HF_ENDPOINT=https://huggingface.co  # Or use mirror: https://hf-mirror.com
export HF_TOKEN=your_token_here  # Only for gated models
export HF_HOME=/path/to/cache  # Optional: custom cache directory

# Start the service - models download automatically
# Environment variables are automatically passed to the container
vllm-sr serve
```

## Documentation üìñ

For comprehensive documentation including detailed setup instructions, architecture guides, and API references, visit:

Complete Documentation at Read the **[Docs](https://vllm-semantic-router.com/)**

The documentation includes:

- **[Installation Guide](https://vllm-semantic-router.com/docs/installation/)** - Complete setup instructions
- **[System Architecture](https://vllm-semantic-router.com/docs/overview/architecture/system-architecture/)** - Technical deep dive
- **[Model Training](https://vllm-semantic-router.com/docs/training/training-overview/)** - How classification models work
- **[API Reference](https://vllm-semantic-router.com/docs/api/router/)** - Complete API documentation
- **[Dashboard](https://vllm-semantic-router.com/docs/overview/dashboard)** - vLLM Semantic Router Dashboard

## Community üëã

For questions, feedback, or to contribute, please join `#semantic-router` channel in vLLM Slack.

### Community Meetings üìÖ

We host bi-weekly community meetings to sync up with contributors across different time zones:

- **First Tuesday of the month**: 9:00-10:00 AM EST (accommodates US EST, EU, and Asia Pacific contributors)
  - [Zoom Link](https://us05web.zoom.us/j/84122485631?pwd=BB88v03mMNLVHn60YzVk4PihuqBV9d.1)
  - [Google Calendar Invite](https://us05web.zoom.us/meeting/tZAsdeuspj4sGdVraOOR4UaXSstrH2jjPYFq/calendar/google/add?meetingMasterEventId=4jjzUKSLSLiBHtIKZpGc3g)
  - [ics file](https://drive.google.com/file/d/15wO8cg0ZjNxdr8OtGiZyAgkSS8_Wry0J/view?usp=sharing)
- **Third Tuesday of the month**: 1:00-2:00 PM EST (accommodates US EST and California contributors)
  - [Zoom Link](https://us06web.zoom.us/j/86871492845?pwd=LcTtXm9gtGu23JeWqXxbnLLCCvbumB.1)
  - [Google Calendar Invite](https://us05web.zoom.us/meeting/tZIlcOispzkiHtH2dlkWlLym68bEqvuf3MU5/calendar/google/add?meetingMasterEventId=PqWz2vk7TOCszPXqconGAA)
  - [ics file](https://drive.google.com/file/d/1T54mwYpXXoV9QfR76I56BFBPNbykSsTw/view?usp=sharing)
- Meeting Recordings: [YouTube](https://www.youtube.com/@vLLMSemanticRouter/videos)

Join us to discuss the latest developments, share ideas, and collaborate on the project!

## Citation

If you find Semantic Router helpful in your research or projects, please consider citing it:

```
@misc{semanticrouter2025,
  title={vLLM Semantic Router},
  author={vLLM Semantic Router Team},
  year={2025},
  howpublished={\url{https://github.com/vllm-project/semantic-router}},
}
```

## Star History üî•

We opened the project at Aug 31, 2025. We love open source  and collaboration ‚ù§Ô∏è

[![Star History Chart](https://api.star-history.com/svg?repos=vllm-project/semantic-router&amp;type=Date)](https://www.star-history.com/#vllm-project/semantic-router&amp;Date)

## Sponsors üëã

We are grateful to our sponsors who support us:

---

[**AMD**](https://www.amd.com) provides us with GPU resources and [ROCm‚Ñ¢](https://www.amd.com/en/products/software/rocm.html) Software for training and researching the frontier router models, enhancing e2e testing, and building online models playground.

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://www.amd.com&quot;&gt;
  &lt;img src=&quot;website/static/img/amd-logo.svg&quot; alt=&quot;AMD&quot; width=&quot;40%&quot;/&gt;
&lt;/a&gt;
&lt;/div&gt;

---
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[kgateway-dev/kgateway]]></title>
            <link>https://github.com/kgateway-dev/kgateway</link>
            <guid>https://github.com/kgateway-dev/kgateway</guid>
            <pubDate>Fri, 09 Jan 2026 00:05:14 GMT</pubDate>
            <description><![CDATA[The Cloud-Native API Gateway and AI Gateway]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/kgateway-dev/kgateway">kgateway-dev/kgateway</a></h1>
            <p>The Cloud-Native API Gateway and AI Gateway</p>
            <p>Language: Go</p>
            <p>Stars: 5,201</p>
            <p>Forks: 626</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/kgateway-dev/kgateway.dev/main/static/logo-dark.svg&quot; alt=&quot;kgateway&quot; width=&quot;400&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://raw.githubusercontent.com/kgateway-dev/kgateway.dev/main/static/logo.svg&quot; alt=&quot;kgateway&quot; width=&quot;400&quot;&gt;
    &lt;img alt=&quot;kgateway&quot; src=&quot;https://raw.githubusercontent.com/kgateway-dev/kgateway.dev/main/static/logo.svg&quot;&gt;
  &lt;/picture&gt;
  &lt;br/&gt;
  The most widely deployed gateway in Kubernetes for microservices and AI agents
&lt;/h1&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/kgateway-dev/kgateway/releases&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/v/release/kgateway-dev/kgateway?style=flat&amp;label=Latest%20version&quot; alt=&quot;Release&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://opensource.org/licenses/Apache-2.0&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/License-Apache2.0-brightgreen.svg?style=flat&quot; alt=&quot;License: Apache 2.0&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/kgateway-dev/kgateway&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/kgateway-dev/kgateway.svg?style=flat&amp;logo=github&amp;label=Stars&quot; alt=&quot;Stars&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.bestpractices.dev/projects/10534&quot;&gt;&lt;img src=&quot;https://www.bestpractices.dev/projects/10534/badge&quot; alt=&quot;OpenSSF Best Practices&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

## About kgateway

Kgateway is the most mature and widely deployed gateway in the market today. Built on open source and open standards, **kgateway is a dual control plane that implements the [Kubernetes Gateway API](https://gateway-api.sigs.k8s.io/) for both [Envoy](https://github.com/envoyproxy/envoy) and [agentgateway](https://github.com/agentgateway/agentgateway)**. This unique architecture enables kgateway to provide unified API connectivity spanning from traditional HTTP/gRPC workloads to advanced AI agent orchestration.

With a control plane that scales from lightweight microgateway deployments between services, to massively parallel centralized gateways handling billions of API calls, to advanced AI gateway use cases for safety, security, and governance, kgateway brings omni-directional API connectivity to any cloud and any environment.

### Use Cases

Kgateway is designed for:

* **Advanced Ingress Controller and Next-Gen API Gateway**: Aggregate web APIs and apply functions like authentication, authorization and rate limiting in one place. Powered by [Envoy](https://www.envoyproxy.io) or [agentgateway](https://github.com/agentgateway/agentgateway) and programmed with the [Gateway API](https://gateway-api.sigs.k8s.io/), kgateway is a world-leading Cloud Native ingress.

* **AI Gateway for LLM Consumption**: Protect models, tools, agents, and data from inappropriate access. Manage traffic to LLM providers, enrich prompts at a system level, and apply prompt guards for safety and compliance.

* **Inference Gateway for Generative Models**: Intelligently route to AI inference workloads in Kubernetes environments utilizing the [Inference Extension](https://gateway-api-inference-extension.sigs.k8s.io/) project.

* **Native MCP and Agent-to-Agent Gateway**: Federate Model Context Protocol tool services and secure agent-to-agent communications with a single scalable endpoint powered by agentgateway.

* **Hybrid Application Migration**: Route to backends implemented as microservices, serverless functions or legacy apps. Gradually migrate from legacy code while maintaining existing systems.

Kgateway is feature-rich, fast, and flexible. It excels in function-level routing, supports legacy apps, microservices and serverless, offers robust discovery capabilities, integrates seamlessly with open-source projects, and is designed to support hybrid applications with various technologies, architectures, protocols, and clouds.

### History

The project was launched in 2018 as **Gloo** by Solo.io and has been [production-ready since 2019](https://www.solo.io/blog/announcing-gloo-1-0-a-production-ready-envoy-based-api-gateway). Since then, it has steadily evolved to become the most trusted and feature-rich API gateway for Kubernetes, processing billions of API requests for many of the world&#039;s biggest companies. Please see [the migration plan](https://github.com/kgateway-dev/kgateway/issues/10363) for more information about the transition from Gloo to kgateway.

## Get involved

- [Join us on our Slack channel](https://kgateway.dev/slack/)
- [Check out the docs](https://kgateway.dev/docs)
- [Read the kgateway blog](https://kgateway.dev/blog/)
- [Learn more about the community](https://github.com/kgateway-dev/community)
- [Watch a video on our YouTube channel](https://www.youtube.com/@kgateway-dev)
- Follow us on [X](https://x.com/kgatewaydev), [Bluesky](https://bsky.app/profile/kgateway.dev), [Mastodon](https://mastodon.social/@kgateway) or [LinkedIn](https://www.linkedin.com/company/kgateway/)

## Contributing to kgateway

Please refer to [devel/contributing/README.md](/devel/contributing/README.md) as a starting point for contributing to the project.

## Releasing kgateway

Please refer to [devel/contributing/releasing.md](devel/contributing/releasing.md) as a starting point for understanding releases of the project.

## Security

See our [SECURITY.md](SECURITY.md) file for details.

## Thanks

Kgateway would not be possible without the valuable open source work of projects in the community. We would like to extend a special thank-you to [Envoy](https://www.envoyproxy.io) and [agentgateway](https://github.com/agentgateway/agentgateway), the two data planes upon which we build our dual control plane architecture.

## Contributors

Thanks to all contributors who are helping to make kgateway better!

&lt;a href=&quot;https://github.com/kgateway-dev/kgateway/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=kgateway-dev/kgateway&quot; /&gt;
&lt;/a&gt;

## Star History

&lt;a href=&quot;https://www.star-history.com/#kgateway-dev/kgateway&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=kgateway-dev/kgateway&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=kgateway-dev/kgateway&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star history of kgateway-dev/kgateway over time&quot; src=&quot;https://api.star-history.com/svg?repos=kgateway-dev/kgateway&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;

---

&lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/cncf/artwork/main/other/cncf-sandbox/horizontal/color/cncf-sandbox-horizontal-color.svg&quot; width=&quot;300&quot; alt=&quot;Cloud Native Computing Foundation logo&quot;/&gt;
    &lt;p&gt;kgateway is a &lt;a href=&quot;https://cncf.io&quot;&gt;Cloud Native Computing Foundation&lt;/a&gt; sandbox project.&lt;/p&gt;
&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[open-telemetry/opentelemetry-go-contrib]]></title>
            <link>https://github.com/open-telemetry/opentelemetry-go-contrib</link>
            <guid>https://github.com/open-telemetry/opentelemetry-go-contrib</guid>
            <pubDate>Fri, 09 Jan 2026 00:05:13 GMT</pubDate>
            <description><![CDATA[Collection of extensions for OpenTelemetry-Go.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/open-telemetry/opentelemetry-go-contrib">open-telemetry/opentelemetry-go-contrib</a></h1>
            <p>Collection of extensions for OpenTelemetry-Go.</p>
            <p>Language: Go</p>
            <p>Stars: 1,575</p>
            <p>Forks: 724</p>
            <p>Stars today: 0 stars today</p>
            <h2>README</h2><pre># OpenTelemetry-Go Contrib

[![build_and_test](https://github.com/open-telemetry/opentelemetry-go-contrib/workflows/build_and_test/badge.svg)](https://github.com/open-telemetry/opentelemetry-go-contrib/actions?query=workflow%3Abuild_and_test+branch%3Amain)
[![codecov.io](https://codecov.io/gh/open-telemetry/opentelemetry-go-contrib/coverage.svg?branch=main)](https://app.codecov.io/gh/open-telemetry/opentelemetry-go-contrib?branch=main)
[![Docs](https://godoc.org/go.opentelemetry.io/contrib?status.svg)](https://pkg.go.dev/go.opentelemetry.io/contrib)
[![Go Report Card](https://goreportcard.com/badge/go.opentelemetry.io/contrib)](https://goreportcard.com/report/go.opentelemetry.io/contrib)
[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/opentelemetry-go-contrib.svg)](https://issues.oss-fuzz.com/issues?q=project:opentelemetry-go-contrib)
[![Slack](https://img.shields.io/badge/slack-@cncf/otel--go-brightgreen.svg?logo=slack)](https://cloud-native.slack.com/archives/C01NPAXACKT)

Collection of 3rd-party packages for [OpenTelemetry-Go](https://github.com/open-telemetry/opentelemetry-go).

## Contents

- [Examples](./examples/): Examples of OpenTelemetry libraries usage.
- [Instrumentation](./instrumentation/): Packages providing OpenTelemetry instrumentation for 3rd-party libraries.
- [Propagators](./propagators/): Packages providing OpenTelemetry context propagators for 3rd-party propagation formats.
- [Detectors](./detectors/): Packages providing OpenTelemetry resource detectors for 3rd-party cloud computing environments.
- [Exporters](./exporters/): Packages providing OpenTelemetry exporters for 3rd-party export formats.
- [Samplers](./samplers/): Packages providing additional implementations of OpenTelemetry samplers.
- [Bridges](./bridges/): Packages providing adapters for 3rd-party instrumentation frameworks.
- [Processors](./processors/): Packages providing additional implementations of OpenTelemetry processors.

## Project Status

This project contains both stable and unstable modules.
Refer to the module for its version or our [versioning manifest](./versions.yaml).

Project versioning information and stability guarantees can be found in the [versioning documentation](https://github.com/open-telemetry/opentelemetry-go/blob/a724cf884287e04785eaa91513d26a6ef9699288/VERSIONING.md).

Progress and status specific to this repository is tracked in our local [project boards](https://github.com/open-telemetry/opentelemetry-go-contrib/projects?query=is%3Aopen) and [milestones](https://github.com/open-telemetry/opentelemetry-go-contrib/milestones).

### Compatibility

OpenTelemetry-Go Contrib ensures compatibility with the current supported
versions of
the [Go language](https://golang.org/doc/devel/release#policy):

&gt; Each major Go release is supported until there are two newer major releases.
&gt; For example, Go 1.5 was supported until the Go 1.7 release, and Go 1.6 was supported until the Go 1.8 release.

For versions of Go that are no longer supported upstream, opentelemetry-go-contrib will
stop ensuring compatibility with these versions in the following manner:

- A minor release of opentelemetry-go-contrib will be made to add support for the new
  supported release of Go.
- The following minor release of opentelemetry-go-contrib will remove compatibility
  testing for the oldest (now archived upstream) version of Go. This, and
  future, releases of opentelemetry-go-contrib may include features only supported by
  the currently supported versions of Go.

This project is tested on the following systems.

| OS       | Go Version | Architecture |
| -------- | ---------- | ------------ |
| Ubuntu   | 1.25       | amd64        |
| Ubuntu   | 1.24       | amd64        |
| Ubuntu   | 1.25       | 386          |
| Ubuntu   | 1.24       | 386          |
| macOS    | 1.25       | amd64        |
| macOS    | 1.24       | amd64        |
| macOS    | 1.25       | arm64        |
| macOS    | 1.24       | arm64        |
| Windows  | 1.25       | amd64        |
| Windows  | 1.24       | amd64        |
| Windows  | 1.25       | 386          |
| Windows  | 1.24       | 386          |

While this project should work for other systems, no compatibility guarantees
are made for those systems currently.

## Contributing

For information on how to contribute, consult [the contributing guidelines](./CONTRIBUTING.md)
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[hashicorp/vault]]></title>
            <link>https://github.com/hashicorp/vault</link>
            <guid>https://github.com/hashicorp/vault</guid>
            <pubDate>Fri, 09 Jan 2026 00:05:12 GMT</pubDate>
            <description><![CDATA[A tool for secrets management, encryption as a service, and privileged access management]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hashicorp/vault">hashicorp/vault</a></h1>
            <p>A tool for secrets management, encryption as a service, and privileged access management</p>
            <p>Language: Go</p>
            <p>Stars: 33,759</p>
            <p>Forks: 4,507</p>
            <p>Stars today: 8 stars today</p>
            <h2>README</h2><pre># Vault [![build](https://github.com/hashicorp/vault/actions/workflows/build.yml/badge.svg)](https://github.com/hashicorp/vault/actions/workflows/build.yml) [![ci](https://github.com/hashicorp/vault/actions/workflows/ci.yml/badge.svg)](https://github.com/hashicorp/vault/actions/workflows/ci.yml)  [![vault enterprise](https://img.shields.io/badge/vault-enterprise-yellow.svg?colorB=7c8797&amp;colorA=000000)](https://www.hashicorp.com/products/vault/?utm_source=github&amp;utm_medium=banner&amp;utm_campaign=github-vault-enterprise)

----

**Please note**: We take Vault&#039;s security and our users&#039; trust very seriously. If you believe you have found a security issue in Vault, _please responsibly disclose_ by contacting us at [security@hashicorp.com](mailto:security@hashicorp.com).

----

- Website: [developer.hashicorp.com/vault](https://developer.hashicorp.com/vault)
- Announcement list: [Google Groups](https://groups.google.com/group/hashicorp-announce)
- Discussion forum: [Discuss](https://discuss.hashicorp.com/c/vault)
- Documentation: [https://developer.hashicorp.com/vault/docs](https://developer.hashicorp.com/vault/docs)
- Tutorials: [https://developer.hashicorp.com/vault/tutorials](https://developer.hashicorp.com/vault/tutorials)
- Certification exam: [https://developer.hashicorp.com/certifications/security-automation](https://developer.hashicorp.com/certifications/security-automation)
- Documentation source: [https://github.com/hashicorp/web-unified-docs](https://github.com/hashicorp/web-unified-docs)

&lt;img width=&quot;300&quot; alt=&quot;Vault Logo&quot; src=&quot;https://github.com/hashicorp/vault/blob/f22d202cde2018f9455dec755118a9b84586e082/Vault_PrimaryLogo_Black.png&quot;&gt;

Vault is a tool for securely accessing secrets. A secret is anything that you want to tightly control access to, such as API keys, passwords, certificates, and more. Vault provides a unified interface to any secret, while providing tight access control and recording a detailed audit log.

A modern system requires access to a multitude of secrets: database credentials, API keys for external services, credentials for service-oriented architecture communication, etc. Understanding who is accessing what secrets is already very difficult and platform-specific. Adding on key rolling, secure storage, and detailed audit logs is almost impossible without a custom solution. This is where Vault steps in.

The key features of Vault are:

* **Secure Secret Storage**: Vault can store arbitrary key/value pairs. Vault encrypts data before writing it to persistent
  storage, so gaining access to the raw storage isn&#039;t enough to access
  your secrets. Vault can write to disk, [Consul](https://www.consul.io),
  and more.

* **Dynamic Secrets**: Vault can generate secrets on-demand for some
  systems, such as AWS or SQL databases. For example, when an application
  needs to access an S3 bucket, it asks Vault for credentials, and Vault
  will generate an AWS keypair with valid permissions on demand. After
  creating these dynamic secrets, Vault will also automatically revoke them
  after the lease is up.

* **Data Encryption**: Vault can encrypt and decrypt data without storing
  it. This allows security teams to define encryption parameters and
  developers to store encrypted data in a location such as a SQL database without
  having to design their own encryption methods.

* **Leasing and Renewal**: Vault associates a **lease** with each secret.
  At the end of the lease, Vault automatically revokes the
  secret. Clients are able to renew leases via built-in renew APIs.

* **Revocation**: Vault has built-in support for secret revocation. Vault
  can revoke not only single secrets, but a tree of secrets, for example,
  all secrets read by a specific user, or all secrets of a particular type.
  Revocation assists in key rolling as well as locking down systems in the
  case of an intrusion.

Documentation, Getting Started, and Certification Exams
-------------------------------

Documentation is available on the [Vault website](https://developer.hashicorp.com/vault/docs).

If you&#039;re new to Vault and want to get started with security automation, please
check out our [Getting Started guides](https://learn.hashicorp.com/collections/vault/getting-started)
on HashiCorp&#039;s learning platform. There are also [additional guides](https://learn.hashicorp.com/vault)
to continue your learning.

For examples of how to interact with Vault from inside your application in different programming languages, see the [vault-examples](https://github.com/hashicorp/vault-examples) repo. An out-of-the-box [sample application](https://github.com/hashicorp/hello-vault-go) is also available.

Show off your Vault knowledge by passing a certification exam. Visit the
[certification page](https://www.hashicorp.com/certification/#hashicorp-certified-vault-associate)
for information about exams and find [study materials](https://learn.hashicorp.com/collections/vault/certification)
on HashiCorp&#039;s learning platform.

Developing Vault
--------------------

If you wish to work on Vault itself or any of its built-in systems, you&#039;ll
first need [Go](https://www.golang.org) installed on your machine.

For local dev first make sure Go is properly installed, including setting up a
[GOPATH](https://golang.org/doc/code.html#GOPATH), then setting the 
[GOBIN](https://pkg.go.dev/cmd/go#hdr-Environment_variables) variable to `$GOPATH/bin`. 
Ensure that `$GOPATH/bin` is in your path as some distributions bundle the old version 
of build tools. 

Next, clone this repository. Vault uses [Go Modules](https://github.com/golang/go/wiki/Modules),
so it is recommended that you clone the repository ***outside*** of the GOPATH.
You can then download any required build tools by bootstrapping your environment:

```sh
$ make bootstrap
...
```

To compile a development version of Vault, run `make` or `make dev`. This will
put the Vault binary in the `bin` and `$GOPATH/bin` folders:

```sh
$ make dev
...
$ bin/vault
...
```

To compile a development version of Vault with the UI, run `make static-dist dev-ui`. This will
put the Vault binary in the `bin` and `$GOPATH/bin` folders:

```sh
$ make static-dist dev-ui
...
$ bin/vault
...
```

To run tests, type `make test`. Note: this requires Docker to be installed. If
this exits with exit status 0, then everything is working!

```sh
$ make test
...
```

If you&#039;re developing a specific package, you can run tests for just that
package by specifying the `TEST` variable. For example below, only
`vault` package tests will be run.

```sh
$ make test TEST=./vault
...
```

### Troubleshooting

If you encounter an error like `could not read Username for &#039;https://github.com&#039;` you may need to adjust your git config like so:

```sh
$ git config --global --add url.&quot;git@github.com:&quot;.insteadOf &quot;https://github.com/&quot;
```


### Importing Vault

This repository publishes two libraries that may be imported by other projects:
`github.com/hashicorp/vault/api` and `github.com/hashicorp/vault/sdk`.

Note that this repository also contains Vault (the product), and as with most Go
projects, Vault uses Go modules to manage its dependencies. The mechanism to do
that is the [go.mod](./go.mod) file. As it happens, the presence of that file
also makes it theoretically possible to import Vault as a dependency into other
projects. Some other projects have made a practice of doing so in order to take
advantage of testing tooling that was developed for testing Vault itself. This
is not, and has never been, a supported way to use the Vault project. We aren&#039;t 
likely to fix bugs relating to failure to import `github.com/hashicorp/vault` 
into your project.

See also the section &quot;Docker-based tests&quot; below.

### Acceptance Tests

Vault has comprehensive [acceptance tests](https://en.wikipedia.org/wiki/Acceptance_testing)
covering most of the features of the secret and auth methods.

If you&#039;re working on a feature of a secret or auth method and want to
verify it is functioning (and also hasn&#039;t broken anything else), we recommend
running the acceptance tests.

**Warning:** The acceptance tests create/destroy/modify *real resources*, which
may incur real costs in some cases. In the presence of a bug, it is technically
possible that broken backends could leave dangling data behind. Therefore,
please run the acceptance tests at your own risk. At the very least,
we recommend running them in their own private account for whatever backend
you&#039;re testing.

To run the acceptance tests, invoke `make testacc`:

```sh
$ make testacc TEST=./builtin/logical/consul
...
```

The `TEST` variable is required, and you should specify the folder where the
backend is. The `TESTARGS` variable is recommended to filter down to a specific
resource to test, since testing all of them at once can sometimes take a very
long time.

Acceptance tests typically require other environment variables to be set for
things such as access keys. The test itself should error early and tell
you what to set, so it is not documented here.

For more information on Vault Enterprise features, visit the [Vault Enterprise site](https://www.hashicorp.com/products/vault/?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=github-vault-enterprise).

### Docker-based Tests

We have created an experimental new testing mechanism inspired by NewTestCluster.
An example of how to use it:

```go
import (
  &quot;testing&quot;
  &quot;github.com/hashicorp/vault/sdk/helper/testcluster/docker&quot;
)

func Test_Something_With_Docker(t *testing.T) {
  opts := &amp;docker.DockerClusterOptions{
    ImageRepo: &quot;hashicorp/vault&quot;, // or &quot;hashicorp/vault-enterprise&quot;
    ImageTag:    &quot;latest&quot;,
  }
  cluster := docker.NewTestDockerCluster(t, opts)
  defer cluster.Cleanup()
  
  client := cluster.Nodes()[0].APIClient()
  _, err := client.Logical().Read(&quot;sys/storage/raft/configuration&quot;)
  if err != nil {
    t.Fatal(err)
  }
}
```

Or for Enterprise:

```go
import (
  &quot;testing&quot;
  &quot;github.com/hashicorp/vault/sdk/helper/testcluster/docker&quot;
)

func Test_Something_With_Docker(t *testing.T) {
  opts := &amp;docker.DockerClusterOptions{
    ImageRepo: &quot;hashicorp/vault-enterprise&quot;,
    ImageTag:  &quot;latest&quot;,
	VaultLicense: licenseString, // not a path, the actual license bytes
  }
  cluster := docker.NewTestDockerCluster(t, opts)
  defer cluster.Cleanup()
}
```

Here is a more realistic example of how we use it in practice.  DefaultOptions uses 
`hashicorp/vault`:`latest` as the repo and tag, but it also looks at the environment
variable VAULT_BINARY. If populated, it will copy the local file referenced by
VAULT_BINARY into the container. This is useful when testing local changes.

Instead of setting the VaultLicense option, you can set the VAULT_LICENSE_CI environment
variable, which is better than committing a license to version control.

Optionally you can set COMMIT_SHA, which will be appended to the image name we
build as a debugging convenience.

```go
func Test_Custom_Build_With_Docker(t *testing.T) {
  opts := docker.DefaultOptions(t)
  cluster := docker.NewTestDockerCluster(t, opts)
  defer cluster.Cleanup()
}
```

There are a variety of helpers in the `github.com/hashicorp/vault/sdk/helper/testcluster`
package, e.g. these tests below will create a pair of 3-node clusters and link them using
PR or DR replication respectively, and fail if the replication state doesn&#039;t become healthy
before the passed context expires.

Again, as written, these depend on having a Vault Enterprise binary locally and the env
var VAULT_BINARY set to point to it, as well as having VAULT_LICENSE_CI set.

```go
func TestStandardPerfReplication_Docker(t *testing.T) {
  opts := docker.DefaultOptions(t)
  r, err := docker.NewReplicationSetDocker(t, opts)
  if err != nil {
      t.Fatal(err)
  }
  defer r.Cleanup()

  ctx, cancel := context.WithTimeout(context.Background(), time.Minute)
  defer cancel()
  err = r.StandardPerfReplication(ctx)
  if err != nil {
    t.Fatal(err)
  }
}

func TestStandardDRReplication_Docker(t *testing.T) {
  opts := docker.DefaultOptions(t)
  r, err := docker.NewReplicationSetDocker(t, opts)
  if err != nil {
    t.Fatal(err)
  }
  defer r.Cleanup()

  ctx, cancel := context.WithTimeout(context.Background(), time.Minute)
  defer cancel()
  err = r.StandardDRReplication(ctx)
  if err != nil {
    t.Fatal(err)
  }
}
```

Finally, here&#039;s an example of running an existing OSS docker test with a custom binary:

```bash
$ GOOS=linux make dev
$ VAULT_BINARY=$(pwd)/bin/vault go test -run &#039;TestRaft_Configuration_Docker&#039; ./vault/external_tests/raft/raft_binary
ok      github.com/hashicorp/vault/vault/external_tests/raft/raft_binary        20.960s
```
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[sirupsen/logrus]]></title>
            <link>https://github.com/sirupsen/logrus</link>
            <guid>https://github.com/sirupsen/logrus</guid>
            <pubDate>Fri, 09 Jan 2026 00:05:11 GMT</pubDate>
            <description><![CDATA[Structured, pluggable logging for Go.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sirupsen/logrus">sirupsen/logrus</a></h1>
            <p>Structured, pluggable logging for Go.</p>
            <p>Language: Go</p>
            <p>Stars: 25,660</p>
            <p>Forks: 2,279</p>
            <p>Stars today: 4 stars today</p>
            <h2>README</h2><pre># Logrus &lt;img src=&quot;http://i.imgur.com/hTeVwmJ.png&quot; width=&quot;40&quot; height=&quot;40&quot; alt=&quot;:walrus:&quot; class=&quot;emoji&quot; title=&quot;:walrus:&quot;/&gt; [![Build Status](https://github.com/sirupsen/logrus/workflows/CI/badge.svg)](https://github.com/sirupsen/logrus/actions?query=workflow%3ACI) [![Go Reference](https://pkg.go.dev/badge/github.com/sirupsen/logrus.svg)](https://pkg.go.dev/github.com/sirupsen/logrus)

Logrus is a structured logger for Go (golang), completely API compatible with
the standard library logger.

**Logrus is in maintenance-mode.** We will not be introducing new features. It&#039;s
simply too hard to do in a way that won&#039;t break many people&#039;s projects, which is
the last thing you want from your Logging library (again...).

This does not mean Logrus is dead. Logrus will continue to be maintained for
security, (backwards compatible) bug fixes, and performance (where we are
limited by the interface).

I believe Logrus&#039; biggest contribution is to have played a part in today&#039;s
widespread use of structured logging in Golang. There doesn&#039;t seem to be a
reason to do a major, breaking iteration into Logrus V2, since the fantastic Go
community has built those independently. Many fantastic alternatives have sprung
up. Logrus would look like those, had it been re-designed with what we know
about structured logging in Go today. Check out, for example,
[Zerolog][zerolog], [Zap][zap], and [Apex][apex].

[zerolog]: https://github.com/rs/zerolog
[zap]: https://github.com/uber-go/zap
[apex]: https://github.com/apex/log

**Seeing weird case-sensitive problems?** It&#039;s in the past been possible to
import Logrus as both upper- and lower-case. Due to the Go package environment,
this caused issues in the community and we needed a standard. Some environments
experienced problems with the upper-case variant, so the lower-case was decided.
Everything using `logrus` will need to use the lower-case:
`github.com/sirupsen/logrus`. Any package that isn&#039;t, should be changed.

To fix Glide, see [these
comments](https://github.com/sirupsen/logrus/issues/553#issuecomment-306591437).
For an in-depth explanation of the casing issue, see [this
comment](https://github.com/sirupsen/logrus/issues/570#issuecomment-313933276).

Nicely color-coded in development (when a TTY is attached, otherwise just
plain text):

![Colored](http://i.imgur.com/PY7qMwd.png)

With `logrus.SetFormatter(&amp;logrus.JSONFormatter{})`, for easy parsing by logstash
or Splunk:

```text
{&quot;animal&quot;:&quot;walrus&quot;,&quot;level&quot;:&quot;info&quot;,&quot;msg&quot;:&quot;A group of walrus emerges from the
ocean&quot;,&quot;size&quot;:10,&quot;time&quot;:&quot;2014-03-10 19:57:38.562264131 -0400 EDT&quot;}

{&quot;level&quot;:&quot;warning&quot;,&quot;msg&quot;:&quot;The group&#039;s number increased tremendously!&quot;,
&quot;number&quot;:122,&quot;omg&quot;:true,&quot;time&quot;:&quot;2014-03-10 19:57:38.562471297 -0400 EDT&quot;}

{&quot;animal&quot;:&quot;walrus&quot;,&quot;level&quot;:&quot;info&quot;,&quot;msg&quot;:&quot;A giant walrus appears!&quot;,
&quot;size&quot;:10,&quot;time&quot;:&quot;2014-03-10 19:57:38.562500591 -0400 EDT&quot;}

{&quot;animal&quot;:&quot;walrus&quot;,&quot;level&quot;:&quot;info&quot;,&quot;msg&quot;:&quot;Tremendously sized cow enters the ocean.&quot;,
&quot;size&quot;:9,&quot;time&quot;:&quot;2014-03-10 19:57:38.562527896 -0400 EDT&quot;}

{&quot;level&quot;:&quot;fatal&quot;,&quot;msg&quot;:&quot;The ice breaks!&quot;,&quot;number&quot;:100,&quot;omg&quot;:true,
&quot;time&quot;:&quot;2014-03-10 19:57:38.562543128 -0400 EDT&quot;}
```

With the default `logrus.SetFormatter(&amp;logrus.TextFormatter{})` when a TTY is not
attached, the output is compatible with the
[logfmt](https://pkg.go.dev/github.com/kr/logfmt) format:

```text
time=&quot;2015-03-26T01:27:38-04:00&quot; level=debug msg=&quot;Started observing beach&quot; animal=walrus number=8
time=&quot;2015-03-26T01:27:38-04:00&quot; level=info msg=&quot;A group of walrus emerges from the ocean&quot; animal=walrus size=10
time=&quot;2015-03-26T01:27:38-04:00&quot; level=warning msg=&quot;The group&#039;s number increased tremendously!&quot; number=122 omg=true
time=&quot;2015-03-26T01:27:38-04:00&quot; level=debug msg=&quot;Temperature changes&quot; temperature=-4
time=&quot;2015-03-26T01:27:38-04:00&quot; level=panic msg=&quot;It&#039;s over 9000!&quot; animal=orca size=9009
time=&quot;2015-03-26T01:27:38-04:00&quot; level=fatal msg=&quot;The ice breaks!&quot; err=&amp;{0x2082280c0 map[animal:orca size:9009] 2015-03-26 01:27:38.441574009 -0400 EDT panic It&#039;s over 9000!} number=100 omg=true
```
To ensure this behaviour even if a TTY is attached, set your formatter as follows:

```go
logrus.SetFormatter(&amp;logrus.TextFormatter{
    DisableColors: true,
    FullTimestamp: true,
})
```

#### Logging Method Name

If you wish to add the calling method as a field, instruct the logger via:

```go
logrus.SetReportCaller(true)
```
This adds the caller as &#039;method&#039; like so:

```json
{&quot;animal&quot;:&quot;penguin&quot;,&quot;level&quot;:&quot;fatal&quot;,&quot;method&quot;:&quot;github.com/sirupsen/arcticcreatures.migrate&quot;,&quot;msg&quot;:&quot;a penguin swims by&quot;,
&quot;time&quot;:&quot;2014-03-10 19:57:38.562543129 -0400 EDT&quot;}
```

```text
time=&quot;2015-03-26T01:27:38-04:00&quot; level=fatal method=github.com/sirupsen/arcticcreatures.migrate msg=&quot;a penguin swims by&quot; animal=penguin
```
Note that this does add measurable overhead - the cost will depend on the version of Go, but is
between 20 and 40% in recent tests with 1.6 and 1.7.  You can validate this in your
environment via benchmarks:

```bash
go test -bench=.*CallerTracing
```

#### Case-sensitivity

The organization&#039;s name was changed to lower-case--and this will not be changed
back. If you are getting import conflicts due to case sensitivity, please use
the lower-case import: `github.com/sirupsen/logrus`.

#### Example

The simplest way to use Logrus is simply the package-level exported logger:

```go
package main

import &quot;github.com/sirupsen/logrus&quot;

func main() {
  logrus.WithFields(logrus.Fields{
    &quot;animal&quot;: &quot;walrus&quot;,
  }).Info(&quot;A walrus appears&quot;)
}
```

Note that it&#039;s completely api-compatible with the stdlib logger, so you can
replace your `log` imports everywhere with `log &quot;github.com/sirupsen/logrus&quot;`
and you&#039;ll now have the flexibility of Logrus. You can customize it all you
want:

```go
package main

import (
  &quot;os&quot;

  log &quot;github.com/sirupsen/logrus&quot;
)

func init() {
  // Log as JSON instead of the default ASCII formatter.
  log.SetFormatter(&amp;log.JSONFormatter{})

  // Output to stdout instead of the default stderr
  // Can be any io.Writer, see below for File example
  log.SetOutput(os.Stdout)

  // Only log the warning severity or above.
  log.SetLevel(log.WarnLevel)
}

func main() {
  log.WithFields(log.Fields{
    &quot;animal&quot;: &quot;walrus&quot;,
    &quot;size&quot;:   10,
  }).Info(&quot;A group of walrus emerges from the ocean&quot;)

  log.WithFields(log.Fields{
    &quot;omg&quot;:    true,
    &quot;number&quot;: 122,
  }).Warn(&quot;The group&#039;s number increased tremendously!&quot;)

  log.WithFields(log.Fields{
    &quot;omg&quot;:    true,
    &quot;number&quot;: 100,
  }).Fatal(&quot;The ice breaks!&quot;)

  // A common pattern is to re-use fields between logging statements by re-using
  // the logrus.Entry returned from WithFields()
  contextLogger := log.WithFields(log.Fields{
    &quot;common&quot;: &quot;this is a common field&quot;,
    &quot;other&quot;: &quot;I also should be logged always&quot;,
  })

  contextLogger.Info(&quot;I&#039;ll be logged with common and other field&quot;)
  contextLogger.Info(&quot;Me too&quot;)
}
```

For more advanced usage such as logging to multiple locations from the same
application, you can also create an instance of the `logrus` Logger:

```go
package main

import (
  &quot;os&quot;

  &quot;github.com/sirupsen/logrus&quot;
)

// Create a new instance of the logger. You can have any number of instances.
var logger = logrus.New()

func main() {
  // The API for setting attributes is a little different than the package level
  // exported logger. See Godoc. 
  logger.Out = os.Stdout

  // You could set this to any `io.Writer` such as a file
  // file, err := os.OpenFile(&quot;logrus.log&quot;, os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0666)
  // if err == nil {
  //  logger.Out = file
  // } else {
  //  logger.Info(&quot;Failed to log to file, using default stderr&quot;)
  // }

  logger.WithFields(logrus.Fields{
    &quot;animal&quot;: &quot;walrus&quot;,
    &quot;size&quot;:   10,
  }).Info(&quot;A group of walrus emerges from the ocean&quot;)
}
```

#### Fields

Logrus encourages careful, structured logging through logging fields instead of
long, unparseable error messages. For example, instead of: `logrus.Fatalf(&quot;Failed
to send event %s to topic %s with key %d&quot;)`, you should log the much more
discoverable:

```go
logrus.WithFields(logrus.Fields{
  &quot;event&quot;: event,
  &quot;topic&quot;: topic,
  &quot;key&quot;: key,
}).Fatal(&quot;Failed to send event&quot;)
```

We&#039;ve found this API forces you to think about logging in a way that produces
much more useful logging messages. We&#039;ve been in countless situations where just
a single added field to a log statement that was already there would&#039;ve saved us
hours. The `WithFields` call is optional.

In general, with Logrus using any of the `printf`-family functions should be
seen as a hint you should add a field, however, you can still use the
`printf`-family functions with Logrus.

#### Default Fields

Often it&#039;s helpful to have fields _always_ attached to log statements in an
application or parts of one. For example, you may want to always log the
`request_id` and `user_ip` in the context of a request. Instead of writing
`logger.WithFields(logrus.Fields{&quot;request_id&quot;: request_id, &quot;user_ip&quot;: user_ip})` on
every line, you can create a `logrus.Entry` to pass around instead:

```go
requestLogger := logger.WithFields(logrus.Fields{&quot;request_id&quot;: request_id, &quot;user_ip&quot;: user_ip})
requestLogger.Info(&quot;something happened on that request&quot;) // will log request_id and user_ip
requestLogger.Warn(&quot;something not great happened&quot;)
```

#### Hooks

You can add hooks for logging levels. For example to send errors to an exception
tracking service on `Error`, `Fatal` and `Panic`, info to StatsD or log to
multiple places simultaneously, e.g. syslog.

Logrus comes with [built-in hooks](hooks/). Add those, or your custom hook, in
`init`:

```go
package main

import (
  &quot;log/syslog&quot;

  &quot;github.com/sirupsen/logrus&quot;
  airbrake &quot;gopkg.in/gemnasium/logrus-airbrake-hook.v2&quot;
  logrus_syslog &quot;github.com/sirupsen/logrus/hooks/syslog&quot;
)

func init() {

  // Use the Airbrake hook to report errors that have Error severity or above to
  // an exception tracker. You can create custom hooks, see the Hooks section.
  logrus.AddHook(airbrake.NewHook(123, &quot;xyz&quot;, &quot;production&quot;))

  hook, err := logrus_syslog.NewSyslogHook(&quot;udp&quot;, &quot;localhost:514&quot;, syslog.LOG_INFO, &quot;&quot;)
  if err != nil {
    logrus.Error(&quot;Unable to connect to local syslog daemon&quot;)
  } else {
    logrus.AddHook(hook)
  }
}
```
Note: Syslog hooks also support connecting to local syslog (Ex. &quot;/dev/log&quot; or &quot;/var/run/syslog&quot; or &quot;/var/run/log&quot;). For the detail, please check the [syslog hook README](hooks/syslog/README.md).

A list of currently known service hooks can be found in this wiki [page](https://github.com/sirupsen/logrus/wiki/Hooks)


#### Level logging

Logrus has seven logging levels: Trace, Debug, Info, Warning, Error, Fatal and Panic.

```go
logrus.Trace(&quot;Something very low level.&quot;)
logrus.Debug(&quot;Useful debugging information.&quot;)
logrus.Info(&quot;Something noteworthy happened!&quot;)
logrus.Warn(&quot;You should probably take a look at this.&quot;)
logrus.Error(&quot;Something failed but I&#039;m not quitting.&quot;)
// Calls os.Exit(1) after logging
logrus.Fatal(&quot;Bye.&quot;)
// Calls panic() after logging
logrus.Panic(&quot;I&#039;m bailing.&quot;)
```

You can set the logging level on a `Logger`, then it will only log entries with
that severity or anything above it:

```go
// Will log anything that is info or above (warn, error, fatal, panic). Default.
logrus.SetLevel(logrus.InfoLevel)
```

It may be useful to set `logrus.Level = logrus.DebugLevel` in a debug or verbose
environment if your application has that.

Note: If you want different log levels for global (`logrus.SetLevel(...)`) and syslog logging, please check the [syslog hook README](hooks/syslog/README.md#different-log-levels-for-local-and-remote-logging).

#### Entries

Besides the fields added with `WithField` or `WithFields` some fields are
automatically added to all logging events:

1. `time`. The timestamp when the entry was created.
2. `msg`. The logging message passed to `{Info,Warn,Error,Fatal,Panic}` after
   the `AddFields` call. E.g. `Failed to send event.`
3. `level`. The logging level. E.g. `info`.

#### Environments

Logrus has no notion of environment.

If you wish for hooks and formatters to only be used in specific environments,
you should handle that yourself. For example, if your application has a global
variable `Environment`, which is a string representation of the environment you
could do:

```go
import (
  &quot;github.com/sirupsen/logrus&quot;
)

func init() {
  // do something here to set environment depending on an environment variable
  // or command-line flag
  if Environment == &quot;production&quot; {
    logrus.SetFormatter(&amp;logrus.JSONFormatter{})
  } else {
    // The TextFormatter is default, you don&#039;t actually have to do this.
    logrus.SetFormatter(&amp;logrus.TextFormatter{})
  }
}
```

This configuration is how `logrus` was intended to be used, but JSON in
production is mostly only useful if you do log aggregation with tools like
Splunk or Logstash.

#### Formatters

The built-in logging formatters are:

* `logrus.TextFormatter`. Logs the event in colors if stdout is a tty, otherwise
  without colors.
  * *Note:* to force colored output when there is no TTY, set the `ForceColors`
    field to `true`.  To force no colored output even if there is a TTY  set the
    `DisableColors` field to `true`. For Windows, see
    [github.com/mattn/go-colorable](https://github.com/mattn/go-colorable).
  * When colors are enabled, levels are truncated to 4 characters by default. To disable
    truncation set the `DisableLevelTruncation` field to `true`.
  * When outputting to a TTY, it&#039;s often helpful to visually scan down a column where all the levels are the same width. Setting the `PadLevelText` field to `true` enables this behavior, by adding padding to the level text.
  * All options are listed in the [generated docs](https://pkg.go.dev/github.com/sirupsen/logrus#TextFormatter).
* `logrus.JSONFormatter`. Logs fields as JSON.
  * All options are listed in the [generated docs](https://pkg.go.dev/github.com/sirupsen/logrus#JSONFormatter).

Third-party logging formatters:

* [`FluentdFormatter`](https://github.com/joonix/log). Formats entries that can be parsed by Kubernetes and Google Container Engine.
* [`GELF`](https://github.com/fabienm/go-logrus-formatters). Formats entries so they comply to Graylog&#039;s [GELF 1.1 specification](http://docs.graylog.org/en/2.4/pages/gelf.html).
* [`logstash`](https://github.com/bshuster-repo/logrus-logstash-hook). Logs fields as [Logstash](http://logstash.net) Events.
* [`prefixed`](https://github.com/x-cray/logrus-prefixed-formatter). Displays log entry source along with alternative layout.
* [`zalgo`](https://github.com/aybabtme/logzalgo). Invoking the Power of Zalgo.
* [`nested-logrus-formatter`](https://github.com/antonfisher/nested-logrus-formatter). Converts logrus fields to a nested structure.
* [`powerful-logrus-formatter`](https://github.com/zput/zxcTool). get fileName, log&#039;s line number and the latest function&#039;s name when print log; Save log to files.
* [`caption-json-formatter`](https://github.com/nolleh/caption_json_formatter). logrus&#039;s message json formatter with human-readable caption added.

You can define your formatter by implementing the `Formatter` interface,
requiring a `Format` method. `Format` takes an `*Entry`. `entry.Data` is a
`Fields` type (`map[string]interface{}`) with all your fields as well as the
default ones (see Entries section above):

```go
type MyJSONFormatter struct{}

logrus.SetFormatter(new(MyJSONFormatter))

func (f *MyJSONFormatter) Format(entry *Entry) ([]byte, error) {
  // Note this doesn&#039;t include Time, Level and Message which are available on
  // the Entry. Consult `godoc` on information about those fields or read the
  // source of the official loggers.
  serialized, err := json.Marshal(entry.Data)
    if err != nil {
      return nil, fmt.Errorf(&quot;Failed to marshal fields to JSON, %w&quot;, err)
    }
  return append(serialized, &#039;\n&#039;), nil
}
```

#### Logger as an `io.Writer`

Logrus can be transformed into an `io.Writer`. That writer is the end of an `io.Pipe` and it is your responsibility to close it.

```go
w := logger.Writer()
defer w.Close()

srv := http.Server{
    // create a stdlib log.Logger that writes to
    // logrus.Logger.
    ErrorLog: log.New(w, &quot;&quot;, 0),
}
```

Each line written to that writer will be printed the usual way, using formatters
and hooks. The level for those entries is `info`.

This means that we can override the standard library logger easily:

```go
logger := logrus.New()
logger.Formatter = &amp;logrus.JSONFormatter{}

// Use logrus for standard log output
// Note that `log` here references stdlib&#039;s log
// Not logrus imported under the name `log`.
log.SetOutput(logger.Writer())
```

#### Rotation

Log rotation is not provided with Logrus. Log rotation should be done by an
external program (like `logrotate(8)`) that can compress and delete old log
entries. It should not be a feature of the application-level logger.

#### Tools

| Tool | Description |
| ---- | ----------- |
|[Logrus Mate](https://github.com/gogap/logrus_mate)|Logrus mate is a tool for Logrus to manage loggers, you can initial logger&#039;s level, hook and formatter by config file, the logger will be generated with different configs in different environments.|
|[Logrus Viper Helper](https://github.com/heirko/go-contrib/tree/master/logrusHelper)|An Helper around Logrus to wrap with spf13/Viper to load configuration with fangs! And to simplify Logrus configuration use some behavior of [Logrus Mate](https://github.com/gogap/logrus_mate). [sample](https://github.com/heirko/iris-contrib/blob/master/middleware/logrus-logger/example) |

#### Testing

Logrus has a built-in facility for asserting the presence of log messages. This is implemented through the `test` hook and provides:

* decorators for existing logger (`test.NewLocal` and `test.NewGlobal`) which basically just adds the `test` hook
* a test logger (`test.NewNullLogger`) that just records log messages (and does not output any):

```go
import(
  &quot;testing&quot;

  &quot;github.com/sirupsen/logrus&quot;
  &quot;github.com/sirupsen/logrus/hooks/test&quot;
  &quot;github.com/stretchr/testify/assert&quot;
)

func TestSomething(t*testing.T){
  logger, hook := test.NewNullLogger()
  logger.Error(&quot;Helloerror&quot;)

  assert.Equal(t, 1, len(hook.Entries))
  assert.Equal(t, logrus.ErrorLevel, hook.LastEntry().Level)
  assert.Equal(t, &quot;Helloerror&quot;, hook.LastEntry().Message)

  hook.Reset()
  assert.Nil(t, hook.LastEntry())
}
```

#### Fatal handlers

Logrus can register one or more functions that will be called when any `fatal`
level message is logged. The registered handlers will be executed before
logrus performs an `os.Exit(1)`. This behavior may be helpful if callers need
to gracefully shut down. Unlike a `panic(&quot;Something went wrong...&quot;)` call which can be intercepted with a deferred `recover` a call to `os.Exit(1)` can not be intercepted.

```go
// ...
handler := func() {
  // gracefully shut down something...
}
logrus.RegisterExitHandler(handler)
// ...
```

#### Thread safety

By default, Logger is protected by a mutex for concurrent writes. The mutex is held when calling hooks and writing logs.
If you are sure such locking is not needed, you can call logger.SetNoLock() to disable the locking.

Situations when locking is not needed include:

* You have no hooks registered, or hooks calling is already thread-safe.

* Writing to logger.Out is already thread-safe, for example:

  1) logger.Out is protected by locks.

  2) logger.Out is an os.File handler opened with `O_APPEND` flag, and every write is smaller than 4k. (This allows multi-thread/multi-process writing)

     (Refer to http://www.notthewizard.com/2014/06/17/are-files-appends-really-atomic/)
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[crossplane/crossplane]]></title>
            <link>https://github.com/crossplane/crossplane</link>
            <guid>https://github.com/crossplane/crossplane</guid>
            <pubDate>Fri, 09 Jan 2026 00:05:10 GMT</pubDate>
            <description><![CDATA[The Cloud Native Control Plane]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/crossplane/crossplane">crossplane/crossplane</a></h1>
            <p>The Cloud Native Control Plane</p>
            <p>Language: Go</p>
            <p>Stars: 11,274</p>
            <p>Forks: 1,124</p>
            <p>Stars today: 5 stars today</p>
            <h2>README</h2><pre>[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/3260/badge)](https://www.bestpractices.dev/projects/3260) ![CI](https://github.com/crossplane/crossplane/workflows/CI/badge.svg) [![Go Report Card](https://goreportcard.com/badge/github.com/crossplane/crossplane)](https://goreportcard.com/report/github.com/crossplane/crossplane)

![Crossplane](banner.png)

Crossplane is a framework for building cloud native control planes without
needing to write code. It has a highly extensible backend that enables you to
build a control plane that can orchestrate applications and infrastructure no
matter where they run, and a highly configurable frontend that puts you in
control of the schema of the declarative API it offers.

Crossplane is a [Cloud Native Computing Foundation][cncf] project.

## Get Started

Crossplane&#039;s [Get Started Docs] covers install and resource quickstarts.

## Releases

[![GitHub release](https://img.shields.io/github/release/crossplane/crossplane/all.svg)](https://github.com/crossplane/crossplane/releases) [![Artifact Hub](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/crossplane)](https://artifacthub.io/packages/helm/crossplane/crossplane)

Currently maintained releases, as well as the next few upcoming releases are
listed below. For more information take a look at the Crossplane [release cycle
documentation].

| Release | Release Date  |   EOL    |
|:-------:|:-------------:|:--------:|
|  v1.20  | May 21, 2025  | Feb 2026 |
|  v2.0   |  Aug 8, 2025  | May 2026 |
|  v2.1   |  Nov 5, 2025  | Aug 2026 |
|  v2.2   | Early Feb &#039;26 | Nov 2026 |
|  v2.3   | Early May &#039;26 | Feb 2027 |
|  v2.4   | Early Aug &#039;26 | May 2027 |

You can subscribe to the [community calendar] to track all release dates, and
find the most recent releases on the [releases] page.

The release process is fully documented in the [`crossplane/release`] repo.

## Roadmap

The public roadmap for Crossplane is published as a GitHub project board. Issues
added to the roadmap have been triaged and identified as valuable to the
community, and therefore a priority for the project that we expect to invest in.

The maintainer team regularly triages requests from the community to identify
features and issues of suitable scope and impact to include in this roadmap. The
community is encouraged to show their support for potential roadmap issues by
adding a :+1: reaction, leaving descriptive comments, and attending the
[regular community meetings] to discuss their requirements and use cases.

The maintainer team updates the roadmap on an as needed basis, in response to
demand, priority, and available resources. The public roadmap can be updated at
any time.

Milestones assigned to any issues in the roadmap are intended to give a sense of
overall priority and the expected order of delivery. They should be considered
approximate estimations and are **not** a strict commitment to a specific
delivery timeline.

[Crossplane Roadmap]

## Get Involved

[![Slack](https://img.shields.io/badge/slack-crossplane-red?logo=slack)](https://slack.crossplane.io) [![Bluesky Follow](https://img.shields.io/badge/bluesky-Follow-blue?logo=bluesky)](https://bsky.app/profile/crossplane.io) [![Twitter Follow](https://img.shields.io/twitter/follow/crossplane_io?logo=X&amp;label=Follow&amp;style=flat)](https://twitter.com/intent/follow?screen_name=crossplane_io&amp;user_id=788180534543339520) [![YouTube Channel Subscribers](https://img.shields.io/youtube/channel/subscribers/UC19FgzMBMqBro361HbE46Fw)](https://www.youtube.com/@Crossplane)

Crossplane is a community driven project; we welcome your contribution. To file
a bug, suggest an improvement, or request a new feature please open an [issue
against Crossplane] or the relevant provider. Refer to our [contributing guide]
for more information on how you can help.

* Discuss Crossplane on [Slack].
* Follow us on [Bluesky], [Twitter], or [LinkedIn].
* Contact us via [Email].
* Join our regular community meetings.
* Provide feedback on our [roadmap and releases board].

The Crossplane community meeting takes place every 4 weeks on [Thursday at
10:00am Pacific Time][community meeting time]. You can find the up to date
meeting schedule on the [Community Calendar][community calendar].

Anyone who wants to discuss the direction of the project, design and
implementation reviews, or raise general questions with the broader community is
encouraged to join.

* Meeting link: &lt;https://zoom-lfx.platform.linuxfoundation.org/meeting/98901510164?password=c60c41ae-1e1e-42d0-9a74-16de2fbb66f9&gt;
* [Current agenda and past meeting notes]
* [Past meeting recordings]
* [Community Calendar][community calendar]

### Special Interest Groups (SIG)

The Crossplane project supports SIGs as discussion groups that bring together
community members with shared interests. SIGs have no decision making authority
or ownership responsibilities. They serve purely as collaborative forums for
community discussion.

If you&#039;re interested in any of the areas below, consider joining the discussion
in their Slack channels. To propose a new SIG that isn&#039;t represented, reach out
through any of the contact methods in the [get involved] section.

Each SIG collaborates primarily in Slack, and some groups hold regular meetings
that you can find in the [Community Calendar][community calendar].

- [#sig-cli][sig-cli]
- [#sig-composition-environments][sig-composition-environments-slack]
- [#sig-composition-functions][sig-composition-functions-slack]
- [#sig-deletion-ordering][sig-deletion-ordering-slack]
- [#sig-devex][sig-devex-slack]
- [#sig-docs][sig-docs-slack]
- [#sig-e2e-testing][sig-e2e-testing-slack]
- [#sig-observability][sig-observability-slack]
- [#sig-observe-only][sig-observe-only-slack]
- [#sig-provider-families][sig-provider-families-slack]
- [#sig-secret-stores][sig-secret-stores-slack]
- [#sig-upjet][sig-upjet-slack]

## Adopters

A list of publicly known users of the Crossplane project can be found in [ADOPTERS.md].  We
encourage all users of Crossplane to add themselves to this list - we want to see the community&#039;s
growing success!

## License

Crossplane is under the Apache 2.0 license.

[![FOSSA Status](https://app.fossa.io/api/projects/git%2Bgithub.com%2Fcrossplane%2Fcrossplane.svg?type=large)](https://app.fossa.io/projects/git%2Bgithub.com%2Fcrossplane%2Fcrossplane?ref=badge_large)

&lt;!-- Named links --&gt;

[Crossplane]: https://crossplane.io
[release cycle documentation]: https://docs.crossplane.io/knowledge-base/guides/release-cycle
[install]: https://crossplane.io/docs/latest
[Slack]: https://slack.crossplane.io
[Bluesky]: https://bsky.app/profile/crossplane.io
[Twitter]: https://twitter.com/crossplane_io
[LinkedIn]: https://www.linkedin.com/company/crossplane/
[Email]: mailto:crossplane-info@lists.cncf.io
[issue against Crossplane]: https://github.com/crossplane/crossplane/issues
[contributing guide]: contributing/README.md
[community meeting time]: https://www.thetimezoneconverter.com/?t=10:00&amp;tz=PT%20%28Pacific%20Time%29
[Current agenda and past meeting notes]: https://docs.google.com/document/d/1q_sp2jLQsDEOX7Yug6TPOv7Fwrys6EwcF5Itxjkno7Y/edit?usp=sharing
[Past meeting recordings]: https://www.youtube.com/playlist?list=PL510POnNVaaYYYDSICFSNWFqNbx1EMr-M
[roadmap and releases board]: https://github.com/orgs/crossplane/projects/20/views/9?pane=info
[cncf]: https://www.cncf.io/
[Get Started Docs]: https://docs.crossplane.io/latest/get-started/get-started-with-composition
[community calendar]: https://zoom-lfx.platform.linuxfoundation.org/meetings/crossplane?view=month
[releases]: https://github.com/crossplane/crossplane/releases
[`crossplane/release`]: https://github.com/crossplane/release
[ADOPTERS.md]: ADOPTERS.md
[regular community meetings]: https://github.com/crossplane/crossplane/blob/main/README.md#get-involved
[Crossplane Roadmap]: https://github.com/orgs/crossplane/projects/20/views/9?pane=info
[get involved]: https://github.com/crossplane/crossplane/blob/main/README.md#get-involved
[sig-cli]: https://crossplane.slack.com/archives/C08V9PMLRQA
[sig-composition-environments-slack]: https://crossplane.slack.com/archives/C05BP6QFLUW
[sig-composition-functions-slack]: https://crossplane.slack.com/archives/C031Y29CSAE
[sig-deletion-ordering-slack]: https://crossplane.slack.com/archives/C05BP8W5ALW
[sig-devex-slack]: https://crossplane.slack.com/archives/C05U1LLM3B2
[sig-docs-slack]: https://crossplane.slack.com/archives/C02CAQ52DPU
[sig-e2e-testing-slack]: https://crossplane.slack.com/archives/C05C8CCTVNV
[sig-observability-slack]: https://crossplane.slack.com/archives/C061GNH3LA0
[sig-observe-only-slack]: https://crossplane.slack.com/archives/C04D5988QEA
[sig-provider-families-slack]: https://crossplane.slack.com/archives/C056YAQRV16
[sig-secret-stores-slack]: https://crossplane.slack.com/archives/C05BY7DKFV2
[sig-upjet-slack]: https://crossplane.slack.com/archives/C05T19TB729
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[aquasecurity/trivy]]></title>
            <link>https://github.com/aquasecurity/trivy</link>
            <guid>https://github.com/aquasecurity/trivy</guid>
            <pubDate>Fri, 09 Jan 2026 00:05:09 GMT</pubDate>
            <description><![CDATA[Find vulnerabilities, misconfigurations, secrets, SBOM in containers, Kubernetes, code repositories, clouds and more]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/aquasecurity/trivy">aquasecurity/trivy</a></h1>
            <p>Find vulnerabilities, misconfigurations, secrets, SBOM in containers, Kubernetes, code repositories, clouds and more</p>
            <p>Language: Go</p>
            <p>Stars: 30,879</p>
            <p>Forks: 2,869</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;docs/imgs/logo.png&quot; width=&quot;200&quot;&gt;

[![GitHub Release][release-img]][release]
[![Test][test-img]][test]
[![Go Report Card][go-report-img]][go-report]
[![License: Apache-2.0][license-img]][license]
[![GitHub Downloads][github-downloads-img]][release]
![Docker Pulls][docker-pulls]

[üìñ Documentation][docs]
&lt;/div&gt;

Trivy ([pronunciation][pronunciation]) is a comprehensive and versatile security scanner.
Trivy has *scanners* that look for security issues, and *targets* where it can find those issues.

Targets (what Trivy can scan):

- Container Image
- Filesystem
- Git Repository (remote)
- Virtual Machine Image
- Kubernetes

Scanners (what Trivy can find there):

- OS packages and software dependencies in use (SBOM)
- Known vulnerabilities (CVEs)
- IaC issues and misconfigurations
- Sensitive information and secrets
- Software licenses

Trivy supports most popular programming languages, operating systems, and platforms. For a complete list, see the [Scanning Coverage] page.

To learn more, go to the [Trivy homepage][homepage] for feature highlights, or to the [Documentation site][docs] for detailed information.

## Quick Start

### Get Trivy

Trivy is available in most common distribution channels. The full list of installation options is available in the [Installation] page. Here are a few popular examples:

- `brew install trivy`
- `docker run aquasec/trivy`
- Download binary from &lt;https://github.com/aquasecurity/trivy/releases/latest/&gt;
- See [Installation] for more

Trivy is integrated with many popular platforms and applications. The complete list of integrations is available in the [Ecosystem] page. Here are a few popular examples:

- [GitHub Actions](https://github.com/aquasecurity/trivy-action)
- [Kubernetes operator](https://github.com/aquasecurity/trivy-operator)
- [VS Code plugin](https://github.com/aquasecurity/trivy-vscode-extension)
- See [Ecosystem] for more

### Canary builds
There are canary builds ([Docker Hub](https://hub.docker.com/r/aquasec/trivy/tags?page=1&amp;name=canary), [GitHub](https://github.com/aquasecurity/trivy/pkgs/container/trivy/75776514?tag=canary), [ECR](https://gallery.ecr.aws/aquasecurity/trivy#canary) images and [binaries](https://github.com/aquasecurity/trivy/actions/workflows/canary.yaml)) generated with every push to the main branch.

Please be aware: canary builds might have critical bugs, so they are not recommended for use in production.

### General usage

```bash
trivy &lt;target&gt; [--scanners &lt;scanner1,scanner2&gt;] &lt;subject&gt;
```

Examples:

```bash
trivy image python:3.4-alpine
```

&lt;details&gt;
&lt;summary&gt;Result&lt;/summary&gt;

https://user-images.githubusercontent.com/1161307/171013513-95f18734-233d-45d3-aaf5-d6aec687db0e.mov

&lt;/details&gt;

```bash
trivy fs --scanners vuln,secret,misconfig myproject/
```

&lt;details&gt;
&lt;summary&gt;Result&lt;/summary&gt;

https://user-images.githubusercontent.com/1161307/171013917-b1f37810-f434-465c-b01a-22de036bd9b3.mov

&lt;/details&gt;

```bash
trivy k8s --report summary cluster
```

&lt;details&gt;
&lt;summary&gt;Result&lt;/summary&gt;

![k8s summary](docs/imgs/trivy-k8s.png)

&lt;/details&gt;

## FAQ

### How to pronounce the name &quot;Trivy&quot;?

`tri` is pronounced like **tri**gger, `vy` is pronounced like en**vy**.

## Want more? Check out Aqua

If you liked Trivy, you will love Aqua which builds on top of Trivy to provide even more enhanced capabilities for a complete security management offering.  
You can find a high level comparison table specific to Trivy users [here](https://trivy.dev/docs/latest/commercial/compare/).
In addition check out the &lt;https://aquasec.com&gt; website for more information about our products and services.
If you&#039;d like to contact Aqua or request a demo, please use this form: &lt;https://www.aquasec.com/demo&gt;

## Community

Trivy is an [Aqua Security][aquasec] open source project.  
Learn about our open source work and portfolio [here][oss].  
Contact us about any matter by opening a GitHub Discussion [here][discussions]

Please ensure to abide by our [Code of Conduct][code-of-conduct] during all interactions.

[test]: https://github.com/aquasecurity/trivy/actions/workflows/test.yaml
[test-img]: https://github.com/aquasecurity/trivy/actions/workflows/test.yaml/badge.svg
[go-report]: https://goreportcard.com/report/github.com/aquasecurity/trivy
[go-report-img]: https://goreportcard.com/badge/github.com/aquasecurity/trivy
[release]: https://github.com/aquasecurity/trivy/releases
[release-img]: https://img.shields.io/github/release/aquasecurity/trivy.svg?logo=github
[github-downloads-img]: https://img.shields.io/github/downloads/aquasecurity/trivy/total?logo=github
[docker-pulls]: https://img.shields.io/docker/pulls/aquasec/trivy?logo=docker&amp;label=docker%20pulls%20%2F%20trivy
[license]: https://github.com/aquasecurity/trivy/blob/main/LICENSE
[license-img]: https://img.shields.io/badge/License-Apache%202.0-blue.svg
[homepage]: https://trivy.dev
[docs]: https://trivy.dev/docs/latest/
[pronunciation]: #how-to-pronounce-the-name-trivy
[code-of-conduct]: https://github.com/aquasecurity/community/blob/main/CODE_OF_CONDUCT.md

[Installation]:https://trivy.dev/docs/latest/getting-started/installation/
[Ecosystem]: https://trivy.dev/docs/latest/ecosystem/
[Scanning Coverage]: https://trivy.dev/docs/latest/coverage/

[alpine]: https://ariadne.space/2021/06/08/the-vulnerability-remediation-lifecycle-of-alpine-containers/
[rego]: https://www.openpolicyagent.org/docs/latest/#rego
[sigstore]: https://www.sigstore.dev/

[aquasec]: https://aquasec.com
[oss]: https://www.aquasec.com/products/open-source-projects/
[discussions]: https://github.com/aquasecurity/trivy/discussions
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
    </channel>
</rss>