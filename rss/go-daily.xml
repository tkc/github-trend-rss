<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for go - Go Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for go.</description>
        <lastBuildDate>Thu, 11 Dec 2025 00:05:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[KaijuEngine/kaiju]]></title>
            <link>https://github.com/KaijuEngine/kaiju</link>
            <guid>https://github.com/KaijuEngine/kaiju</guid>
            <pubDate>Thu, 11 Dec 2025 00:05:01 GMT</pubDate>
            <description><![CDATA[General purpose 3D and 2D game engine using Go (golang) and Vulkan with built in editor]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/KaijuEngine/kaiju">KaijuEngine/kaiju</a></h1>
            <p>General purpose 3D and 2D game engine using Go (golang) and Vulkan with built in editor</p>
            <p>Language: Go</p>
            <p>Stars: 2,740</p>
            <p>Forks: 87</p>
            <p>Stars today: 1,480 stars today</p>
            <h2>README</h2><pre># Kaiju Engine
Kaiju is a 2D/3D game engine written in Go (Golang) backed by Vulkan. The goal of the engine is to use a modern, easy, systems level programming language, with a focus on simplicity, to create a new kind of game engine.

- üìÑ 2D / üßä 3D Game Engine
- ü™ü Windows
- üêß Linux
- ü§ñ Android (NEW, support now functional)
- üçé Mac (support is [currently WIP](https://github.com/KaijuEngine/kaiju/pull/489))
- ü§ñüëâ‚å®Ô∏è Local AI (LLM) interop
- ‚ö†Ô∏èüößüèóÔ∏èüë∑‚Äç‚ôÇÔ∏è Work in progress, under heavy development
- üöö Faster builds than other game engines
- üî• Better performance than other game engines (9x faster than Unity out of the box)
- üíæ Less memory than other engines

## Join the community
- [GitHub repository](https://github.com/KaijuEngine/kaiju)
- [Mailing list](https://www.freelists.org/list/kaijuengine) &lt;- Recommended for detailed updates
- [Discord server](https://discord.gg/8rFPEu8U52)
- [Brent Farris on X/Twitter](https://twitter.com/ShieldCrush)

## Why Kaiju?
The current version of the base engine renders extremely fast, faster than most would think a garbage collected language could go. In my testing a release mode build of a game in Unity with nothing but a black background and a cube runs at about 1,600 FPS. In Kaiju, the same thing runs at around 5,400 FPS on the same machine. In fact, a complete game, with audio, custom cursors, real time PBR rendering with real time shadows, UI, and more runs at 2,712 FPS (in &quot;debug&quot; mode) [screenshots or it didn&#039;t happen](https://x.com/ShieldCrush/status/1943516032674537958).

## Why Go (golang)?
I love C, and because I love C and found out that Ken Thompson played a part in designing Go, I gave Go a chance. It has been such a joy to use and work with I decided to port my C game engine to Go. Go is a modern system-level language that allows me to write code the way I want to write code and even have the opportunity to do some crazy things if I want to (no strings attached). Also the simplicity and &quot;just works&quot; of writing Assembly code was a great boost to my happiness.

What&#039;s more, it&#039;s a language that other developers can easily learn and jump right into extending the engine/editor. No need for developers to re-figure out some bespoke macros or crazy templating nonsense. It&#039;s flat, easy, straight forward, and the foot-gun is hidden behind some walls, but there if you want it. Furthermore, developers can write their games in Go directly, no need for some alternative language that is different from the engine code (but we&#039;ll include Lua for modding).

## What about the Garbage Collector?!
I am creating this section because I get asked about it when I mention &quot;Go&quot;, possibly not realizing that most public game engines use a garbage collector (GC).

The GC is actually a feature I&#039;m happy with (shocker coming from a C guy). Well, the reason is simple, if you&#039;re going to make a game engine that the public will use and needs to be stable, you need a garbage collector. Unity has C# (and possibly an internal GC as well), Unreal has a GC (and it could use a tune up if you ask me), Godot has a GC albeit their scripting language or when you use C#. It is actually very important for public engines to have a GC because people are only human and make a lot of mistakes, mistakes they&#039;ll blame on you (the engine developer) before they blame themselves.

Coincidentally, the overall design I have for the engine plays very well with the GC and last I measured, I have a net-0 heap allocation while running (may need a new review). If you don&#039;t abuse the GC, you shouldn&#039;t generally feel it, it runs concurrently as well.

I&#039;ll be the first to admit, I think the developers of Go can create a better GC than I can, and probably better than Unreal and Unity too.

## ‚ö†Ô∏è WORK IN PROGRESS ‚ö†Ô∏è
Though the engine is production ready, the editor **_is not_**, feel free to join and contribute to its development.

For the latest updates, please join the [Discord](https://discord.gg/HYj7Dh7ke3) or check my [Twitter/X](https://twitter.com/ShieldCrush).

Please review the Ad-Hoc [editor readme](https://github.com/KaijuEngine/kaiju/blob/master/src/editor/README.md)

## Compiling the engine
&gt; **Windows developers must install the 64-bit Go toolchain (`windows-amd64`).**
&gt; The 32-bit (`windows-386`) Go distribution will not compile Kaiju‚Äôs Vulkan backend.
Please see the [documentation](https://kaijuengine.org/engine_developers/build_from_source/) on how to get started and compile the engine

## Editor previews
‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è **Please note, this video is not professional at all. It&#039;s one I made to aid in the [Mac port pull request](https://github.com/KaijuEngine/kaiju/pull/489), but shows many features.**

[(YouTube) Compatibility requirements video for Mac](https://www.youtube.com/watch?v=B36gYYSNRDc)

### Older videos
[full-project-run-cycle.mp4](https://github.com/user-attachments/assets/04c75879-23af-40fa-9773-33cd22cc9552)

[clanker.mp4](https://github.com/user-attachments/assets/6be56b37-589b-4197-86e7-18b1153f7e07)

[working-code-binding.mp4](https://github.com/user-attachments/assets/b7edcbfb-0c78-482f-8eb1-f40910fbaabf)

[content-tagging.mp4](https://github.com/user-attachments/assets/15122db6-efda-4458-bf69-f384def5aa31)

[status-bar-update.mp4](https://github.com/user-attachments/assets/6f3d6511-5db0-405f-b264-af041c199bd0)

[focus-and-transform-hotkeys](https://github.com/user-attachments/assets/95a9bcdc-55fe-4317-9200-412f84a494ce)

## Star history
[![Star History Chart](https://api.star-history.com/svg?repos=KaijuEngine/kaiju&amp;type=Date)](https://star-history.com/#KaijuEngine/kaiju&amp;Date)   
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[hashicorp/terraform]]></title>
            <link>https://github.com/hashicorp/terraform</link>
            <guid>https://github.com/hashicorp/terraform</guid>
            <pubDate>Thu, 11 Dec 2025 00:05:00 GMT</pubDate>
            <description><![CDATA[Terraform enables you to safely and predictably create, change, and improve infrastructure. It is a source-available tool that codifies APIs into declarative configuration files that can be shared amongst team members, treated as code, edited, reviewed, and versioned.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hashicorp/terraform">hashicorp/terraform</a></h1>
            <p>Terraform enables you to safely and predictably create, change, and improve infrastructure. It is a source-available tool that codifies APIs into declarative configuration files that can be shared amongst team members, treated as code, edited, reviewed, and versioned.</p>
            <p>Language: Go</p>
            <p>Stars: 47,252</p>
            <p>Forks: 10,140</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre># Terraform

- Website: https://developer.hashicorp.com/terraform
- Forums: [HashiCorp Discuss](https://discuss.hashicorp.com/c/terraform-core)
- Documentation: [https://developer.hashicorp.com/terraform/docs](https://developer.hashicorp.com/terraform/docs)
- Tutorials: [HashiCorp&#039;s Learn Platform](https://developer.hashicorp.com/terraform/tutorials)
- Certification Exam: [HashiCorp Certified: Terraform Associate](https://www.hashicorp.com/certification/#hashicorp-certified-terraform-associate)

&lt;img alt=&quot;Terraform&quot; src=&quot;https://www.datocms-assets.com/2885/1731373310-terraform_white.svg&quot; width=&quot;600px&quot;&gt;

Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can manage existing and popular service providers as well as custom in-house solutions.

The key features of Terraform are:

- **Infrastructure as Code**: Infrastructure is described using a high-level configuration syntax. This allows a blueprint of your datacenter to be versioned and treated as you would any other code. Additionally, infrastructure can be shared and re-used.

- **Execution Plans**: Terraform has a &quot;planning&quot; step where it generates an execution plan. The execution plan shows what Terraform will do when you call apply. This lets you avoid any surprises when Terraform manipulates infrastructure.

- **Resource Graph**: Terraform builds a graph of all your resources, and parallelizes the creation and modification of any non-dependent resources. Because of this, Terraform builds infrastructure as efficiently as possible, and operators get insight into dependencies in their infrastructure.

- **Change Automation**: Complex changesets can be applied to your infrastructure with minimal human interaction. With the previously mentioned execution plan and resource graph, you know exactly what Terraform will change and in what order, avoiding many possible human errors.

For more information, refer to the [What is Terraform?](https://www.terraform.io/intro) page on the Terraform website.

## Getting Started &amp; Documentation

Documentation is available on the [Terraform website](https://developer.hashicorp.com/terraform):

- [Introduction](https://developer.hashicorp.com/terraform/intro)
- [Documentation](https://developer.hashicorp.com/terraform/docs)

If you&#039;re new to Terraform and want to get started creating infrastructure, please check out our [Getting Started guides](https://learn.hashicorp.com/terraform#getting-started) on HashiCorp&#039;s learning platform. There are also [additional guides](https://learn.hashicorp.com/terraform#operations-and-development) to continue your learning.

Show off your Terraform knowledge by passing a certification exam. Visit the [certification page](https://www.hashicorp.com/certification/) for information about exams and find [study materials](https://learn.hashicorp.com/terraform/certification/terraform-associate) on HashiCorp&#039;s learning platform.

## Developing Terraform

This repository contains only Terraform core, which includes the command line interface and the main graph engine. Providers are implemented as plugins, and Terraform can automatically download providers that are published on [the Terraform Registry](https://registry.terraform.io). HashiCorp develops some providers, and others are developed by other organizations. For more information, refer to [Plugin development](https://developer.hashicorp.com/terraform/plugin).

- To learn more about compiling Terraform and contributing suggested changes, refer to [the contributing guide](.github/CONTRIBUTING.md).

- To learn more about how we handle bug reports, refer to the [bug triage guide](./BUGPROCESS.md).

- To learn how to contribute to the Terraform documentation, refer to the [Web Unified Docs repository](https://github.com/hashicorp/web-unified-docs).

## License

[Business Source License 1.1](https://github.com/hashicorp/terraform/blob/main/LICENSE)
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[moby/buildkit]]></title>
            <link>https://github.com/moby/buildkit</link>
            <guid>https://github.com/moby/buildkit</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:59 GMT</pubDate>
            <description><![CDATA[concurrent, cache-efficient, and Dockerfile-agnostic builder toolkit]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/moby/buildkit">moby/buildkit</a></h1>
            <p>concurrent, cache-efficient, and Dockerfile-agnostic builder toolkit</p>
            <p>Language: Go</p>
            <p>Stars: 9,582</p>
            <p>Forks: 1,341</p>
            <p>Stars today: 12 stars today</p>
            <h2>README</h2><pre>[![asciicinema example](https://asciinema.org/a/gPEIEo1NzmDTUu2bEPsUboqmU.png)](https://asciinema.org/a/gPEIEo1NzmDTUu2bEPsUboqmU)

# BuildKit &lt;!-- omit in toc --&gt;

[![GitHub Release](https://img.shields.io/github/release/moby/buildkit.svg?style=flat-square)](https://github.com/moby/buildkit/releases/latest)
[![PkgGoDev](https://img.shields.io/badge/go.dev-docs-007d9c?style=flat-square&amp;logo=go&amp;logoColor=white)](https://pkg.go.dev/github.com/moby/buildkit/client/llb)
[![CI BuildKit Status](https://img.shields.io/github/actions/workflow/status/moby/buildkit/buildkit.yml?label=buildkit&amp;logo=github&amp;style=flat-square)](https://github.com/moby/buildkit/actions?query=workflow%3Abuildkit)
[![CI Frontend Status](https://img.shields.io/github/actions/workflow/status/moby/buildkit/frontend.yml?label=frontend&amp;logo=github&amp;style=flat-square)](https://github.com/moby/buildkit/actions?query=workflow%3Afrontend)
[![Go Report Card](https://goreportcard.com/badge/github.com/moby/buildkit?style=flat-square)](https://goreportcard.com/report/github.com/moby/buildkit)
[![Codecov](https://img.shields.io/codecov/c/github/moby/buildkit?logo=codecov&amp;style=flat-square)](https://codecov.io/gh/moby/buildkit)

BuildKit is a toolkit for converting source code to build artifacts in an efficient, expressive and repeatable manner.

Key features:

-   Automatic garbage collection
-   Extendable frontend formats
-   Concurrent dependency resolution
-   Efficient instruction caching
-   Build cache import/export
-   Nested build job invocations
-   Distributable workers
-   Multiple output formats
-   Pluggable architecture
-   Execution without root privileges

Read the proposal from https://github.com/moby/moby/issues/32925

Introductory blog post https://blog.mobyproject.org/introducing-buildkit-17e056cc5317

Join `#buildkit` channel on [Docker Community Slack](https://dockr.ly/comm-slack)

&gt; [!NOTE]
&gt; If you are visiting this repo for the usage of BuildKit-only Dockerfile features
&gt; like `RUN --mount=type=(bind|cache|tmpfs|secret|ssh)`, please refer to the
&gt; [Dockerfile reference](https://docs.docker.com/engine/reference/builder/).

&gt; [!NOTE]
&gt; `docker build` [uses Buildx and BuildKit by default](https://docs.docker.com/build/architecture/) since Docker Engine 23.0.
&gt; You don&#039;t need to read this document unless you want to use the full-featured
&gt; standalone version of BuildKit.

&lt;!-- START doctoc generated TOC please keep comment here to allow auto update --&gt;
&lt;!-- DON&#039;T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE --&gt;

- [Used by](#used-by)
- [Quick start](#quick-start)
  - [Linux Setup](#linux-setup)
  - [Windows Setup](#windows-setup)
  - [macOS Setup](#macos-setup)
  - [Build from source](#build-from-source)
  - [Exploring LLB](#exploring-llb)
  - [Exploring Dockerfiles](#exploring-dockerfiles)
    - [Building a Dockerfile with `buildctl`](#building-a-dockerfile-with-buildctl)
    - [Building a Dockerfile using external frontend](#building-a-dockerfile-using-external-frontend)
  - [Output](#output)
    - [Image/Registry](#imageregistry)
    - [Local directory](#local-directory)
    - [Docker tarball](#docker-tarball)
    - [OCI tarball](#oci-tarball)
    - [containerd image store](#containerd-image-store)
- [Cache](#cache)
  - [Garbage collection](#garbage-collection)
  - [Export cache](#export-cache)
    - [Inline (push image and cache together)](#inline-push-image-and-cache-together)
    - [Registry (push image and cache separately)](#registry-push-image-and-cache-separately)
    - [Local directory](#local-directory-1)
    - [GitHub Actions cache (experimental)](#github-actions-cache-experimental)
    - [S3 cache (experimental)](#s3-cache-experimental)
    - [Azure Blob Storage cache (experimental)](#azure-blob-storage-cache-experimental)
  - [Consistent hashing](#consistent-hashing)
- [Metadata](#metadata)
- [Systemd socket activation](#systemd-socket-activation)
- [Expose BuildKit as a TCP service](#expose-buildkit-as-a-tcp-service)
  - [Load balancing](#load-balancing)
- [Containerizing BuildKit](#containerizing-buildkit)
  - [Podman](#podman)
  - [Nerdctl](#nerdctl)
  - [Kubernetes](#kubernetes)
  - [Daemonless](#daemonless)
- [OpenTelemetry support](#opentelemetry-support)
- [Running BuildKit without root privileges](#running-buildkit-without-root-privileges)
- [Building multi-platform images](#building-multi-platform-images)
  - [Configuring `buildctl`](#configuring-buildctl)
    - [Color Output Controls](#color-output-controls)
    - [Number of log lines (for active steps in tty mode)](#number-of-log-lines-for-active-steps-in-tty-mode)
- [Contributing](#contributing)

&lt;!-- END doctoc generated TOC please keep comment here to allow auto update --&gt;

## Used by

BuildKit is used by the following projects:

-   [Moby &amp; Docker](https://github.com/moby/moby/pull/37151) (`DOCKER_BUILDKIT=1 docker build`)
-   [img](https://github.com/genuinetools/img)
-   [OpenFaaS Cloud](https://github.com/openfaas/openfaas-cloud)
-   [container build interface](https://github.com/containerbuilding/cbi)
-   [Tekton Pipelines](https://github.com/tektoncd/catalog) (formerly [Knative Build Templates](https://github.com/knative/build-templates))
-   [the Sanic build tool](https://github.com/distributed-containers-inc/sanic)
-   [vab](https://github.com/stellarproject/vab)
-   [Rio](https://github.com/rancher/rio)
-   [kim](https://github.com/rancher/kim)
-   [PouchContainer](https://github.com/alibaba/pouch)
-   [Docker buildx](https://github.com/docker/buildx)
-   [Okteto Cloud](https://okteto.com/)
-   [Earthly earthfiles](https://github.com/vladaionescu/earthly)
-   [Gitpod](https://github.com/gitpod-io/gitpod)
-   [Dagger](https://dagger.io)
-   [envd](https://github.com/tensorchord/envd/)
-   [Depot](https://depot.dev)
-   [Namespace](https://namespace.so)
-   [Unikraft](https://unikraft.org)
-   [DevZero](https://devzero.io)
-   [dacc](https://github.com/r2d4/dacc)

## Quick start

:information_source: For Kubernetes deployments, see [`examples/kubernetes`](./examples/kubernetes).

BuildKit is composed of the `buildkitd` daemon and the `buildctl` client.
While the `buildctl` client is available for Linux, macOS, and Windows, the `buildkitd` daemon is only available for Linux and *Windows currently.

The latest binaries of BuildKit are available [here](https://github.com/moby/buildkit/releases) for Linux, macOS, and Windows.


### Linux Setup

The `buildkitd` daemon requires the following components to be installed:
-   [runc](https://github.com/opencontainers/runc) or [crun](https://github.com/containers/crun)
-   [containerd](https://github.com/containerd/containerd) (if you want to use containerd worker)

**Starting the `buildkitd` daemon:**
You need to run `buildkitd` as the root user on the host.

```bash
$ sudo buildkitd
```

To run `buildkitd` as a non-root user, see [`docs/rootless.md`](docs/rootless.md).

The buildkitd daemon supports two worker backends: OCI (runc) and containerd.

By default, the OCI (runc) worker is used. You can set `--oci-worker=false --containerd-worker=true` to use the containerd worker.

We are open to adding more backends.

To start the buildkitd daemon using systemd socket activation, you can install the buildkit systemd unit files.
See [Systemd socket activation](#systemd-socket-activation)

The buildkitd daemon listens gRPC API on `/run/buildkit/buildkitd.sock` by default, but you can also use TCP sockets.
See [Expose BuildKit as a TCP service](#expose-buildkit-as-a-tcp-service).

### Windows Setup

See instructions and notes at [`docs/windows.md`](./docs/windows.md).

### macOS Setup

[Homebrew formula](https://formulae.brew.sh/formula/buildkit) (unofficial) is available for macOS.
```console
$ brew install buildkit
```

The Homebrew formula does not contain the daemon (`buildkitd`).

For example, [Lima](https://lima-vm.io) can be used for launching the daemon inside a Linux VM.
```console
brew install lima
limactl start template://buildkit
export BUILDKIT_HOST=&quot;unix://$HOME/.lima/buildkit/sock/buildkitd.sock&quot;
```

### Build from source

To build BuildKit from source, see [`.github/CONTRIBUTING.md`](./.github/CONTRIBUTING.md).

For a `buildctl` reference, see [this document](./docs/reference/buildctl.md).

### Exploring LLB

BuildKit builds are based on a binary intermediate format called LLB that is used for defining the dependency graph for processes running part of your build. tl;dr: LLB is to Dockerfile what LLVM IR is to C.

-   Marshaled as Protobuf messages
-   Concurrently executable
-   Efficiently cacheable
-   Vendor-neutral (i.e. non-Dockerfile languages can be easily implemented)

See [`solver/pb/ops.proto`](./solver/pb/ops.proto) for the format definition, and see [`./examples/README.md`](./examples/README.md) for example LLB applications.

Currently, the following high-level languages have been implemented for LLB:

-   Dockerfile (See [Exploring Dockerfiles](#exploring-dockerfiles))
-   [Buildpacks](https://github.com/tonistiigi/buildkit-pack)
-   [Mockerfile](https://matt-rickard.com/building-a-new-dockerfile-frontend/)
-   [Gockerfile](https://github.com/po3rin/gockerfile)
-   [bldr (Pkgfile)](https://github.com/talos-systems/bldr/)
-   [HLB](https://github.com/openllb/hlb)
-   [Earthfile (Earthly)](https://github.com/earthly/earthly)
-   [Cargo Wharf (Rust)](https://github.com/denzp/cargo-wharf)
-   [Nix](https://github.com/reproducible-containers/buildkit-nix)
-   [mopy (Python)](https://github.com/cmdjulian/mopy)
-   [envd (starlark)](https://github.com/tensorchord/envd/)
-   [Blubber](https://gitlab.wikimedia.org/repos/releng/blubber)
-   [Bass](https://github.com/vito/bass)
-   [kraft.yaml (Unikraft)](https://github.com/unikraft/kraftkit/tree/staging/tools/dockerfile-llb-frontend)
-   [r2d4/llb (JSON Gateway)](https://github.com/r2d4/llb)
-   [Mass√©](https://github.com/marxarelli/masse)
-   [DALEC](https://github.com/project-dalec/dalec)
-   (open a PR to add your own language)

### Exploring Dockerfiles

Frontends are components that run inside BuildKit and convert any build definition to LLB. There is a special frontend called gateway (`gateway.v0`) that allows using any image as a frontend.

During development, Dockerfile frontend (`dockerfile.v0`) is also part of the BuildKit repo. In the future, this will be moved out, and Dockerfiles can be built using an external image.

#### Building a Dockerfile with `buildctl`

```bash
buildctl build \
    --frontend=dockerfile.v0 \
    --local context=. \
    --local dockerfile=.
# or
buildctl build \
    --frontend=dockerfile.v0 \
    --local context=. \
    --local dockerfile=. \
    --opt target=foo \
    --opt build-arg:foo=bar
```

`--local` exposes local source files from client to the builder. `context` and `dockerfile` are the names Dockerfile frontend looks for build context and Dockerfile location.

If the Dockerfile has a different filename it can be specified with `--opt filename=./Dockerfile-alternative`.

#### Building a Dockerfile using external frontend

External versions of the Dockerfile frontend are pushed to https://hub.docker.com/r/docker/dockerfile-upstream and https://hub.docker.com/r/docker/dockerfile and can be used with the gateway frontend. The source for the external frontend is currently located in `./frontend/dockerfile/cmd/dockerfile-frontend` but will move out of this repository in the future ([#163](https://github.com/moby/buildkit/issues/163)). For automatic build from master branch of this repository `docker/dockerfile-upstream:master` or `docker/dockerfile-upstream:master-labs` image can be used.

```bash
buildctl build \
    --frontend gateway.v0 \
    --opt source=docker/dockerfile \
    --local context=. \
    --local dockerfile=.
buildctl build \
    --frontend gateway.v0 \
    --opt source=docker/dockerfile \
    --opt context=https://github.com/moby/moby.git \
    --opt build-arg:APT_MIRROR=cdn-fastly.deb.debian.org
```

### Output

By default, the build result and intermediate cache will only remain internally in BuildKit. An output needs to be specified to retrieve the result.

#### Image/Registry

```bash
buildctl build ... --output type=image,name=docker.io/username/image,push=true
```

To export the image to multiple registries:

```bash
buildctl build ... --output type=image,\&quot;name=docker.io/username/image,docker.io/username2/image2\&quot;,push=true
```

To export the cache embed with the image and pushing them to registry together, type `registry` is required to import the cache, you should specify `--export-cache type=inline` and `--import-cache type=registry,ref=...`. To export the cache to a local directly, you should specify `--export-cache type=local`.
Details in [Export cache](#export-cache).

```bash
buildctl build ...\
  --output type=image,name=docker.io/username/image,push=true \
  --export-cache type=inline \
  --import-cache type=registry,ref=docker.io/username/image
```

Keys supported by image output:
* `name=&lt;value&gt;`: specify image name(s)
* `push=true`: push after creating the image
* `push-by-digest=true`: push unnamed image
* `registry.insecure=true`: push to insecure HTTP registry
* `oci-mediatypes=true`: use OCI mediatypes in configuration JSON instead of Docker&#039;s
* `oci-artifact=false`: use OCI artifact format for attestations
* `unpack=true`: unpack image after creation (for use with containerd)
* `dangling-name-prefix=&lt;value&gt;`: name image with `prefix@&lt;digest&gt;`, used for anonymous images
* `name-canonical=true`: add additional canonical name `name@&lt;digest&gt;`
* `compression=&lt;uncompressed|gzip|estargz|zstd&gt;`: choose compression type for layers newly created and cached, gzip is default value. estargz should be used with `oci-mediatypes=true`.
* `compression-level=&lt;value&gt;`: compression level for gzip, estargz (0-9) and zstd (0-22)
* `rewrite-timestamp=true`: rewrite the file timestamps to the `SOURCE_DATE_EPOCH` value.
   See [`docs/build-repro.md`](docs/build-repro.md) for how to specify the `SOURCE_DATE_EPOCH` value.
* `force-compression=true`: forcefully apply `compression` option to all layers (including already existing layers)
* `store=true`: store the result images to the worker&#039;s (e.g. containerd) image store as well as ensures that the image has all blobs in the content store (default `true`). Ignored if the worker doesn&#039;t have image store (e.g. OCI worker).
* `annotation.&lt;key&gt;=&lt;value&gt;`: attach an annotation with the respective `key` and `value` to the built image
  * Using the extended syntaxes, `annotation-&lt;type&gt;.&lt;key&gt;=&lt;value&gt;`, `annotation[&lt;platform&gt;].&lt;key&gt;=&lt;value&gt;` and both combined with `annotation-&lt;type&gt;[&lt;platform&gt;].&lt;key&gt;=&lt;value&gt;`, allows configuring exactly where to attach the annotation.
  * `&lt;type&gt;` specifies what object to attach to, and can be any of `manifest` (the default), `manifest-descriptor`, `index` and `index-descriptor`
  * `&lt;platform&gt;` specifies which objects to attach to (by default, all), and is the same key passed into the `platform` opt, see [`docs/multi-platform.md`](docs/multi-platform.md).
  * See [`docs/annotations.md`](docs/annotations.md) for more details.

If credentials are required, `buildctl` will attempt to read Docker configuration file `$DOCKER_CONFIG/config.json`.
`$DOCKER_CONFIG` defaults to `~/.docker`.

#### Local directory

The local client will copy the files directly to the client. This is useful if BuildKit is being used for building something else than container images.

```bash
buildctl build ... --output type=local,dest=path/to/output-dir
```

To export specific files use multi-stage builds with a scratch stage and copy the needed files into that stage with `COPY --from`.

```dockerfile
...
FROM scratch as testresult

COPY --from=builder /usr/src/app/testresult.xml .
...
```

```bash
buildctl build ... --opt target=testresult --output type=local,dest=path/to/output-dir
```

With a [multi-platform build](docs/multi-platform.md), a subfolder matching
each target platform will be created in the destination directory:

```dockerfile
FROM busybox AS build
ARG TARGETOS
ARG TARGETARCH
RUN mkdir /out &amp;&amp; echo foo &gt; /out/hello-$TARGETOS-$TARGETARCH

FROM scratch
COPY --from=build /out /
```

```bash
$ buildctl build \
  --frontend dockerfile.v0 \
  --opt platform=linux/amd64,linux/arm64 \
  --output type=local,dest=./bin/release

$ tree ./bin
./bin/
‚îî‚îÄ‚îÄ release
    ‚îú‚îÄ‚îÄ linux_amd64
    ‚îÇ   ‚îî‚îÄ‚îÄ hello-linux-amd64
    ‚îî‚îÄ‚îÄ linux_arm64
        ‚îî‚îÄ‚îÄ hello-linux-arm64
```

You can set `platform-split=false` to merge files from all platforms together
into same directory:

```bash
$ buildctl build \
  --frontend dockerfile.v0 \
  --opt platform=linux/amd64,linux/arm64 \
  --output type=local,dest=./bin/release,platform-split=false

$ tree ./bin
./bin/
‚îî‚îÄ‚îÄ release
    ‚îú‚îÄ‚îÄ hello-linux-amd64
    ‚îî‚îÄ‚îÄ hello-linux-arm64
```

Tar exporter is similar to local exporter but transfers the files through a tarball.

```bash
buildctl build ... --output type=tar,dest=out.tar
buildctl build ... --output type=tar &gt; out.tar
```

#### Docker tarball

```bash
# exported tarball is also compatible with OCI spec
buildctl build ... --output type=docker,name=myimage | docker load
```

#### OCI tarball

```bash
buildctl build ... --output type=oci,dest=path/to/output.tar
buildctl build ... --output type=oci &gt; output.tar
```

#### containerd image store

The containerd worker needs to be used

```bash
buildctl build ... --output type=image,name=docker.io/username/image
ctr --namespace=buildkit images ls
```

To change the containerd namespace, you need to change `worker.containerd.namespace` in [`/etc/buildkit/buildkitd.toml`](./docs/buildkitd.toml.md).

## Cache

To show local build cache (`/var/lib/buildkit`):

```bash
buildctl du -v
```

To prune local build cache:
```bash
buildctl prune
```

### Garbage collection

See [`./docs/buildkitd.toml.md`](./docs/buildkitd.toml.md).

### Export cache

BuildKit supports the following cache exporters:
* `inline`: embed the cache into the image, and push them to the registry together
* `registry`: push the image and the cache separately
* `local`: export to a local directory
* `gha`: export to GitHub Actions cache

In most case you want to use the `inline` cache exporter.
However, note that the `inline` cache exporter only supports `min` cache mode. 
To enable `max` cache mode, push the image and the cache separately by using `registry` cache exporter.

`inline` and `registry` exporters both store the cache in the registry. For importing the cache, `type=registry` is sufficient for both, as specifying the cache format is not necessary.

#### Inline (push image and cache together)

```bash
buildctl build ... \
  --output type=image,name=docker.io/username/image,push=true \
  --export-cache type=inline \
  --import-cache type=registry,ref=docker.io/username/image
```

Note that the inline cache is not imported unless [`--import-cache type=registry,ref=...`](#registry-push-image-and-cache-separately) is provided.

Inline cache embeds cache metadata into the image config. The layers in the image will be left untouched compared to the image with no cache information.

:information_source: Docker-integrated BuildKit (`DOCKER_BUILDKIT=1 docker build`) and `docker buildx`requires 
`--build-arg BUILDKIT_INLINE_CACHE=1` to be specified to enable the `inline` cache exporter.
However, the standalone `buildctl` does NOT require `--opt build-arg:BUILDKIT_INLINE_CACHE=1` and the build-arg is simply ignored.

#### Registry (push image and cache separately)

```bash
buildctl build ... \
  --output type=image,name=localhost:5000/myrepo:image,push=true \
  --export-cache type=registry,ref=localhost:5000/myrepo:buildcache \
  --import-cache type=registry,ref=localhost:5000/myrepo:buildcache
```

`--export-cache` options:
* `type=registry`
* `mode=&lt;min|max&gt;`: specify cache layers to export (default: `min`)
  * `min`: only export layers for the resulting image
  * `max`: export all the layers of all intermediate steps
* `ref=&lt;ref&gt;`: specify repository re

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[akuity/kargo]]></title>
            <link>https://github.com/akuity/kargo</link>
            <guid>https://github.com/akuity/kargo</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:58 GMT</pubDate>
            <description><![CDATA[Application lifecycle orchestration]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/akuity/kargo">akuity/kargo</a></h1>
            <p>Application lifecycle orchestration</p>
            <p>Language: Go</p>
            <p>Stars: 2,933</p>
            <p>Forks: 302</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre>![Kargo by Akuity, creators of Argo](./ui/public/kargo-logo-white.png#gh-dark-mode-only)
![Kargo by Akuity, creators of Argo](kargo-logo.png#gh-light-mode-only)

![CI](https://github.com/akuity/kargo/actions/workflows/ci.yaml/badge.svg)
[![codecov](https://codecov.io/gh/akuity/kargo/branch/main/graph/badge.svg?token=FGUq4netA6)](https://codecov.io/gh/akuity/kargo)
[![Netlify Status](https://api.netlify.com/api/v1/badges/71b4c2e1-5e8b-4927-ad1f-b475bae59e90/deploy-status)](https://app.netlify.com/sites/docs-kargo-io/deploys)
[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](CODE_OF_CONDUCT.md)
[![Discord](https://img.shields.io/discord/1138942074998235187?logo=discord&amp;logoColor=ffffff&amp;label=discord
)](https://akuity.community)

Kargo builds upon
[GitOps](https://opengitops.dev/) principles to manage and automate the
promotion of software artifacts through the many stages of their lifecycle.

![Kargo Dashboard](./docs/static/img/kargo-ui.png)

## Getting Started

Read about Kargo in our [docs](https://docs.kargo.io), get hands-on right away
with our [Quickstart](https://docs.kargo.io/quickstart) tutorial, or watch the
*Multi-Stage Deployment Pipelines the GitOps Way* talk presented by Jesse Suen &amp;
Kent Rancourt of [Akuity](https://akuity.io/) at GitOpsCon EU 2024:

[![Multi-Stage Deployment Pipelines the GitOps Way - Kargo](https://img.youtube.com/vi/0B_JODxyK0w/0.jpg)](https://youtu.be/0B_JODxyK0w)

## Support &amp; Feedback

To report a bug or request a feature, please open an
[issue](https://github.com/akuity/kargo/issues). For general questions, please
start a [discussion](https://github.com/akuity/kargo/discussions) instead.

Please also feel free to join fellow users in discusions in our
[Discord Community](https://akuity.community)!

If you are interested in enterprise-scale Kargo hosted, managed, and
professionally supported by Akuity, inquire at https://akuity.io/kargo-enterprise.

## Contributing

The Kargo project accepts contributions via GitHub pull requests. If you wish to
implement a bug fix or new feature, please engage our maintainers by opening an
[issue](https://github.com/akuity/kargo/issues) first.

Visit our
[Kargo Contributor Guide](https://docs.kargo.io/contributor-guide/) for more
info on how to start hacking on Kargo quickly and easily.

## Code of Conduct

Participation in the Kargo project is governed by the
[Contributor Covenant Code of Conduct](https://docs.kargo.io/contributor-guide/code-of-conduct/).
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[Tencent/WeKnora]]></title>
            <link>https://github.com/Tencent/WeKnora</link>
            <guid>https://github.com/Tencent/WeKnora</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:57 GMT</pubDate>
            <description><![CDATA[LLM-powered framework for deep document understanding, semantic retrieval, and context-aware answers using RAG paradigm.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Tencent/WeKnora">Tencent/WeKnora</a></h1>
            <p>LLM-powered framework for deep document understanding, semantic retrieval, and context-aware answers using RAG paradigm.</p>
            <p>Language: Go</p>
            <p>Stars: 7,941</p>
            <p>Forks: 908</p>
            <p>Stars today: 87 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;img src=&quot;./docs/images/logo.png&quot; alt=&quot;WeKnora Logo&quot; height=&quot;120&quot;/&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://weknora.weixin.qq.com&quot; target=&quot;_blank&quot;&gt;
        &lt;img alt=&quot;ÂÆòÊñπÁΩëÁ´ô&quot; src=&quot;https://img.shields.io/badge/ÂÆòÊñπÁΩëÁ´ô-WeKnora-4e6b99&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://chatbot.weixin.qq.com&quot; target=&quot;_blank&quot;&gt;
        &lt;img alt=&quot;ÂæÆ‰ø°ÂØπËØùÂºÄÊîæÂπ≥Âè∞&quot; src=&quot;https://img.shields.io/badge/ÂæÆ‰ø°ÂØπËØùÂºÄÊîæÂπ≥Âè∞-5ac725&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/Tencent/WeKnora/blob/main/LICENSE&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/License-MIT-ffffff?labelColor=d4eaf7&amp;color=2e6cc4&quot; alt=&quot;License&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;./CHANGELOG.md&quot;&gt;
        &lt;img alt=&quot;Version&quot; src=&quot;https://img.shields.io/badge/version-0.2.0-2e6cc4?labelColor=d4eaf7&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
| &lt;b&gt;English&lt;/b&gt; | &lt;a href=&quot;./README_CN.md&quot;&gt;&lt;b&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;./README_JA.md&quot;&gt;&lt;b&gt;Êó•Êú¨Ë™û&lt;/b&gt;&lt;/a&gt; |
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;h4 align=&quot;center&quot;&gt;

  [Overview](#-overview) ‚Ä¢ [Architecture](#-architecture) ‚Ä¢ [Key Features](#-key-features) ‚Ä¢ [Getting Started](#-getting-started) ‚Ä¢ [API Reference](#-api-reference) ‚Ä¢ [Developer Guide](#-developer-guide)
  
  &lt;/h4&gt;
&lt;/p&gt;

# üí° WeKnora - LLM-Powered Document Understanding &amp; Retrieval Framework

## üìå Overview

[**WeKnora**](https://weknora.weixin.qq.com) is an LLM-powered framework designed for deep document understanding and semantic retrieval, especially for handling complex, heterogeneous documents. 

It adopts a modular architecture that combines multimodal preprocessing, semantic vector indexing, intelligent retrieval, and large language model inference. At its core, WeKnora follows the **RAG (Retrieval-Augmented Generation)** paradigm, enabling high-quality, context-aware answers by combining relevant document chunks with model reasoning.

**Website:** https://weknora.weixin.qq.com

## ‚ú® Latest Updates

**v0.2.0 Highlights:**

- ü§ñ **Agent Mode**: New ReACT Agent mode that can call built-in tools, MCP tools, and web search, providing comprehensive summary reports through multiple iterations and reflection
- üìö **Multi-Type Knowledge Bases**: Support for FAQ and document knowledge base types, with new features including folder import, URL import, tag management, and online entry
- ‚öôÔ∏è **Conversation Strategy**: Support for configuring Agent models, normal mode models, retrieval thresholds, and Prompts, with precise control over multi-turn conversation behavior
- üåê **Web Search**: Support for extensible web search engines with built-in DuckDuckGo search engine
- üîå **MCP Tool Integration**: Support for extending Agent capabilities through MCP, with built-in uvx and npx launchers, supporting multiple transport methods
- üé® **New UI**: Optimized conversation interface with Agent mode/normal mode switching, tool call process display, and comprehensive knowledge base management interface upgrade
- ‚ö° **Infrastructure Upgrade**: Introduced MQ async task management, support for automatic database migration, and fast development mode

## üîí Security Notice

**Important:** Starting from v0.1.3, WeKnora includes login authentication functionality to enhance system security. For production deployments, we strongly recommend:

- Deploy WeKnora services in internal/private network environments rather than public internet
- Avoid exposing the service directly to public networks to prevent potential information leakage
- Configure proper firewall rules and access controls for your deployment environment
- Regularly update to the latest version for security patches and improvements

## üèóÔ∏è Architecture

![weknora-architecture.png](./docs/images/architecture.png)

WeKnora employs a modern modular design to build a complete document understanding and retrieval pipeline. The system primarily includes document parsing, vector processing, retrieval engine, and large model inference as core modules, with each component being flexibly configurable and extendable.

## üéØ Key Features

- **ü§ñ Agent Mode**: Support for ReACT Agent mode that can use built-in tools to retrieve knowledge bases, MCP tools, and web search tools to access external services, providing comprehensive summary reports through multiple iterations and reflection
- **üîç Precise Understanding**: Structured content extraction from PDFs, Word documents, images and more into unified semantic views
- **üß† Intelligent Reasoning**: Leverages LLMs to understand document context and user intent for accurate Q&amp;A and multi-turn conversations
- **üìö Multi-Type Knowledge Bases**: Support for FAQ and document knowledge base types, with folder import, URL import, tag management, and online entry capabilities
- **üîß Flexible Extension**: All components from parsing and embedding to retrieval and generation are decoupled for easy customization
- **‚ö° Efficient Retrieval**: Hybrid retrieval strategies combining keywords, vectors, and knowledge graphs, with cross-knowledge base retrieval support
- **üåê Web Search**: Support for extensible web search engines with built-in DuckDuckGo search engine
- **üîå MCP Tool Integration**: Support for extending Agent capabilities through MCP, with built-in uvx and npx launchers, supporting multiple transport methods
- **‚öôÔ∏è Conversation Strategy**: Support for configuring Agent models, normal mode models, retrieval thresholds, and Prompts, with precise control over multi-turn conversation behavior
- **üéØ User-Friendly**: Intuitive web interface and standardized APIs for zero technical barriers
- **üîí Secure &amp; Controlled**: Support for local deployment and private cloud, ensuring complete data sovereignty

## üìä Application Scenarios

| Scenario | Applications | Core Value |
|---------|----------|----------|
| **Enterprise Knowledge Management** | Internal document retrieval, policy Q&amp;A, operation manual search | Improve knowledge discovery efficiency, reduce training costs |
| **Academic Research Analysis** | Paper retrieval, research report analysis, scholarly material organization | Accelerate literature review, assist research decisions |
| **Product Technical Support** | Product manual Q&amp;A, technical documentation search, troubleshooting | Enhance customer service quality, reduce support burden |
| **Legal &amp; Compliance Review** | Contract clause retrieval, regulatory policy search, case analysis | Improve compliance efficiency, reduce legal risks |
| **Medical Knowledge Assistance** | Medical literature retrieval, treatment guideline search, case analysis | Support clinical decisions, improve diagnosis quality |

## üß© Feature Matrix

| Module | Support | Description |
|---------|---------|------|
| Agent Mode | ‚úÖ ReACT Agent Mode | Support for using built-in tools to retrieve knowledge bases, MCP tools, and web search, with cross-knowledge base retrieval and multiple iterations |
| Knowledge Base Types | ‚úÖ FAQ / Document | Support for creating FAQ and document knowledge base types, with folder import, URL import, tag management, and online entry |
| Document Formats | ‚úÖ PDF / Word / Txt / Markdown / Images (with OCR / Caption) | Support for structured and unstructured documents with text extraction from images |
| Model Management | ‚úÖ Centralized configuration, built-in model sharing | Centralized model configuration with model selection in knowledge base settings, support for multi-tenant shared built-in models |
| Embedding Models | ‚úÖ Local models, BGE / GTE APIs, etc. | Customizable embedding models, compatible with local deployment and cloud vector generation APIs |
| Vector DB Integration | ‚úÖ PostgreSQL (pgvector), Elasticsearch | Support for mainstream vector index backends, flexible switching for different retrieval scenarios |
| Retrieval Strategies | ‚úÖ BM25 / Dense Retrieval / GraphRAG | Support for sparse/dense recall and knowledge graph-enhanced retrieval with customizable retrieve-rerank-generate pipelines |
| LLM Integration | ‚úÖ Support for Qwen, DeepSeek, etc., with thinking/non-thinking mode switching | Compatible with local models (e.g., via Ollama) or external API services with flexible inference configuration |
| Conversation Strategy | ‚úÖ Agent models, normal mode models, retrieval thresholds, Prompt configuration | Support for configuring Agent models, normal mode models, retrieval thresholds, online Prompt configuration, precise control over multi-turn conversation behavior |
| Web Search | ‚úÖ Extensible search engines, DuckDuckGo | Support for extensible web search engines with built-in DuckDuckGo search engine |
| MCP Tools | ‚úÖ uvx, npx launchers, Stdio/HTTP Streamable/SSE | Support for extending Agent capabilities through MCP, with built-in uvx and npx launchers, supporting three transport methods |
| QA Capabilities | ‚úÖ Context-aware, multi-turn dialogue, prompt templates | Support for complex semantic modeling, instruction control and chain-of-thought Q&amp;A with configurable prompts and context windows |
| E2E Testing | ‚úÖ Retrieval+generation process visualization and metric evaluation | End-to-end testing tools for evaluating recall hit rates, answer coverage, BLEU/ROUGE and other metrics |
| Deployment Modes | ‚úÖ Support for local deployment / Docker images | Meets private, offline deployment and flexible operation requirements, with fast development mode support |
| User Interfaces | ‚úÖ Web UI + RESTful API | Interactive interface and standard API endpoints, with Agent mode/normal mode switching and tool call process display |
| Task Management | ‚úÖ MQ async tasks, automatic database migration | MQ-based async task state maintenance, support for automatic database schema and data migration during version upgrades |

## üöÄ Getting Started

### üõ† Prerequisites

Make sure the following tools are installed on your system:

* [Docker](https://www.docker.com/)
* [Docker Compose](https://docs.docker.com/compose/)
* [Git](https://git-scm.com/)

### üì¶ Installation

#### ‚ë† Clone the repository

```bash
# Clone the main repository
git clone https://github.com/Tencent/WeKnora.git
cd WeKnora
```

#### ‚ë° Configure environment variables

```bash
# Copy example env file
cp .env.example .env

# Edit .env and set required values
# All variables are documented in the .env.example comments
```

#### ‚ë¢ Start the services (include Ollama)

Check the images that need to be started in the .env file.

```bash
./scripts/start_all.sh
```

or

```bash
make start-all
```

#### ‚ë¢.0 Start ollama services (Optional)

```bash
ollama serve &gt; /dev/null 2&gt;&amp;1 &amp;
```

#### ‚ë¢.1 Activate different combinations of features

- Minimum core services
```bash
docker compose up -d
```

- All features enabled
```bash
docker-compose --profile full up -d
```

- Tracing logs required
```bash
docker-compose --profile jaeger up -d
```

- Neo4j knowledge graph required
```bash
docker-compose --profile neo4j up -d
```

- Minio file storage service required
```bash
docker-compose --profile minio up -d
```

- Multiple options combination
```bash
docker-compose --profile neo4j --profile minio up -d
```

#### ‚ë£ Stop the services

```bash
./scripts/start_all.sh --stop
# Or
make stop-all
```

### üåê Access Services

Once started, services will be available at:

* Web UI: `http://localhost`
* Backend API: `http://localhost:8080`
* Jaeger Tracing: `http://localhost:16686`

### üîå Using WeChat Dialog Open Platform

WeKnora serves as the core technology framework for the [WeChat Dialog Open Platform](https://chatbot.weixin.qq.com), providing a more convenient usage approach:

- **Zero-code Deployment**: Simply upload knowledge to quickly deploy intelligent Q&amp;A services within the WeChat ecosystem, achieving an &quot;ask and answer&quot; experience
- **Efficient Question Management**: Support for categorized management of high-frequency questions, with rich data tools to ensure accurate, reliable, and easily maintainable answers
- **WeChat Ecosystem Integration**: Through the WeChat Dialog Open Platform, WeKnora&#039;s intelligent Q&amp;A capabilities can be seamlessly integrated into WeChat Official Accounts, Mini Programs, and other WeChat scenarios, enhancing user interaction experiences

### üîó Access WeKnora via MCP Server

#### 1Ô∏è‚É£ Clone the repository
```
git clone https://github.com/Tencent/WeKnora
```

#### 2Ô∏è‚É£ Configure MCP Server
&gt; It is recommended to directly refer to the [MCP Configuration Guide](./mcp-server/MCP_CONFIG.md) for configuration.

Configure the MCP client to connect to the server:
```json
{
  &quot;mcpServers&quot;: {
    &quot;weknora&quot;: {
      &quot;args&quot;: [
        &quot;path/to/WeKnora/mcp-server/run_server.py&quot;
      ],
      &quot;command&quot;: &quot;python&quot;,
      &quot;env&quot;:{
        &quot;WEKNORA_API_KEY&quot;:&quot;Enter your WeKnora instance, open developer tools, check the request header x-api-key starting with sk&quot;,
        &quot;WEKNORA_BASE_URL&quot;:&quot;http(s)://your-weknora-address/api/v1&quot;
      }
    }
  }
}
```

Run directly using stdio command:
```
pip install weknora-mcp-server
python -m weknora-mcp-server
```

## üîß Initialization Configuration Guide

To help users quickly configure various models and reduce trial-and-error costs, we&#039;ve improved the original configuration file initialization method by adding a Web UI interface for model configuration. Before using, please ensure the code is updated to the latest version. The specific steps are as follows:
If this is your first time using this project, you can skip steps ‚ë†‚ë° and go directly to steps ‚ë¢‚ë£.

### ‚ë† Stop the services

```bash
./scripts/start_all.sh --stop
```

### ‚ë° Clear existing data tables (recommended when no important data exists)

```bash
make clean-db
```

### ‚ë¢ Compile and start services

```bash
./scripts/start_all.sh
```

### ‚ë£ Access Web UI

http://localhost

On your first visit, you will be automatically redirected to the registration/login page. After completing registration, please create a new knowledge base and finish the relevant settings on its configuration page.

## üì± Interface Showcase

### Web UI Interface

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;b&gt;Knowledge Base Management&lt;/b&gt;&lt;br/&gt;&lt;img src=&quot;./docs/images/knowledgebases.png&quot; alt=&quot;Knowledge Base Management&quot;&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Conversation Settings&lt;/b&gt;&lt;br/&gt;&lt;img src=&quot;./docs/images/settings.png&quot; alt=&quot;Conversation Settings&quot;&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td colspan=&quot;2&quot;&gt;&lt;b&gt;Agent Mode Tool Call Process&lt;/b&gt;&lt;br/&gt;&lt;img src=&quot;./docs/images/agent-qa.png&quot; alt=&quot;Agent Mode Tool Call Process&quot;&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

**Knowledge Base Management:** Support for creating FAQ and document knowledge base types, with multiple import methods including drag-and-drop, folder import, and URL import. Automatically identifies document structures and extracts core knowledge to establish indexes. Supports tag management and online entry. The system clearly displays processing progress and document status, achieving efficient knowledge base management.

**Agent Mode:** Support for ReACT Agent mode that can use built-in tools to retrieve knowledge bases, call user-configured MCP tools and web search tools to access external services, providing comprehensive summary reports through multiple iterations and reflection. Supports cross-knowledge base retrieval, allowing selection of multiple knowledge bases for simultaneous retrieval.

**Conversation Strategy:** Support for configuring Agent models, normal mode models, retrieval thresholds, and online Prompt configuration, with precise control over multi-turn conversation behavior and retrieval execution methods. The conversation input box supports Agent mode/normal mode switching, enabling/disabling web search, and selecting conversation models.

### Document Knowledge Graph

WeKnora supports transforming documents into knowledge graphs, displaying the relationships between different sections of the documents. Once the knowledge graph feature is enabled, the system analyzes and constructs an internal semantic association network that not only helps users understand document content but also provides structured support for indexing and retrieval, enhancing the relevance and breadth of search results.

For detailed configuration, please refer to the [Knowledge Graph Configuration Guide](./docs/KnowledgeGraph.md).

### MCP Server

Please refer to the [MCP Configuration Guide](./mcp-server/MCP_CONFIG.md) for the necessary setup.

## üìò API Reference

Troubleshooting FAQ: [Troubleshooting FAQ](./docs/QA.md)

Detailed API documentation is available at: [API Docs](./docs/API.md)

## üß≠ Developer Guide

### ‚ö° Fast Development Mode (Recommended)

If you need to frequently modify code, **you don&#039;t need to rebuild Docker images every time**! Use fast development mode:

```bash
# Method 1: Using Make commands (Recommended)
make dev-start      # Start infrastructure
make dev-app        # Start backend (new terminal)
make dev-frontend   # Start frontend (new terminal)

# Method 2: One-click start
./scripts/quick-dev.sh

# Method 3: Using scripts
./scripts/dev.sh start     # Start infrastructure
./scripts/dev.sh app       # Start backend (new terminal)
./scripts/dev.sh frontend  # Start frontend (new terminal)
```

**Development Advantages:**
- ‚úÖ Frontend modifications auto hot-reload (no restart needed)
- ‚úÖ Backend modifications quick restart (5-10 seconds, supports Air hot-reload)
- ‚úÖ No need to rebuild Docker images
- ‚úÖ Support IDE breakpoint debugging

**Detailed Documentation:** [Development Environment Quick Start](./docs/ÂºÄÂèëÊåáÂçó.md)

### üìÅ Directory Structure

```
WeKnora/
‚îú‚îÄ‚îÄ client/      # go client
‚îú‚îÄ‚îÄ cmd/         # Main entry point
‚îú‚îÄ‚îÄ config/      # Configuration files
‚îú‚îÄ‚îÄ docker/      # docker images files
‚îú‚îÄ‚îÄ docreader/   # Document parsing app
‚îú‚îÄ‚îÄ docs/        # Project documentation
‚îú‚îÄ‚îÄ frontend/    # Frontend app
‚îú‚îÄ‚îÄ internal/    # Core business logic
‚îú‚îÄ‚îÄ mcp-server/  # MCP server
‚îú‚îÄ‚îÄ migrations/  # DB migration scripts
‚îî‚îÄ‚îÄ scripts/     # Shell scripts
```

## ü§ù Contributing

We welcome community contributions! For suggestions, bugs, or feature requests, please submit an [Issue](https://github.com/Tencent/WeKnora/issues) or directly create a Pull Request.

### üéØ How to Contribute

- üêõ **Bug Fixes**: Discover and fix system defects
- ‚ú® **New Features**: Propose and implement new capabilities
- üìö **Documentation**: Improve project documentation
- üß™ **Test Cases**: Write unit and integration tests
- üé® **UI/UX Enhancements**: Improve user interface and experience

### üìã Contribution Process

1. **Fork the project** to your GitHub account
2. **Create a feature branch** `git checkout -b feature/amazing-feature`
3. **Commit changes** `git commit -m &#039;Add amazing feature&#039;`
4. **Push branch** `git push origin feature/amazing-feature`
5. **Create a Pull Request** with detailed description of changes

### üé® Code Standards

- Follow [Go Code Review Comments](https://github.com/golang/go/wiki/CodeReviewComments)
- Format code using `gofmt`
- Add necessary unit tests
- Update relevant documentation

### üìù Commit Guidelines

Use [Conventional Commits](https://www.conventionalcommits.org/) standard:

```
feat: Add document batch upload functionality
fix: Resolve vector retrieval precision issue
docs: Update API documentation
test: Add retrieval engine test cases
refactor: Restructure document parsing module
```

## üë• Contributors

Thanks to these excellent contributors:

[![Contributors](https://contrib.rocks/image?repo=Tencent/WeKnora)](https://github.com/Tencent/WeKnora/graphs/contributors)

## üìÑ License

This project is licensed under the [MIT License](./LICENSE).
You are free to use, modify, and distribute the code with proper attribution.

## üìà Project Statistics

&lt;a href=&quot;https://www.star-history.com/#Tencent/WeKnora&amp;type=date&amp;legend=top-left&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Tencent/WeKnora&amp;type=date&amp;theme=dark&amp;legend=top-left&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Tencent/WeKnora&amp;type=date&amp;legend=top-left&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=Tencent/WeKnora&amp;type=date&amp;legend=top-left&quot; /&gt;
 &lt;/picture&gt;


... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[inngest/inngest]]></title>
            <link>https://github.com/inngest/inngest</link>
            <guid>https://github.com/inngest/inngest</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:56 GMT</pubDate>
            <description><![CDATA[The leading workflow orchestration platform. Run stateful step functions and AI workflows on serverless, servers, or the edge.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/inngest/inngest">inngest/inngest</a></h1>
            <p>The leading workflow orchestration platform. Run stateful step functions and AI workflows on serverless, servers, or the edge.</p>
            <p>Language: Go</p>
            <p>Stars: 4,352</p>
            <p>Forks: 216</p>
            <p>Stars today: 20 stars today</p>
            <h2>README</h2><pre># [![Inngest](https://github.com/inngest/.github/raw/main/profile/github-readme-banner-2025-06-20.png)](https://www.inngest.com)

[![Latest release](https://img.shields.io/github/v/release/inngest/inngest?include_prereleases&amp;sort=semver)](https://github.com/inngest/inngest/releases)
[![Test Status](https://img.shields.io/github/actions/workflow/status/inngest/inngest/go.yaml?branch=main&amp;label=tests)](https://github.com/inngest/inngest/actions?query=branch%3Amain)
[![Discord](https://img.shields.io/discord/842170679536517141?label=discord)](https://www.inngest.com/discord)
[![Twitter Follow](https://img.shields.io/twitter/follow/inngest?style=social)](https://twitter.com/inngest)

[Inngest](https://www.inngest.com/?ref=github-inngest-readme)&#039;s durable functions replace queues, state management, and scheduling to enable any developer to write reliable step functions faster without touching infrastructure.

1. Write durable functions using any of [**our language SDKs**](#sdks)
2. Run the [**Inngest Dev Server**](#getting-started) for a complete local development experience, with production parity.
3. Deploy your functions to your own infrastructure
4. Sync your application&#039;s functions with the [**Inngest Platform**](https://www.inngest.com/?ref=github-inngest-readme) or a [self-hosted Inngest server](#self-hosting).
5. Inngest invokes your functions securely via HTTPS whenever triggering events are received.

### An example durable function

Inngest Functions enable developers to run reliable background logic, from background jobs to complex workflows. An Inngest Function is composed of three key parts that provide robust support for retrying, scheduling, and coordinating complex sequences of operations:

- [**Triggers**](https://www.inngest.com/docs/features/events-triggers?ref=github-inngest-readme) - Events, Cron schedules or webhook events that trigger the function.
- [**Flow Control**](https://www.inngest.com/docs/guides/flow-control?ref=github-inngest-readme) - Configure how the function runs are enqueued and executed including concurrency, throttling, debouncing, rate limiting, and prioritization.
- [**Steps**](/docs/features/inngest-functions/steps-workflows?ref=github-inngest-readme) - Steps are fundamental building blocks of Inngest, turning your Inngest Functions into reliable workflows that can runs for months and recover from failures.

Here is an example function that limits concurrency for each unique user id and performs two steps that will be retried on error:

```typescript
export default inngest.createFunction(
  {
    id: &quot;import-product-images&quot;,
    concurrency: {
      key: &quot;event.data.userId&quot;,
      limit: 10
    }
  },
  { event: &quot;shop/product.imported&quot; },
  async ({ event, step }) =&gt; {
    // Here goes the business logic
    // By wrapping code in steps, each will be retried automatically on failure
    const s3Urls = await step.run(&quot;copy-images-to-s3&quot;, async () =&gt; {
      return copyAllImagesToS3(event.data.imageURLs);
    });
    // You can include numerous steps in your function
    await step.run(&quot;resize-images&quot;, async () =&gt; {
      await resizer.bulk({ urls: s3Urls, quality: 0.9, maxWidth: 1024 });
    })
  };
);

// Elsewhere in your code (e.g. in your API endpoint):
await inngest.send({
  name: &quot;shop/product.imported&quot;,
  data: {
    userId: &quot;01J8G44701QYGE0DH65PZM8DPM&quot;,
    imageURLs: [
      &quot;https://useruploads.acme.com/q2345678/1094.jpg&quot;,
      &quot;https://useruploads.acme.com/q2345678/1095.jpg&quot;
    ],
  },
});
```

## Learn more

- [Getting started](#getting-started)
- [SDKs](#sdks)
- [Project Architecture](#project-architecture)
- [Self-hosting](#self-hosting)
- [Community](#community)

## Getting started

Run the Inngest Dev Server using our CLI:

```
npx inngest-cli@latest dev
```

Open the Inngest Dev Server dashboard at http://localhost:8288:

![Screenshot of the Inngest dashboard served by the Inngest Dev Server](.github/assets/dashboard-screenshot-2024-09-23.png)

Follow our [Next.js](https://www.inngest.com/docs/getting-started/nextjs-quick-start?ref=github-inngest-readme), [Node.js](https://www.inngest.com/docs/getting-started/nodejs-quick-start?ref=github-inngest-readme) or [Python](https://www.inngest.com/docs/getting-started/python-quick-start?ref=github-inngest-readme) quick start guides.

## SDKs

- **TypeScript / JavaScript** ([inngest-js](https://github.com/inngest/inngest-js)) - [Reference](https://www.inngest.com/docs/reference/typescript?ref=github-inngest-readme)
- **Python** ([inngest-py](https://github.com/inngest/inngest-py)) - [Reference](https://www.inngest.com/docs/reference/python?ref=github-inngest-readme)
- **Go** ([inngestgo](https://github.com/inngest/inngestgo)) - [Reference](https://pkg.go.dev/github.com/inngest/inngestgo)
- **Kotlin / Java** ([inngest-kt](https://github.com/inngest/inngest-kt))

## Project Architecture

To understand how self-hosting works, it&#039;s valuable to understand the architecture and system components at a high level. We&#039;ll take a look at a simplified architecture diagram and walk through the system.

&lt;br /&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;.github/assets/architecture-2024-09-23.png&quot; alt=&quot;System Architecture&quot; width=&quot;660&quot; /&gt;
&lt;/p&gt;

- **Event API** - Receives events from SDKs via HTTP requests. Authenticates client requests via [Event Keys](https://www.inngest.com/docs/events/creating-an-event-key?ref=github-inngest-readme). The Event API publishes event payloads to an internal event stream.
- **Event stream** - Acts as buffer between the _Event API_ and the _Runner_.
- **Runner** - Consumes incoming events and performs several actions:
  - Scheduling of new ‚Äúfunction runs‚Äù (aka jobs) given the event type, creating initial run state in the _State store_ database. Runs are added to queues given the function&#039;s flow control configuration.
  - Resume functions paused via [`waitForEvent`](https://www.inngest.com/docs/features/inngest-functions/steps-workflows/wait-for-event?ref=github-inngest-readme) with matching expressions.
  - Cancels running functions with matching [`cancelOn`](https://www.inngest.com/docs/features/inngest-functions/cancellation/cancel-on-events?ref=github-inngest-readme) expressions
  - Writes ingested events to a database for historical record and future replay.
- **Queue** - A multi-tenant aware, multi-tier queue designed for fairness and various [flow control](https://www.inngest.com/docs/guides/flow-control?ref=github-inngest-readme) methods (concurrency, throttling, prioritization, debouncing, rate limiting) and [batching](https://www.inngest.com/docs/guides/batching?ref=github-inngest-readme).
- **Executor** - Responsible for executing functions, from initial execution, step execution, writing incremental function run state to the _State store_, and retries after failures.
- **State store (database)** - Persists data for pending and ongoing function runs. Data includes initial triggering event(s), step output and step errors.
- **Database** - Persists system data and history including Apps, Functions, Events, Function run results.
- **API** - GraphQL and REST APIs for programmatic access and management of system resources.
- **Dashboard UI** - The UI to manage apps, functions and view function run history.

&lt;br /&gt;

## Community

- [**Join our Discord community for support, to give us feedback, or chat with us**](https://www.inngest.com/discord).
- [Post a question or idea to our GitHub discussion board](https://github.com/orgs/inngest/discussions)
- [Read the documentation](https://www.inngest.com/docs?ref=github-inngest-readme)
- [Explore our public roadmap](http://roadmap.inngest.com/)
- [Follow us on Twitter](https://twitter.com/inngest)
- [Join our mailing list](https://www.inngest.com/mailing-list) for release notes and project updates

## Contributing

We embrace contributions in many forms, including documentation, typos, bug reports or fixes. Check out our [contributing guide](/docs/CONTRIBUTING.md) to get started. Each of our open source [SDKs](#sdks) are open to contributions as well.

Additionally, Inngest&#039;s website documentation is available for contribution in [the `inngest/website` repo](https://github.com/inngest/website).

## Self-hosting

Self-hosting the Inngest server is possible and easy to get started with. Learn more about self-hosting Inngest in [our docs guide](https://www.inngest.com/docs/self-hosting?ref=github-inngest-readme).

## License

The Inngest server and CLI are available under the Server Side Public License and delayed open source publication (DOSP) under Apache 2.0. [View the license here](/LICENSE.md).

All Inngest [SDKs](#sdks) are all available under the Apache 2.0 license.
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[labstack/echo]]></title>
            <link>https://github.com/labstack/echo</link>
            <guid>https://github.com/labstack/echo</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:55 GMT</pubDate>
            <description><![CDATA[High performance, minimalist Go web framework]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/labstack/echo">labstack/echo</a></h1>
            <p>High performance, minimalist Go web framework</p>
            <p>Language: Go</p>
            <p>Stars: 31,901</p>
            <p>Forks: 2,305</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre>[![Sourcegraph](https://sourcegraph.com/github.com/labstack/echo/-/badge.svg?style=flat-square)](https://sourcegraph.com/github.com/labstack/echo?badge)
[![GoDoc](http://img.shields.io/badge/go-documentation-blue.svg?style=flat-square)](https://pkg.go.dev/github.com/labstack/echo/v4)
[![Go Report Card](https://goreportcard.com/badge/github.com/labstack/echo?style=flat-square)](https://goreportcard.com/report/github.com/labstack/echo)
[![GitHub Workflow Status (with event)](https://img.shields.io/github/actions/workflow/status/labstack/echo/echo.yml?style=flat-square)](https://github.com/labstack/echo/actions)
[![Codecov](https://img.shields.io/codecov/c/github/labstack/echo.svg?style=flat-square)](https://codecov.io/gh/labstack/echo)
[![Forum](https://img.shields.io/badge/community-forum-00afd1.svg?style=flat-square)](https://github.com/labstack/echo/discussions)
[![Twitter](https://img.shields.io/badge/twitter-@labstack-55acee.svg?style=flat-square)](https://twitter.com/labstack)
[![License](http://img.shields.io/badge/license-mit-blue.svg?style=flat-square)](https://raw.githubusercontent.com/labstack/echo/master/LICENSE)

## Echo

High performance, extensible, minimalist Go web framework.

* [Official website](https://echo.labstack.com)
* [Quick start](https://echo.labstack.com/docs/quick-start)
* [Middlewares](https://echo.labstack.com/docs/category/middleware)

Help and questions: [Github Discussions](https://github.com/labstack/echo/discussions)


### Feature Overview

- Optimized HTTP router which smartly prioritize routes
- Build robust and scalable RESTful APIs
- Group APIs
- Extensible middleware framework
- Define middleware at root, group or route level
- Data binding for JSON, XML and form payload
- Handy functions to send variety of HTTP responses
- Centralized HTTP error handling
- Template rendering with any template engine
- Define your format for the logger
- Highly customizable
- Automatic TLS via Let‚Äôs Encrypt
- HTTP/2 support

## Sponsors

&lt;div&gt;
  &lt;a href=&quot;https://encore.dev&quot; style=&quot;display: inline-flex; align-items: center; gap: 10px&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/78424526/214602214-52e0483a-b5fc-4d4c-b03e-0b7b23e012df.svg&quot; height=&quot;28px&quot; alt=&quot;encore icon&quot;&gt;&lt;/img&gt;
  &lt;b&gt;Encore ‚Äì the platform for building Go-based cloud backends&lt;/b&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;br/&gt;

Click [here](https://github.com/sponsors/labstack) for more information on sponsorship.

## [Guide](https://echo.labstack.com/guide)

### Installation

```sh
// go get github.com/labstack/echo/{version}
go get github.com/labstack/echo/v4
```
Latest version of Echo supports last four Go major [releases](https://go.dev/doc/devel/release) and might work with older versions.

### Example

```go
package main

import (
  &quot;github.com/labstack/echo/v4&quot;
  &quot;github.com/labstack/echo/v4/middleware&quot;
  &quot;log/slog&quot;
  &quot;net/http&quot;
)

func main() {
  // Echo instance
  e := echo.New()

  // Middleware
  e.Use(middleware.Logger())
  e.Use(middleware.Recover())

  // Routes
  e.GET(&quot;/&quot;, hello)

  // Start server
  if err := e.Start(&quot;:8080&quot;); err != nil &amp;&amp; !errors.Is(err, http.ErrServerClosed) {
    slog.Error(&quot;failed to start server&quot;, &quot;error&quot;, err)
  }
}

// Handler
func hello(c echo.Context) error {
  return c.String(http.StatusOK, &quot;Hello, World!&quot;)
}
```

# Official middleware repositories

Following list of middleware is maintained by Echo team.

| Repository                                                                   | Description                                                                                                                                                                                                                                                                                                                   |
|------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [github.com/labstack/echo-jwt](https://github.com/labstack/echo-jwt)         | [JWT](https://github.com/golang-jwt/jwt) middleware                                                                                                                                                                                                                                                                           | 
| [github.com/labstack/echo-contrib](https://github.com/labstack/echo-contrib) | [casbin](https://github.com/casbin/casbin), [gorilla/sessions](https://github.com/gorilla/sessions), [jaegertracing](https://github.com/uber/jaeger-client-go), [prometheus](https://github.com/prometheus/client_golang/), [pprof](https://pkg.go.dev/net/http/pprof), [zipkin](https://github.com/openzipkin/zipkin-go) middlewares | 

# Third-party middleware repositories

Be careful when adding 3rd party middleware. Echo teams does not have time or manpower to guarantee safety and quality
of middlewares in this list.

| Repository                                                                                           | Description                                                                                                                                                                                              |
|------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [deepmap/oapi-codegen](https://github.com/deepmap/oapi-codegen)                                      | Automatically generate RESTful API documentation with [OpenAPI](https://swagger.io/specification/) Client and Server Code Generator                                                                      |
| [github.com/swaggo/echo-swagger](https://github.com/swaggo/echo-swagger)                             | Automatically generate RESTful API documentation with [Swagger](https://swagger.io/) 2.0.                                                                                                                |
| [github.com/ziflex/lecho](https://github.com/ziflex/lecho)                                           | [Zerolog](https://github.com/rs/zerolog) logging library wrapper for Echo logger interface.                                                                                                              |
| [github.com/brpaz/echozap](https://github.com/brpaz/echozap)                                         | Uber¬¥s [Zap](https://github.com/uber-go/zap) logging library wrapper for Echo logger interface.                                                                                                          |
| [github.com/samber/slog-echo](https://github.com/samber/slog-echo)                                         | Go [slog](https://pkg.go.dev/golang.org/x/exp/slog) logging library wrapper for Echo logger interface.                                                                                                          |
| [github.com/darkweak/souin/plugins/echo](https://github.com/darkweak/souin/tree/master/plugins/echo) | HTTP cache system based on [Souin](https://github.com/darkweak/souin) to automatically get your endpoints cached. It supports some distributed and non-distributed storage systems depending your needs. |
| [github.com/mikestefanello/pagoda](https://github.com/mikestefanello/pagoda)                         | Rapid, easy full-stack web development starter kit built with Echo.                                                                                                                                      |
| [github.com/go-woo/protoc-gen-echo](https://github.com/go-woo/protoc-gen-echo)                       | ProtoBuf generate Echo server side code                                                                                                                                                                  |

Please send a PR to add your own library here.

## Contribute

**Use issues for everything**

- For a small change, just send a PR.
- For bigger changes open an issue for discussion before sending a PR.
- PR should have:
  - Test case
  - Documentation
  - Example (If it makes sense)
- You can also contribute by:
  - Reporting issues
  - Suggesting new features or enhancements
  - Improve/fix documentation

## Credits

- [Vishal Rana](https://github.com/vishr) (Author)
- [Nitin Rana](https://github.com/nr17) (Consultant)
- [Roland Lammel](https://github.com/lammel) (Maintainer)
- [Martti T.](https://github.com/aldas) (Maintainer)
- [Pablo Andres Fuente](https://github.com/pafuent) (Maintainer)
- [Contributors](https://github.com/labstack/echo/graphs/contributors)

## License

[MIT](https://github.com/labstack/echo/blob/master/LICENSE)
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[open-telemetry/opentelemetry-collector-contrib]]></title>
            <link>https://github.com/open-telemetry/opentelemetry-collector-contrib</link>
            <guid>https://github.com/open-telemetry/opentelemetry-collector-contrib</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:54 GMT</pubDate>
            <description><![CDATA[Contrib repository for the OpenTelemetry Collector]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib">open-telemetry/opentelemetry-collector-contrib</a></h1>
            <p>Contrib repository for the OpenTelemetry Collector</p>
            <p>Language: Go</p>
            <p>Stars: 4,224</p>
            <p>Forks: 3,185</p>
            <p>Stars today: 5 stars today</p>
            <h2>README</h2><pre>---

&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;
    &lt;a href=&quot;https://opentelemetry.io/docs/collector/getting-started/&quot;&gt;Getting Started&lt;/a&gt;
    &amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://github.com/open-telemetry/opentelemetry-collector/blob/main/CONTRIBUTING.md&quot;&gt;Getting Involved&lt;/a&gt;
    &amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://cloud-native.slack.com/archives/C01N6P7KR6W&quot;&gt;Getting In Touch&lt;/a&gt;
  &lt;/strong&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/open-telemetry/opentelemetry-collector-contrib/actions/workflows/build-and-test.yml?query=branch%3Amain&quot;&gt;
    &lt;img alt=&quot;Build Status&quot; src=&quot;https://img.shields.io/github/actions/workflow/status/open-telemetry/opentelemetry-collector-contrib/build-and-test.yml?branch=main&amp;style=for-the-badge&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://goreportcard.com/report/github.com/open-telemetry/opentelemetry-collector-contrib&quot;&gt;
    &lt;img alt=&quot;Go Report Card&quot; src=&quot;https://goreportcard.com/badge/github.com/open-telemetry/opentelemetry-collector-contrib?style=for-the-badge&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://codecov.io/gh/open-telemetry/opentelemetry-collector-contrib/branch/main/&quot;&gt;
    &lt;img alt=&quot;Codecov Status&quot; src=&quot;https://img.shields.io/codecov/c/github/open-telemetry/opentelemetry-collector-contrib?style=for-the-badge&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/open-telemetry/opentelemetry-collector-contrib/releases&quot;&gt;
    &lt;img alt=&quot;GitHub release (latest by date including pre-releases)&quot; src=&quot;https://img.shields.io/github/v/release/open-telemetry/opentelemetry-collector-contrib?include_prereleases&amp;style=for-the-badge&quot;&gt;
  &lt;/a&gt;
  &lt;img alt=&quot;Beta&quot; src=&quot;https://img.shields.io/badge/status-beta-informational?style=for-the-badge&amp;logo=data:image/png;base64,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&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;
    &lt;a href=&quot;https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/vision.md&quot;&gt;Vision&lt;/a&gt;
    &amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/observability.md&quot;&gt;Observability&lt;/a&gt;
    &amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/security-best-practices.md&quot;&gt;Security&lt;/a&gt;
  &lt;/strong&gt;
&lt;/p&gt;

---

# OpenTelemetry Collector Contrib

This is a repository for OpenTelemetry Collector components that are not suitable for the  [core repository](https://github.com/open-telemetry/opentelemetry-collector) of the collector. 

The official distributions, core and contrib, are available as part of the [opentelemetry-collector-releases](https://github.com/open-telemetry/opentelemetry-collector-releases) repository. Some of the components in this repository are part of the &quot;core&quot; distribution, such as the Jaeger and Prometheus components, but most of the components here are only available as part of the &quot;contrib&quot; distribution. Users of the OpenTelemetry Collector are also encouraged to build their own custom distributions with the [OpenTelemetry Collector Builder](https://github.com/open-telemetry/opentelemetry-collector/tree/main/cmd/builder), using the components they need from the core repository, the contrib repository, and possibly third-party or internal repositories.

Each component has its own support levels, as defined in the following sections. For each signal that a component supports, there&#039;s a stability level, setting the right expectations. It is possible then that a component will be **Stable** for traces but **Alpha** for metrics and **Development** for logs.

## Stability levels

Stability level for components in this repository follow the [definitions](https://github.com/open-telemetry/opentelemetry-collector#stability-levels) from the OpenTelemetry Collector repository.

## Gated features

Some features are hidden behind feature gates before they are part of the main code path for the component. Note that the feature gates themselves might be at different [lifecycle stages](https://github.com/open-telemetry/opentelemetry-collector/tree/main/featuregate#feature-lifecycle).

## Support

Each component is supported either by the community of OpenTelemetry Collector Contrib maintainers, as defined by the GitHub group [@open-telemetry/collector-contrib-maintainer](https://github.com/orgs/open-telemetry/teams/collector-contrib-maintainer), or by specific vendors. See the individual README files for information about the specific components.

The OpenTelemetry Collector Contrib maintainers may at any time downgrade specific components if they are deemed unmaintained or if they pose a risk to the repository and/or binary distribution.

Even though the OpenTelemetry Collector Contrib maintainers are ultimately responsible for the components hosted here, actual support will likely be provided by individual contributors, typically a code owner for the specific component.

## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md).

### Maintainers

- [Alex Boten](https://github.com/codeboten), Honeycomb
- [Andrzej Stencel](https://github.com/andrzej-stencel), Elastic
- [Antoine Toulme](https://github.com/atoulme), Splunk
- [Bogdan Drutu](https://github.com/bogdandrutu), Snowflake
- [Christos Markou](https://github.com/ChrsMark), Elastic
- [Dmitrii Anoshin](https://github.com/dmitryax), Splunk
- [Edmo Vamerlatti Costa](https://github.com/edmocosta), Elastic
- [Evan Bradley](https://github.com/evan-bradley), Dynatrace
- [Pablo Baeyens](https://github.com/mx-psi), DataDog
- [Sean Marciniak](https://github.com/MovieStoreGuy), Splunk
- [Tyler Helmuth](https://github.com/TylerHelmuth), Honeycomb
- [Yang Song](https://github.com/songy23), DataDog

For more information about the maintainer role, see the [community repository](https://github.com/open-telemetry/community/blob/main/guides/contributor/membership.md#maintainer).

### Approvers

- [Andrew Wilkins](https://github.com/axw), Elastic
- [Arthur Silva Sens](https://github.com/ArthurSens), Grafana Labs
- [Braydon Kains](https://github.com/braydonk), Google
- [Curtis Robert](https://github.com/crobert-1), Splunk
- [David Ashpole](https://github.com/dashpole), Google
- [Matt Wear](https://github.com/mwear), Lightstep
- [Paulo Janotti](https://github.com/pjanotti), Splunk
- [Sam DeHaan](https://github.com/dehaansa), Grafana Labs
- [Vihas Makwana](https://github.com/VihasMakwana), Elastic
- [Ziqi Zhao](https://github.com/fatsheep9146), Alibaba

For more information about the approver role, see the [community repository](https://github.com/open-telemetry/community/blob/main/guides/contributor/membership.md#approver).

### Triagers

- [Benedikt Bongartz](https://github.com/frzifus), Red Hat
- [Bogdan Stancu](https://github.com/bogdan-st), Adobe
- [Constan√ßa Manteigas](https://github.com/constanca-m), Elastic
- [Douglas Camata](https://github.com/douglascamata), Coralogix
- [Florian Bacher](https://github.com/bacherfl), Dynatrace
- [Israel Blancas](https://github.com/iblancasa), Coralogix
- [James Moessis](https://github.com/jamesmoessis), Atlassian
- [Jared Tan](https://github.com/JaredTan95), DaoCloud
- [Murphy Chen](https://github.com/Frapschen), DaoCloud
- [Ondrej Dubaj](https://github.com/odubajDT), Dynatrace
- [Paulo Dias](https://github.com/paulojmdias), Five9
- [Roger Coll](https://github.com/rogercoll), Elastic
- Actively seeking contributors to triage issues

For more information about the triager role, see the [community repository](https://github.com/open-telemetry/community/blob/main/guides/contributor/membership.md#triager).

### Emeritus Maintainers

- [Daniel Jaglowski](https://github.com/djaglowski)
- [Juraci Paix√£o Kr√∂hling](https://github.com/jpkrohling)
- [Tigran Najaryan](https://github.com/tigrannajaryan)

For more information about the emeritus role, see the [community repository](https://github.com/open-telemetry/community/blob/main/guides/contributor/membership.md#emeritus-maintainerapprovertriager).

### Emeritus Approvers

- [Anthony Mirabella](https://github.com/Aneurysm9)
- [Bryan Aguilar](https://github.com/bryan-aguilar)
- [Przemek Maciolek](https://github.com/pmm-sumo)
- [Ruslan Kovalov](https://github.com/kovrus)

For more information about the emeritus role, see the [community repository](https://github.com/open-telemetry/community/blob/main/guides/contributor/membership.md#emeritus-maintainerapprovertriager).

### Emeritus Triagers

- [Alolita Sharma](https://github.com/alolita)
- [Gabriel Aszalos](https://github.com/gbbr)
- [Goutham Veeramachaneni](https://github.com/gouthamve)
- [Punya Biswal](https://github.com/punya)
- [Steve Flanders](https://github.com/flands)

For more information about the emeritus role, see the [community repository](https://github.com/open-telemetry/community/blob/main/guides/contributor/membership.md#emeritus-maintainerapprovertriager).

### No Over-Representation

A community member cannot be promoted to be a Collector contrib maintainer if, after their promotion, the resulting maintainers group has more than one-fourth (25%) of the members affiliated with the same employer. Job changes and similar events might result in over-representation, and no new maintainers from the same company can be promoted until representation is balanced again. In the event of confusion or concern, the OpenTelemetry Collector SIG will defer to the CNCF definition of &quot;same employer&quot;.

## PRs and Reviews

When creating a PR please follow the process [described
here](https://github.com/open-telemetry/opentelemetry-collector/blob/main/CONTRIBUTING.md#how-to-structure-prs-to-get-expedient-reviews).

New PRs will be automatically associated with the reviewers based on
[CODEOWNERS](.github/CODEOWNERS). PRs will be also automatically assigned to one of the
maintainers or approvers for facilitation.

The facilitator is responsible for helping the PR author and reviewers to make progress
or if progress cannot be made for closing the PR.

If the reviewers do not have approval rights the facilitator is also responsible
for the official approval that is required for the PR to be merged and if the facilitator
is a maintainer they are responsible for merging the PR as well.

The facilitator is not required to perform a thorough review, but they are encouraged to
enforce Collector best practices and consistency across the codebase and component
behavior. The facilitators will typically rely on codeowner&#039;s detailed review of the code
when making the final approval decision. 
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[open-telemetry/opentelemetry-collector]]></title>
            <link>https://github.com/open-telemetry/opentelemetry-collector</link>
            <guid>https://github.com/open-telemetry/opentelemetry-collector</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:53 GMT</pubDate>
            <description><![CDATA[OpenTelemetry Collector]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/open-telemetry/opentelemetry-collector">open-telemetry/opentelemetry-collector</a></h1>
            <p>OpenTelemetry Collector</p>
            <p>Language: Go</p>
            <p>Stars: 6,399</p>
            <p>Forks: 1,810</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre>---

&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;
    &lt;a href=&quot;https://opentelemetry.io/docs/collector/getting-started/&quot;&gt;Getting Started&lt;/a&gt;
    &amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;CONTRIBUTING.md&quot;&gt;Getting Involved&lt;/a&gt;
    &amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://cloud-native.slack.com/archives/C01N6P7KR6W&quot;&gt;Getting In Touch&lt;/a&gt;
  &lt;/strong&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/open-telemetry/opentelemetry-collector/actions/workflows/build-and-test.yml?query=branch%3Amain&quot;&gt;
    &lt;img alt=&quot;Build Status&quot; src=&quot;https://img.shields.io/github/actions/workflow/status/open-telemetry/opentelemetry-collector/build-and-test.yml?branch=main&amp;style=for-the-badge&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://goreportcard.com/report/github.com/open-telemetry/opentelemetry-collector&quot;&gt;
    &lt;img alt=&quot;Go Report Card&quot; src=&quot;https://goreportcard.com/badge/github.com/open-telemetry/opentelemetry-collector?style=for-the-badge&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://codecov.io/gh/open-telemetry/opentelemetry-collector/branch/main/&quot;&gt;
    &lt;img alt=&quot;Codecov Status&quot; src=&quot;https://img.shields.io/codecov/c/github/open-telemetry/opentelemetry-collector?style=for-the-badge&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/open-telemetry/opentelemetry-collector/releases&quot;&gt;
    &lt;img alt=&quot;GitHub release (latest by date including pre-releases)&quot; src=&quot;https://img.shields.io/github/v/release/open-telemetry/opentelemetry-collector?include_prereleases&amp;style=for-the-badge&quot;&gt;
  &lt;/a&gt;
  &lt;/br&gt;
  &lt;a href=&quot;https://www.bestpractices.dev/projects/8404&quot;&gt;&lt;img src=&quot;https://www.bestpractices.dev/projects/8404/badge&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&amp;can=1&amp;q=proj:opentelemetry&quot;&gt;
    &lt;img alt=&quot;Fuzzing Status&quot; src=&quot;https://oss-fuzz-build-logs.storage.googleapis.com/badges/opentelemetry.svg&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;
    &lt;a href=&quot;docs/vision.md&quot;&gt;Vision&lt;/a&gt;
    &amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://opentelemetry.io/docs/collector/configuration/&quot;&gt;Configuration&lt;/a&gt;
    &amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://opentelemetry.io/docs/collector/internal-telemetry/#use-internal-telemetry-to-monitor-the-collector&quot;&gt;Monitoring&lt;/a&gt;
    &amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;docs/security-best-practices.md&quot;&gt;Security&lt;/a&gt;
    &amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://pkg.go.dev/go.opentelemetry.io/collector&quot;&gt;Package&lt;/a&gt;
  &lt;/strong&gt;
&lt;/p&gt;

---

# &lt;img src=&quot;https://opentelemetry.io/img/logos/opentelemetry-logo-nav.png&quot; alt=&quot;OpenTelemetry Icon&quot; width=&quot;45&quot; height=&quot;&quot;&gt; OpenTelemetry Collector

The OpenTelemetry Collector offers a vendor-agnostic implementation on how to
receive, process and export telemetry data. In addition, it removes the need
to run, operate and maintain multiple agents/collectors in order to support
open-source telemetry data formats (e.g. Jaeger, Prometheus, etc.) to
multiple open-source or commercial back-ends.

Objectives:

- Usable: Reasonable default configuration, supports popular protocols, runs and collects out of the box.
- Performant: Highly stable and performant under varying loads and configurations.
- Observable: An exemplar of an observable service.
- Extensible: Customizable without touching the core code.
- Unified: Single codebase, deployable as an agent or collector with support for traces, metrics and logs.

## Community

The OpenTelemetry Collector SIG is present at the [#otel-collector](https://cloud-native.slack.com/archives/C01N6P7KR6W)
channel on the CNCF Slack and [meets once a week](https://github.com/open-telemetry/community#implementation-sigs) via
video calls. Everyone is invited to join those calls, which typically serves the following purposes:

- meet the humans behind the project
- get an opinion about specific proposals
- look for a sponsor for a proposed component after trying already via GitHub and Slack
- get attention to a specific pull-request that got stuck and is difficult to discuss asynchronously

We rotate our video calls between three time slots, in order to
allow everyone to join at least once every three meetings. The rotation order is as follows:

Tuesday:

- [17:00 PT](https://dateful.com/convert/pst-pdt-pacific-time?t=1700)

Wednesday:

- [09:00 PT](https://dateful.com/convert/pst-pdt-pacific-time?t=0900)
- [05:00 PT](https://dateful.com/convert/pst-pdt-pacific-time?t=0500)

Contributors to the project are also welcome to have ad-hoc meetings for synchronous discussions about specific points.
Post a note in #otel-collector-dev on Slack inviting others, specifying the topic to be discussed. Unless there are strong
reasons to keep the meeting private, please make it an open invitation for other contributors to join. Try also to
identify who would be the other contributors interested on that topic and in which timezones they are.

Remember that our source of truth is GitHub: every decision made via Slack or video calls has to be recorded in the
relevant GitHub issue. Ideally, the agenda items from the meeting notes would include a link to the issue or pull
request where a discussion is happening already. We acknowledge that not everyone can join Slack or the synchronous
calls and don&#039;t want them to feel excluded.

## Supported OTLP version

This code base is currently built against using OTLP protocol v1.5.0,
considered Stable. [See the OpenTelemetry Protocol Stability
definition
here.](https://github.com/open-telemetry/opentelemetry-proto?tab=readme-ov-file#stability-definition)

## Stability levels

See [Stability Levels and versioning](docs/component-stability.md) for more details.

## Compatibility

When used as a library, the OpenTelemetry Collector attempts to track the currently supported versions of Go, as [defined by the Go team](https://go.dev/doc/devel/release#policy).
Removing support for an unsupported Go version is not considered a breaking change.

Support for Go versions on the OpenTelemetry Collector is updated as follows:

1. The first release after the release of a new Go minor version `N` will add build and tests steps for the new Go minor version.
2. The first release after the release of a new Go minor version `N` will remove support for Go version `N-2`.

Official OpenTelemetry Collector distro binaries will be built with a release in the latest Go minor version series.

## Verifying the images signatures

&gt; [!NOTE]
&gt; To verify a signed artifact or blob, first [install Cosign](https://docs.sigstore.dev/cosign/system_config/installation/), then follow the instructions below.

We are signing the images `otel/opentelemetry-collector` and `otel/opentelemetry-collector-contrib` using [sigstore cosign](https://github.com/sigstore/cosign) tool and to verify the signatures you can run the following command:

```console
$ cosign verify \
  --certificate-identity=https://github.com/open-telemetry/opentelemetry-collector-releases/.github/workflows/base-release.yaml@refs/tags/&lt;RELEASE_TAG&gt; \
  --certificate-oidc-issuer=https://token.actions.githubusercontent.com \
  &lt;OTEL_COLLECTOR_IMAGE&gt;
```

where:

- `&lt;RELEASE_TAG&gt;`: is the release that you want to validate
- `&lt;OTEL_COLLECTOR_IMAGE&gt;`: is the image that you want to check

Example:

```console
$ cosign verify --certificate-identity=https://github.com/open-telemetry/opentelemetry-collector-releases/.github/workflows/base-release.yaml@refs/tags/v0.98.0 --certificate-oidc-issuer=https://token.actions.githubusercontent.com ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib:0.98.0

Verification for ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib:0.98.0 --
The following checks were performed on each of these signatures:
  - The cosign claims were validated
  - Existence of the claims in the transparency log was verified offline
  - The code-signing certificate was verified using trusted certificate authority certificates

[{&quot;critical&quot;:{&quot;identity&quot;:{&quot;docker-reference&quot;:&quot;ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib&quot;},&quot;image&quot;:{&quot;docker-manifest-digest&quot;:&quot;sha256:5cea85bcbc734a3c0a641368e5a4ea9d31b472997e9f2feca57eeb4a147fcf1a&quot;},&quot;type&quot;:&quot;cosign container image signature&quot;},&quot;optional&quot;:{&quot;1.3.6.1.4.1.57264.1.1&quot;:&quot;https://token.actions.githubusercontent.com&quot;,&quot;1.3.6.1.4.1.57264.1.2&quot;:&quot;push&quot;,&quot;1.3.6.1.4.1.57264.1.3&quot;:&quot;9e20bf5c142e53070ccb8320a20315fffb41469e&quot;,&quot;1.3.6.1.4.1.57264.1.4&quot;:&quot;Release Contrib&quot;,&quot;1.3.6.1.4.1.57264.1.5&quot;:&quot;open-telemetry/opentelemetry-collector-releases&quot;,&quot;1.3.6.1.4.1.57264.1.6&quot;:&quot;refs/tags/v0.98.0&quot;,&quot;Bundle&quot;:{&quot;SignedEntryTimestamp&quot;:&quot;MEUCIQDdlmNeKXQrHnonwWiHLhLLwFDVDNoOBCn2sv85J9P8mgIgDQFssWJImo1hn38VlojvSCL7Qq5FMmtnGu0oLsNdOm8=&quot;,&quot;Payload&quot;:{&quot;body&quot;:&quot;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&quot;,&quot;integratedTime&quot;:1712809120,&quot;logIndex&quot;:84797936,&quot;logID&quot;:&quot;c0d23d6ad406973f9559f3ba2d1ca01f84147d8ffc5b8445c224f98b9591801d&quot;}},&quot;Issuer&quot;:&quot;https://token.actions.githubusercontent.com&quot;,&quot;Subject&quot;:&quot;https://github.com/open-telemetry/opentelemetry-collector-releases/.github/workflows/base-release.yaml@refs/tags/v0.98.0&quot;,&quot;githubWorkflowName&quot;:&quot;Release Contrib&quot;,&quot;githubWorkflowRef&quot;:&quot;refs/tags/v0.98.0&quot;,&quot;githubWorkflowRepository&quot;:&quot;open-telemetry/opentelemetry-collector-releases&quot;,&quot;githubWorkflowSha&quot;:&quot;9e20bf5c142e53070ccb8320a20315fffb41469e&quot;,&quot;githubWorkflowTrigger&quot;:&quot;push&quot;}},{&quot;critical&quot;:{&quot;identity&quot;:{&quot;docker-reference&quot;:&quot;ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib&quot;},&quot;image&quot;:{&quot;docker-manifest-digest&quot;:&quot;sha256:5cea85bcbc734a3c0a641368e5a4ea9d31b472997e9f2feca57eeb4a147fcf1a&quot;},&quot;type&quot;:&quot;cosign container image signature&quot;},&quot;optional&quot;:{&quot;1.3.6.1.4.1.57264.1.1&quot;:&quot;https://token.actions.githubusercontent.com&quot;,&quot;1.3.6.1.4.1.57264.1.2&quot;:&quot;push&quot;,&quot;1.3.6.1.4.1.57264.1.3&quot;:&quot;9e20bf5c142e53070ccb8320a20315fffb41469e&quot;,&quot;1.3.6.1.4.1.57264.1.4&quot;:&quot;Release Contrib&quot;,&quot;1.3.6.1.4.1.57264.1.5&quot;:&quot;open-telemetry/opentelemetry-collector-releases&quot;,&quot;1.3.6.1.4.1.57264.1.6&quot;:&quot;refs/tags/v0.98.0&quot;,&quot;Bundle&quot;:{&quot;SignedEntryTimestamp&quot;:&quot;MEUCIQD1ehDnPO6fzoPIpeQ3KFuYHHBiX7RcEbpo9B2r7JAlzwIgZ1bsuQz7gAXbNU1IEdsTQgfAnRk3xVXO16GnKXM2sAQ=&quot;,&quot;Payload&quot;:{&quot;body&quot;:&quot;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

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[modelcontextprotocol/registry]]></title>
            <link>https://github.com/modelcontextprotocol/registry</link>
            <guid>https://github.com/modelcontextprotocol/registry</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:52 GMT</pubDate>
            <description><![CDATA[A community driven registry service for Model Context Protocol (MCP) servers.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/modelcontextprotocol/registry">modelcontextprotocol/registry</a></h1>
            <p>A community driven registry service for Model Context Protocol (MCP) servers.</p>
            <p>Language: Go</p>
            <p>Stars: 6,083</p>
            <p>Forks: 523</p>
            <p>Stars today: 75 stars today</p>
            <h2>README</h2><pre># MCP Registry

The MCP registry provides MCP clients with a list of MCP servers, like an app store for MCP servers.

[**üì§ Publish my MCP server**](docs/modelcontextprotocol-io/quickstart.mdx) | [**‚ö°Ô∏è Live API docs**](https://registry.modelcontextprotocol.io/docs) | [**üëÄ Ecosystem vision**](docs/design/ecosystem-vision.md) | üìñ **[Full documentation](./docs)**

## Development Status

**2025-10-24 update**: The Registry API has entered an **API freeze (v0.1)** üéâ. For the next month or more, the API will remain stable with no breaking changes, allowing integrators to confidently implement support. This freeze applies to v0.1 while development continues on v0. We&#039;ll use this period to validate the API in real-world integrations and gather feedback to shape v1 for general availability. Thank you to everyone for your contributions and patience‚Äîyour involvement has been key to getting us here!

**2025-09-08 update**: The registry has launched in preview üéâ ([announcement blog post](https://blog.modelcontextprotocol.io/posts/2025-09-08-mcp-registry-preview/)). While the system is now more stable, this is still a preview release and breaking changes or data resets may occur. A general availability (GA) release will follow later. We&#039;d love your feedback in [GitHub discussions](https://github.com/modelcontextprotocol/registry/discussions/new?category=ideas) or in the [#registry-dev Discord](https://discord.com/channels/1358869848138059966/1369487942862504016) ([joining details here](https://modelcontextprotocol.io/community/communication)).

Current key maintainers:
- **Adam Jones** (Anthropic) [@domdomegg](https://github.com/domdomegg)  
- **Tadas Antanavicius** (PulseMCP) [@tadasant](https://github.com/tadasant)
- **Toby Padilla** (GitHub) [@toby](https://github.com/toby)
- **Radoslav (Rado) Dimitrov** (Stacklok) [@rdimitrov](https://github.com/rdimitrov)

## Contributing

We use multiple channels for collaboration - see [modelcontextprotocol.io/community/communication](https://modelcontextprotocol.io/community/communication).

Often (but not always) ideas flow through this pipeline:

- **[Discord](https://modelcontextprotocol.io/community/communication)** - Real-time community discussions
- **[Discussions](https://github.com/modelcontextprotocol/registry/discussions)** - Propose and discuss product/technical requirements
- **[Issues](https://github.com/modelcontextprotocol/registry/issues)** - Track well-scoped technical work  
- **[Pull Requests](https://github.com/modelcontextprotocol/registry/pulls)** - Contribute work towards issues

### Quick start:

#### Pre-requisites

- **Docker**
- **Go 1.24.x**
- **ko** - Container image builder for Go ([installation instructions](https://ko.build/install/))
- **golangci-lint v2.4.0**

#### Running the server

```bash
# Start full development environment
make dev-compose
```

This starts the registry at [`localhost:8080`](http://localhost:8080) with PostgreSQL. The database uses ephemeral storage and is reset each time you restart the containers, ensuring a clean state for development and testing.

**Note:** The registry uses [ko](https://ko.build) to build container images. The `make dev-compose` command automatically builds the registry image with ko and loads it into your local Docker daemon before starting the services.

By default, the registry seeds from the production API with a filtered subset of servers (to keep startup fast). This ensures your local environment mirrors production behavior and all seed data passes validation. For offline development you can seed from a file without validation with `MCP_REGISTRY_SEED_FROM=data/seed.json MCP_REGISTRY_ENABLE_REGISTRY_VALIDATION=false make dev-compose`.

The setup can be configured with environment variables in [docker-compose.yml](./docker-compose.yml) - see [.env.example](./.env.example) for a reference.

&lt;details&gt;
&lt;summary&gt;Alternative: Running a pre-built Docker image&lt;/summary&gt;

Pre-built Docker images are automatically published to GitHub Container Registry:

```bash
# Run latest stable release
docker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:latest

# Run latest from main branch (continuous deployment)
docker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:main

# Run specific release version
docker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:v1.0.0

# Run development build from main branch
docker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:main-20250906-abc123d
```

**Available tags:** 
- **Releases**: `latest`, `v1.0.0`, `v1.1.0`, etc.
- **Continuous**: `main` (latest main branch build)
- **Development**: `main-&lt;date&gt;-&lt;sha&gt;` (specific commit builds)

&lt;/details&gt;

#### Publishing a server

To publish a server, we&#039;ve built a simple CLI. You can use it with:

```bash
# Build the latest CLI
make publisher

# Use it!
./bin/mcp-publisher --help
```

See [the publisher guide](./docs/modelcontextprotocol-io/quickstart.mdx) for more details.

#### Other commands

```bash
# Run lint, unit tests and integration tests
make check
```

There are also a few more helpful commands for development. Run `make help` to learn more, or look in [Makefile](./Makefile).

&lt;!--
For Claude and other AI tools: Always prefer make targets over custom commands where possible.
--&gt;

## Architecture

### Project Structure

```
‚îú‚îÄ‚îÄ cmd/                     # Application entry points
‚îÇ   ‚îî‚îÄ‚îÄ publisher/           # Server publishing tool
‚îú‚îÄ‚îÄ data/                    # Seed data
‚îú‚îÄ‚îÄ deploy/                  # Deployment configuration (Pulumi)
‚îú‚îÄ‚îÄ docs/                    # Documentation
‚îú‚îÄ‚îÄ internal/                # Private application code
‚îÇ   ‚îú‚îÄ‚îÄ api/                 # HTTP handlers and routing
‚îÇ   ‚îú‚îÄ‚îÄ auth/                # Authentication (GitHub OAuth, JWT, namespace blocking)
‚îÇ   ‚îú‚îÄ‚îÄ config/              # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ database/            # Data persistence (PostgreSQL)
‚îÇ   ‚îú‚îÄ‚îÄ service/             # Business logic
‚îÇ   ‚îú‚îÄ‚îÄ telemetry/           # Metrics and monitoring
‚îÇ   ‚îî‚îÄ‚îÄ validators/          # Input validation
‚îú‚îÄ‚îÄ pkg/                     # Public packages
‚îÇ   ‚îú‚îÄ‚îÄ api/                 # API types and structures
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ v0/              # Version 0 API types
‚îÇ   ‚îî‚îÄ‚îÄ model/               # Data models for server.json
‚îú‚îÄ‚îÄ scripts/                 # Development and testing scripts
‚îú‚îÄ‚îÄ tests/                   # Integration tests
‚îî‚îÄ‚îÄ tools/                   # CLI tools and utilities
    ‚îî‚îÄ‚îÄ validate-*.sh        # Schema validation tools
```

### Authentication

Publishing supports multiple authentication methods:
- **GitHub OAuth** - For publishing by logging into GitHub
- **GitHub OIDC** - For publishing from GitHub Actions
- **DNS verification** - For proving ownership of a domain and its subdomains
- **HTTP verification** - For proving ownership of a domain

The registry validates namespace ownership when publishing. E.g. to publish...:
- `io.github.domdomegg/my-cool-mcp` you must login to GitHub as `domdomegg`, or be in a GitHub Action on domdomegg&#039;s repos
- `me.adamjones/my-cool-mcp` you must prove ownership of `adamjones.me` via DNS or HTTP challenge

## Community Projects

Check out [community projects](docs/community-projects.md) to explore notable registry-related work created by the community.

## More documentation

See the [documentation](./docs) for more details if your question has not been answered here!
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[microsoft/typescript-go]]></title>
            <link>https://github.com/microsoft/typescript-go</link>
            <guid>https://github.com/microsoft/typescript-go</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:51 GMT</pubDate>
            <description><![CDATA[Staging repo for development of native port of TypeScript]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/typescript-go">microsoft/typescript-go</a></h1>
            <p>Staging repo for development of native port of TypeScript</p>
            <p>Language: Go</p>
            <p>Stars: 23,259</p>
            <p>Forks: 760</p>
            <p>Stars today: 33 stars today</p>
            <h2>README</h2><pre># TypeScript 7

[Not sure what this is? Read the announcement post!](https://devblogs.microsoft.com/typescript/typescript-native-port/)

## Preview

A preview build is available on npm as [`@typescript/native-preview`](https://www.npmjs.com/package/@typescript/native-preview).

```sh
npm install @typescript/native-preview
npx tsgo # Use this as you would tsc.
```

A preview VS Code extension is [available on the VS Code marketplace](https://marketplace.visualstudio.com/items?itemName=TypeScriptTeam.native-preview).

To use this, set this in your VS Code settings:

```json
{
    &quot;typescript.experimental.useTsgo&quot;: true
}
```

## What Works So Far?

This is still a work in progress and is not yet at full feature parity with TypeScript. Bugs may exist. Please check this list carefully before logging a new issue or assuming an intentional change.

| Feature | Status | Notes |
|---------|--------|-------|
| Program creation | done | Same files and module resolution as TS 5.9. Not all resolution modes supported yet. |
| Parsing/scanning | done | Exact same syntax errors as TS 5.9 |
| Commandline and `tsconfig.json` parsing | done | Done, though `tsconfig` errors may not be as helpful. |
| Type resolution | done | Same types as TS 5.9. |
| Type checking | done | Same errors, locations, and messages as TS 5.9. Types printback in errors may display differently. |
| JavaScript-specific inference and JSDoc | in progress | Mostly complete, but intentionally lacking some features. Declaration emit not complete. |
| JSX | done | - |
| Declaration emit | in progress | Most common features are in place, but some edge cases and feature flags are still unhandled. |
| Emit (JS output) | in progress | `target: esnext` well-supported, other targets may have gaps. |
| Watch mode | prototype | Watches files and rebuilds, but no incremental rechecking. Not optimized. |
| Build mode / project references | done | - |
| Incremental build | done | - |
| Language service (LSP) | in progress | Most functionality. More features coming soon. |
| API | not ready | - |

Definitions:

 * **done** aka &quot;believed done&quot;: We&#039;re not currently aware of any deficits or major left work to do. OK to log bugs
 * **in progress**: currently being worked on; some features may work and some might not. OK to log panics, but nothing else please
 * **prototype**: proof-of-concept only; do not log bugs
 * **not ready**: either haven&#039;t even started yet, or far enough from ready that you shouldn&#039;t bother messing with it yet

## Other Notes

Long-term, we expect that this repo and its contents will be merged into `microsoft/TypeScript`.
As a result, the repo and issue tracker for typescript-go will eventually be closed, so treat discussions/issues accordingly.

For a list of intentional changes with respect to TypeScript 5.9, see CHANGES.md.

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit [Contributor License Agreements](https://cla.opensource.microsoft.com).

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
[Microsoft&#039;s Trademark &amp; Brand Guidelines](https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party&#039;s policies.
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[XIU2/CloudflareSpeedTest]]></title>
            <link>https://github.com/XIU2/CloudflareSpeedTest</link>
            <guid>https://github.com/XIU2/CloudflareSpeedTest</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:50 GMT</pubDate>
            <description><![CDATA[üå©„ÄåËá™ÈÄâ‰ºòÈÄâ IP„ÄçÊµãËØï Cloudflare CDN Âª∂ËøüÂíåÈÄüÂ∫¶ÔºåËé∑ÂèñÊúÄÂø´ IP ÔºÅÂΩìÁÑ∂‰πüÊîØÊåÅÂÖ∂‰ªñ CDN / Â§ö‰∏™Ëß£Êûê IP ÁöÑÁΩëÁ´ô ~]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/XIU2/CloudflareSpeedTest">XIU2/CloudflareSpeedTest</a></h1>
            <p>üå©„ÄåËá™ÈÄâ‰ºòÈÄâ IP„ÄçÊµãËØï Cloudflare CDN Âª∂ËøüÂíåÈÄüÂ∫¶ÔºåËé∑ÂèñÊúÄÂø´ IP ÔºÅÂΩìÁÑ∂‰πüÊîØÊåÅÂÖ∂‰ªñ CDN / Â§ö‰∏™Ëß£Êûê IP ÁöÑÁΩëÁ´ô ~</p>
            <p>Language: Go</p>
            <p>Stars: 23,673</p>
            <p>Forks: 4,692</p>
            <p>Stars today: 30 stars today</p>
            <h2>README</h2><pre># XIU2/CloudflareSpeedTest

[![Go Version](https://img.shields.io/github/go-mod/go-version/XIU2/CloudflareSpeedTest.svg?style=flat-square&amp;label=Go&amp;color=00ADD8&amp;logo=go)](https://github.com/XIU2/CloudflareSpeedTest/)
[![Release Version](https://img.shields.io/github/v/release/XIU2/CloudflareSpeedTest.svg?style=flat-square&amp;label=Release&amp;color=00ADD8&amp;logo=github)](https://github.com/XIU2/CloudflareSpeedTest/releases/latest)
[![GitHub license](https://img.shields.io/github/license/XIU2/CloudflareSpeedTest.svg?style=flat-square&amp;label=License&amp;color=00ADD8&amp;logo=github)](https://github.com/XIU2/CloudflareSpeedTest/)
[![GitHub Star](https://img.shields.io/github/stars/XIU2/CloudflareSpeedTest.svg?style=flat-square&amp;label=Star&amp;color=00ADD8&amp;logo=github)](https://github.com/XIU2/CloudflareSpeedTest/)
[![GitHub Fork](https://img.shields.io/github/forks/XIU2/CloudflareSpeedTest.svg?style=flat-square&amp;label=Fork&amp;color=00ADD8&amp;logo=github)](https://github.com/XIU2/CloudflareSpeedTest/)

ÂõΩÂ§ñÂæàÂ§öÁΩëÁ´ôÈÉΩÂú®‰ΩøÁî® Cloudflare CDNÔºå‰ΩÜÂàÜÈÖçÁªô‰∏≠ÂõΩÂÜÖÂú∞ËÆøÂÆ¢ÁöÑ IP Âπ∂‰∏çÂèãÂ•ΩÔºàÂª∂ËøüÈ´ò„ÄÅ‰∏¢ÂåÖÂ§ö„ÄÅÈÄüÂ∫¶ÊÖ¢Ôºâ„ÄÇ  
ËôΩÁÑ∂ Cloudflare ÂÖ¨ÂºÄ‰∫ÜÊâÄÊúâ [IP ÊÆµ](https://www.cloudflare.com/zh-cn/ips/) Ôºå‰ΩÜÊÉ≥Ë¶ÅÂú®Ëøô‰πàÂ§ö IP ‰∏≠ÊâæÂà∞ÈÄÇÂêàËá™Â∑±ÁöÑÔºåÊÄïÊòØË¶ÅÁ¥ØÊ≠ªÔºå‰∫éÊòØÂ∞±Êúâ‰∫ÜËøô‰∏™ËΩØ‰ª∂„ÄÇ

**„ÄåËá™ÈÄâ‰ºòÈÄâ IP„ÄçÊµãËØï Cloudflare CDN Âª∂ËøüÂíåÈÄüÂ∫¶ÔºåËé∑ÂèñÊúÄÂø´ IP (IPv4+IPv6)**ÔºÅÂ•ΩÁî®ÁöÑËØù**ÁÇπ‰∏™`‚≠ê`ÈºìÂä±‰∏Ä‰∏ãÂè≠~**

&gt; _ÂàÜ‰∫´ÊàëÂÖ∂‰ªñÂºÄÊ∫êÈ°πÁõÆÔºö[**TrackersList.com** - ÂÖ®ÁΩëÁÉ≠Èó® BT Tracker ÂàóË°®ÔºÅÊúâÊïàÊèêÈ´ò BT ‰∏ãËΩΩÈÄüÂ∫¶~](https://github.com/XIU2/TrackersListCollection) &lt;img src=&quot;https://img.shields.io/github/stars/XIU2/TrackersListCollection.svg?style=flat-square&amp;label=Star&amp;color=4285dd&amp;logo=github&quot; height=&quot;16px&quot; /&gt;_  
&gt; _[**UserScript** - üêµ Github È´òÈÄü‰∏ãËΩΩ„ÄÅÁü•‰πéÂ¢ûÂº∫„ÄÅËá™Âä®Êó†ÁºùÁøªÈ°µ„ÄÅÊä§ÁúºÊ®°Âºè Á≠âÂçÅÂá†‰∏™**Ê≤πÁå¥ËÑöÊú¨**~](https://github.com/XIU2/UserScript) &lt;img src=&quot;https://img.shields.io/github/stars/XIU2/UserScript.svg?style=flat-square&amp;label=Star&amp;color=4285dd&amp;logo=github&quot; height=&quot;16px&quot; /&gt;_  
&gt; _[**SNIProxy** - üß∑ Ëá™Áî®ÁöÑÁÆÄÂçï SNI ProxyÔºàÊîØÊåÅÂÖ®Âπ≥Âè∞„ÄÅÂÖ®Á≥ªÁªü„ÄÅÂâçÁΩÆ‰ª£ÁêÜ„ÄÅÈÖçÁΩÆÁÆÄÂçïÁ≠â~](https://github.com/XIU2/SNIProxy) &lt;img src=&quot;https://img.shields.io/github/stars/XIU2/SNIProxy.svg?style=flat-square&amp;label=Star&amp;color=4285dd&amp;logo=github&quot; height=&quot;16px&quot; /&gt;_  

ÂΩìÁÑ∂‰∫ÜÔºåÊú¨È°πÁõÆ‰πüÊîØÊåÅÂØπ **`ÂÖ∂‰ªñ CDN / Â§ö‰∏™Ëß£Êûê IP ÁöÑÁΩëÁ´ô`** Âª∂ËøüÊµãÈÄüÔºå‰ΩÜÁõ∏ÂØπÂ∫îÁöÑ‰∏ãËΩΩÊµãÈÄüÂú∞ÂùÄÈúÄËá™Ë°åÂØªÊâæ„ÄÇ

&gt; [!IMPORTANT]
&gt; Cloudflare CDN Â∑≤**ÊòéÊñáÁ¶ÅÊ≠¢‰ª£ÁêÜ**ÊñπÂºè‰ΩøÁî®ÔºåÂØπ‰∫é**‰ª£ÁêÜÂ•ó CDN** ÁöÑËá™Ë°åÊâøÊãÖÈ£éÈô©ÔºåËØ∑ÂãøËøáÂ∫¶‰æùËµñ [#382](https://github.com/XIU2/CloudflareSpeedTest/discussions/382) [#383](https://github.com/XIU2/CloudflareSpeedTest/discussions/383)

****
## \# Âø´ÈÄü‰ΩøÁî®

### ‰∏ãËΩΩËøêË°å

1. ‰∏ãËΩΩÁºñËØëÂ•ΩÁöÑÂèØÊâßË°åÊñá‰ª∂Ôºà [Github Releases](https://github.com/XIU2/CloudflareSpeedTest/releases) / [ËìùÂ•è‰∫ë](https://xiu.lanzoub.com/b0742hkxe) ÔºâÂπ∂Ëß£Âéã„ÄÇ  
2. ÂèåÂáªËøêË°å `cfst.exe` Êñá‰ª∂ÔºàWindows Á≥ªÁªüÔºâÔºåÁ≠âÂæÖÊµãÈÄüÂÆåÊàê...

&lt;details&gt;
&lt;summary&gt;&lt;code&gt;&lt;strong&gt;„Äå ÁÇπÂáªÊü•Áúã Windows Á≥ªÁªü‰∏ãÂÖ∂‰ªñÂÆâË£ÖÊñπÂºè„Äç&lt;/strong&gt;&lt;/code&gt;&lt;/summary&gt;

****

Â¶ÇÊûú‰Ω†Êúâ scoop(Windows ‰∏ãÁöÑÂëΩ‰ª§Ë°åÂÆâË£ÖÁ®ãÂ∫è)ÔºåÂàôÂèØ‰ª•ËøôÊ†∑ÂÆâË£Ö:

```sh
# Ê∑ªÂä†ÊúÄÂ§ö‰∫∫‰ΩøÁî®ÁöÑ‰∏≠ÊñáËΩØ‰ª∂ÂåÖ‰ªìÂ∫ìÔºödorado
scoop bucket add dorado https://github.com/chawyehsu/dorado
# ÂÆâË£Öcloudflare-speedtest
scoop install dorado/cloudflare-speedtest
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;code&gt;&lt;strong&gt;„Äå ÁÇπÂáªÊü•Áúã Linux Á≥ªÁªü‰∏ãÁöÑ‰ΩøÁî®Á§∫‰æã „Äç&lt;/strong&gt;&lt;/code&gt;&lt;/summary&gt;

****

‰ª•‰∏ãÂëΩ‰ª§‰ªÖ‰∏∫Á§∫‰æãÔºåÁâàÊú¨Âè∑ÂíåÊñá‰ª∂ÂêçËØ∑ÂâçÂæÄ [**Releases**](https://github.com/XIU2/CloudflareSpeedTest/releases) Êü•Áúã„ÄÇ

``` yaml
# Â¶ÇÊûúÊòØÁ¨¨‰∏ÄÊ¨°‰ΩøÁî®ÔºåÂàôÂª∫ËÆÆÂàõÂª∫Êñ∞Êñá‰ª∂Â§πÔºàÂêéÁª≠Êõ¥Êñ∞Êó∂ÔºåË∑≥ËøáËØ•Ê≠•È™§Ôºâ
mkdir cfst

# ËøõÂÖ•Êñá‰ª∂Â§πÔºàÂêéÁª≠Êõ¥Êñ∞ÔºåÂè™ÈúÄË¶Å‰ªéËøôÈáåÈáçÂ§ç‰∏ãÈù¢ÁöÑ‰∏ãËΩΩ„ÄÅËß£ÂéãÂëΩ‰ª§Âç≥ÂèØÔºâ
cd cfst

# ‰∏ãËΩΩ CFST ÂéãÁº©ÂåÖÔºàËá™Ë°åÊ†πÊçÆÈúÄÊ±ÇÊõøÊç¢ URL ‰∏≠ [ÁâàÊú¨Âè∑] Âíå [Êñá‰ª∂Âêç]Ôºâ
wget -N https://github.com/XIU2/CloudflareSpeedTest/releases/download/v2.3.4/cfst_linux_amd64.tar.gz
# Â¶ÇÊûú‰Ω†ÊòØÂú®ÂõΩÂÜÖÁΩëÁªúÁéØÂ¢É‰∏≠‰∏ãËΩΩÔºåÈÇ£‰πàËØ∑‰ΩøÁî®‰∏ãÈù¢ËøôÂá†‰∏™ÈïúÂÉèÂä†ÈÄü‰πã‰∏ÄÔºö
# wget -N https://ghfast.top/https://github.com/XIU2/CloudflareSpeedTest/releases/download/v2.3.4/cfst_linux_arm64.tar.gz
# wget -N https://wget.la/https://github.com/XIU2/CloudflareSpeedTest/releases/download/v2.3.4/cfst_linux_arm64.tar.gz
# wget -N https://ghproxy.net/https://github.com/XIU2/CloudflareSpeedTest/releases/download/v2.3.4/cfst_linux_arm64.tar.gz
# wget -N https://gh-proxy.com/https://github.com/XIU2/CloudflareSpeedTest/releases/download/v2.3.4/cfst_linux_arm64.tar.gz
# wget -N https://hk.gh-proxy.com/https://github.com/XIU2/CloudflareSpeedTest/releases/download/v2.3.4/cfst_linux_arm64.tar.gz
# Â¶ÇÊûú‰∏ãËΩΩÂ§±Ë¥•ÁöÑËØùÔºåÂ∞ùËØïÂà†Èô§ -N ÂèÇÊï∞ÔºàÂ¶ÇÊûúÊòØ‰∏∫‰∫ÜÊõ¥Êñ∞ÔºåÂàôËÆ∞ÂæóÊèêÂâçÂà†Èô§ÊóßÂéãÁº©ÂåÖ rm cfst_linux_amd64.tar.gz Ôºâ

# Ëß£ÂéãÔºà‰∏çÈúÄË¶ÅÂà†Èô§ÊóßÊñá‰ª∂Ôºå‰ºöÁõ¥Êé•Ë¶ÜÁõñÔºåËá™Ë°åÊ†πÊçÆÈúÄÊ±ÇÊõøÊç¢ Êñá‰ª∂ÂêçÔºâ
tar -zxf cfst_linux_amd64.tar.gz

# Ëµã‰∫àÊâßË°åÊùÉÈôê
chmod +x cfst

# ËøêË°åÔºà‰∏çÂ∏¶ÂèÇÊï∞Ôºâ
./cfst

# ËøêË°åÔºàÂ∏¶ÂèÇÊï∞Á§∫‰æãÔºâ
./cfst -tl 200 -dn 20
```

&gt; Â¶ÇÊûúÂπ≥**ÂùáÂª∂ËøüÈùûÂ∏∏‰Ωé**ÔºàÂ¶Ç 0.xxÔºâÔºåÂàôËØ¥Êòé CFST **ÊµãÈÄüÊó∂Ëµ∞‰∫Ü‰ª£ÁêÜ**ÔºåËØ∑ÂÖàÂÖ≥Èó≠‰ª£ÁêÜËΩØ‰ª∂ÂêéÂÜçÊµãÈÄü„ÄÇ  
&gt; Â¶ÇÊûúÂú®**Ë∑ØÁî±Âô®**‰∏äËøêË°åÔºåÂª∫ËÆÆÂÖàÂÖ≥Èó≠Ë∑ØÁî±Âô®ÂÜÖÁöÑ‰ª£ÁêÜÔºàÊàñÂ∞ÜÂÖ∂ÊéíÈô§ÔºâÔºåÂê¶ÂàôÊµãÈÄüÁªìÊûúÂèØËÉΩ‰ºö**‰∏çÂáÜÁ°Æ/Êó†Ê≥ï‰ΩøÁî®**„ÄÇ

&lt;/details&gt;

****

&gt; _Âú®**ÊâãÊú∫**‰∏äÁã¨Á´ãËøêË°å CFST ÊµãÈÄüÁöÑÁÆÄÂçïÊïôÁ®ãÔºö**[Android](https://github.com/XIU2/CloudflareSpeedTest/discussions/61)„ÄÅ[Android APP](https://github.com/xianshenglu/cloudflare-ip-tester-app)„ÄÅ[IOS](https://github.com/XIU2/CloudflareSpeedTest/discussions/321)**_

&gt; [!NOTE]
&gt; Ê≥®ÊÑèÔºÅÊú¨ËΩØ‰ª∂‰ªÖÈÄÇÁî®‰∫éÁΩëÁ´ôÔºå**‰∏çÊîØÊåÅÁªô‰ΩøÁî® UDP ÂçèËÆÆÁöÑ Cloudflare WARP ‰ºòÈÄâ IP**ÔºåÂÖ∑‰ΩìËßÅÔºö[#392](https://github.com/XIU2/CloudflareSpeedTest/discussions/392)

### ÁªìÊûúÁ§∫‰æã

ÊµãÈÄüÂÆåÊØïÂêéÔºåÈªòËÆ§‰ºöÊòæÁ§∫**ÊúÄÂø´ÁöÑ 10 ‰∏™ IP**ÔºåÁ§∫‰æãÔºà‰ªÖ‰∏∫ËæìÂá∫ÂÜÖÂÆπÁ§∫‰æãÔºâÔºö

``` bash
IP Âú∞ÂùÄ           Â∑≤ÂèëÈÄÅ  Â∑≤Êé•Êî∂  ‰∏¢ÂåÖÁéá  Âπ≥ÂùáÂª∂Ëøü  ‰∏ãËΩΩÈÄüÂ∫¶(MB/s)  Âú∞Âå∫Á†Å
104.27.200.69     4      4       0.00   146.23    28.64          LAX
172.67.60.78      4      4       0.00   139.82    15.02          SEA
104.25.140.153    4      4       0.00   146.49    14.90          SJC
104.27.192.65     4      4       0.00   140.28    14.07          LAX
172.67.62.214     4      4       0.00   139.29    12.71          LAX
104.27.207.5      4      4       0.00   145.92    11.95          LAX
172.67.54.193     4      4       0.00   146.71    11.55          LAX
104.22.66.8       4      4       0.00   147.42    11.11          SEA
104.27.197.63     4      4       0.00   131.29    10.26          FRA
172.67.58.91      4      4       0.00   140.19    9.14           SJC
...

# Â¶ÇÊûúÂπ≥ÂùáÂª∂ËøüÈùûÂ∏∏‰ΩéÔºàÂ¶Ç 0.xxÔºâÔºåÂàôËØ¥Êòé CFST ÊµãÈÄüÊó∂Ëµ∞‰∫Ü‰ª£ÁêÜÔºåËØ∑ÂÖàÂÖ≥Èó≠‰ª£ÁêÜËΩØ‰ª∂ÂêéÂÜçÊµãÈÄü„ÄÇ
# Â¶ÇÊûúÂú®Ë∑ØÁî±Âô®‰∏äËøêË°åÔºåËØ∑ÂÖàÂÖ≥Èó≠Ë∑ØÁî±Âô®ÂÜÖÁöÑ‰ª£ÁêÜÔºàÊàñÂ∞ÜÂÖ∂ÊéíÈô§ÔºâÔºåÂê¶ÂàôÊµãÈÄüÁªìÊûúÂèØËÉΩ‰ºö‰∏çÂáÜÁ°Æ/Êó†Ê≥ï‰ΩøÁî®„ÄÇ

# Âõ†‰∏∫ÊØèÊ¨°ÊµãÈÄüÈÉΩÊòØÂú®ÊØè‰∏™ IP ÊÆµ‰∏≠ÈöèÊú∫ IPÔºåÊâÄ‰ª•ÊØèÊ¨°ÁöÑÊµãÈÄüÁªìÊûúÈÉΩ‰∏çÂèØËÉΩÁõ∏ÂêåÔºåËøôÊòØÊ≠£Â∏∏ÁöÑÔºÅ

# Ê≥®ÊÑèÔºÅÊàëÂèëÁé∞ÁîµËÑëÂºÄÊú∫ÂêéÁ¨¨‰∏ÄÊ¨°ÊµãÈÄüÂª∂Ëøü‰ºöÊòéÊòæÂÅèÈ´òÔºàÊâãÂä® TCPing ‰πü‰∏ÄÊ†∑ÔºâÔºåÂêéÁª≠ÊµãÈÄüÈÉΩÊ≠£Â∏∏
# Âõ†Ê≠§Âª∫ËÆÆÂ§ßÂÆ∂ÂºÄÊú∫ÂêéÁ¨¨‰∏ÄÊ¨°Ê≠£ÂºèÊµãÈÄüÂâçÔºåÂÖàÈöè‰æøÊµãÂá†‰∏™ IPÔºàÊó†ÈúÄÁ≠âÂæÖÂª∂ËøüÊµãÈÄüÂÆåÊàêÔºåÂè™Ë¶ÅËøõÂ∫¶Êù°Âä®‰∫ÜÂ∞±ÂèØ‰ª•Áõ¥Êé•ÂÖ≥‰∫ÜÔºâ

# ËΩØ‰ª∂Âú® ÈªòËÆ§ÂèÇÊï∞ ‰∏ãÁöÑÊï¥‰∏™ÊµÅÁ®ãÂ§ßÊ¶ÇÊ≠•È™§Ôºö
# 1. Âª∂ËøüÊµãÈÄüÔºàÈªòËÆ§ TCPing Ê®°ÂºèÔºåHTTPing Ê®°ÂºèÈúÄË¶ÅÊâãÂä®Âä†‰∏äÂèÇÊï∞Ôºâ
# 2. Âª∂ËøüÊéíÂ∫èÔºàÂª∂Ëøü ‰ªé‰ΩéÂà∞È´ò ÊéíÂ∫èÂπ∂ÊåâÊù°‰ª∂ËøáÊª§Ôºå‰∏çÂêå‰∏¢ÂåÖÁéá‰ºöÂàÜÂºÄÊéíÂ∫èÔºåÂõ†Ê≠§ÂèØËÉΩ‰ºöÊúâ‰∏Ä‰∫õÂª∂Ëøü‰Ωé‰ΩÜ‰∏¢ÂåÖÁöÑ IP ÊéíÂà∞ÂêéÈù¢Ôºâ
# 3. ‰∏ãËΩΩÊµãÈÄüÔºà‰ªéÂª∂ËøüÊúÄ‰ΩéÁöÑ IP ÂºÄÂßã‰æùÊ¨°‰∏ãËΩΩÊµãÈÄüÔºåÈªòËÆ§ÊµãÂ§ü 10 ‰∏™Â∞±‰ºöÂÅúÊ≠¢Ôºâ
# 4. ÈÄüÂ∫¶ÊéíÂ∫èÔºàÈÄüÂ∫¶‰ªéÈ´òÂà∞‰ΩéÊéíÂ∫èÔºâ
# 5. ËæìÂá∫ÁªìÊûúÔºàÈÄöËøáÂèÇÊï∞ÊéßÂà∂ÊòØÂê¶ËæìÂá∫Âà∞ÂëΩ‰ª§Ë°å(-p 0)ÊàñËæìÂá∫Âà∞Êñá‰ª∂(-o &quot;&quot;)Ôºâ

# Ê≥®ÊÑèÔºöËæìÂá∫ÁöÑÁªìÊûúÊñá‰ª∂ result.csv ÈÄöËøáÂæÆËΩØ Excel Ë°®Ê†ºÊâìÂºÄ‰ºö‰∏≠Êñá‰π±Á†ÅÔºåËøôÊòØÊ≠£Â∏∏ÁöÑÔºåÂÖ∂‰ªñË°®Ê†ºËΩØ‰ª∂/ËÆ∞‰∫ãÊú¨ÈÉΩÊòæÁ§∫Ê≠£Â∏∏
```

ÊµãÈÄüÁªìÊûúÁ¨¨‰∏ÄË°åÂ∞±ÊòØ**Êó¢‰∏ãËΩΩÈÄüÂ∫¶ÊúÄÂø´„ÄÅÂèàÂπ≥ÂùáÂª∂ËøüÊúÄ‰ΩéÁöÑÊúÄÂø´ IP**ÔºÅ

ÂÆåÊï¥ÁªìÊûú‰øùÂ≠òÂú®ÂΩìÂâçÁõÆÂΩï‰∏ãÁöÑ `result.csv` Êñá‰ª∂‰∏≠ÔºåÁî®**ËÆ∞‰∫ãÊú¨/Ë°®Ê†ºËΩØ‰ª∂**ÊâìÂºÄÔºåÊ†ºÂºèÂ¶Ç‰∏ãÔºö

```
IP Âú∞ÂùÄ,Â∑≤ÂèëÈÄÅ,Â∑≤Êé•Êî∂,‰∏¢ÂåÖÁéá,Âπ≥ÂùáÂª∂Ëøü,‰∏ãËΩΩÈÄüÂ∫¶(MB/s),Âú∞Âå∫Á†Å
104.27.200.69,4,4,0.00,146.23,28.64,LAX
```

&gt; [!NOTE]
&gt; _Â¶ÇÊûú‰Ω†ÂèëÁé∞**‰∏ãËΩΩÈÄüÂ∫¶‰∏∫ 0.00**ÔºåÈÇ£‰πàÂèØ‰ª•Áî®**Ë∞ÉËØïÊ®°Âºè `-debug`** ÊéíÊü•‰∏Ä‰∏ãÔºåËØ¶ËßÅÔºö[**# ‰∏ãËΩΩÊµãÈÄüÈÉΩÊòØ 0.00 Ôºü**](https://github.com/XIU2/CloudflareSpeedTest#-%E4%B8%8B%E8%BD%BD%E6%B5%8B%E9%80%9F%E9%83%BD%E6%98%AF-000-)_

&gt; _Â§ßÂÆ∂ÂèØ‰ª•ÊåâËá™Â∑±ÈúÄÊ±ÇÔºåÂØπÂÆåÊï¥ÁªìÊûú**Ëøõ‰∏ÄÊ≠•Á≠õÈÄâÂ§ÑÁêÜ**ÔºåÊàñËÄÖÂéªÁúã‰∏ÄÁúãËøõÈò∂‰ΩøÁî®**ÊåáÂÆöËøáÊª§Êù°‰ª∂**ÔºÅ_

****
## \# ËøõÈò∂‰ΩøÁî®

Áõ¥Êé•ËøêË°å‰ΩøÁî®ÁöÑÊòØÈªòËÆ§ÂèÇÊï∞ÔºåÂ¶ÇÊûúÊÉ≥Ë¶ÅÊµãÈÄüÁªìÊûúÊõ¥ÂÖ®Èù¢„ÄÅÊõ¥Á¨¶ÂêàËá™Â∑±ÁöÑË¶ÅÊ±ÇÔºåÂèØ‰ª•Ëá™ÂÆö‰πâÂèÇÊï∞„ÄÇ

```Dart
C:\&gt;cfst.exe -h

CloudflareSpeedTest vX.X.X
ÊµãËØïÂêÑ‰∏™ CDN ÊàñÁΩëÁ´ôÊâÄÊúâ IP ÁöÑÂª∂ËøüÂíåÈÄüÂ∫¶ÔºåËé∑ÂèñÊúÄÂø´ IP (IPv4+IPv6)ÔºÅ
https://github.com/XIU2/CloudflareSpeedTest

ÂèÇÊï∞Ôºö
    -n 200
        Âª∂ËøüÊµãÈÄüÁ∫øÁ®ãÔºõË∂äÂ§öÂª∂ËøüÊµãÈÄüË∂äÂø´ÔºåÊÄßËÉΩÂº±ÁöÑËÆæÂ§á (Â¶ÇË∑ØÁî±Âô®) ËØ∑ÂãøÂ§™È´òÔºõ(ÈªòËÆ§ 200 ÊúÄÂ§ö 1000)
    -t 4
        Âª∂ËøüÊµãÈÄüÊ¨°Êï∞ÔºõÂçï‰∏™ IP Âª∂ËøüÊµãÈÄüÁöÑÊ¨°Êï∞Ôºõ(ÈªòËÆ§ 4 Ê¨°)
    -dn 10
        ‰∏ãËΩΩÊµãÈÄüÊï∞ÈáèÔºõÂª∂ËøüÊµãÈÄüÂπ∂ÊéíÂ∫èÂêéÔºå‰ªéÊúÄ‰ΩéÂª∂ËøüËµ∑‰∏ãËΩΩÊµãÈÄüÁöÑÊï∞ÈáèÔºõ(ÈªòËÆ§ 10 ‰∏™)
    -dt 10
        ‰∏ãËΩΩÊµãÈÄüÊó∂Èó¥ÔºõÂçï‰∏™ IP ‰∏ãËΩΩÊµãÈÄüÊúÄÈïøÊó∂Èó¥Ôºå‰∏çËÉΩÂ§™Áü≠Ôºõ(ÈªòËÆ§ 10 Áßí)
    -tp 443
        ÊåáÂÆöÊµãÈÄüÁ´ØÂè£ÔºõÂª∂ËøüÊµãÈÄü/‰∏ãËΩΩÊµãÈÄüÊó∂‰ΩøÁî®ÁöÑÁ´ØÂè£Ôºõ(ÈªòËÆ§ 443 Á´ØÂè£)
    -url https://cf.xiu2.xyz/url
        ÊåáÂÆöÊµãÈÄüÂú∞ÂùÄÔºõÂª∂ËøüÊµãÈÄü(HTTPing)/‰∏ãËΩΩÊµãÈÄüÊó∂‰ΩøÁî®ÁöÑÂú∞ÂùÄÔºåÈªòËÆ§Âú∞ÂùÄ‰∏ç‰øùËØÅÂèØÁî®ÊÄßÔºåÂª∫ËÆÆËá™Âª∫Ôºõ
        ÂΩì‰∏ãËΩΩÊµãÈÄüÊó∂ÔºåËΩØ‰ª∂‰ºö‰ªé HTTP ÂìçÂ∫îÂ§¥‰∏≠Ëé∑ÂèñËØ• IP ÂΩìÂâçÂú∞Âå∫Á†ÅÔºàÊîØÊåÅ Cloudflare„ÄÅAWS CloudFront„ÄÅFastly„ÄÅGcore„ÄÅCDN77„ÄÅBunny Á≠â CDNÔºâÂπ∂ÊòæÁ§∫Âá∫Êù•„ÄÇ

    -httping
        ÂàáÊç¢ÊµãÈÄüÊ®°ÂºèÔºõÂª∂ËøüÊµãÈÄüÊ®°ÂºèÊîπ‰∏∫ HTTP ÂçèËÆÆÔºåÊâÄÁî®ÊµãËØïÂú∞ÂùÄ‰∏∫ [-url] ÂèÇÊï∞Ôºõ(ÈªòËÆ§ TCPing)
        ÂΩì‰ΩøÁî® HTTP ÊµãÈÄüÊ®°ÂºèÊó∂ÔºåËΩØ‰ª∂‰ºö‰ªé HTTP ÂìçÂ∫îÂ§¥‰∏≠Ëé∑ÂèñËØ• IP ÂΩìÂâçÂú∞Âå∫Á†ÅÔºàÊîØÊåÅ Cloudflare„ÄÅAWS CloudFront„ÄÅFastly„ÄÅGcore„ÄÅCDN77„ÄÅBunny Á≠â CDNÔºâÂπ∂ÊòæÁ§∫Âá∫Êù•„ÄÇ
        Ê≥®ÊÑèÔºöHTTPing Êú¨Ë¥®‰∏ä‰πüÁÆó‰∏ÄÁßç ÁΩëÁªúÊâ´Êèè Ë°å‰∏∫ÔºåÂõ†Ê≠§Â¶ÇÊûú‰Ω†Âú®ÊúçÂä°Âô®‰∏äÈù¢ËøêË°åÔºåÈúÄË¶ÅÈôç‰ΩéÂπ∂Âèë(-n)ÔºåÂê¶ÂàôÂèØËÉΩ‰ºöË¢´‰∏Ä‰∫õ‰∏•Ê†ºÁöÑÂïÜÂÆ∂ÊöÇÂÅúÊúçÂä°„ÄÇ
        Â¶ÇÊûú‰Ω†ÈÅáÂà∞ HTTPing È¶ñÊ¨°ÊµãÈÄüÂèØÁî® IP Êï∞ÈáèÊ≠£Â∏∏ÔºåÂêéÁª≠ÊµãÈÄüË∂äÊù•Ë∂äÂ∞ëÁîöËá≥Áõ¥Êé•‰∏∫ 0Ôºå‰ΩÜÂÅú‰∏ÄÊÆµÊó∂Èó¥ÂêéÂèàÊÅ¢Â§ç‰∫ÜÁöÑÊÉÖÂÜµÔºåÈÇ£‰πà‰πüÂèØËÉΩÊòØË¢´ ËøêËê•ÂïÜ„ÄÅCloudflare CDN ËÆ§‰∏∫‰Ω†Âú®ÁΩëÁªúÊâ´ÊèèËÄå Ëß¶Âèë‰∏¥Êó∂ÈôêÂà∂Êú∫Âà∂ÔºåÂõ†Ê≠§Êâç‰ºöËøá‰∏Ä‰ºöÂÑøÂ∞±ÊÅ¢Â§ç‰∫ÜÔºåÂª∫ËÆÆÈôç‰ΩéÂπ∂Âèë(-n)ÂáèÂ∞ëËøôÁßçÊÉÖÂÜµÁöÑÂèëÁîü„ÄÇ
    -httping-code 200
        ÊúâÊïàÁä∂ÊÄÅ‰ª£Á†ÅÔºõHTTPing Âª∂ËøüÊµãÈÄüÊó∂ÁΩëÈ°µËøîÂõûÁöÑÊúâÊïà HTTP Áä∂ÊÄÅÁ†ÅÔºå‰ªÖÈôê‰∏Ä‰∏™Ôºõ(ÈªòËÆ§ 200 301 302)
    -cfcolo HKG,KHH,NRT,LAX,SEA,SJC,FRA,MAD
        ÂåπÈÖçÊåáÂÆöÂú∞Âå∫ÔºõIATA Êú∫Âú∫Âú∞Âå∫Á†ÅÊàñÂõΩÂÆ∂/ÂüéÂ∏ÇÁ†ÅÔºåËã±ÊñáÈÄóÂè∑ÂàÜÈöîÔºåÂ§ßÂ∞èÂÜôÂùáÂèØÔºå‰ªÖ HTTPing Ê®°ÂºèÂèØÁî®Ôºõ(ÈªòËÆ§ ÊâÄÊúâÂú∞Âå∫)
        ÊîØÊåÅ Cloudflare„ÄÅAWS CloudFront„ÄÅFastly„ÄÅGcore„ÄÅCDN77„ÄÅBunny Á≠â CDN
        ÂÖ∂‰∏≠ Cloudflare„ÄÅAWS CloudFront„ÄÅFastly ‰ΩøÁî®ÁöÑÊòØ IATA ‰∏âÂ≠óÊú∫Âú∫Âú∞Âå∫Á†ÅÔºåÂ¶ÇÔºöHKG,LAX
        ÂÖ∂‰∏≠ CDN77„ÄÅBunny ‰ΩøÁî®ÁöÑÊòØ ‰∫åÂ≠óÂõΩÂÆ∂/Âå∫ÂüüÁ†ÅÔºåÂ¶ÇÔºöUS,CN
        ÂÖ∂‰∏≠ Gcore ‰ΩøÁî®ÁöÑÊòØ ‰∫åÂ≠óÂüéÂ∏ÇÁ†ÅÔºåÂ¶ÇÔºöFR,AM
        Âõ†Ê≠§Â§ßÂÆ∂‰ΩøÁî® -cfcolo ÊåáÂÆöÂú∞Âå∫Á†ÅÊó∂Ë¶ÅÊ†πÊçÆ‰∏çÂêåÁöÑ CDN Êù•ÊåáÂÆö‰∏çÂêåÁ±ªÂûãÁöÑÂú∞Âå∫Á†Å„ÄÇ

    -tl 200
        Âπ≥ÂùáÂª∂Ëøü‰∏äÈôêÔºõÂè™ËæìÂá∫‰Ωé‰∫éÊåáÂÆöÂπ≥ÂùáÂª∂ËøüÁöÑ IPÔºåÂêÑ‰∏ä‰∏ãÈôêÊù°‰ª∂ÂèØÊê≠ÈÖç‰ΩøÁî®Ôºõ(ÈªòËÆ§ 9999 ms)
    -tll 40
        Âπ≥ÂùáÂª∂Ëøü‰∏ãÈôêÔºõÂè™ËæìÂá∫È´ò‰∫éÊåáÂÆöÂπ≥ÂùáÂª∂ËøüÁöÑ IPÔºõ(ÈªòËÆ§ 0 ms)
    -tlr 0.2
        ‰∏¢ÂåÖÂá†Áéá‰∏äÈôêÔºõÂè™ËæìÂá∫‰Ωé‰∫é/Á≠â‰∫éÊåáÂÆö‰∏¢ÂåÖÁéáÁöÑ IPÔºåËåÉÂõ¥ 0.00~1.00Ôºå0 ËøáÊª§Êéâ‰ªª‰Ωï‰∏¢ÂåÖÁöÑ IPÔºõ(ÈªòËÆ§ 1.00)
    -sl 5
        ‰∏ãËΩΩÈÄüÂ∫¶‰∏ãÈôêÔºõÂè™ËæìÂá∫È´ò‰∫éÊåáÂÆö‰∏ãËΩΩÈÄüÂ∫¶ÁöÑ IPÔºåÂáëÂ§üÊåáÂÆöÊï∞Èáè [-dn] Êâç‰ºöÂÅúÊ≠¢ÊµãÈÄüÔºõ(ÈªòËÆ§ 0.00 MB/s)

    -p 10
        ÊòæÁ§∫ÁªìÊûúÊï∞ÈáèÔºõÊµãÈÄüÂêéÁõ¥Êé•ÊòæÁ§∫ÊåáÂÆöÊï∞ÈáèÁöÑÁªìÊûúÔºå‰∏∫ 0 Êó∂‰∏çÊòæÁ§∫ÁªìÊûúÁõ¥Êé•ÈÄÄÂá∫Ôºõ(ÈªòËÆ§ 10 ‰∏™)
    -f ip.txt
        IPÊÆµÊï∞ÊçÆÊñá‰ª∂ÔºõÂ¶ÇË∑ØÂæÑÂê´ÊúâÁ©∫Ê†ºËØ∑Âä†‰∏äÂºïÂè∑ÔºõÊîØÊåÅÂÖ∂‰ªñ CDN IPÊÆµÔºõ(ÈªòËÆ§ ip.txt)
    -ip 1.1.1.1,2.2.2.2/24,2606:4700::/32
        ÊåáÂÆöIPÊÆµÊï∞ÊçÆÔºõÁõ¥Êé•ÈÄöËøáÂèÇÊï∞ÊåáÂÆöË¶ÅÊµãÈÄüÁöÑ IP ÊÆµÊï∞ÊçÆÔºåËã±ÊñáÈÄóÂè∑ÂàÜÈöîÔºõ(ÈªòËÆ§ Á©∫)
    -o result.csv
        ÂÜôÂÖ•ÁªìÊûúÊñá‰ª∂ÔºõÂ¶ÇË∑ØÂæÑÂê´ÊúâÁ©∫Ê†ºËØ∑Âä†‰∏äÂºïÂè∑ÔºõÂÄº‰∏∫Á©∫Êó∂‰∏çÂÜôÂÖ•Êñá‰ª∂ [-o &quot;&quot;]Ôºõ(ÈªòËÆ§ result.csv)
        Ê≥®ÊÑèÔºöÂú®‰∏Ä‰∫õÁéØÂ¢É‰∏ã‰ΩøÁî® -o &quot;&quot; ÂèØËÉΩ‰ºöË¢´ÂøΩÁï•ÊéâËøô‰∏™Á©∫ÂèÇÊï∞ÂØºËá¥Êä•ÈîôÔºåÂèØÂä†‰∏™Á©∫Ê†º -o &quot; &quot; Ëß£ÂÜ≥

    -dd
        Á¶ÅÁî®‰∏ãËΩΩÊµãÈÄüÔºõÁ¶ÅÁî®ÂêéÊµãÈÄüÁªìÊûú‰ºöÊåâÂª∂ËøüÊéíÂ∫è (ÈªòËÆ§Êåâ‰∏ãËΩΩÈÄüÂ∫¶ÊéíÂ∫è)Ôºõ(ÈªòËÆ§ ÂêØÁî®)
    -allip
        ÊµãÈÄüÂÖ®ÈÉ®ÁöÑIPÔºõÂØπ IP ÊÆµ‰∏≠ÁöÑÊØè‰∏™ IP (‰ªÖÊîØÊåÅ IPv4) ËøõË°åÊµãÈÄüÔºõ(ÈªòËÆ§ ÊØè‰∏™ /24 ÊÆµÈöèÊú∫ÊµãÈÄü‰∏Ä‰∏™ IP)

    -debug
        Ë∞ÉËØïËæìÂá∫Ê®°ÂºèÔºõ‰ºöÂú®‰∏Ä‰∫õÈùûÈ¢ÑÊúüÊÉÖÂÜµ‰∏ãËæìÂá∫Êõ¥Â§öÊó•Âøó‰ª•‰æøÂà§Êñ≠ÂéüÂõ†Ôºõ(ÈªòËÆ§ ÂÖ≥Èó≠)
        ÁõÆÂâçËØ•ÂäüËÉΩ‰ªÖÈíàÂØπ HTTPing Âª∂ËøüÊµãÈÄüËøáÁ®ã Âèä ‰∏ãËΩΩÊµãÈÄüËøáÁ®ãÔºåÂΩìËøáÁ®ã‰∏≠Âõ†‰∏∫ÂêÑÁßçÂéüÂõ†ÂØºËá¥ÂΩìÂâç IP ÊµãÈÄü‰∏≠Êñ≠ÈÉΩ‰ºöËæìÂá∫ÈîôËØØÂéüÂõ†
        ‰æãÂ¶ÇÔºöHTTPing Âª∂ËøüÊµãÈÄüËøáÁ®ã‰∏≠ÔºåÂõ†‰∏∫ HTTP Áä∂ÊÄÅÁ†Å‰∏çÁ¨¶ÂêàÊàñÊµãÈÄüÂú∞ÂùÄÊúâÈóÆÈ¢òÊàñË∂ÖÊó∂Á≠âÂéüÂõ†ËÄåÁªàÊ≠¢ÊµãÈÄü
        ‰æãÂ¶ÇÔºö‰∏ãËΩΩÊµãÈÄüËøáÁ®ã‰∏≠ÔºåÂõ†‰∏∫‰∏ãËΩΩÊµãÈÄüÂú∞ÂùÄÊúâÈóÆÈ¢òÔºàË¢´ÈòªÊñ≠„ÄÅ403Áä∂ÊÄÅÁ†Å„ÄÅË∂ÖÊó∂ÔºâÁ≠âÂéüÂõ†ËÄåÁªàÊ≠¢ÊµãÈÄüÔºàÂØºËá¥ÊòæÁ§∫ 0.00Ôºâ

    -v
        ÊâìÂç∞Á®ãÂ∫èÁâàÊú¨ + Ê£ÄÊü•ÁâàÊú¨Êõ¥Êñ∞
    -h
        ÊâìÂç∞Â∏ÆÂä©ËØ¥Êòé
```

### ÁïåÈù¢Ëß£Èáä

‰∏∫‰∫ÜÈÅøÂÖçÂ§ßÂÆ∂ÂØπÊµãÈÄüËøáÁ®ã‰∏≠ÁöÑ**ËæìÂá∫ÂÜÖÂÆπ‰∫ßÁîüËØØËß£ÔºàÂèØÁî®„ÄÅÈòüÂàóÁ≠âÊï∞Â≠óÔºå‰∏ãËΩΩÊµãÈÄü‰∏ÄÂçäÂ∞±&quot;‰∏≠Êñ≠&quot;Ôºü‰∏ãËΩΩÊµãÈÄü&quot;Âç°‰Ωè&quot;‰∏çÂä®ÔºüÔºâ**ÔºåÊàëÁâπÊÑèËß£Èáä‰∏ã„ÄÇ

&lt;details&gt;
&lt;summary&gt;&lt;code&gt;&lt;strong&gt;„Äå ÁÇπÂáªÂ±ïÂºÄ Êü•ÁúãÂÜÖÂÆπ „Äç&lt;/strong&gt;&lt;/code&gt;&lt;/summary&gt;

****

&gt; ËØ•Á§∫‰æãÊääÂ∏∏Áî®ÂèÇÊï∞ÈÉΩÁªôÂä†‰∏ä‰∫ÜÔºåÂç≥‰∏∫Ôºö`-tll 40 -tl 150 -sl 1 -dn 5`ÔºåÊúÄÂêéËæìÂá∫ÁªìÊûúÂ¶Ç‰∏ãÔºö

```python
# XIU2/CloudflareSpeedTest vX.X.X

ÂºÄÂßãÂª∂ËøüÊµãÈÄüÔºàÊ®°ÂºèÔºöTCP, Á´ØÂè£Ôºö443, ËåÉÂõ¥Ôºö40 ~ 150 ms, ‰∏¢ÂåÖÔºö1.00)
321 / 321 [-----------------------------------------------------------] ÂèØÁî®: 30
ÂºÄÂßã‰∏ãËΩΩÊµãÈÄüÔºà‰∏ãÈôêÔºö1.00 MB/s, Êï∞ÈáèÔºö5, ÈòüÂàóÔºö10Ôºâ
3 / 5 [-----------------------------------------‚Üó--------------------]
IP Âú∞ÂùÄ           Â∑≤ÂèëÈÄÅ  Â∑≤Êé•Êî∂  ‰∏¢ÂåÖÁéá  Âπ≥ÂùáÂª∂Ëøü  ‰∏ãËΩΩÈÄüÂ∫¶(MB/s)  Âú∞Âå∫Á†Å
XXX.XXX.XXX.XXX   4      4       0.00   83.32     3.66           LAX
XXX.XXX.XXX.XXX   4      4       0.00   107.81    2.49           LAX
XXX.XXX.XXX.XXX   4      3       0.25   149.59    1.04           N/A

ÂÆåÊï¥ÊµãÈÄüÁªìÊûúÂ∑≤ÂÜôÂÖ• result.csv Êñá‰ª∂ÔºåÂèØ‰ΩøÁî®ËÆ∞‰∫ãÊú¨/Ë°®Ê†ºËΩØ‰ª∂Êü•Áúã„ÄÇ
Êåâ‰∏ã ÂõûËΩ¶ÈîÆ Êàñ Ctrl+C ÈÄÄÂá∫„ÄÇ
```

****

&gt; ÂàöÊé•Ëß¶ CFST ÁöÑ‰∫∫ÔºåÂèØËÉΩ‰ºöËø∑ÊÉë**ÊòéÊòéÂª∂ËøüÊµãÈÄüÂèØÁî® IP Êúâ 30 ‰∏™ÔºåÊÄé‰πàÊúÄÂêéÂè™Ââ©‰∏ã 3 ‰∏™‰∫ÜÂë¢Ôºü**  
&gt; ‰∏ãËΩΩÊµãÈÄüÈáåÁöÑÈòüÂàóÂèàÊòØ‰ªÄ‰πàÊÑèÊÄùÔºüÈöæÈÅìÊàë‰∏ãËΩΩÊµãÈÄüËøòË¶ÅÊéíÈòüÔºü

CFST ‰ºöÂÖàÂª∂ËøüÊµãÈÄüÔºåÂú®ËøôËøáÁ®ã‰∏≠ËøõÂ∫¶Êù°Âè≥‰æß‰ºöÂÆûÊó∂ÊòæÁ§∫ÂèØÁî® IP Êï∞ÈáèÔºà`ÂèØÁî®: 30`ÔºâÔºå‰ΩÜÊ≥®ÊÑèËØ•ÂèØÁî®Êï∞ÈáèÊåáÁöÑÊòØ**ÊµãËØïÈÄöËøáÊ≤°ÊúâË∂ÖÊó∂ÁöÑ IP Êï∞Èáè**ÔºåÂíåÂª∂Ëøü‰∏ä‰∏ãÈôê„ÄÅ‰∏¢ÂåÖÊù°‰ª∂Êó†ÂÖ≥„ÄÇÂΩìÂª∂ËøüÊµãÈÄüÂÆåÊàêÂêéÔºåÂõ†‰∏∫ËøòÊåáÂÆö‰∫Ü**Âª∂Ëøü‰∏ä‰∏ãÈôê„ÄÅ‰∏¢ÂåÖ**ÁöÑÊù°‰ª∂ÔºåÊâÄ‰ª•ÊåâÁÖßÊù°‰ª∂ËøáÊª§ÂêéÂè™Ââ©‰∏ã `10` ‰∏™‰∫ÜÔºà‰πüÂ∞±ÊòØÁ≠âÂæÖ‰∏ãËΩΩÊµãÈÄüÁöÑ `ÈòüÂàóÔºö10`Ôºâ„ÄÇ

Âç≥‰ª•‰∏äÁ§∫‰æã‰∏≠Ôºå`321` ‰∏™ IP Âª∂ËøüÊµãÈÄüÂÆåÊàêÂêéÔºåÂè™Êúâ `30` ‰∏™ IP ÊµãËØïÈÄöËøáÊ≤°ÊúâË∂ÖÊó∂ÔºåÁÑ∂ÂêéÊ†πÊçÆÂª∂Ëøü‰∏ä‰∏ãÈôêËåÉÂõ¥Ôºö`40 ~ 150 ms` Âèä‰∏¢ÂåÖ‰∏äÈôêÊù°‰ª∂ËøáÊª§ÂêéÔºåÂè™Ââ©‰∏ã `10` ‰∏™Êª°Ë∂≥Ë¶ÅÊ±ÇÁöÑ IP ‰∫Ü„ÄÇÂ¶ÇÊûú‰Ω† `-dd` Á¶ÅÁî®‰∫Ü‰∏ãËΩΩÊµãÈÄüÔºåÈÇ£‰πàÂ∞±‰ºöÁõ¥Êé•ËæìÂá∫Ëøô `10` ‰∏™ IP ‰∫Ü„ÄÇÂΩìÁÑ∂ËØ•Á§∫‰æãÂπ∂Êú™Á¶ÅÁî®ÔºåÂõ†Ê≠§Êé•‰∏ãÊù•ËΩØ‰ª∂‰ºöÁªßÁª≠ÂØπËøô `10` ‰∏™ IP ËøõË°å‰∏ãËΩΩÊµãÈÄüÔºà`ÈòüÂàóÔºö10`Ôºâ„ÄÇ

&gt; Âõ†‰∏∫‰∏ãËΩΩÊµãÈÄüÊòØÂçïÁ∫øÁ®ã‰∏Ä‰∏™‰∏™ IP Êå®ÁùÄÊéíÈòüÊµãÈÄüÁöÑÔºåÂõ†Ê≠§Á≠âÂæÖ‰∏ãËΩΩÊµãÈÄüÁöÑ IP Êï∞ÈáèÊâç‰ºöÂè´ÂÅö `ÈòüÂàó`„ÄÇ

****

&gt; ‰Ω†ÂèØËÉΩÊ≥®ÊÑèÂà∞‰∫ÜÔºå**ÊòéÊòéÊåáÂÆö‰∫ÜË¶ÅÊâæÂà∞ 5 ‰∏™Êª°Ë∂≥‰∏ãËΩΩÈÄüÂ∫¶Êù°‰ª∂ÁöÑ IPÔºåÊÄé‰πàÊâç 3 ‰∏™Â∞± ‚Äú‰∏≠Êñ≠‚Äù ‰∫ÜÂë¢Ôºü**

‰∏ãËΩΩÊµãÈÄüËøõÂ∫¶Êù°‰∏≠ÁöÑ `3 / 5`ÔºåÂâçËÄÖÊåáÁöÑÊòØÊâæÂà∞‰∫Ü `3` ‰∏™Êª°Ë∂≥‰∏ãËΩΩÈÄüÂ∫¶‰∏ãÈôêÊù°‰ª∂ÁöÑ IPÔºàÂç≥‰∏ãËΩΩÈÄüÂ∫¶È´ò‰∫é `1 MB/s` ÔºâÔºåÂêéËÄÖ `5` ÊåáÁöÑÊòØ‰Ω†Ë¶ÅÊ±ÇÊâæÂà∞ `5` ‰∏™Êª°Ë∂≥‰∏ãËΩΩÈÄüÂ∫¶‰∏ãÈôêÊù°‰ª∂ÁöÑ IPÔºà`-dn 5`Ôºâ„ÄÇ

&gt; Âè¶Â§ñÔºåÊèêÈÜí‰∏Ä‰∏ãÔºåÂ¶ÇÊûú‰Ω†ÊåáÂÆöÁöÑ `-dn` Â§ß‰∫é‰∏ãËΩΩÊµãÈÄüÈòüÂàóÔºåÊØîÂ¶Ç‰Ω†Âª∂ËøüÊµãÈÄüÂêéÂè™Ââ©‰∏ã `4` ‰∏™ IP ‰∫ÜÔºåÈÇ£‰πà‰∏ãËΩΩÊµãÈÄüËøõÂ∫¶Êù°‰∏≠ÂêéÈù¢ÁöÑÊï∞Â≠óÂ∞±‰ºöÂíå‰∏ãËΩΩÊµãÈÄüÈòüÂàó‰∏ÄÊ†∑ÈÉΩÊòØ `4` ‰∏™ÔºåËÄåÈùû‰Ω† `-dn` ÊåáÂÆöÁöÑ `5` ‰∏™‰∫Ü„ÄÇ

ËΩØ‰ª∂Âú®ÊµãÈÄüÂÆåËøô `10` ‰∏™ IP ÂêéÔºåÂè™ÊâæÂà∞‰∫Ü `3` ‰∏™‰∏ãËΩΩÈÄüÂ∫¶È´ò‰∫é `1 MB/s` ÁöÑ IPÔºåÂâ©‰∏ãÁöÑ `7` ‰∏™ IP ÈÉΩÊòØ ‚Äú‰∏çÂèäÊ†º‚Äù ÁöÑ„ÄÇ

Âõ†Ê≠§ÔºåËøô‰∏çÊòØ `‚ÄúÊØèÊ¨°ÊµãÈÄüÈÉΩ‰∏çÂà∞ 5 Â∞±‰∏≠Êñ≠‰∫Ü‚Äù`ÔºåËÄåÊòØÊâÄÊúâ IP ÈÉΩ‰∏ãËΩΩÊµãÈÄüÂÆå‰∫ÜÔºå‰ΩÜÂç¥Âè™ÊâæÂà∞‰∫Ü `3` ‰∏™Êª°Ë∂≥Êù°‰ª∂ÁöÑ„ÄÇ

****

ËøòÊúâ‰∏ÄÁßçÊÉÖÂÜµÔºåÈÇ£Â∞±ÊòØÂΩìÂèØÁî® IP ÂæàÂ§öÊó∂ÔºàÂá†ÁôæÂá†ÂçÉÔºâÔºå‰Ω†ËøòËÆæÁΩÆ‰∫Ü‰∏ãËΩΩÈÄüÂ∫¶Êù°‰ª∂ÔºåÈÇ£‰πàÂèØËÉΩÂ∞±‰ºöÈÅáÂà∞Ôºö**ÊÄé‰πà‰∏ãËΩΩÊµãÈÄüËøõÂ∫¶Êù°ËÄÅÊòØÂç°Âú® `X / 5` ‰∫ÜÂë¢Ôºü**

ËøôÂÖ∂ÂÆûÂπ∂‰∏çÊòØÂç°‰Ωè‰∫ÜÔºåËÄåÊòØÂè™ÊúâÂΩìÊâæÂà∞‰∏Ä‰∏™Êª°Ë∂≥Êù°‰ª∂ÁöÑ IP Êó∂ÔºåËøõÂ∫¶Êù°Êâç‰ºö +1ÔºåÂõ†Ê≠§Â¶ÇÊûú‰∏ÄÁõ¥Êâæ‰∏çÂà∞ÔºåÈÇ£‰πà CFST Â∞±‰ºö‰∏ÄÁõ¥‰∏ãËΩΩÊµãÈÄü‰∏ãÂéªÔºåÂõ†Ê≠§Âú®Ë°®Áé∞‰∏∫ËøõÂ∫¶Êù°Âç°‰Ωè‰∏çÂä®Ôºå‰ΩÜËøô‰πüÊòØÂú®ÊèêÈÜí‰Ω†Ôºö‰Ω†ËÆæÁΩÆÁöÑ‰∏ãËΩΩÈÄüÂ∫¶Êù°‰ª∂ÂØπ‰Ω†Êù•ËØ¥Â∑≤ÁªèÈ´ò‰∫éÂÆûÈôÖ‰∫ÜÔºå‰Ω†ÈúÄË¶ÅÈÄÇÂΩìË∞É‰ΩéÈ¢ÑÊúü„ÄÇ

****

Â¶ÇÊûú‰∏çÊÉ≥ÈÅáÂà∞ËøôÁßçÂÖ®ÈÉ®ÊµãÈÄü‰∏ÄÈÅçÈÉΩÊ≤°Âá†‰∏™Êª°Ë∂≥Êù°‰ª∂ÁöÑÊÉÖÂÜµÔºåÈÇ£‰πàÂ∞±Ë¶Å**Ë∞É‰Ωé‰∏ãËΩΩÈÄüÂ∫¶‰∏äÈôêÂèÇÊï∞ `-sl`**ÔºåÊàñËÄÖÁßªÈô§„ÄÇ

Âõ†‰∏∫Âè™Ë¶ÅÊåáÂÆö‰∫Ü `-sl` ÂèÇÊï∞ÔºåÈÇ£‰πàÂè™Ë¶ÅÊ≤°ÊúâÂáëÂ§ü `-dn` ÁöÑÊï∞ÈáèÔºàÈªòËÆ§ 10 ‰∏™ÔºâÔºåÂ∞±‰ºö‰∏ÄÁõ¥ÊµãÈÄü‰∏ãÂéªÔºåÁõ¥Âà∞ÂáëÂ§üÊàñÂÖ®ÈÉ®ÊµãÈÄüÂÆå„ÄÇÁßªÈô§ `-sl` Âπ∂Ê∑ªÂä† `-dn 20` ÂèÇÊï∞ÔºåËøôÊ†∑Â∞±ÊòØÂè™ÊµãÈÄüÂª∂ËøüÊúÄ‰ΩéÁöÑÂâç 20 ‰∏™ IPÔºåÊµãÈÄüÂÆåÂ∞±ÂÅúÊ≠¢ÔºåËäÇÁúÅÊó∂Èó¥„ÄÇ

****

Âè¶Â§ñÔºåÂ¶ÇÊûúÂÖ®ÈÉ®ÈòüÂàó IP ÈÉΩÊµãÈÄüÂÆå‰∫ÜÔºå‰ΩÜ‰∏Ä‰∏™Êª°Ë∂≥‰∏ãËΩΩÈÄüÂ∫¶Êù°‰ª∂ÁöÑ IP ÈÉΩÊ≤°ÊúâÔºå‰Ω†ÂèØËÉΩÈúÄË¶ÅË∞É‰ΩéÈ¢ÑÊúüÁöÑ‰∏ãËΩΩÊµãÈÄü‰∏ãÈôêÊù°‰ª∂Ôºå‰ΩÜ‰Ω†ÈúÄË¶ÅÁü•ÈÅìÂΩìÂâçÁöÑÂ§ßÊ¶ÇÊµãÈÄüÈÄüÂ∫¶ÈÉΩÂú®‰ªÄ‰πàËåÉÂõ¥ÔºåÈÇ£‰πà‰Ω†Â∞±ÂèØ‰ª•Âä†‰∏ä `-debug` ÂèÇÊï∞ÂºÄÂêØË∞ÉËØïÊ®°ÂºèÔºåËøôÊ†∑ÂÜçÈÅáÂà∞ËøôÁßçÊÉÖÂÜµÊó∂ÔºåÂ∞±‰ºö**ÂøΩÁï•Êù°‰ª∂ËøîÂõûÊâÄÊúâÊµãÈÄüÁªìÊûú**Ôºå‰Ω†Â∞±ËÉΩÁúãÂà∞Ëøô‰∫õ IP ÁöÑ‰∏ãËΩΩÈÄüÂ∫¶ÈÉΩÊúâÂ§öÂ∞ëÔºåÂøÉÈáå‰πüÂ∞±ÊúâÊï∞‰∫ÜÔºåÁÑ∂Âêé**ÈÄÇÂΩìË∞É‰Ωé `-sl` ÂÜçËØïËØï**„ÄÇ

&gt; Ê≥®ÊÑèÔºåÂ¶ÇÊûú‰Ω†**Ê≤°ÊúâÊåáÂÆö**‰∏ãËΩΩÊµãÈÄü‰∏ãÈôê `-sl` Êù°‰ª∂ÔºåÈÇ£‰πàÊó†ËÆ∫‰ªÄ‰πàÊÉÖÂÜµ‰∏ã CFST ÈÉΩ‰ºö**ËæìÂá∫ÊâÄÊúâÊµãÈÄüÁªìÊûú**„ÄÇ

ÂêåÊ†∑ÔºåÂª∂ËøüÊµãÈÄüÊñπÈù¢Ôºå`ÂèØÁî®: 30`„ÄÅ`ÈòüÂàóÔºö10` Ëøô‰∏§‰∏™Êï∞ÂÄº‰πüÂèØ‰ª•ËÆ©‰Ω†Ê∏ÖÊ•öÔºå‰Ω†ËÆæÁΩÆÁöÑÂª∂ËøüÊù°‰ª∂ÂØπ‰Ω†Êù•ËØ¥ÊòØÂê¶Ëøá‰∫éËãõÂàª„ÄÇÂ¶ÇÊûúÂèØÁî® IP ‰∏ÄÂ§ßÂ†ÜÔºå‰ΩÜÊù°‰ª∂ËøáÊª§ÂêéÂè™Ââ©‰∏ã 2„ÄÅ3 ‰∏™ÔºåÈÇ£‰∏çÁî®ËØ¥Â∞±Áü•ÈÅìÈúÄË¶Å**Ë∞É‰ΩéÈ¢ÑÊúüÁöÑÂª∂Ëøü/‰∏¢ÂåÖÊù°‰ª∂**‰∫Ü„ÄÇ

Ëøô‰∏§‰∏™Êú∫Âà∂Ôºå‰∏Ä‰∏™ÊòØÂëäËØâ‰Ω†**Âª∂Ëøü‰∏¢ÂåÖÊù°‰ª∂**ÊòØÂê¶ÂêàÈÄÇÁöÑÔºå‰∏Ä‰∏™ÊòØÂëäËØâ‰Ω†**‰∏ãËΩΩÈÄüÂ∫¶Êù°‰ª∂**ÊòØÂê¶ÂêàÈÄÇÁöÑ„ÄÇ

&lt;/details&gt;

****

### ‰ΩøÁî®Á§∫‰æã

Windows Ë¶ÅÊåáÂÆöÂèÇÊï∞ÈúÄË¶ÅÂú® CMD ‰∏≠ËøêË°åÔºåÊàñËÄÖÊääÂèÇÊï∞Ê∑ªÂä†Âà∞Âø´Êç∑ÊñπÂºèÁõÆÊ†á‰∏≠„ÄÇ

&gt; [!TIP]
&gt; - ÂêÑÂèÇÊï∞ÂùáÊúâ**ÈªòËÆ§ÂÄº**ÔºåÂΩì‰ΩøÁî®ÈªòËÆ§ÂÄºÊó∂ÂèÇÊï∞ÂèØ‰ª•ÁúÅÁï•Ôºà**ÊåâÈúÄÈÄâÊã©**ÔºâÔºåÂèÇÊï∞**‰∏çÂàÜÂâçÂêéÈ°∫Â∫è**„ÄÇ  
&gt; - Windows **PowerShell** Âè™ÈúÄÊää‰∏ãÈù¢ÂëΩ‰ª§‰∏≠ÁöÑ `cfst.exe` Êîπ‰∏∫ `.\cfst.exe` Âç≥ÂèØ„ÄÇ  
&gt; - Linux / macOS Á≥ªÁªüÂè™ÈúÄË¶ÅÊää‰∏ãÈù¢ÂëΩ‰ª§‰∏≠ÁöÑ `cfst.exe` Êîπ‰∏∫ `./cfst` Âç≥ÂèØ„ÄÇ

****

#### \# CMD Â∏¶ÂèÇÊï∞ËøêË°å

ÂØπÂëΩ‰ª§Ë°åÁ®ãÂ∫è‰∏çÁÜüÊÇâÁöÑ‰∫∫ÔºåÂèØËÉΩ‰∏çÁü•ÈÅìËØ•Â¶Ç‰ΩïÂ∏¶ÂèÇÊï∞ËøêË°åÔºåÊàëÂ∞±ÁÆÄÂçïËØ¥‰∏Ä‰∏ã„ÄÇ

&lt;details&gt;
&lt;summary&gt;&lt;code&gt;&lt;strong&gt;„Äå ÁÇπÂáªÂ±ïÂºÄ Êü•ÁúãÂÜÖÂÆπ „Äç&lt;/strong&gt;&lt;/code&gt;&lt;/summary&gt;

****

ÂæàÂ§ö‰∫∫ÊâìÂºÄ CMD Áõ¥Êé•Â∞±‰ª•**ÁªùÂØπË∑ØÂæÑ**ËøêË°å CFST ‰ºöÊä•ÈîôÔºåËøôÊòØÂõ†‰∏∫ÈªòËÆ§ÁöÑ `-f ip.txt` ÂèÇÊï∞ÊòØÁõ∏ÂØπË∑ØÂæÑÔºåÈúÄË¶ÅÊåáÂÆöÁªùÂØπË∑ØÂæÑÁöÑ ip.txt ÊâçË°åÔºå‰ΩÜËøôÊ†∑ÊØïÁ´üÂ§™È∫ªÁÉ¶‰∫ÜÔºåÂõ†Ê≠§ËøòÊòØÂª∫ËÆÆËøõÂÖ• CFST Á®ãÂ∫èÁõÆÂΩï‰∏ãÔºå‰ª•**Áõ∏ÂØπË∑ØÂæÑ**ÊñπÂºèËøêË°åÔºö

**ÊñπÂºè ‰∏Ä**Ôºö
1. ÊâìÂºÄ CFST Á®ãÂ∫èÊâÄÂú®ÁõÆÂΩï  
2. Á©∫ÁôΩÂ§ÑÊåâ‰∏ã &lt;kbd&gt;Shift + Èº†Ê†áÂè≥ÈîÆ&lt;/kbd&gt; ÊòæÁ§∫Âè≥ÈîÆËèúÂçï  
3. ÈÄâÊã© **\[Âú®Ê≠§Â§ÑÊâìÂºÄÂëΩ‰ª§Á™óÂè£\]** Êù•ÊâìÂºÄ CMD Á™óÂè£ÔºåÊ≠§Êó∂ÈªòËÆ§Â∞±‰Ωç‰∫éÂΩìÂâçÁõÆÂΩï‰∏ã  
4. ËæìÂÖ•Â∏¶ÂèÇÊï∞ÁöÑÂëΩ‰ª§ÔºåÂ¶ÇÔºö`cfst.exe -tl 200 -dn 20` Âç≥ÂèØËøêË°å

**ÊñπÂºè ‰∫å**Ôºö
1. ÊâìÂºÄ CFST Á®ãÂ∫èÊâÄÂú®ÁõÆÂΩï  
2. Áõ¥Êé•Âú®Êñá‰ª∂Â§πÂú∞ÂùÄÊ†è‰∏≠ÂÖ®ÈÄâ(ÊàñÊ∏ÖÁ©∫)Âπ∂ËæìÂÖ• `cmd` ÂõûËΩ¶Â∞±ËÉΩÊâìÂºÄ CMD Á™óÂè£ÔºåÊ≠§Êó∂ÈªòËÆ§Â∞±‰Ωç‰∫éÂΩìÂâçÁõÆÂΩï‰∏ã  
4. ËæìÂÖ•Â∏¶ÂèÇÊï∞ÁöÑÂëΩ‰ª§ÔºåÂ¶ÇÔºö`cfst.exe -tl 200 -dn 20` Âç≥ÂèØËøêË°å

&gt; ÂΩìÁÑ∂‰Ω†‰πüÂèØ‰ª•Èöè‰æøÊâìÂºÄ‰∏Ä‰∏™ CMD Á™óÂè£ÔºåÁÑ∂ÂêéËæìÂÖ•Â¶Ç `cd /d &quot;D:\Program Files\cfst&quot;` Êù•ËøõÂÖ•Á®ãÂ∫èÁõÆÂΩï

&gt; **ÊèêÁ§∫**ÔºöÂ¶ÇÊûúÁî®ÁöÑÊòØ **PowerShell** Âè™ÈúÄÊääÂëΩ‰ª§‰∏≠ÁöÑ `cfst.exe` Êîπ‰∏∫ `.\cfst.exe` Âç≥ÂèØ„ÄÇ  
&gt; **Ê≥®ÊÑè**ÔºöÂú® **PowerShell** ‰∏ã‰ΩøÁî® `-o &quot;&quot;` ‰ºöË¢´ÂøΩÁï•ÊéâÁ©∫ÂèÇÊï∞ÂØºËá¥Êä•ÈîôÔºåÂèØÂä†‰∏™Á©∫Ê†º `-o &quot; &quot;` Ëß£ÂÜ≥

&lt;/details&gt;

****

#### \# Windows Âø´Êç∑ÊñπÂºèÂ∏¶ÂèÇÊï∞ËøêË°å

Â¶ÇÊûú‰∏çÁªèÂ∏∏‰øÆÊîπËøêË°åÂèÇÊï∞ÔºàÊØîÂ¶ÇÂπ≥Êó∂ÈÉΩÊòØÁõ¥Êé•ÂèåÂáªËøêË°åÔºâÁöÑ‰∫∫ÔºåÂª∫ËÆÆ‰ΩøÁî®Âø´Êç∑ÊñπÂºèÔºåÊõ¥Êñπ‰æøÁÇπ„ÄÇ

&lt;details&gt;
&lt;summary&gt;&lt;code&gt;&lt;strong&gt;„Äå ÁÇπÂáªÂ±ïÂºÄ Êü•ÁúãÂÜÖÂÆπ „Äç&lt;/strong&gt;&lt;/code&gt;&lt;/summary&gt;

****

Âè≥ÈîÆ `cfst.exe` Êñá‰ª∂ - **\[ÂàõÂª∫Âø´Êç∑ÊñπÂºè\]**ÔºåÁÑ∂ÂêéÂè≥ÈîÆËØ•Âø´Êç∑ÊñπÂºè - **\[Â±ûÊÄß\]**Ôºå‰øÆÊîπÂÖ∂**ÁõÆÊ†á**Ôºö

``` bash
# Â¶ÇÊûúË¶Å‰∏çËæìÂá∫ÁªìÊûúÊñá‰ª∂ÔºåÈÇ£‰πàËØ∑Âä†‰∏ä -o &quot; &quot;ÔºåÂºïÂè∑ÈáåÁöÑÊòØÁ©∫Ê†ºÔºà‰∏çÂä†Á©∫Ê†º‰ºöÂØºËá¥ËØ•Á©∫ÂèÇÊï∞Ë¢´ÂøΩÁï•‰ªéËÄåÊä•ÈîôÔºâ„ÄÇ
D:\ABC\cfst\cfst.exe -tl 200 -dn 20 -o &quot; &quot;

# Â¶ÇÊûúÊñá‰ª∂Ë∑ØÂæÑÂåÖÂê´ÂºïÂè∑ÔºåÂàôÈúÄË¶ÅÊääÂêØÂä®ÂèÇÊï∞ÊîæÂú®ÂºïÂè∑Â§ñÈù¢ÔºåËÆ∞ÂæóÂºïÂè∑Âíå - ‰πãÈó¥ÊúâÁ©∫Ê†º„ÄÇ
&quot;D:\Program Files\cfst\cfst.exe&quot; -tl 200 -dn 20 -o &quot; &quot;

# Ê≥®ÊÑèÔºÅÂø´Êç∑ÊñπÂºè - Ëµ∑Âßã‰ΩçÁΩÆ ‰∏çËÉΩÊòØÁ©∫ÁöÑÔºåÂê¶ÂàôÂ∞±‰ºöÂõ†‰∏∫ÁªùÂØπË∑ØÂæÑËÄåÊâæ‰∏çÂà∞ ip.txt Êñá‰ª∂
```

&lt;/details&gt;

****

#### \# IPv4/IPv6

&lt;details&gt;
&lt;summary&gt;&lt;code&gt;&lt;strong&gt;„Äå ÁÇπÂáªÂ±ïÂºÄ Êü•ÁúãÂÜÖÂÆπ „Äç&lt;/strong&gt;&lt;/code&gt;&lt;/summary&gt;

****
``` bash
# ÊåáÂÆöËá™Â∏¶ÁöÑ IPv4 Êï∞ÊçÆÊñá‰ª∂ÂèØÊµãÈÄüËøô‰∫õ IPv4 Âú∞ÂùÄÔºà-f ÈªòËÆ§ÂÄºÂ∞±ÊòØ ip.txtÔºåÊâÄ‰ª•ËØ•ÂèÇÊï∞ÂèØÁúÅÁï•Ôºâ
cfst.exe -f ip.txt

# ÊåáÂÆöËá™Â∏¶ÁöÑ IPv6 Êï∞ÊçÆÊñá‰ª∂ÂèØÊµãÈÄüËøô‰∫õ IPv6 Âú∞ÂùÄ
# Âè¶Â§ñÔºåv2.1.0 ÁâàÊú¨ÂêéÊîØÊåÅ IPv4+IPv6 Ê∑∑ÂêàÊµãÈÄüÂπ∂ÁßªÈô§‰∫Ü -ipv6 ÂèÇÊï∞ÔºåÂõ†Ê≠§‰∏Ä‰∏™Êñá‰ª∂ÂÜÖÂèØ‰ª•ÂêåÊó∂ÂåÖÂê´ IPv4+IPv6 Âú∞ÂùÄ
cfst.exe -f ipv6.txt

# ‰πüÂèØ‰ª•Áõ¥Êé•ÈÄöËøáÂèÇÊï∞ÊåáÂÆöË¶ÅÊµãÈÄüÁöÑ IP
cfst.exe -ip 1.1.1.1,2606:4700::/32
```

&gt; ÊµãÈÄü IPv6 Êó∂ÔºåÂèØËÉΩ‰ºöÊ≥®ÊÑèÂà∞ÊØèÊ¨°ÊµãÈÄüÊï∞ÈáèÈÉΩ‰∏ç‰∏ÄÊ†∑Ôºå‰∫ÜËß£ÂéüÂõ†Ôºö [#120](https://github.com/XIU2/CloudflareSpeedTest/issues/120)  
&gt; Âõ†‰∏∫ IPv6 Â§™Â§öÔºà‰ª•‰∫ø‰∏∫Âçï‰ΩçÔºâÔºå‰∏îÁªùÂ§ßÈÉ®ÂàÜ IP ÊÆµÂéãÊ†πÊú™ÂêØÁî®ÔºåÊâÄ‰ª•ÊàëÂè™Êâ´‰∫Ü‰∏ÄÈÉ®ÂàÜÂèØÁî®ÁöÑ IPv6 ÊÆµÂÜôÂà∞ `ipv6.txt` Êñá‰ª∂‰∏≠ÔºåÊúâÂÖ¥Ë∂£ÁöÑÂèØ‰ª•Ëá™Ë°åÊâ´ÊèèÂ¢ûÂà†ÔºåASN Êï∞ÊçÆÊ∫êÊù•Ëá™Ôºö[bgp.he.net](https://bgp.he.net/AS13335#_prefixes6)

&lt;/details&gt;

****

#### \# HTTPing

&lt;details&gt;
&lt;summary&gt;&lt;code&gt;&lt;strong&gt;„Äå ÁÇπÂáªÂ±ïÂºÄ Êü•ÁúãÂÜÖÂÆπ „Äç&lt;/strong&gt;&lt;/code&gt;&lt;/summary&gt;

****

ÁõÆÂâçÊúâ‰∏§ÁßçÂª∂ËøüÊµãÈÄüÊ®°ÂºèÔºåÂàÜÂà´‰∏∫ **TCP ÂçèËÆÆ„ÄÅHTTP ÂçèËÆÆ**„ÄÇ  
TCP ÂçèËÆÆËÄóÊó∂Êõ¥Áü≠„ÄÅÊ∂àËÄóËµÑÊ∫êÊõ¥Â∞ëÔºåË∂ÖÊó∂Êó∂Èó¥‰∏∫ 1 ÁßíÔºåËØ•ÂçèËÆÆ‰∏∫ÈªòËÆ§Ê®°Âºè„ÄÇ  
HTTP ÂçèËÆÆÈÄÇÁî®‰∫éÂø´ÈÄüÊµãËØïÊüêÂüüÂêçÊåáÂêëÊüê IP Êó∂ÊòØÂê¶ÂèØ‰ª•ËÆøÈóÆÔºåË∂ÖÊó∂Êó∂Èó¥‰∏∫ 2 Áßí„ÄÇ  
Âêå‰∏Ä‰∏™ IPÔºåÂêÑÂçèËÆÆÂéª Ping ÂæóÂà∞ÁöÑÂª∂Ëøü‰∏ÄËà¨‰∏∫Ôºö**ICMP &lt; TCP &lt; HTTP**ÔºåË∂äÈù†Âè≥ÂØπ‰∏¢ÂåÖÁ≠âÁΩëÁªúÊ≥¢Âä®Ë∂äÊïèÊÑü„ÄÇ

&gt; Ê≥®ÊÑèÔºöHTTPing Êú¨Ë¥®‰∏ä‰πüÁÆó‰∏ÄÁßç**ÁΩëÁªúÊâ´Êèè**Ë°å‰∏∫ÔºåÂõ†Ê≠§Â¶ÇÊûú‰Ω†Âú®ÊúçÂä°Âô®‰∏äÈù¢ËøêË°åÔºåÈúÄË¶Å**Èôç‰ΩéÂπ∂Âèë**(`-n`)ÔºåÂê¶ÂàôÂèØËÉΩ‰ºöË¢´‰∏Ä‰∫õ‰∏•Ê†ºÁöÑÂïÜÂÆ∂ÊöÇÂÅúÊúçÂä°„ÄÇÂ¶ÇÊûú‰Ω†ÈÅáÂà∞ HTTPing È¶ñÊ¨°ÊµãÈÄüÂèØÁî® IP Êï∞ÈáèÊ≠£Â∏∏ÔºåÂêéÁª≠ÊµãÈÄüË∂äÊù•Ë∂äÂ∞ëÁîöËá≥Áõ¥Êé•‰∏∫ 0Ôºå‰ΩÜÂÅú‰∏ÄÊÆµÊó∂Èó¥ÂêéÂèàÊÅ¢Â§ç‰∫ÜÁöÑÊÉÖÂÜµÔºåÈÇ£‰πà‰πüÂèØËÉΩÊòØË¢´ ËøêËê•ÂïÜ„ÄÅCloudflare CDN ËÆ§‰∏∫‰Ω†Âú®ÁΩëÁªúÊâ´ÊèèËÄå**Ëß¶Âèë‰∏¥Êó∂ÈôêÂà∂Êú∫Âà∂**ÔºåÂõ†Ê≠§Êâç‰ºöËøá‰∏Ä‰ºöÂÑøÂ∞±ÊÅ¢Â§ç‰∫ÜÔºåÂª∫ËÆÆ**Èôç‰ΩéÂπ∂Âèë**(`-n`)ÂáèÂ∞ëËøôÁßçÊÉÖÂÜµÁöÑÂèëÁîü„ÄÇ

&gt; Âè¶Â§ñÔºåÊú¨ËΩØ‰ª∂ HTTPing ‰ªÖËé∑Âèñ**ÂìçÂ∫îÂ§¥(response headers)**ÔºåÂπ∂‰∏çËé∑ÂèñÊ≠£ÊñáÂÜÖÂÆπÔºàÂç≥ URL Êñá‰ª∂Â§ßÂ∞è‰∏çÂΩ±Âìç HTTPing ÊµãËØïÔºå‰ΩÜÂ¶ÇÊûú‰Ω†ËøòË¶Å‰∏ãËΩΩÊµãÈÄüÁöÑËØùÔºåÈÇ£‰πàËøòÊòØÈúÄË¶Å‰∏Ä‰∏™Â§ßÊñá‰ª∂ÁöÑÔºâÔºåÁ±ª‰ºº‰∫é curl -i ÂäüËÉΩ„ÄÇ

&gt; Âè¶Â§ñÔºåHTTPing ËøáÁ®ã‰∏≠ÔºåËΩØ‰ª∂‰ºö‰ªé HTTP ÂìçÂ∫îÂ§¥‰∏≠Ëé∑ÂèñËØ• IP ÂΩìÂâçÂú∞Âå∫Á†ÅÔºàÊîØÊåÅ Cloudflare„ÄÅAWS CloudFront„ÄÅFastly„ÄÅGcore„ÄÅCDN77„ÄÅBunny Á≠â CDNÔºâÂπ∂ÊòæÁ§∫Âá∫Êù•ÔºåËÄå TCPing ËøáÁ®ã‰∏≠Êó†Ê≥ïËøôÊ†∑ÂÅöÔºà‰ΩÜ ‰∏ãËΩΩÊµãÈÄü Êó∂‰πü‰ºöËøôÊ†∑ÂÅöÊù•Ëé∑ÂèñÂú∞Âå∫Á†ÅÔºåÊØïÁ´ü‰∏ãËΩΩÊµãÈÄü‰πüÊòØ‰∏™ HTTP ÈìæÊé•Ôºâ

``` bash
# Âè™ÈúÄÂä†‰∏ä -httping ÂèÇÊï∞Âç≥ÂèØÂàáÊç¢Âà∞ HTTP ÂçèËÆÆÂª∂ËøüÊµãÈÄüÊ®°Âºè
cfst.exe -httping

# ËΩØ‰ª∂‰ºöÊ†πÊçÆËÆøÈóÆÊó∂ÁΩëÈ°µËøîÂõûÁöÑÊúâÊïà HTTP Áä∂ÊÄÅÁ†ÅÊù•Âà§Êñ≠ÂèØÁî®ÊÄßÔºàÂΩìÁÑ∂Ë∂ÖÊó∂‰πüÁÆóÔºâÔºåÈªòËÆ§ÂØπËøîÂõû 200 301 302 Ëøô‰∏â‰∏™ HTTP Áä∂ÊÄÅÁ†ÅÁöÑËßÜ‰∏∫ÊúâÊïàÔºåÂèØ‰ª•ÊâãÂä®ÊåáÂÆöËÆ§‰∏∫ÊúâÊïàÁöÑ HTTP Áä∂ÊÄÅÁ†ÅÔºå‰ΩÜÂè™ËÉΩÊåáÂÆö‰∏Ä‰∏™Ôºà‰Ω†ÈúÄË¶ÅÊèêÂâçÁ°ÆÂÆöÊµãËØïÂú∞ÂùÄÊ≠£Â∏∏ÊÉÖÂÜµ‰∏ã‰ºöËøîÂõûÂì™‰∏™Áä∂ÊÄÅÁ†ÅÔºâ
cfst.exe -httping -httping-code 200

# ÈÄöËøá -url ÂèÇÊï∞Êù•ÊåáÂÆö HTTPing ÊµãËØïÂú∞ÂùÄÔºàÂèØ‰ª•ÊòØ‰ªªÊÑèÁΩëÈ°µ URLÔºå‰∏çÂ±ÄÈôê‰∫éÂÖ∑‰ΩìÊñá‰ª∂Âú∞ÂùÄÔºâ
cfst.exe -httping -url https://cf.xiu2.xyz/url
# Â¶ÇÊûú‰Ω†Ë¶Å HTTPing ÊµãËØïÂÖ∂‰ªñÁΩëÁ´ô/CDNÔºåÈÇ£‰πàÊåáÂÆö‰∏Ä‰∏™ËØ•ÁΩëÁ´ô/‰ΩøÁî®ËØ• CDN ÁöÑÂú∞ÂùÄÔºàÂõ†‰∏∫ËΩØ‰ª∂ÈªòËÆ§Âú∞ÂùÄÊòØ Cloudflare ÁöÑÔºåÂè™ËÉΩÁî®‰∫éÊµãËØï Cloudflare ÁöÑ IPÔºâ

# Ê≥®ÊÑèÔºöÂ¶ÇÊûúÊµãÈÄüÂú∞ÂùÄ‰∏∫ HTTP ÂçèËÆÆÔºåËÆ∞ÂæóÂä†‰∏ä -tp 80ÔºàËøô‰∏™ÂèÇÊï∞‰ºöÂΩ±Âìç Âª∂ËøüÊµãÈÄü/‰∏ãËΩΩÊµãÈÄü Êó∂‰ΩøÁî®ÁöÑÁ´ØÂè£Ôºâ
# ÂêåÁêÜÔºåÂ¶ÇÊûúË¶ÅÊµãÈÄü 80 Á´ØÂè£ÔºåÈÇ£‰πà‰πüÈúÄË¶ÅÂä†‰∏ä -url ÂèÇÊï∞Êù•ÊåáÂÆö‰∏Ä‰∏™ http:// ÂçèËÆÆÁöÑÂú∞ÂùÄÊâçË°åÔºà‰∏îËØ•Âú∞ÂùÄ‰∏ç‰ºöÂº∫Âà∂ÈáçÂÆöÂêëËá≥ HTTPSÔºâÔºåÂ¶ÇÊûúÊòØÈùû 80 443 Á´ØÂè£ÔºåÈÇ£‰πàÈúÄË¶ÅÁ°ÆÂÆöËØ•‰∏ãËΩΩÊµãÈÄüÂú∞ÂùÄÊòØÂê¶ÊîØÊåÅÈÄöËøáËØ•Á´ØÂè£ËÆøÈóÆ„ÄÇ
cfst.exe -httping -tp 80 -url http://cdn.cloudflare.steamstatic.com/steam/apps/5952/movie_max.webm
```

&lt;/details&gt;

****

#### \# ÂåπÈÖçÊåáÂÆöÂú∞Âå∫

&lt;details&gt;
&lt;summary&gt;&lt;code&gt;&lt;strong&gt;„Äå ÁÇπÂáªÂ±ïÂºÄ Êü•ÁúãÂÜÖÂÆπ „Äç&lt;/strong&gt;&lt;/code&gt;&lt;/summary&gt;

****

Cloudflare CDN ÁöÑËäÇÁÇπ IP ÊòØ Anycast IPÔºåÂç≥ÊØè‰∏™ IP ÂØπÂ∫îÁöÑÊúçÂä°Âô®ËäÇÁÇπÂèäÂú∞Âå∫‰∏çÊòØÂõ∫ÂÆöÁöÑÔºåËÄåÊòØÂä®ÊÄÅÂèòÂåñÁöÑÔºå**‰∏çÂêåÂú∞Âå∫„ÄÅ‰∏çÂêåËøêËê•ÂïÜ„ÄÅ‰∏çÂêåÊó∂Èó¥ÊÆµ**ËÆøÈóÆ**Âêå‰∏Ä‰∏™ IP** ÂàÜÈÖçÂà∞ÁöÑÊúçÂä°Âô®ËäÇÁÇπÂú∞Âå∫ÂíåË∑ØÁ∫ø‰πüÈÉΩÊòØ‰∏ç‰∏ÄÊ†∑ÁöÑÔºàÊØîÂ¶ÇÂêå‰∏Ä‰∏™ IPÔºåÁæéÂõΩ‰∫∫ËÆøÈóÆÂ∞±ÊòØÂàÜÈÖçÂà∞Â∞±ËøëÁöÑÁæéÂõΩËäÇÁÇπÊúçÂä°Âô®ÔºåÊó•Êú¨‰∫∫ËÆøÈóÆÂàôÂ∞±ÂèàÂèòÊàê‰∫ÜÂ∞±ËøëÁöÑÊó•Êú¨ËäÇÁÇπÊúçÂä°Âô®‰∫ÜÔºåÂõΩÂÜÖÂÜÖÂú∞Â∞±ÊØîËæÉÁâπÊÆä‰∫ÜÔºåÂè™ËÉΩÁªô‰Ω†ÂàÜÈÖçÂà∞ÂÖ∂‰ªñÂõΩÂÆ∂ÔºåÂΩìÁÑ∂‰∏çÂêåÁöÑ IP ÊÆµË∑ØÁî±ÂèòÂåñ/ÂàÜÈÖçÈÄªËæë‰πüÊòØ‰∏çÂêåÁöÑÔºåÊúâÁöÑ IP ÊÆµ‰ºöËæÉ‰∏∫Âõ∫ÂÆöÔºâ„ÄÇ

&gt; **Ê≥®ÊÑè**ÔºÅËôΩÁÑ∂ Cloudflare CDN ÊúâÂæàÂ§ö‰∫öÊ¥≤ËäÇÁÇπÔºå‰ΩÜ**‰∏ç‰ª£Ë°®‰Ω†Â∞±ËÉΩÁî®‰∏ä**ÔºåÊñ∞Âä†Âù°‰∫∫ÊµãÈÄüÂèØËÉΩÈöè‰æø‰∏ÄÊäì‰∏ÄÂ§ßÊääÁöÑÊñ∞Âä†Âù°ËäÇÁÇπÔºå‰ΩÜ‰Ω†ÂÖ®ÈÉ®Êâ´‰∏ÄÈÅçÂèØËÉΩÈÉΩÈÅá‰∏çÂà∞‰∏Ä‰∏™ÔºåÂõ†‰∏∫ËøôÊòØÁî± CDN ÊéßÂà∂ÁöÑ„ÄÇAnycast IP ÁöÑË∑ØÁî±ÊòØÁªèÂ∏∏ÂèòÁöÑÔºåÂêå‰∏Ä‰∏™ IP ‰ªäÂ§©ÂèØËÉΩÊòØÁæéÂõΩÔºåÊòéÂ§©‰Ω†ÂÜçËÆøÈóÆÂèØËÉΩÂ∞±ÂèàÂàÜÈÖçÂà∞Ê¨ßÊ¥≤ËäÇÁÇπ‰∫ÜÔºàÂΩìÁÑ∂ËøôÂè™ÊòØ‰∏™‰æãÂ≠êÔºå‰∏ÄËà¨Ê≤°ÊúâÈÇ£‰πàÈ¢ëÁπÅÔºåËøô‰πüÂíåÂæàÂ§öÂõ†Á¥†ÊúâÂÖ≥ÔºåÊØîÂ¶ÇÁ∫øË∑ØÊã•Â°ûÁ®ãÂ∫¶ÔºåÊàêÊú¨ÂèòÂä®Á≠âÔºâÔºåÂõ†Ê≠§**‰∏çË¶ÅÂØπËØ•ÂäüËÉΩÊúâËøáÈ´òÊúüÂæÖ**~

ÊàñËÄÖ‰Ω†Èöè‰æøÊâæ‰∏™ Cloudflare CDN ÁöÑ IPÔºàÊØîÂ¶ÇÂÆòÁΩëÂüüÂêçÁöÑËß£Êûê IP `104.16.123.96`ÔºâÔºåÁÑ∂ÂêéÂéªÈÇ£‰∫õÊúâÂÖ®ÁêÉËäÇÁÇπÁöÑ[Âú®Á∫ø Ping ÊµãËØï](https://ping.sx/ping?t=104.16.123.96)ÁΩëÁ´ôÔºå‰Ω†Â∞±‰ºöÂèëÁé∞Ëøô‰∏™ IP Âú®ÂÖ®ÁêÉÂ§ßÈÉ®ÂàÜÂú∞Âå∫ÁöÑÂª∂ËøüÈÉΩÊòØ‰∏™‰ΩçÊï∞ÔºàËÄå‰∏îÂæàÂ§öÈÉΩÊòØ 0.X msÔºâÔºåÂ∞±ÁÆó‰∏Ä‰∫õÂú∞ÊñπÂª∂ËøüÈ´ò‰∏Ä‰∫õ‰ΩÜ‰πüÂü∫Êú¨ÈÉΩÊéßÂà∂Âú® Âá†ÂçÅmsÔºåÂè™ÊúâÂú®ÂõΩÂÜÖÊâç‰ºöÂèëÁé∞Á™ÅÁÑ∂ÂèòÊàê‰∫Ü ‰∏äÁôæms ‰∫Ü„ÄÇ

ËøôÂ∞±ÊòØ Anycast ÊäÄÊúØÔºå‰πüÂ∞±Âè™ÊúâÂõΩÂÜÖÂ§ßÈôÜËøôÁßçÁâπÊÆäÁöÑÁΩëÁªúÊÉÖÂÜµÔºåÊâçÈúÄË¶ÅÂØπ Anycast ÁöÑ CDN IP ËøõË°å‰ºòÈÄâ„ÄÇ

Âõ†Ê≠§ÔºåÂØπ‰∫éËøôÁßç Anycast IP ÁöÑÂÆûÈôÖÊúçÂä°Âô®‰ΩçÁΩÆÔºåÂ∞±‰∏çËÉΩÈù†ÈÇ£‰∫õÂú®Á∫ø IP Âú∞ÂùÄ‰ΩçÁΩÆÊü•ËØ¢ÁΩëÁ´ôÊù•Âà§Êñ≠‰∫Ü„ÄÇ

Èô§‰∫ÜÈÄöËøá **HTTP ÂìçÂ∫îÂ§¥**Ëé∑ÂèñÂú∞Âå∫Á†ÅÂ§ñÔºàËØ•ÂäüËÉΩÁöÑÂÆûÁé∞ÊñπÂºèÔºâÔºåËøòÂèØ‰ª•ÊâãÂä®ËÆøÈóÆ `http://CloudflareIP/cdn-cgi/trace` Êù•Ëé∑Áü• CDN ÂàÜÈÖçÁªô‰Ω†ÁöÑÂÆûÈôÖËäÇÁÇπÂú∞Âå∫Á†Å„ÄÇ

&gt; ËØ•ÂäüËÉΩÊîØÊåÅ **Cloudflare„ÄÅAWS CloudFront„ÄÅFastly„ÄÅGcore„ÄÅCDN77„ÄÅBunny** Á≠â CDN„ÄÇ  
&gt; ‰ΩÜÊ≥®ÊÑèÔºå‰∏çÊòØÊâÄÊúâ CDN ÈÉΩÊîØÊåÅ Anycast ÊäÄÊúØÁöÑÔºåÂæàÂ§ö CDN ‰ºöÈôêÂà∂‰∏Ä‰∏™ÁΩëÁ´ôËÉΩ‰ΩøÁî®ÁöÑ IP ËåÉÂõ¥„ÄÇ

&gt; ÂÖ∂‰∏≠ **Cloudflare„ÄÅAWS CloudFront„ÄÅFastly** ÈÉΩ‰ΩøÁî®ÁöÑÊòØ **`IATA ‰∏âÂ≠óÊú∫Âú∫Âú∞Âå∫Á†Å`**ÔºåÂ¶ÇÔºöHKG,LAX  
&gt; ËÄå **CDN77„ÄÅBunny** ‰ΩøÁî®ÁöÑÊòØ **`‰∫åÂ≠óÂõΩÂÆ∂/Âå∫ÂüüÁ†Å`**ÔºåÂ¶ÇÔºöUS,CN  
&gt; **Gcore** Âàô‰ΩøÁî®ÁöÑÊòØ **`‰∫åÂ≠óÂüéÂ∏ÇÁ†Å`**ÔºåÂ¶ÇÔºöFR,AM  
&gt; Âõ†Ê≠§Â§ßÂÆ∂‰ΩøÁî® `-cfcolo` ÊåáÂÆöÂú∞Âå∫Á†ÅÊó∂Ë¶ÅÊ†πÊçÆ‰∏çÂêåÁöÑ CDN Êù•ÊåáÂÆö‰∏çÂêåÁ±ªÂûãÁöÑÂú∞Âå∫Á†Å„ÄÇ

&gt; **Ê≥®ÊÑè**ÔºöÂ¶ÇÊûú‰Ω†Ë¶ÅÁî®‰∫éÁ≠õÈÄâ AWS CloudFront CDN Âú∞Âå∫ÔºåÈÇ£‰πàË¶ÅÈÄöËøá `-url` ÂèÇÊï∞ÊåáÂÆö‰∏Ä‰∏™‰ΩøÁî® AWS CloudFront CDN ÁöÑ‰∏ãËΩΩÊµãÈÄüÂú∞ÂùÄÔºàÂõ†‰∏∫ËΩØ‰ª∂ÈªòËÆ§‰∏ãËΩΩÊµãÈÄüÂú∞ÂùÄÊòØ Cloudflare CDN ÁöÑÔºâÔºåÂè¶Â§ñÊúâÊó∂ÂÄô HTTPing Ê®°ÂºèÊµãÈÄü‰∏Ä‰∫õ AWS CloudFront Âú∞ÂùÄ‰ºöËøîÂõû 403 ÈîôËØØÔºåËøôÁßçÊÉÖÂÜµ‰∏ãÈúÄË¶ÅÂä†‰∏ä `-httping-code 403` ÊâçËÉΩÊ≠£Á°ÆËé∑ÂèñÂú∞Âå∫Á†Å„ÄÇ

``` bash
# ÊåáÂÆöÂú∞Âå∫ÂêçÂêéÔºåÂª∂ËøüÊµãÈÄüÂêéÂæóÂà∞ÁöÑÁªìÊûúÂ∞±ÈÉΩÊòØÊåáÂÆöÂú∞Âå∫ÁöÑ IP ‰∫ÜÔºàÂ¶ÇÊûúÊ≤°ÊúâÊåáÂÆö -dd ÁöÑËØùÂàô‰ºöÁªßÁª≠ËøõË°å‰∏ãËΩΩÊµãÈÄüÔºâ
# Â¶ÇÊûúÂª∂ËøüÊµãÈÄüÂêéÁªìÊûú‰∏∫ 0ÔºåÂàôËØ¥ÊòéÊ≤°ÊúâÊâæÂà∞‰ªª‰Ωï‰∏Ä‰∏™ÔºàÊú™Ë∂ÖÊó∂ÂèØÁî®ÁöÑÔºâÊåáÂÆöÂú∞Âå∫ÁöÑ IP„ÄÇ
# ËäÇÁÇπÂú∞Âå∫Âêç‰∏∫ÂΩìÂú∞ IATA Êú∫Âú∫Âú∞Âå∫Á†ÅÊàñÂõΩÂÆ∂/ÂüéÂ∏ÇÁ†ÅÔºåÊåáÂÆöÂ§ö‰∏™Êó∂Áî®Ëã±ÊñáÈÄóÂè∑ÂàÜÈöîÔºåv2.2.3 ÁâàÊú¨ÂêéÊîØÊåÅÂ∞èÂÜô

cfst.exe -httping -cfcolo HKG,KHH,NRT,LAX,SEA,SJC,FRA,MAD

# Ê≥®ÊÑèÔºåËØ•ÂèÇÊï∞Âè™ÊúâÂú® HTTPing Âª∂ËøüÊµãÈÄüÊ®°Âºè‰∏ãÊâçÂèØÁî®ÔºàÂõ†‰∏∫ËΩØ‰ª∂ÊòØÈÄöËøá HTTP ÈìæÊé•‰∏≠ÁöÑÂìçÂ∫îÂ§¥Êù•Ëé∑ÂæóËØ• IP ÁöÑÂÆûÈôÖÂú∞Âå∫Á†ÅÔºâ

# Âè¶Â§ñÔºåHTTPing ËøáÁ®ã‰∏≠ÔºåËΩØ‰ª∂‰ºö‰ªé HTTP ÂìçÂ∫îÂ§¥‰∏≠Ëé∑ÂèñËØ• IP ÂΩìÂâçÂú∞Âå∫Á†ÅÔºàÊîØÊåÅ Cloudflare„ÄÅAWS CloudFront„ÄÅFastly„ÄÅGcore„ÄÅCDN77„ÄÅBunny Á≠â CDNÔºâÂπ∂ÊòæÁ§∫Âá∫Êù•ÔºåËÄå TCPing ËøáÁ®ã‰∏≠Êó†Ê≥ïËøôÊ†∑ÂÅöÔºà‰ΩÜ ‰∏ãËΩΩÊµãÈÄü Êó∂‰πü‰ºöËøôÊ†∑ÂÅöÊù•Ëé∑ÂèñÂú∞Âå∫Á†ÅÔºåÊØïÁ´ü‰∏ãËΩΩÊµãÈÄü‰πüÊòØ‰∏™ HTTP ÈìæÊé•Ôºâ
```

&gt; **`IATA ‰∏âÂ≠óÊú∫Âú∫Âú∞Âå∫Á†Å`**ÔºåÂèØËßÅÔºöhttps://www.cloudflarestatus.com/  
&gt; **`‰∫åÂ≠óÂõΩÂÆ∂Á†Å`**ÔºåÂèØËßÅÔºö[https://zh.wikipedia.org/wiki/ISO_3166-1‰∫å‰ΩçÂ≠óÊØç‰ª£Á†Å#Ê≠£ÂºèÂàÜÈÖç‰ª£Á†Å](https://zh.wikipedia.org/wiki/ISO_3166-1%E4%BA%8C%E4%BD%8D%E5%AD%97%E6%AF%8D%E4%BB%A3%E7%A0%81#%E6%AD%A3%E5%BC%8F%E5%88%86%E9%85%8D%E4%BB%A3%E7%A0%81)

&lt;/details&gt;

****

#### \# Êñá‰ª∂Áõ∏ÂØπ/ÁªùÂØπË∑ØÂæÑ

&lt;details&gt;
&lt;summary&gt;&lt;code&gt;&lt;strong&gt;„Äå ÁÇπÂáªÂ±ïÂºÄ Êü•ÁúãÂÜÖÂÆπ „Äç&lt;/strong&gt;&lt;/code&gt;&lt;/summary&gt;

****

``` bash
# ÊåáÂÆö IPv4 Êï∞ÊçÆÊñá‰ª∂Ôºå‰∏çÊòæÁ§∫ÁªìÊûúÁõ¥Êé•ÈÄÄÂá∫ÔºåËæìÂá∫ÁªìÊûúÂà∞Êñá‰ª∂Ôºà-p ÂÄº‰∏∫ 0Ôºâ
cfst.exe -f 1.txt -p 0 -dd

# ÊåáÂÆö IPv4 Êï∞ÊçÆÊñá‰ª∂Ôºå‰∏çËæìÂá∫ÁªìÊûúÂà∞Êñá‰ª∂ÔºåÁõ¥Êé•ÊòæÁ§∫ÁªìÊûúÔºà-p ÂÄº‰∏∫ 10 Êù°Ôºå-o ÂÄº‰∏∫Á©∫‰ΩÜÂºïÂè∑‰∏çËÉΩÂ∞ëÔºâ
cfst.exe -f 2.txt -o &quot;&quot; -p 10 -dd

# ÊåáÂÆö IPv4 Êï∞ÊçÆÊñá‰ª∂ Âèä ËæìÂá∫ÁªìÊûúÂà∞Êñá‰ª∂ÔºàÁõ∏ÂØπË∑ØÂæÑÔºåÂç≥ÂΩìÂâçÁõÆÂΩï‰∏ãÔºåÂ¶ÇÂê´Á©∫Ê†ºËØ∑Âä†‰∏äÂºïÂè∑Ôºâ
cfst.exe -f 3.txt -o result.txt -dd


# ÊåáÂÆö IPv4 Êï∞ÊçÆÊñá‰ª∂ Âèä ËæìÂá∫ÁªìÊûúÂà∞Êñá‰ª∂ÔºàÁõ∏ÂØπË∑ØÂæÑÔºåÂç≥ÂΩìÂâçÁõÆÂΩïÂÜÖÁöÑ abc Êñá‰ª∂Â§π‰∏ãÔºåÂ¶ÇÂê´Á©∫Ê†ºËØ∑Âä†‰∏äÂºïÂè∑Ôºâ
# LinuxÔºàCFST Á®ãÂ∫èÊâÄÂú®ÁõÆÂΩïÂÜÖÁöÑ abc Êñá‰ª∂Â§π‰∏ãÔºâ
./cfst -f abc/3.txt -o abc/result.txt -dd

# WindowsÔºàÊ≥®ÊÑèÊòØÂèçÊñúÊù†Ôºâ
cfst.exe -f abc\3.txt -o abc\result.txt -dd


# ÊåáÂÆö IPv4 Êï∞ÊçÆÊñá‰ª∂ Âèä ËæìÂá∫ÁªìÊûúÂà∞Êñá‰ª∂ÔºàÁªùÂØπË∑ØÂæÑÔºåÂç≥ C:\abc\ ÁõÆÂΩï‰∏ãÔºåÂ¶ÇÂê´Á©∫Ê†ºËØ∑Âä†‰∏äÂºïÂè∑Ôºâ
# LinuxÔºà/abc/ ÁõÆÂΩï‰∏ãÔºâ
./cfst -f /abc/4.txt -o /abc/result.csv -dd

# WindowsÔºàÊ≥®ÊÑèÊòØÂèçÊñúÊù†Ôºâ
cfst.exe -f C:\abc\4.txt -o C:\abc\result.csv -dd


# Â¶ÇÊûúË¶Å‰ª•„ÄêÁªùÂØπË∑ØÂæÑ„ÄëËøêË°å CFSTÔºåÈÇ£‰πà -f / -o ÂèÇÊï∞‰∏≠ÁöÑÊñá‰ª∂Âêç‰πüÂøÖÈ°ªÊòØ„ÄêÁªùÂØπË∑ØÂæÑ„ÄëÔºåÂê¶Âàô‰ºöÊä•ÈîôÊâæ‰∏çÂà∞Êñá‰ª∂ÔºÅ
# LinuxÔºà/abc/ ÁõÆÂΩï‰∏ãÔºâ
/abc/cfst -f /abc/4.txt -o /abc/result.csv -dd

# WindowsÔºàÊ≥®ÊÑèÊòØÂèçÊñúÊù†Ôºâ
C:\abc\cfst.exe -f C:\abc\4.txt -o C:\abc\result.csv -dd
```
&lt;/details&gt;

****

#### \# ÊµãÈÄüÂÖ∂‰ªñÁ´ØÂè£

&lt;details&gt;
&lt;summary&gt;&lt;code&gt;&lt;strong&gt;„Äå ÁÇπÂáªÂ±ïÂºÄ Êü•ÁúãÂÜÖÂÆπ „Äç&lt;/strong&gt;&lt;/code&gt;&lt;/summary&gt;

****

``` bash
# Â¶ÇÊûú‰Ω†ÊÉ≥Ë¶ÅÊµãÈÄüÈùûÈªòËÆ§ 443 ÁöÑÂÖ∂‰ªñÁ´ØÂè£ÔºåÂàôÈúÄË¶ÅÈÄöËøá -tp ÂèÇÊï∞ÊåáÂÆöÔºàËØ•ÂèÇÊï∞‰ºöÂΩ±Âìç Âª∂ËøüÊµãÈÄü/‰∏ãËΩΩÊµãÈÄü Êó∂‰ΩøÁî®ÁöÑÁ´ØÂè£Ôºâ

# Â¶ÇÊûúË¶ÅÂª∂ËøüÊµãÈÄü 80 Á´ØÂè£+‰∏ãËΩΩÊµãÈÄüÔºàÂ¶ÇÊûú -dd Á¶ÅÁî®‰∫Ü‰∏ãËΩΩÊµãÈÄüÂàô‰∏çÈúÄË¶ÅÔºâÔºåÈÇ£‰πàËøòÈúÄË¶ÅÊåáÂÆö http:// ÂçèËÆÆÁöÑ‰∏ãËΩΩÊµãÈÄüÂú∞ÂùÄÊâçË°åÔºà‰∏îËØ•Âú∞ÂùÄ‰∏ç‰ºöÂº∫Âà∂ÈáçÂÆöÂêëËá≥ HTTPSÔºåÂõ†‰∏∫ÈÇ£Ê†∑Â∞±ÂèòÊàê 443 Á´ØÂè£‰∫ÜÔºâ
cfst.exe -tp 80 -url http://cdn.cloudflare.steamstatic.com/steam/apps/5952/movie_max.webm

# Â¶ÇÊûúÊòØÈùû 80 443 ÁöÑÂÖ∂‰ªñÁ´ØÂè£ÔºåÈÇ£‰πàÈúÄË¶ÅÁ°ÆÂÆö‰Ω†‰ΩøÁî®ÁöÑ‰∏ãËΩΩÊµãÈÄüÂú∞ÂùÄÊòØÂê¶ÊîØÊåÅÈÄöËøáËØ•ÈùûÊ†áÁ´ØÂè£ËÆøÈóÆ„ÄÇ
```

&lt;/details&gt;

****

#### \# Ëá™ÂÆö‰πâÊµãÈÄüÂú∞ÂùÄ

&lt;details&gt;
&lt;summary&gt;&lt;code&gt;&lt;strong&gt;„Äå ÁÇπÂáªÂ±ïÂºÄ Êü•ÁúãÂÜÖÂÆπ „Äç&lt;/strong&gt;&lt;/code&gt;&lt;/summary&gt;

****

``` bash
# ËØ•ÂèÇÊï∞ÈÄÇÁî®‰∫é‰∏ãËΩΩÊµãÈÄü Âèä HTTP ÂçèËÆÆÁöÑÂª∂ËøüÊµãÈÄüÔºåÂØπ‰∫éÂêéËÄÖËØ•Âú∞ÂùÄÂèØ‰ª•ÊòØ‰ªªÊÑèÁΩëÈ°µ URLÔºà‰∏çÂ±ÄÈôê‰∫éÂÖ∑‰ΩìÊñá‰ª∂Âú∞ÂùÄÔºâ

# Âú∞ÂùÄË¶ÅÊ±ÇÔºöÂèØ‰ª•Áõ¥Êé•‰∏ãËΩΩ„ÄÅÊñá‰ª∂Â§ßÂ∞èË∂ÖËøá 200MB„ÄÅÁî®ÁöÑÊòØ Cloudflare CDN
cfst.exe -url https://cf.xiu2.xyz/url

# Ê≥®ÊÑèÔºöÂ¶ÇÊûúÊµãÈÄüÂú∞ÂùÄ‰∏∫ HTTP ÂçèËÆÆÔºàËØ•Âú∞ÂùÄ‰∏çËÉΩÂº∫Âà∂ÈáçÂÆöÂêëËá≥ HTTPSÔºâÔºåËÆ∞ÂæóÂä†‰∏ä -tp 80ÔºàËøô

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[aquasecurity/trivy]]></title>
            <link>https://github.com/aquasecurity/trivy</link>
            <guid>https://github.com/aquasecurity/trivy</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:49 GMT</pubDate>
            <description><![CDATA[Find vulnerabilities, misconfigurations, secrets, SBOM in containers, Kubernetes, code repositories, clouds and more]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/aquasecurity/trivy">aquasecurity/trivy</a></h1>
            <p>Find vulnerabilities, misconfigurations, secrets, SBOM in containers, Kubernetes, code repositories, clouds and more</p>
            <p>Language: Go</p>
            <p>Stars: 30,391</p>
            <p>Forks: 2,846</p>
            <p>Stars today: 45 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;docs/imgs/logo.png&quot; width=&quot;200&quot;&gt;

[![GitHub Release][release-img]][release]
[![Test][test-img]][test]
[![Go Report Card][go-report-img]][go-report]
[![License: Apache-2.0][license-img]][license]
[![GitHub Downloads][github-downloads-img]][release]
![Docker Pulls][docker-pulls]

[üìñ Documentation][docs]
&lt;/div&gt;

Trivy ([pronunciation][pronunciation]) is a comprehensive and versatile security scanner.
Trivy has *scanners* that look for security issues, and *targets* where it can find those issues.

Targets (what Trivy can scan):

- Container Image
- Filesystem
- Git Repository (remote)
- Virtual Machine Image
- Kubernetes

Scanners (what Trivy can find there):

- OS packages and software dependencies in use (SBOM)
- Known vulnerabilities (CVEs)
- IaC issues and misconfigurations
- Sensitive information and secrets
- Software licenses

Trivy supports most popular programming languages, operating systems, and platforms. For a complete list, see the [Scanning Coverage] page.

To learn more, go to the [Trivy homepage][homepage] for feature highlights, or to the [Documentation site][docs] for detailed information.

## Quick Start

### Get Trivy

Trivy is available in most common distribution channels. The full list of installation options is available in the [Installation] page. Here are a few popular examples:

- `brew install trivy`
- `docker run aquasec/trivy`
- Download binary from &lt;https://github.com/aquasecurity/trivy/releases/latest/&gt;
- See [Installation] for more

Trivy is integrated with many popular platforms and applications. The complete list of integrations is available in the [Ecosystem] page. Here are a few popular examples:

- [GitHub Actions](https://github.com/aquasecurity/trivy-action)
- [Kubernetes operator](https://github.com/aquasecurity/trivy-operator)
- [VS Code plugin](https://github.com/aquasecurity/trivy-vscode-extension)
- See [Ecosystem] for more

### Canary builds
There are canary builds ([Docker Hub](https://hub.docker.com/r/aquasec/trivy/tags?page=1&amp;name=canary), [GitHub](https://github.com/aquasecurity/trivy/pkgs/container/trivy/75776514?tag=canary), [ECR](https://gallery.ecr.aws/aquasecurity/trivy#canary) images and [binaries](https://github.com/aquasecurity/trivy/actions/workflows/canary.yaml)) generated with every push to the main branch.

Please be aware: canary builds might have critical bugs, so they are not recommended for use in production.

### General usage

```bash
trivy &lt;target&gt; [--scanners &lt;scanner1,scanner2&gt;] &lt;subject&gt;
```

Examples:

```bash
trivy image python:3.4-alpine
```

&lt;details&gt;
&lt;summary&gt;Result&lt;/summary&gt;

https://user-images.githubusercontent.com/1161307/171013513-95f18734-233d-45d3-aaf5-d6aec687db0e.mov

&lt;/details&gt;

```bash
trivy fs --scanners vuln,secret,misconfig myproject/
```

&lt;details&gt;
&lt;summary&gt;Result&lt;/summary&gt;

https://user-images.githubusercontent.com/1161307/171013917-b1f37810-f434-465c-b01a-22de036bd9b3.mov

&lt;/details&gt;

```bash
trivy k8s --report summary cluster
```

&lt;details&gt;
&lt;summary&gt;Result&lt;/summary&gt;

![k8s summary](docs/imgs/trivy-k8s.png)

&lt;/details&gt;

## FAQ

### How to pronounce the name &quot;Trivy&quot;?

`tri` is pronounced like **tri**gger, `vy` is pronounced like en**vy**.

## Want more? Check out Aqua

If you liked Trivy, you will love Aqua which builds on top of Trivy to provide even more enhanced capabilities for a complete security management offering.  
You can find a high level comparison table specific to Trivy users [here](https://trivy.dev/docs/latest/commercial/compare/).
In addition check out the &lt;https://aquasec.com&gt; website for more information about our products and services.
If you&#039;d like to contact Aqua or request a demo, please use this form: &lt;https://www.aquasec.com/demo&gt;

## Community

Trivy is an [Aqua Security][aquasec] open source project.  
Learn about our open source work and portfolio [here][oss].  
Contact us about any matter by opening a GitHub Discussion [here][discussions]

Please ensure to abide by our [Code of Conduct][code-of-conduct] during all interactions.

[test]: https://github.com/aquasecurity/trivy/actions/workflows/test.yaml
[test-img]: https://github.com/aquasecurity/trivy/actions/workflows/test.yaml/badge.svg
[go-report]: https://goreportcard.com/report/github.com/aquasecurity/trivy
[go-report-img]: https://goreportcard.com/badge/github.com/aquasecurity/trivy
[release]: https://github.com/aquasecurity/trivy/releases
[release-img]: https://img.shields.io/github/release/aquasecurity/trivy.svg?logo=github
[github-downloads-img]: https://img.shields.io/github/downloads/aquasecurity/trivy/total?logo=github
[docker-pulls]: https://img.shields.io/docker/pulls/aquasec/trivy?logo=docker&amp;label=docker%20pulls%20%2F%20trivy
[license]: https://github.com/aquasecurity/trivy/blob/main/LICENSE
[license-img]: https://img.shields.io/badge/License-Apache%202.0-blue.svg
[homepage]: https://trivy.dev
[docs]: https://trivy.dev/docs/latest/
[pronunciation]: #how-to-pronounce-the-name-trivy
[code-of-conduct]: https://github.com/aquasecurity/community/blob/main/CODE_OF_CONDUCT.md

[Installation]:https://trivy.dev/docs/latest/getting-started/installation/
[Ecosystem]: https://trivy.dev/docs/latest/ecosystem/
[Scanning Coverage]: https://trivy.dev/docs/latest/coverage/

[alpine]: https://ariadne.space/2021/06/08/the-vulnerability-remediation-lifecycle-of-alpine-containers/
[rego]: https://www.openpolicyagent.org/docs/latest/#rego
[sigstore]: https://www.sigstore.dev/

[aquasec]: https://aquasec.com
[oss]: https://www.aquasec.com/products/open-source-projects/
[discussions]: https://github.com/aquasecurity/trivy/discussions
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[containers/kubernetes-mcp-server]]></title>
            <link>https://github.com/containers/kubernetes-mcp-server</link>
            <guid>https://github.com/containers/kubernetes-mcp-server</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:48 GMT</pubDate>
            <description><![CDATA[Model Context Protocol (MCP) server for Kubernetes and OpenShift]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/containers/kubernetes-mcp-server">containers/kubernetes-mcp-server</a></h1>
            <p>Model Context Protocol (MCP) server for Kubernetes and OpenShift</p>
            <p>Language: Go</p>
            <p>Stars: 861</p>
            <p>Forks: 194</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre># Kubernetes MCP Server

[![GitHub License](https://img.shields.io/github/license/containers/kubernetes-mcp-server)](https://github.com/containers/kubernetes-mcp-server/blob/main/LICENSE)
[![npm](https://img.shields.io/npm/v/kubernetes-mcp-server)](https://www.npmjs.com/package/kubernetes-mcp-server)
[![PyPI - Version](https://img.shields.io/pypi/v/kubernetes-mcp-server)](https://pypi.org/project/kubernetes-mcp-server/)
[![GitHub release (latest SemVer)](https://img.shields.io/github/v/release/containers/kubernetes-mcp-server?sort=semver)](https://github.com/containers/kubernetes-mcp-server/releases/latest)
[![Build](https://github.com/containers/kubernetes-mcp-server/actions/workflows/build.yaml/badge.svg)](https://github.com/containers/kubernetes-mcp-server/actions/workflows/build.yaml)

[‚ú® Features](#features) | [üöÄ Getting Started](#getting-started) | [üé• Demos](#demos) | [‚öôÔ∏è Configuration](#configuration) | [üõ†Ô∏è Tools](#tools-and-functionalities) | [üßë‚Äçüíª Development](#development)

https://github.com/user-attachments/assets/be2b67b3-fc1c-4d11-ae46-93deba8ed98e

## ‚ú® Features &lt;a id=&quot;features&quot;&gt;&lt;/a&gt;

A powerful and flexible Kubernetes [Model Context Protocol (MCP)](https://blog.marcnuri.com/model-context-protocol-mcp-introduction) server implementation with support for **Kubernetes** and **OpenShift**.

- **‚úÖ Configuration**:
  - Automatically detect changes in the Kubernetes configuration and update the MCP server.
  - **View** and manage the current [Kubernetes `.kube/config`](https://blog.marcnuri.com/where-is-my-default-kubeconfig-file) or in-cluster configuration.
- **‚úÖ Generic Kubernetes Resources**: Perform operations on **any** Kubernetes or OpenShift resource.
  - Any CRUD operation (Create or Update, Get, List, Delete).
- **‚úÖ Pods**: Perform Pod-specific operations.
  - **List** pods in all namespaces or in a specific namespace.
  - **Get** a pod by name from the specified namespace.
  - **Delete** a pod by name from the specified namespace.
  - **Show logs** for a pod by name from the specified namespace.
  - **Top** gets resource usage metrics for all pods or a specific pod in the specified namespace.
  - **Exec** into a pod and run a command.
  - **Run** a container image in a pod and optionally expose it.
- **‚úÖ Namespaces**: List Kubernetes Namespaces.
- **‚úÖ Events**: View Kubernetes events in all namespaces or in a specific namespace.
- **‚úÖ Projects**: List OpenShift Projects.
- **‚ò∏Ô∏è Helm**:
  - **Install** a Helm chart in the current or provided namespace.
  - **List** Helm releases in all namespaces or in a specific namespace.
  - **Uninstall** a Helm release in the current or provided namespace.

Unlike other Kubernetes MCP server implementations, this **IS NOT** just a wrapper around `kubectl` or `helm` command-line tools.
It is a **Go-based native implementation** that interacts directly with the Kubernetes API server.

There is **NO NEED** for external dependencies or tools to be installed on the system.
If you&#039;re using the native binaries you don&#039;t need to have Node or Python installed on your system.

- **‚úÖ Lightweight**: The server is distributed as a single native binary for Linux, macOS, and Windows.
- **‚úÖ High-Performance / Low-Latency**: Directly interacts with the Kubernetes API server without the overhead of calling and waiting for external commands.
- **‚úÖ Multi-Cluster**: Can interact with multiple Kubernetes clusters simultaneously (as defined in your kubeconfig files).
- **‚úÖ Cross-Platform**: Available as a native binary for Linux, macOS, and Windows, as well as an npm package, a Python package, and container/Docker image.
- **‚úÖ Configurable**: Supports [command-line arguments](#configuration)  to configure the server behavior.
- **‚úÖ Well tested**: The server has an extensive test suite to ensure its reliability and correctness across different Kubernetes environments.

## üöÄ Getting Started &lt;a id=&quot;getting-started&quot;&gt;&lt;/a&gt;

### Requirements

- Access to a Kubernetes cluster.

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Claude Code&lt;/b&gt;&lt;/summary&gt;

Follow the [dedicated Claude Code getting started guide](docs/GETTING_STARTED_CLAUDE_CODE.md) in our [user documentation](docs/).

For a secure production setup with dedicated ServiceAccount and read-only access, also review the [Kubernetes setup guide](docs/GETTING_STARTED_KUBERNETES.md).

&lt;/details&gt;

### Claude Desktop

#### Using npx

If you have npm installed, this is the fastest way to get started with `kubernetes-mcp-server` on Claude Desktop.

Open your `claude_desktop_config.json` and add the mcp server to the list of `mcpServers`:
``` json
{
  &quot;mcpServers&quot;: {
    &quot;kubernetes&quot;: {
      &quot;command&quot;: &quot;npx&quot;,
      &quot;args&quot;: [
        &quot;-y&quot;,
        &quot;kubernetes-mcp-server@latest&quot;
      ]
    }
  }
}
```

### VS Code / VS Code Insiders

Install the Kubernetes MCP server extension in VS Code Insiders by pressing the following link:

[&lt;img src=&quot;https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square&amp;label=Install%20Server&amp;color=0098FF&quot; alt=&quot;Install in VS Code&quot;&gt;](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522kubernetes%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522-y%2522%252C%2522kubernetes-mcp-server%2540latest%2522%255D%257D)
[&lt;img alt=&quot;Install in VS Code Insiders&quot; src=&quot;https://img.shields.io/badge/VS_Code_Insiders-VS_Code_Insiders?style=flat-square&amp;label=Install%20Server&amp;color=24bfa5&quot;&gt;](https://insiders.vscode.dev/redirect?url=vscode-insiders%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522kubernetes%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522-y%2522%252C%2522kubernetes-mcp-server%2540latest%2522%255D%257D)

Alternatively, you can install the extension manually by running the following command:

```shell
# For VS Code
code --add-mcp &#039;{&quot;name&quot;:&quot;kubernetes&quot;,&quot;command&quot;:&quot;npx&quot;,&quot;args&quot;:[&quot;kubernetes-mcp-server@latest&quot;]}&#039;
# For VS Code Insiders
code-insiders --add-mcp &#039;{&quot;name&quot;:&quot;kubernetes&quot;,&quot;command&quot;:&quot;npx&quot;,&quot;args&quot;:[&quot;kubernetes-mcp-server@latest&quot;]}&#039;
```

### Cursor

Install the Kubernetes MCP server extension in Cursor by pressing the following link:

[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=kubernetes-mcp-server&amp;config=eyJjb21tYW5kIjoibnB4IC15IGt1YmVybmV0ZXMtbWNwLXNlcnZlckBsYXRlc3QifQ%3D%3D)

Alternatively, you can install the extension manually by editing the `mcp.json` file:

```json
{
  &quot;mcpServers&quot;: {
    &quot;kubernetes-mcp-server&quot;: {
      &quot;command&quot;: &quot;npx&quot;,
      &quot;args&quot;: [&quot;-y&quot;, &quot;kubernetes-mcp-server@latest&quot;]
    }
  }
}
```

### Goose CLI

[Goose CLI](https://blog.marcnuri.com/goose-on-machine-ai-agent-cli-introduction) is the easiest (and cheapest) way to get rolling with artificial intelligence (AI) agents.

#### Using npm

If you have npm installed, this is the fastest way to get started with `kubernetes-mcp-server`.

Open your goose `config.yaml` and add the mcp server to the list of `mcpServers`:
```yaml
extensions:
  kubernetes:
    command: npx
    args:
      - -y
      - kubernetes-mcp-server@latest

```

## üé• Demos &lt;a id=&quot;demos&quot;&gt;&lt;/a&gt;

### Diagnosing and automatically fixing an OpenShift Deployment

Demo showcasing how Kubernetes MCP server is leveraged by Claude Desktop to automatically diagnose and fix a deployment in OpenShift without any user assistance.

https://github.com/user-attachments/assets/a576176d-a142-4c19-b9aa-a83dc4b8d941

### _Vibe Coding_ a simple game and deploying it to OpenShift

In this demo, I walk you through the process of _Vibe Coding_ a simple game using VS Code and how to leverage [Podman MCP server](https://github.com/manusa/podman-mcp-server) and Kubernetes MCP server to deploy it to OpenShift.

&lt;a href=&quot;https://www.youtube.com/watch?v=l05jQDSrzVI&quot; target=&quot;_blank&quot;&gt;
 &lt;img src=&quot;docs/images/vibe-coding.jpg&quot; alt=&quot;Vibe Coding: Build &amp; Deploy a Game on Kubernetes&quot; width=&quot;240&quot;  /&gt;
&lt;/a&gt;

### Supercharge GitHub Copilot with Kubernetes MCP Server in VS Code - One-Click Setup!

In this demo, I&#039;ll show you how to set up Kubernetes MCP server in VS code just by clicking a link.

&lt;a href=&quot;https://youtu.be/AI4ljYMkgtA&quot; target=&quot;_blank&quot;&gt;
 &lt;img src=&quot;docs/images/kubernetes-mcp-server-github-copilot.jpg&quot; alt=&quot;Supercharge GitHub Copilot with Kubernetes MCP Server in VS Code - One-Click Setup!&quot; width=&quot;240&quot;  /&gt;
&lt;/a&gt;

## ‚öôÔ∏è Configuration &lt;a id=&quot;configuration&quot;&gt;&lt;/a&gt;

The Kubernetes MCP server can be configured using command line (CLI) arguments.

You can run the CLI executable either by using `npx`, `uvx`, or by downloading the [latest release binary](https://github.com/containers/kubernetes-mcp-server/releases/latest).

```shell
# Run the Kubernetes MCP server using npx (in case you have npm and node installed)
npx kubernetes-mcp-server@latest --help
```

```shell
# Run the Kubernetes MCP server using uvx (in case you have uv and python installed)
uvx kubernetes-mcp-server@latest --help
```

```shell
# Run the Kubernetes MCP server using the latest release binary
./kubernetes-mcp-server --help
```

### Configuration Options

| Option                    | Description                                                                                                                                                                                                                                                                                   |
|---------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `--port`                  | Starts the MCP server in Streamable HTTP mode (path /mcp) and Server-Sent Event (SSE) (path /sse) mode and listens on the specified port .                                                                                                                                                    |
| `--log-level`             | Sets the logging level (values [from 0-9](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md)). Similar to [kubectl logging levels](https://kubernetes.io/docs/reference/kubectl/quick-reference/#kubectl-output-verbosity-and-debugging). |
| `--config`                | (Optional) Path to the main TOML configuration file. See [Drop-in Configuration](#drop-in-configuration) section below for details.                                                                                                                                                          |
| `--config-dir`            | (Optional) Path to drop-in configuration directory. Files are loaded in lexical (alphabetical) order. Defaults to `conf.d` relative to the main config file if `--config` is specified. See [Drop-in Configuration](#drop-in-configuration) section below for details.                       |
| `--kubeconfig`            | Path to the Kubernetes configuration file. If not provided, it will try to resolve the configuration (in-cluster, default location, etc.).                                                                                                                                                    |
| `--list-output`           | Output format for resource list operations (one of: yaml, table) (default &quot;table&quot;)                                                                                                                                                                                                            |
| `--read-only`             | If set, the MCP server will run in read-only mode, meaning it will not allow any write operations (create, update, delete) on the Kubernetes cluster. This is useful for debugging or inspecting the cluster without making changes.                                                          |
| `--disable-destructive`   | If set, the MCP server will disable all destructive operations (delete, update, etc.) on the Kubernetes cluster. This is useful for debugging or inspecting the cluster without accidentally making changes. This option has no effect when `--read-only` is used.                            |
| `--toolsets`              | Comma-separated list of toolsets to enable. Check the [üõ†Ô∏è Tools and Functionalities](#tools-and-functionalities) section for more information.                                                                                                                                               |
| `--disable-multi-cluster` | If set, the MCP server will disable multi-cluster support and will only use the current context from the kubeconfig file. This is useful if you want to restrict the MCP server to a single cluster.                                                                                          |

### Drop-in Configuration &lt;a id=&quot;drop-in-configuration&quot;&gt;&lt;/a&gt;

The Kubernetes MCP server supports flexible configuration through both a main config file and drop-in files. **Both are optional** - you can use either, both, or neither (server will use built-in defaults).

#### Configuration Loading Order

Configuration values are loaded and merged in the following order (later sources override earlier ones):

1. **Internal Defaults** - Always loaded (hardcoded default values)
2. **Main Configuration File** - Optional, loaded via `--config` flag
3. **Drop-in Files** - Optional, loaded from `--config-dir` in **lexical (alphabetical) order**

#### How Drop-in Files Work

- **Default Directory**: If `--config-dir` is not specified, the server looks for drop-in files in `conf.d/` relative to the main config file&#039;s directory (when `--config` is provided)
- **File Naming**: Use numeric prefixes to control loading order (e.g., `00-base.toml`, `10-cluster.toml`, `99-override.toml`)
- **File Extension**: Only `.toml` files are processed; dotfiles (starting with `.`) are ignored
- **Partial Configuration**: Drop-in files can contain only a subset of configuration options
- **Merge Behavior**: Values present in a drop-in file override previous values; missing values are preserved

#### Dynamic Configuration Reload

To reload configuration after modifying config files, send a `SIGHUP` signal to the running server process.

**Prerequisite**: SIGHUP reload requires the server to be started with either the `--config` flag or `--config-dir` flag (or both). If neither is specified, SIGHUP signals will be ignored.

**How to reload:**

```shell
# Find the process ID
ps aux | grep kubernetes-mcp-server

# Send SIGHUP to reload configuration
kill -HUP &lt;pid&gt;

# Or use pkill
pkill -HUP kubernetes-mcp-server
```

The server will:
- Reload the main config file and all drop-in files
- Update configuration values (log level, output format, etc.)
- Rebuild the toolset registry with new tool configurations
- Log the reload status

**Note**: Changing `kubeconfig` or cluster-related settings requires a server restart. Only tool configurations, log levels, and output formats can be reloaded dynamically.

**Note**: SIGHUP reload is not available on Windows. On Windows, restart the server to reload configuration.

#### Example: Using Both Config Methods

**Command (using default `conf.d` directory):**
```shell
kubernetes-mcp-server --config /etc/kubernetes-mcp-server/config.toml
```

**Directory structure:**
```
/etc/kubernetes-mcp-server/
‚îú‚îÄ‚îÄ config.toml              # Main configuration
‚îî‚îÄ‚îÄ conf.d/                  # Default drop-in directory (automatically loaded)
    ‚îú‚îÄ‚îÄ 00-base.toml         # Base overrides
    ‚îú‚îÄ‚îÄ 10-toolsets.toml     # Toolset-specific config
    ‚îî‚îÄ‚îÄ 99-local.toml        # Local overrides
```

**Command (with explicit `--config-dir`):**
```shell
kubernetes-mcp-server --config /etc/kubernetes-mcp-server/config.toml \
                      --config-dir /etc/kubernetes-mcp-server/config.d/
```

**Example drop-in file** (`10-toolsets.toml`):
```toml
# Override only the toolsets - all other config preserved
toolsets = [&quot;core&quot;, &quot;config&quot;, &quot;helm&quot;, &quot;logs&quot;]
```

**Example drop-in file** (`99-local.toml`):
```toml
# Local development overrides
log_level = 9
read_only = true
```

**To apply changes:**
```shell
# Edit config files
vim /etc/kubernetes-mcp-server/conf.d/99-local.toml

# Reload without restarting
pkill -HUP kubernetes-mcp-server
```

## üõ†Ô∏è Tools and Functionalities &lt;a id=&quot;tools-and-functionalities&quot;&gt;&lt;/a&gt;

The Kubernetes MCP server supports enabling or disabling specific groups of tools and functionalities (tools, resources, prompts, and so on) via the `--toolsets` command-line flag or `toolsets` configuration option.
This allows you to control which Kubernetes functionalities are available to your AI tools.
Enabling only the toolsets you need can help reduce the context size and improve the LLM&#039;s tool selection accuracy.

### Available Toolsets

The following sets of tools are available (toolsets marked with ‚úì in the Default column are enabled by default):

&lt;!-- AVAILABLE-TOOLSETS-START --&gt;

| Toolset  | Description                                                                                                                                                          | Default |
|----------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------|
| config   | View and manage the current local Kubernetes configuration (kubeconfig)                                                                                              | ‚úì       |
| core     | Most common tools for Kubernetes management (Pods, Generic Resources, Events, etc.)                                                                                  | ‚úì       |
| helm     | Tools for managing Helm charts and releases                                                                                                                          | ‚úì       |
| kiali    | Most common tools for managing Kiali, check the [Kiali documentation](https://github.com/containers/kubernetes-mcp-server/blob/main/docs/KIALI.md) for more details. |         |
| kubevirt | KubeVirt virtual machine management tools                                                                                                                            |         |

&lt;!-- AVAILABLE-TOOLSETS-END --&gt;

### Tools

In case multi-cluster support is enabled (default) and you have access to multiple clusters, all applicable tools will include an additional `context` argument to specify the Kubernetes context (cluster) to use for that operation.

&lt;!-- AVAILABLE-TOOLSETS-TOOLS-START --&gt;

&lt;details&gt;

&lt;summary&gt;config&lt;/summary&gt;

- **configuration_contexts_list** - List all available context names and associated server urls from the kubeconfig file

- **configuration_view** - Get the current Kubernetes configuration content as a kubeconfig YAML
  - `minified` (`boolean`) - Return a minified version of the configuration. If set to true, keeps only the current-context and the relevant pieces of the configuration for that context. If set to false, all contexts, clusters, auth-infos, and users are returned in the configuration. (Optional, default true)

&lt;/details&gt;

&lt;details&gt;

&lt;summary&gt;core&lt;/summary&gt;

- **events_list** - List all the Kubernetes events in the current cluster from all namespaces
  - `namespace` (`string`) - Optional Namespace to retrieve the events from. If not provided, will list events from all namespaces

- **namespaces_list** - List all the Kubernetes namespaces in the current cluster

- **projects_list** - List all the OpenShift projects in the current cluster

- **nodes_log** - Get logs from a Kubernetes node (kubelet, kube-proxy, or other system logs). This accesses node logs through the Kubernetes API proxy to the kubelet
  - `name` (`string`) **(required)** - Name of the node to get logs from
  - `query` (`string`) **(required)** - query specifies services(s) or files from which to ret

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[github/gh-ost]]></title>
            <link>https://github.com/github/gh-ost</link>
            <guid>https://github.com/github/gh-ost</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:47 GMT</pubDate>
            <description><![CDATA[GitHub's Online Schema-migration Tool for MySQL]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/github/gh-ost">github/gh-ost</a></h1>
            <p>GitHub's Online Schema-migration Tool for MySQL</p>
            <p>Language: Go</p>
            <p>Stars: 13,087</p>
            <p>Forks: 1,342</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre># gh-ost

[![ci](https://github.com/github/gh-ost/actions/workflows/ci.yml/badge.svg)](https://github.com/github/gh-ost/actions/workflows/ci.yml) [![replica-tests](https://github.com/github/gh-ost/actions/workflows/replica-tests.yml/badge.svg)](https://github.com/github/gh-ost/actions/workflows/replica-tests.yml) [![downloads](https://img.shields.io/github/downloads/github/gh-ost/total.svg)](https://github.com/github/gh-ost/releases) [![release](https://img.shields.io/github/release/github/gh-ost.svg)](https://github.com/github/gh-ost/releases)

#### GitHub&#039;s online schema migration for MySQL &lt;img src=&quot;doc/images/gh-ost-logo-light-160.png&quot; align=&quot;right&quot;&gt;

 `gh-ost` is a triggerless online schema migration solution for MySQL. It is testable and provides pausability, dynamic control/reconfiguration, auditing, and many operational perks.

`gh-ost` produces a light workload on the master throughout the migration, decoupled from the existing workload on the migrated table.

It has been designed based on years of experience with existing solutions, and changes the paradigm of table migrations.



## How?

All existing online-schema-change tools operate in similar manner: they create a _ghost_ table in the likeness of your original table, migrate that table while empty, slowly and incrementally copy data from your original table to the _ghost_ table, meanwhile propagating ongoing changes (any `INSERT`, `DELETE`, `UPDATE` applied to your table) to the _ghost_ table. Finally, at the right time, they replace your original table with the _ghost_ table.

`gh-ost` uses the same pattern. However it differs from all existing tools by not using triggers. We have recognized the triggers to be the source of [many limitations and risks](doc/why-triggerless.md).

Instead, `gh-ost` [uses the binary log stream](doc/triggerless-design.md) to capture table changes, and asynchronously applies them onto the _ghost_ table. `gh-ost` takes upon itself some tasks that other tools leave for the database to perform. As result, `gh-ost` has greater control over the migration process; can truly suspend it; can truly decouple the migration&#039;s write load from the master&#039;s workload.

In addition, it offers many [operational perks](doc/perks.md) that make it safer, trustworthy and fun to use.

![gh-ost general flow](doc/images/gh-ost-general-flow.png)

## Highlights

- Build your trust in `gh-ost` by testing it on replicas. `gh-ost` will issue same flow as it would have on the master, to migrate a table on a replica, without actually replacing the original table, leaving the replica with two tables you can then compare and satisfy yourself that the tool operates correctly. This is how we continuously test `gh-ost` in production.
- True pause: when `gh-ost` [throttles](doc/throttle.md), it truly ceases writes on master: no row copies and no ongoing events processing. By throttling, you return your master to its original workload
- Dynamic control: you can [interactively](doc/interactive-commands.md) reconfigure `gh-ost`, even as migration still runs. You may forcibly initiate throttling.
- Auditing: you may query `gh-ost` for status. `gh-ost` listens on unix socket or TCP.
- Control over cut-over phase: `gh-ost` can be instructed to postpone what is probably the most critical step: the swap of tables, until such time that you&#039;re comfortably available. No need to worry about ETA being outside office hours.
- External [hooks](doc/hooks.md) can couple `gh-ost` with your particular environment.

Please refer to the [docs](doc) for more information. No, really, read the [docs](doc).

## Usage

The [cheatsheet](doc/cheatsheet.md) has it all. You may be interested in invoking `gh-ost` in various modes:

- a _noop_ migration (merely testing that the migration is valid and good to go)
- a real migration, utilizing a replica (the migration runs on the master; `gh-ost` figures out identities of servers involved. Required mode if your master uses Statement Based Replication)
- a real migration, run directly on the master (but `gh-ost` prefers the former)
- a real migration on a replica (master untouched)
- a test migration on a replica, the way for you to build trust with `gh-ost`&#039;s operation.

Our tips:

- [Testing above all](doc/testing-on-replica.md), try out `--test-on-replica` first few times. Better yet, make it continuous. We have multiple replicas where we iterate our entire fleet of production tables, migrating them one by one, checksumming the results, verifying migration is good.
- For each master migration, first issue a _noop_
- Then issue the real thing via `--execute`.

More tips:

- Use `--exact-rowcount` for accurate progress indication
- Use `--postpone-cut-over-flag-file` to gain control over cut-over timing
- Get familiar with the [interactive commands](doc/interactive-commands.md)

Also see:

- [requirements and limitations](doc/requirements-and-limitations.md)
- [common questions](doc/questions.md)
- [what if?](doc/what-if.md)
- [the fine print](doc/the-fine-print.md)
- [Community questions](https://github.com/github/gh-ost/issues?q=label%3Aquestion)
- [Using `gh-ost` on AWS RDS](doc/rds.md)
- [Using `gh-ost` on Azure Database for MySQL](doc/azure.md)

## What&#039;s in a name?

Originally this was named `gh-osc`: GitHub Online Schema Change, in the likes of [Facebook online schema change](https://www.facebook.com/notes/mysql-at-facebook/online-schema-change-for-mysql/430801045932/) and [pt-online-schema-change](https://www.percona.com/doc/percona-toolkit/2.2/pt-online-schema-change.html).

But then a rare genetic mutation happened, and the `c` transformed into `t`. And that sent us down the path of trying to figure out a new acronym. `gh-ost` (pronounce: _Ghost_), stands for GitHub&#039;s Online Schema Transmogrifier/Translator/Transformer/Transfigurator

## License

`gh-ost` is licensed under the [MIT license](https://github.com/github/gh-ost/blob/master/LICENSE)

`gh-ost` uses 3rd party libraries, each with their own license. These are found [here](https://github.com/github/gh-ost/tree/master/vendor).

## Community

`gh-ost` is released at a stable state, but with mileage to go. We are [open to pull requests](https://github.com/github/gh-ost/blob/master/.github/CONTRIBUTING.md). Please first discuss your intentions via [Issues](https://github.com/github/gh-ost/issues).

We develop `gh-ost` at GitHub and for the community. We may have different priorities than others. From time to time we may suggest a contribution that is not on our immediate roadmap but which may appeal to others.

Please see [Coding gh-ost](doc/coding-ghost.md) for a guide to getting started developing with gh-ost.

## Download/binaries/source

`gh-ost` is now GA and stable.

`gh-ost` is available in binary format for Linux and Mac OS/X

[Download latest release here](https://github.com/github/gh-ost/releases/latest)

`gh-ost` is a Go project; it is built with Go `1.15` and above. To build on your own, use either:
- [script/build](https://github.com/github/gh-ost/blob/master/script/build) - this is the same build script used by CI hence the authoritative; artifact is `./bin/gh-ost` binary.
- [build.sh](https://github.com/github/gh-ost/blob/master/build.sh) for building `tar.gz` artifacts in `/tmp/gh-ost-release`

Generally speaking, `master` branch is stable, but only [releases](https://github.com/github/gh-ost/releases) are to be used in production.

## Authors

`gh-ost` is designed, authored, reviewed and tested by the database infrastructure team at GitHub:
- [@jonahberquist](https://github.com/jonahberquist)
- [@ggunson](https://github.com/ggunson)
- [@tomkrouper](https://github.com/tomkrouper)
- [@shlomi-noach](https://github.com/shlomi-noach)
- [@jessbreckenridge](https://github.com/jessbreckenridge)
- [@gtowey](https://github.com/gtowey)
- [@timvaillancourt](https://github.com/timvaillancourt)
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[ollama/ollama]]></title>
            <link>https://github.com/ollama/ollama</link>
            <guid>https://github.com/ollama/ollama</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:46 GMT</pubDate>
            <description><![CDATA[Get up and running with OpenAI gpt-oss, DeepSeek-R1, Gemma 3 and other models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ollama/ollama">ollama/ollama</a></h1>
            <p>Get up and running with OpenAI gpt-oss, DeepSeek-R1, Gemma 3 and other models.</p>
            <p>Language: Go</p>
            <p>Stars: 157,426</p>
            <p>Forks: 13,887</p>
            <p>Stars today: 67 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
¬† &lt;a href=&quot;https://ollama.com&quot;&gt;
    &lt;img alt=&quot;ollama&quot; width=&quot;240&quot; src=&quot;https://github.com/ollama/ollama/assets/3325447/0d0b44e2-8f4a-4e99-9b52-a5c1c741c8f7&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

# Ollama

Get up and running with large language models.

### macOS

[Download](https://ollama.com/download/Ollama.dmg)

### Windows

[Download](https://ollama.com/download/OllamaSetup.exe)

### Linux

```shell
curl -fsSL https://ollama.com/install.sh | sh
```

[Manual install instructions](https://docs.ollama.com/linux#manual-install)

### Docker

The official [Ollama Docker image](https://hub.docker.com/r/ollama/ollama) `ollama/ollama` is available on Docker Hub.

### Libraries

- [ollama-python](https://github.com/ollama/ollama-python)
- [ollama-js](https://github.com/ollama/ollama-js)

### Community

- [Discord](https://discord.gg/ollama)
- [Reddit](https://reddit.com/r/ollama)

## Quickstart

To run and chat with [Gemma 3](https://ollama.com/library/gemma3):

```shell
ollama run gemma3
```

## Model library

Ollama supports a list of models available on [ollama.com/library](https://ollama.com/library &#039;ollama model library&#039;)

Here are some example models that can be downloaded:

| Model              | Parameters | Size  | Download                         |
| ------------------ | ---------- | ----- | -------------------------------- |
| Gemma 3            | 1B         | 815MB | `ollama run gemma3:1b`           |
| Gemma 3            | 4B         | 3.3GB | `ollama run gemma3`              |
| Gemma 3            | 12B        | 8.1GB | `ollama run gemma3:12b`          |
| Gemma 3            | 27B        | 17GB  | `ollama run gemma3:27b`          |
| QwQ                | 32B        | 20GB  | `ollama run qwq`                 |
| DeepSeek-R1        | 7B         | 4.7GB | `ollama run deepseek-r1`         |
| DeepSeek-R1        | 671B       | 404GB | `ollama run deepseek-r1:671b`    |
| Llama 4            | 109B       | 67GB  | `ollama run llama4:scout`        |
| Llama 4            | 400B       | 245GB | `ollama run llama4:maverick`     |
| Llama 3.3          | 70B        | 43GB  | `ollama run llama3.3`            |
| Llama 3.2          | 3B         | 2.0GB | `ollama run llama3.2`            |
| Llama 3.2          | 1B         | 1.3GB | `ollama run llama3.2:1b`         |
| Llama 3.2 Vision   | 11B        | 7.9GB | `ollama run llama3.2-vision`     |
| Llama 3.2 Vision   | 90B        | 55GB  | `ollama run llama3.2-vision:90b` |
| Llama 3.1          | 8B         | 4.7GB | `ollama run llama3.1`            |
| Llama 3.1          | 405B       | 231GB | `ollama run llama3.1:405b`       |
| Phi 4              | 14B        | 9.1GB | `ollama run phi4`                |
| Phi 4 Mini         | 3.8B       | 2.5GB | `ollama run phi4-mini`           |
| Mistral            | 7B         | 4.1GB | `ollama run mistral`             |
| Moondream 2        | 1.4B       | 829MB | `ollama run moondream`           |
| Neural Chat        | 7B         | 4.1GB | `ollama run neural-chat`         |
| Starling           | 7B         | 4.1GB | `ollama run starling-lm`         |
| Code Llama         | 7B         | 3.8GB | `ollama run codellama`           |
| Llama 2 Uncensored | 7B         | 3.8GB | `ollama run llama2-uncensored`   |
| LLaVA              | 7B         | 4.5GB | `ollama run llava`               |
| Granite-3.3         | 8B         | 4.9GB | `ollama run granite3.3`          |

&gt; [!NOTE]
&gt; You should have at least 8 GB of RAM available to run the 7B models, 16 GB to run the 13B models, and 32 GB to run the 33B models.

## Customize a model

### Import from GGUF

Ollama supports importing GGUF models in the Modelfile:

1. Create a file named `Modelfile`, with a `FROM` instruction with the local filepath to the model you want to import.

   ```
   FROM ./vicuna-33b.Q4_0.gguf
   ```

2. Create the model in Ollama

   ```shell
   ollama create example -f Modelfile
   ```

3. Run the model

   ```shell
   ollama run example
   ```

### Import from Safetensors

See the [guide](https://docs.ollama.com/import) on importing models for more information.

### Customize a prompt

Models from the Ollama library can be customized with a prompt. For example, to customize the `llama3.2` model:

```shell
ollama pull llama3.2
```

Create a `Modelfile`:

```
FROM llama3.2

# set the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 1

# set the system message
SYSTEM &quot;&quot;&quot;
You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.
&quot;&quot;&quot;
```

Next, create and run the model:

```
ollama create mario -f ./Modelfile
ollama run mario
&gt;&gt;&gt; hi
Hello! It&#039;s your friend Mario.
```

For more information on working with a Modelfile, see the [Modelfile](https://docs.ollama.com/modelfile) documentation.

## CLI Reference

### Create a model

`ollama create` is used to create a model from a Modelfile.

```shell
ollama create mymodel -f ./Modelfile
```

### Pull a model

```shell
ollama pull llama3.2
```

&gt; This command can also be used to update a local model. Only the diff will be pulled.

### Remove a model

```shell
ollama rm llama3.2
```

### Copy a model

```shell
ollama cp llama3.2 my-model
```

### Multiline input

For multiline input, you can wrap text with `&quot;&quot;&quot;`:

```
&gt;&gt;&gt; &quot;&quot;&quot;Hello,
... world!
... &quot;&quot;&quot;
I&#039;m a basic program that prints the famous &quot;Hello, world!&quot; message to the console.
```

### Multimodal models

```
ollama run llava &quot;What&#039;s in this image? /Users/jmorgan/Desktop/smile.png&quot;
```

&gt; **Output**: The image features a yellow smiley face, which is likely the central focus of the picture.

### Pass the prompt as an argument

```shell
ollama run llama3.2 &quot;Summarize this file: $(cat README.md)&quot;
```

&gt; **Output**: Ollama is a lightweight, extensible framework for building and running language models on the local machine. It provides a simple API for creating, running, and managing models, as well as a library of pre-built models that can be easily used in a variety of applications.

### Show model information

```shell
ollama show llama3.2
```

### List models on your computer

```shell
ollama list
```

### List which models are currently loaded

```shell
ollama ps
```

### Stop a model which is currently running

```shell
ollama stop llama3.2
```

### Generate embeddings from the CLI

```shell
ollama run embeddinggemma &quot;Your text to embed&quot;
```

You can also pipe text for scripted workflows:

```shell
echo &quot;Your text to embed&quot; | ollama run embeddinggemma
```

### Start Ollama

`ollama serve` is used when you want to start ollama without running the desktop application.

## Building

See the [developer guide](https://github.com/ollama/ollama/blob/main/docs/development.md)

### Running local builds

Next, start the server:

```shell
./ollama serve
```

Finally, in a separate shell, run a model:

```shell
./ollama run llama3.2
```

## REST API

Ollama has a REST API for running and managing models.

### Generate a response

```shell
curl http://localhost:11434/api/generate -d &#039;{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;prompt&quot;:&quot;Why is the sky blue?&quot;
}&#039;
```

### Chat with a model

```shell
curl http://localhost:11434/api/chat -d &#039;{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;messages&quot;: [
    { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;why is the sky blue?&quot; }
  ]
}&#039;
```

See the [API documentation](./docs/api.md) for all endpoints.

## Community Integrations

### Web &amp; Desktop

- [Open WebUI](https://github.com/open-webui/open-webui)
- [SwiftChat (macOS with ReactNative)](https://github.com/aws-samples/swift-chat)
- [Enchanted (macOS native)](https://github.com/AugustDev/enchanted)
- [Hollama](https://github.com/fmaclen/hollama)
- [Lollms WebUI (Single user)](https://github.com/ParisNeo/lollms-webui)
- [Lollms (Multi users)](https://github.com/ParisNeo/lollms)
- [LibreChat](https://github.com/danny-avila/LibreChat)
- [Bionic GPT](https://github.com/bionic-gpt/bionic-gpt)
- [HTML UI](https://github.com/rtcfirefly/ollama-ui)
- [AI-UI](https://github.com/bajahaw/ai-ui)
- [Saddle](https://github.com/jikkuatwork/saddle)
- [TagSpaces](https://www.tagspaces.org) (A platform for file-based apps, [utilizing Ollama](https://docs.tagspaces.org/ai/) for the generation of tags and descriptions)
- [Chatbot UI](https://github.com/ivanfioravanti/chatbot-ollama)
- [Chatbot UI v2](https://github.com/mckaywrigley/chatbot-ui)
- [Typescript UI](https://github.com/ollama-interface/Ollama-Gui?tab=readme-ov-file)
- [Minimalistic React UI for Ollama Models](https://github.com/richawo/minimal-llm-ui)
- [Ollamac](https://github.com/kevinhermawan/Ollamac)
- [big-AGI](https://github.com/enricoros/big-AGI)
- [Cheshire Cat assistant framework](https://github.com/cheshire-cat-ai/core)
- [Amica](https://github.com/semperai/amica)
- [chatd](https://github.com/BruceMacD/chatd)
- [Ollama-SwiftUI](https://github.com/kghandour/Ollama-SwiftUI)
- [Dify.AI](https://github.com/langgenius/dify)
- [MindMac](https://mindmac.app)
- [NextJS Web Interface for Ollama](https://github.com/jakobhoeg/nextjs-ollama-llm-ui)
- [Msty](https://msty.app)
- [Chatbox](https://github.com/Bin-Huang/Chatbox)
- [WinForm Ollama Copilot](https://github.com/tgraupmann/WinForm_Ollama_Copilot)
- [NextChat](https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web) with [Get Started Doc](https://docs.nextchat.dev/models/ollama)
- [Alpaca WebUI](https://github.com/mmo80/alpaca-webui)
- [OllamaGUI](https://github.com/enoch1118/ollamaGUI)
- [OpenAOE](https://github.com/InternLM/OpenAOE)
- [Odin Runes](https://github.com/leonid20000/OdinRunes)
- [LLM-X](https://github.com/mrdjohnson/llm-x) (Progressive Web App)
- [AnythingLLM (Docker + MacOs/Windows/Linux native app)](https://github.com/Mintplex-Labs/anything-llm)
- [Ollama Basic Chat: Uses HyperDiv Reactive UI](https://github.com/rapidarchitect/ollama_basic_chat)
- [Ollama-chats RPG](https://github.com/drazdra/ollama-chats)
- [IntelliBar](https://intellibar.app/) (AI-powered assistant for macOS)
- [Jirapt](https://github.com/AliAhmedNada/jirapt) (Jira Integration to generate issues, tasks, epics)
- [ojira](https://github.com/AliAhmedNada/ojira) (Jira chrome plugin to easily generate descriptions for tasks)
- [QA-Pilot](https://github.com/reid41/QA-Pilot) (Interactive chat tool that can leverage Ollama models for rapid understanding and navigation of GitHub code repositories)
- [ChatOllama](https://github.com/sugarforever/chat-ollama) (Open Source Chatbot based on Ollama with Knowledge Bases)
- [CRAG Ollama Chat](https://github.com/Nagi-ovo/CRAG-Ollama-Chat) (Simple Web Search with Corrective RAG)
- [RAGFlow](https://github.com/infiniflow/ragflow) (Open-source Retrieval-Augmented Generation engine based on deep document understanding)
- [StreamDeploy](https://github.com/StreamDeploy-DevRel/streamdeploy-llm-app-scaffold) (LLM Application Scaffold)
- [chat](https://github.com/swuecho/chat) (chat web app for teams)
- [Lobe Chat](https://github.com/lobehub/lobe-chat) with [Integrating Doc](https://lobehub.com/docs/self-hosting/examples/ollama)
- [Ollama RAG Chatbot](https://github.com/datvodinh/rag-chatbot.git) (Local Chat with multiple PDFs using Ollama and RAG)
- [BrainSoup](https://www.nurgo-software.com/products/brainsoup) (Flexible native client with RAG &amp; multi-agent automation)
- [macai](https://github.com/Renset/macai) (macOS client for Ollama, ChatGPT, and other compatible API back-ends)
- [RWKV-Runner](https://github.com/josStorer/RWKV-Runner) (RWKV offline LLM deployment tool, also usable as a client for ChatGPT and Ollama)
- [Ollama Grid Search](https://github.com/dezoito/ollama-grid-search) (app to evaluate and compare models)
- [Olpaka](https://github.com/Otacon/olpaka) (User-friendly Flutter Web App for Ollama)
- [Casibase](https://casibase.org) (An open source AI knowledge base and dialogue system combining the latest RAG, SSO, ollama support, and multiple large language models.)
- [OllamaSpring](https://github.com/CrazyNeil/OllamaSpring) (Ollama Client for macOS)
- [LLocal.in](https://github.com/kartikm7/llocal) (Easy to use Electron Desktop Client for Ollama)
- [Shinkai Desktop](https://github.com/dcSpark/shinkai-apps) (Two click install Local AI using Ollama + Files + RAG)
- [AiLama](https://github.com/zeyoyt/ailama) (A Discord User App that allows you to interact with Ollama anywhere in Discord)
- [Ollama with Google Mesop](https://github.com/rapidarchitect/ollama_mesop/) (Mesop Chat Client implementation with Ollama)
- [R2R](https://github.com/SciPhi-AI/R2R) (Open-source RAG engine)
- [Ollama-Kis](https://github.com/elearningshow/ollama-kis) (A simple easy-to-use GUI with sample custom LLM for Drivers Education)
- [OpenGPA](https://opengpa.org) (Open-source offline-first Enterprise Agentic Application)
- [Painting Droid](https://github.com/mateuszmigas/painting-droid) (Painting app with AI integrations)
- [Kerlig AI](https://www.kerlig.com/) (AI writing assistant for macOS)
- [AI Studio](https://github.com/MindWorkAI/AI-Studio)
- [Sidellama](https://github.com/gyopak/sidellama) (browser-based LLM client)
- [LLMStack](https://github.com/trypromptly/LLMStack) (No-code multi-agent framework to build LLM agents and workflows)
- [BoltAI for Mac](https://boltai.com) (AI Chat Client for Mac)
- [Harbor](https://github.com/av/harbor) (Containerized LLM Toolkit with Ollama as default backend)
- [PyGPT](https://github.com/szczyglis-dev/py-gpt) (AI desktop assistant for Linux, Windows, and Mac)
- [Alpaca](https://github.com/Jeffser/Alpaca) (An Ollama client application for Linux and macOS made with GTK4 and Adwaita)
- [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT/blob/master/docs/content/platform/ollama.md) (AutoGPT Ollama integration)
- [Go-CREW](https://www.jonathanhecl.com/go-crew/) (Powerful Offline RAG in Golang)
- [PartCAD](https://github.com/openvmp/partcad/) (CAD model generation with OpenSCAD and CadQuery)
- [Ollama4j Web UI](https://github.com/ollama4j/ollama4j-web-ui) - Java-based Web UI for Ollama built with Vaadin, Spring Boot, and Ollama4j
- [PyOllaMx](https://github.com/kspviswa/pyOllaMx) - macOS application capable of chatting with both Ollama and Apple MLX models.
- [Cline](https://github.com/cline/cline) - Formerly known as Claude Dev is a VS Code extension for multi-file/whole-repo coding
- [Void](https://github.com/voideditor/void) (Open source AI code editor and Cursor alternative)
- [Cherry Studio](https://github.com/kangfenmao/cherry-studio) (Desktop client with Ollama support)
- [ConfiChat](https://github.com/1runeberg/confichat) (Lightweight, standalone, multi-platform, and privacy-focused LLM chat interface with optional encryption)
- [Archyve](https://github.com/nickthecook/archyve) (RAG-enabling document library)
- [crewAI with Mesop](https://github.com/rapidarchitect/ollama-crew-mesop) (Mesop Web Interface to run crewAI with Ollama)
- [Tkinter-based client](https://github.com/chyok/ollama-gui) (Python tkinter-based Client for Ollama)
- [LLMChat](https://github.com/trendy-design/llmchat) (Privacy focused, 100% local, intuitive all-in-one chat interface)
- [Local Multimodal AI Chat](https://github.com/Leon-Sander/Local-Multimodal-AI-Chat) (Ollama-based LLM Chat with support for multiple features, including PDF RAG, voice chat, image-based interactions, and integration with OpenAI.)
- [ARGO](https://github.com/xark-argo/argo) (Locally download and run Ollama and Huggingface models with RAG and deep research on Mac/Windows/Linux)
- [OrionChat](https://github.com/EliasPereirah/OrionChat) - OrionChat is a web interface for chatting with different AI providers
- [G1](https://github.com/bklieger-groq/g1) (Prototype of using prompting strategies to improve the LLM&#039;s reasoning through o1-like reasoning chains.)
- [Web management](https://github.com/lemonit-eric-mao/ollama-web-management) (Web management page)
- [Promptery](https://github.com/promptery/promptery) (desktop client for Ollama.)
- [Ollama App](https://github.com/JHubi1/ollama-app) (Modern and easy-to-use multi-platform client for Ollama)
- [chat-ollama](https://github.com/annilq/chat-ollama) (a React Native client for Ollama)
- [SpaceLlama](https://github.com/tcsenpai/spacellama) (Firefox and Chrome extension to quickly summarize web pages with ollama in a sidebar)
- [YouLama](https://github.com/tcsenpai/youlama) (Webapp to quickly summarize any YouTube video, supporting Invidious as well)
- [DualMind](https://github.com/tcsenpai/dualmind) (Experimental app allowing two models to talk to each other in the terminal or in a web interface)
- [ollamarama-matrix](https://github.com/h1ddenpr0cess20/ollamarama-matrix) (Ollama chatbot for the Matrix chat protocol)
- [ollama-chat-app](https://github.com/anan1213095357/ollama-chat-app) (Flutter-based chat app)
- [Perfect Memory AI](https://www.perfectmemory.ai/) (Productivity AI assists personalized by what you have seen on your screen, heard, and said in the meetings)
- [Hexabot](https://github.com/hexastack/hexabot) (A conversational AI builder)
- [Reddit Rate](https://github.com/rapidarchitect/reddit_analyzer) (Search and Rate Reddit topics with a weighted summation)
- [OpenTalkGpt](https://github.com/adarshM84/OpenTalkGpt) (Chrome Extension to manage open-source models supported by Ollama, create custom models, and chat with models from a user-friendly UI)
- [VT](https://github.com/vinhnx/vt.ai) (A minimal multimodal AI chat app, with dynamic conversation routing. Supports local models via Ollama)
- [Nosia](https://github.com/nosia-ai/nosia) (Easy to install and use RAG platform based on Ollama)
- [Witsy](https://github.com/nbonamy/witsy) (An AI Desktop application available for Mac/Windows/Linux)
- [Abbey](https://github.com/US-Artificial-Intelligence/abbey) (A configurable AI interface server with notebooks, document storage, and YouTube support)
- [Minima](https://github.com/dmayboroda/minima) (RAG with on-premises or fully local workflow)
- [aidful-ollama-model-delete](https://github.com/AidfulAI/aidful-ollama-model-delete) (User interface for simplified model cleanup)
- [Perplexica](https://github.com/ItzCrazyKns/Perplexica) (An AI-powered search engine &amp; an open-source alternative to Perplexity AI)
- [Ollama Chat WebUI for Docker ](https://github.com/oslook/ollama-webui) (Support for local docker deployment, lightweight ollama webui)
- [AI Toolkit for Visual Studio Code](https://aka.ms/ai-tooklit/ollama-docs) (Microsoft-official VS Code extension to chat, test, evaluate models with Ollama support, and use them in your AI applications.)
- [MinimalNextOllamaChat](https://github.com/anilkay/MinimalNextOllamaChat) (Minimal Web UI for Chat and Model Control)
- [Chipper](https://github.com/TilmanGriesel/chipper) AI interface for tinkerers (Ollama, Haystack RAG, Python)
- [ChibiChat](https://github.com/CosmicEventHorizon/ChibiChat) (Kotlin-based Android app to chat with Ollama and Koboldcpp API endpoints)
- [LocalLLM](https://github.com/qusaismael/localllm) (Minimal Web-App to run ollama models on it with a GUI)
- [Ollamazing](https://github.com/buiducnhat/ollamazing) (Web extension to run Ollama models)
- [OpenDeepResearcher-via-searxng](https://github.com/benhaotang/OpenDeepResearcher-via-searxng) (A Deep Research equivalent endpoint with Ollama support for running locally)
- [AntSK](https://github.com/AIDotNet/AntSK) (Out-of-the-box &amp; Adaptable RAG Chatbot)
- [MaxKB](https://github.com/1Panel-dev/MaxKB/) (Ready-to-use &amp; flexible RAG Chatbot)
- [yla](https://github.com/danielekp/yla) (Web interface to freely interact with your customized models)
- [LangBot](https://github.com/RockChinQ/LangBot) (LLM-based instant messaging bots platform, with Agents, RAG features, supports multiple platforms)
- [1Panel](https://github.com/1Panel-dev/1Panel/) (Web-based Linux Server Management Tool)
- [AstrBot](https://github.com/Soulter/AstrBot/) (User-friendly LLM-based multi-platform chatbot with a WebUI, supporting RAG, LLM agents, and plugins integration)
- [Reins](https://github.com/ibrahimcetin/reins) (Easily tweak parameters, customize system prompts per chat, and enhanc

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[thanos-io/thanos]]></title>
            <link>https://github.com/thanos-io/thanos</link>
            <guid>https://github.com/thanos-io/thanos</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:45 GMT</pubDate>
            <description><![CDATA[Highly available Prometheus setup with long term storage capabilities. A CNCF Incubating project.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/thanos-io/thanos">thanos-io/thanos</a></h1>
            <p>Highly available Prometheus setup with long term storage capabilities. A CNCF Incubating project.</p>
            <p>Language: Go</p>
            <p>Stars: 13,852</p>
            <p>Forks: 2,235</p>
            <p>Stars today: 0 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;docs/img/Thanos-logo_fullmedium.png&quot; alt=&quot;Thanos Logo&quot;&gt;&lt;/p&gt;

[![Latest Release](https://img.shields.io/github/release/thanos-io/thanos.svg?style=flat-square)](https://github.com/thanos-io/thanos/releases/latest) [![Go Report Card](https://goreportcard.com/badge/github.com/thanos-io/thanos)](https://goreportcard.com/report/github.com/thanos-io/thanos) [![Go Code reference](https://img.shields.io/badge/code%20reference-go.dev-darkblue.svg)](https://pkg.go.dev/github.com/thanos-io/thanos?tab=subdirectories) [![Slack](https://img.shields.io/badge/join%20slack-%23thanos-brightgreen.svg)](https://slack.cncf.io/) [![Netlify Status](https://api.netlify.com/api/v1/badges/664a5091-934c-4b0e-a7b6-bc12f822a590/deploy-status)](https://app.netlify.com/sites/thanos-io/deploys) [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3048/badge)](https://bestpractices.coreinfrastructure.org/projects/3048)

[![CI](https://github.com/thanos-io/thanos/workflows/CI/badge.svg)](https://github.com/thanos-io/thanos/actions?query=workflow%3ACI) [![CI](https://circleci.com/gh/thanos-io/thanos.svg?style=svg)](https://circleci.com/gh/thanos-io/thanos) [![go](https://github.com/thanos-io/thanos/workflows/go/badge.svg)](https://github.com/thanos-io/thanos/actions?query=workflow%3Ago) [![react](https://github.com/thanos-io/thanos/workflows/react/badge.svg)](https://github.com/thanos-io/thanos/actions?query=workflow%3Areact) [![docs](https://github.com/thanos-io/thanos/workflows/docs/badge.svg)](https://github.com/thanos-io/thanos/actions?query=workflow%3Adocs) [![Gitpod ready-to-code](https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/thanos-io/thanos) [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://github.com/codespaces/new?hide_repo_select=true&amp;ref=main&amp;repo=109162639)

&gt; üì¢ [ThanosCon](https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/co-located-events/thanoscon/) happened on 19th March 2024 as a co-located half-day on KubeCon EU in Paris.

## Overview

Thanos is a set of components that can be composed into a highly available metric system with unlimited storage capacity, which can be added seamlessly on top of existing Prometheus deployments.

Thanos is a [CNCF](https://www.cncf.io/) Incubating project.

Thanos leverages the Prometheus 2.0 storage format to cost-efficiently store historical metric data in any object storage while retaining fast query latencies. Additionally, it provides a global query view across all Prometheus installations and can merge data from Prometheus HA pairs on the fly.

Concretely the aims of the project are:

1. Global query view of metrics.
2. Unlimited retention of metrics.
3. High availability of components, including Prometheus.

## Getting Started

* **[Getting Started](https://thanos.io/tip/thanos/getting-started.md/)**
* [Design](https://thanos.io/tip/thanos/design.md/)
* [Blog posts](docs/getting-started.md#blog-posts)
* [Talks](docs/getting-started.md#talks)
* [Proposals](docs/proposals-done)
* [Integrations](docs/integrations.md)

## Features

* Global querying view across all connected Prometheus servers
* Deduplication and merging of metrics collected from Prometheus HA pairs
* Seamless integration with existing Prometheus setups
* Any object storage as its only, optional dependency
* Downsampling historical data for massive query speedup
* Cross-cluster federation
* Fault-tolerant query routing
* Simple gRPC &quot;Store API&quot; for unified data access across all metric data
* Easy integration points for custom metric providers

## Architecture Overview

Deployment with Sidecar for Kubernetes:

&lt;!---
Source file to copy and edit: https://docs.google.com/drawings/d/1AiMc1qAjASMbtqL6PNs0r9-ynGoZ9LIAtf0b9PjILxw/edit?usp=sharing
--&gt;

![Sidecar](https://docs.google.com/drawings/d/e/2PACX-1vSJd32gPh8-MC5Ko0-P-v1KQ0Xnxa0qmsVXowtkwVGlczGfVW-Vd415Y6F129zvh3y0vHLBZcJeZEoz/pub?w=960&amp;h=720)

Deployment with Receive in order to scale out or implement with other remote write compatible sources:

&lt;!---
Source file to copy and edit: https://docs.google.com/drawings/d/1iimTbcicKXqz0FYtSfz04JmmVFLVO9BjAjEzBm5538w/edit?usp=sharing
--&gt;

![Receive](https://docs.google.com/drawings/d/e/2PACX-1vRdYP__uDuygGR5ym1dxBzU6LEx5v7Rs1cAUKPsl5BZrRGVl5YIj5lsD_FOljeIVOGWatdAI9pazbCP/pub?w=960&amp;h=720)

## Thanos Philosophy

The philosophy of Thanos and our community is borrowing much from UNIX philosophy and the golang programming language.

* Each subcommand should do one thing and do it well
  * e.g. thanos query proxies incoming calls to known store API endpoints merging the result
* Write components that work together
  * e.g. blocks should be stored in native prometheus format
* Make it easy to read, write, and, run components
  * e.g. reduce complexity in system design and implementation

## Releases

Main branch should be stable and usable. Every commit to main builds docker image named `main-&lt;date&gt;-&lt;sha&gt;` in [quay.io/thanos/thanos](https://quay.io/repository/thanos/thanos) and [thanosio/thanos dockerhub (mirror)](https://hub.docker.com/r/thanosio/thanos)

We also perform minor releases every 6 weeks.

During that, we build tarballs for major platforms and release docker images.

See [release process docs](docs/release-process.md) for details.

## Contributing

Contributions are very welcome! See our [CONTRIBUTING.md](CONTRIBUTING.md) for more information.

## Community

Thanos is an open source project and we value and welcome new contributors and members of the community. Here are ways to get in touch with the community:

* Slack: [#thanos](https://slack.cncf.io/)
* Issue Tracker: [GitHub Issues](https://github.com/thanos-io/thanos/issues)

## Adopters

See [`Adopters List`](website/data/adopters.yml).

## Maintainers

See [MAINTAINERS.md](MAINTAINERS.md)
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[anchore/grype]]></title>
            <link>https://github.com/anchore/grype</link>
            <guid>https://github.com/anchore/grype</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:44 GMT</pubDate>
            <description><![CDATA[A vulnerability scanner for container images and filesystems]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/anchore/grype">anchore/grype</a></h1>
            <p>A vulnerability scanner for container images and filesystems</p>
            <p>Language: Go</p>
            <p>Stars: 11,150</p>
            <p>Forks: 716</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
    &lt;img alt=&quot;Grype logo&quot; src=&quot;https://user-images.githubusercontent.com/5199289/136855393-d0a9eef9-ccf1-4e2b-9d7c-7aad16a567e5.png&quot; width=&quot;234&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &amp;nbsp;&lt;a href=&quot;https://github.com/anchore/grype/actions?query=workflow%3A%22Static+Analysis+%2B+Unit+%2B+Integration%22&quot;&gt;&lt;img src=&quot;https://github.com/anchore/grype/workflows/Static%20Analysis%20+%20Unit%20+%20Integration/badge.svg&quot; alt=&quot;Static Analysis + Unit + Integration&quot;&gt;&lt;/a&gt;&amp;nbsp;
    &amp;nbsp;&lt;a href=&quot;https://github.com/anchore/grype/actions/workflows/validations.yaml&quot;&gt;&lt;img src=&quot;https://github.com/anchore/grype/workflows/Validations/badge.svg&quot; alt=&quot;Validations&quot;&gt;&lt;/a&gt;&amp;nbsp;
    &amp;nbsp;&lt;a href=&quot;https://goreportcard.com/report/github.com/anchore/grype&quot;&gt;&lt;img src=&quot;https://goreportcard.com/badge/github.com/anchore/grype&quot; alt=&quot;Go Report Card&quot;&gt;&lt;/a&gt;&amp;nbsp;
    &amp;nbsp;&lt;a href=&quot;https://github.com/anchore/grype/releases/latest&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/release/anchore/grype.svg&quot; alt=&quot;GitHub release&quot;&gt;&lt;/a&gt;&amp;nbsp;
    &amp;nbsp;&lt;a href=&quot;https://github.com/anchore/grype&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/go-mod/go-version/anchore/grype.svg&quot; alt=&quot;GitHub go.mod Go version&quot;&gt;&lt;/a&gt;&amp;nbsp;
    &lt;br&gt;
    &amp;nbsp;&lt;a href=&quot;https://github.com/anchore/grype/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-Apache%202.0-blue.svg&quot; alt=&quot;License: Apache-2.0&quot;&gt;&lt;/a&gt;&amp;nbsp;
    &amp;nbsp;&lt;a href=&quot;https://anchore.com/discourse&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discourse-Join-blue?logo=discourse&quot; alt=&quot;Join our Discourse&quot;&gt;&lt;/a&gt;&amp;nbsp;
    &amp;nbsp;&lt;a rel=&quot;me&quot; href=&quot;https://fosstodon.org/@grype&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Mastodon-Follow-blue?logoColor=white&amp;amp;logo=mastodon&quot; alt=&quot;Follow on Mastodon&quot;&gt;&lt;/a&gt;&amp;nbsp;
    &amp;nbsp;&lt;a href=&quot;https://scorecard.dev/viewer/?uri=github.com/anchore/grype&quot;&gt;&lt;img src=&quot;https://api.securityscorecards.dev/projects/github.com/anchore/grype/badge&quot; alt=&quot;OpenSSF Scorecard&quot;&gt;&lt;/a&gt;&amp;nbsp;
    &amp;nbsp;&lt;a href=&quot;https://www.bestpractices.dev/projects/6708&quot;&gt;&lt;img src=&quot;https://www.bestpractices.dev/projects/6708/badge&quot; alt=&quot;OpenSSF Best Practices&quot;&gt;&lt;/a&gt;&amp;nbsp;
&lt;p&gt;

A vulnerability scanner for container images and filesystems. Easily [install the binary](#installation) to try it out. Works with [Syft](https://github.com/anchore/syft), the powerful SBOM (software bill of materials) tool for container images and filesystems.

### Join our community meetings!

- Calendar: https://calendar.google.com/calendar/u/0/r?cid=Y182OTM4dGt0MjRtajI0NnNzOThiaGtnM29qNEBncm91cC5jYWxlbmRhci5nb29nbGUuY29t
- Agenda: https://docs.google.com/document/d/1ZtSAa6fj2a6KRWviTn3WoJm09edvrNUp4Iz_dOjjyY8/edit?usp=sharing (join [this group](https://groups.google.com/g/anchore-oss-community) for write access)
- All are welcome!

For commercial support options with Syft or Grype, please [contact Anchore](https://get.anchore.com/contact/).

![grype-demo](https://user-images.githubusercontent.com/590471/90276236-9868f300-de31-11ea-8068-4268b6b68529.gif)

## Features

- Scan the contents of a container image or filesystem to find known vulnerabilities.
- Find vulnerabilities for major operating system packages:
  - Alpine
  - Amazon Linux
  - Azure Linux (previously CBL-Mariner)
  - BusyBox
  - CentOS
  - Debian
  - Echo
  - Distroless
  - MinimOS
  - Oracle Linux
  - Red Hat (RHEL)
  - Ubuntu
  - Wolfi
- Find vulnerabilities for language-specific packages:
  - Ruby (Gems)
  - Java (JAR, WAR, EAR, JPI, HPI)
  - JavaScript (NPM, Yarn)
  - Python (Egg, Wheel, Poetry, requirements.txt/setup.py files)
  - Dotnet (deps.json)
  - Golang (go.mod)
  - PHP (Composer)
  - Rust (Cargo)
- Supports Docker, OCI and [Singularity](https://github.com/sylabs/singularity) image formats.
- [OpenVEX](https://github.com/openvex) support for filtering and augmenting scanning results.

If you encounter an issue, please [let us know using the issue tracker](https://github.com/anchore/grype/issues).

## Installation

### Recommended

```bash
curl -sSfL https://get.anchore.io/grype | sudo sh -s -- -b /usr/local/bin
```
Install script options:
-	`-b`: Specify a custom installation directory (defaults to `./bin`)
-	`-d`: More verbose logging levels (`-d` for debug, `-dd` for trace)
-	`-v`: Verify the signature of the downloaded artifact before installation (requires [`cosign`](https://github.com/sigstore/cosign) to be installed)

### Chocolatey

The chocolatey distribution of grype is community-maintained and not distributed by the anchore team.

```bash
choco install grype -y
```

### Homebrew

```bash
brew tap anchore/grype
brew install grype
```

### MacPorts

On macOS, Grype can additionally be installed from the [community-maintained port](https://ports.macports.org/port/grype/) via MacPorts:

```bash
sudo port install grype
```

**Note**: Currently, Grype is built only for macOS and Linux.

### From source

See [DEVELOPING.md](DEVELOPING.md#native-development) for instructions to build and run from source.

### GitHub Actions

If you&#039;re using GitHub Actions, you can use our [Grype-based action](https://github.com/marketplace/actions/anchore-container-scan) to run vulnerability scans on your code or container images during your CI workflows.

## Verifying the artifacts

Checksums are applied to all artifacts, and the resulting checksum file is signed using cosign.

You need the following tool to verify signature:

- [Cosign](https://docs.sigstore.dev/cosign/system_config/installation/)

Verification steps are as follow:

1. Download the files you want, and the checksums.txt, checksums.txt.pem and checksums.txt.sig files from the [releases](https://github.com/anchore/grype/releases) page:

2. Verify the signature:

```shell
cosign verify-blob &lt;path to checksum.txt&gt; \
--certificate &lt;path to checksums.txt.pem&gt; \
--signature &lt;path to checksums.txt.sig&gt; \
--certificate-identity-regexp &#039;https://github\.com/anchore/grype/\.github/workflows/.+&#039; \
--certificate-oidc-issuer &quot;https://token.actions.githubusercontent.com&quot;
```

3. Once the signature is confirmed as valid, you can proceed to validate that the SHA256 sums align with the downloaded artifact:

```shell
sha256sum --ignore-missing -c checksums.txt
```

## Getting started

[Install the binary](#installation), and make sure that `grype` is available in your path. To scan for vulnerabilities in an image:

```
grype &lt;image&gt;
```

The above command scans for vulnerabilities visible in the container (i.e., the squashed representation of the image). To include software from all image layers in the vulnerability scan, regardless of its presence in the final image, provide `--scope all-layers`:

```
grype &lt;image&gt; --scope all-layers
```

To run grype from a Docker container so it can scan a running container, use the following command:

```yml
docker run --rm \
--volume /var/run/docker.sock:/var/run/docker.sock \
--name Grype anchore/grype:latest \
$(ImageName):$(ImageTag)
```

## Supported sources

Grype can scan a variety of sources beyond those found in Docker.

```
# scan a container image archive (from the result of `docker image save ...`, `podman save ...`, or `skopeo copy` commands)
grype path/to/image.tar

# scan a Singularity Image Format (SIF) container
grype path/to/image.sif

# scan a directory
grype dir:path/to/dir
```

Sources can be explicitly provided with a scheme:

```
podman:yourrepo/yourimage:tag          use images from the Podman daemon
docker:yourrepo/yourimage:tag          use images from the Docker daemon
docker-archive:path/to/yourimage.tar   use a tarball from disk for archives created from &quot;docker save&quot;
oci-archive:path/to/yourimage.tar      use a tarball from disk for OCI archives (from Skopeo or otherwise)
oci-dir:path/to/yourimage              read directly from a path on disk for OCI layout directories (from Skopeo or otherwise)
singularity:path/to/yourimage.sif      read directly from a Singularity Image Format (SIF) container on disk
dir:path/to/yourproject                read directly from a path on disk (any directory)
file:path/to/yourfile                  read directly from a file on disk
sbom:path/to/syft.json                 read Syft JSON from path on disk
registry:yourrepo/yourimage:tag        pull image directly from a registry (no container runtime required)
```

If an image source is not provided and cannot be detected from the given reference it is assumed the image should be pulled from the Docker daemon.
If docker is not present, then the Podman daemon is attempted next, followed by reaching out directly to the image registry last.


This default behavior can be overridden with the `default-image-pull-source` configuration option (See [Configuration](https://github.com/anchore/grype#configuration) for more details).

Use SBOMs for even faster vulnerability scanning in Grype:

```
# Then scan for new vulnerabilities as frequently as needed
grype sbom:./sbom.json

# (You can also pipe the SBOM into Grype)
cat ./sbom.json | grype
```

Grype supports input of [Syft](https://github.com/anchore/syft), [SPDX](https://spdx.dev/), and [CycloneDX](https://cyclonedx.org/)
SBOM formats. If Syft has generated any of these file types, they should have the appropriate information to work properly with Grype.
It is also possible to use SBOMs generated by other tools with varying degrees of success. Two things that make Grype matching
more successful are the inclusion of CPE and Linux distribution information. If an SBOM does not include any CPE information, it
is possible to generate these based on package information using the `--add-cpes-if-none` flag. To specify a distribution,
use the `--distro &lt;distro&gt;:&lt;version&gt;` flag. A full example is:

```
grype --add-cpes-if-none --distro alpine:3.10 sbom:some-alpine-3.10.spdx.json
```

## Threat &amp; Risk Prioritization

This section explains the columns and UI cues that help prioritize remediation efforts:

- **Severity**: String-based severity derived from CVSS scores that indicates the significance of a vulnerability in levels.
  This balances concerns such as ease of exploitability, and the potential to affect
  confidentiality, integrity, and availability of software and services.

- **EPSS**:
  [Exploit Prediction Scoring System](https://www.first.org/epss/model) is a metric expressing the likelihood
  that a vulnerability will be 
  exploited in the wild over the next 30 days (on a 0‚Äì1 scale); higher values signal a greater likelihood of 
  exploitation.
  The table output shows the EPSS percentile, a one-way transform of the EPSS score showing the 
  proportion of all scored vulnerabilities with an equal or lower probability.
  Percentiles linearize a heavily skewed distribution, making threshold choice (e.g. ‚Äúonly CVEs above the 
  90th percentile‚Äù) straightforward.

- **KEV Indicator**: Flags entries from CISA‚Äôs [Known Exploited Vulnerabilities Catalog](https://www.cisa.gov/known-exploited-vulnerabilities-catalog)
  --an authoritative list of flaws observed being exploited in the wild.

- **Risk Score**: A composite 0‚Äì100 metric calculated as:
  ```markdown
  risk = min(1, threat * average(severity)) * 100
  ```
  Where: 
  - `severity` is the average of all CVSS scores and string severity for a vulnerability (scaled between 0‚Äì1).
  - `threat` is the EPSS score (between 0‚Äì1). If the vulnerability is on the KEV list then `threat` is 
    `1.05`, or `1.1` if the vulnerability is associated with a ransomware campaign.
  This metric is one way to combine EPSS and CVSS suggested in the [EPSS user guide](https://www.first.org/epss/user-guide).

- **Suggested Fixes**: All possible fixes for a package are listed, however, when multiple fixes are available, we de-emphasize all 
  upgrade paths except for the minimal upgrade path (which highlights the smallest, safest version bump).

Results default to sorting by Risk Score and can be overridden with `--sort-by &lt;value&gt;`:

- `severity`: sort by severity
- `epss`: sort by EPSS percentile (aka, &quot;threat&quot;)
- `risk`: sort by risk score
- `kev`: just like risk, except that KEV entries are always above non-KEV entries
- `package`: sort by package name, version, type
- `vulnerability`: sort by vulnerability ID

### Supported versions

Software updates are always applied to the latest version of Grype; fixes are not backported to any previous versions of Grype.

In terms of database updates, any version of Grype before v0.51.0 (Oct 2022, before schema v5) will not receive
vulnerability database updates. You can still build vulnerability databases for unsupported Grype releases by using previous
releases of [vunnel](https://github.com/anchore/vunnel) to gather the upstream data and [grype-db](https://github.com/anchore/grype-db)
to build databases for unsupported schemas.

Only the latest database schema is considered to be supported. When a new database schema is introduced then the one it replaces is
marked as deprecated. Deprecated schemas will continue to receive updates for at least one year after they are marked
as deprecated at which point they will no longer be supported.

### Working with attestations
Grype supports scanning SBOMs as input via stdin. Users can use [cosign](https://github.com/sigstore/cosign) to verify attestations
with an SBOM as its content to scan an image for vulnerabilities:
```
COSIGN_EXPERIMENTAL=1 cosign verify-attestation caphill4/java-spdx-tools:latest \
| jq -r .payload \
| base64 --decode \
| jq -r .predicate.Data \
| grype
```

### Vulnerability Summary

#### Basic Grype Vulnerability Data Shape

```json
 {
  &quot;vulnerability&quot;: {
    ...
  },
  &quot;relatedVulnerabilities&quot;: [
    ...
  ],
  &quot;matchDetails&quot;: [
    ...
  ],
  &quot;artifact&quot;: {
    ...
  }
}
```

- **Vulnerability**: All information on the specific vulnerability that was directly matched on (e.g. ID, severity, CVSS score, fix information, links for more information)
- **RelatedVulnerabilities**: Information pertaining to vulnerabilities found to be related to the main reported vulnerability. Perhaps the vulnerability we matched on was a GitHub Security Advisory, which has an upstream CVE (in the authoritative national vulnerability database). In these cases we list the upstream vulnerabilities here.
- **MatchDetails**: This section tries to explain what we searched for while looking for a match and exactly what details on the package and vulnerability that led to a match.
- **Artifact**: This is a subset of the information that we know about the package (when compared to the [Syft](https://github.com/anchore/syft) json output, we summarize the metadata section).
  This has information about where within the container image or directory we found the package, what kind of package it is, licensing info, pURLs, CPEs, etc.

### Excluding file paths

Grype can exclude files and paths from being scanned within a source by using glob expressions
with one or more `--exclude` parameters:

```
grype &lt;source&gt; --exclude &#039;./out/**/*.json&#039; --exclude /etc
```

**Note:** in the case of _image scanning_, since the entire filesystem is scanned it is
possible to use absolute paths like `/etc` or `/usr/**/*.txt` whereas _directory scans_
exclude files _relative to the specified directory_. For example: scanning `/usr/foo` with
`--exclude ./package.json` would exclude `/usr/foo/package.json` and `--exclude &#039;**/package.json&#039;`
would exclude all `package.json` files under `/usr/foo`. For _directory scans_,
it is required to begin path expressions with `./`, `*/`, or `**/`, all of which
will be resolved _relative to the specified scan directory_. Keep in mind, your shell
may attempt to expand wildcards, so put those parameters in single quotes, like:
`&#039;**/*.json&#039;`.

### External Sources

Grype can be configured to incorporate external data sources for added fidelity in vulnerability matching. This
feature is currently disabled by default. To enable this feature add the following to the grype config:

```yaml
external-sources:
  enable: true
  maven:
    search-upstream-by-sha1: true
    base-url: https://search.maven.org/solrsearch/select
    rate-limit: 300ms # Time between Maven API requests
```

You can also configure the base-url if you&#039;re using another registry as your maven endpoint.

The rate at which Maven API requests are made can be configured to match your environment&#039;s requirements. The default is 300ms between requests.

### Output formats

The output format for Grype is configurable as well:

```
grype &lt;image&gt; -o &lt;format&gt;
```

Where the formats available are:

- `table`: A columnar summary (default).
- `cyclonedx`: An XML report conforming to the [CycloneDX 1.6 specification](https://cyclonedx.org/specification/overview/).
- `cyclonedx-json`: A JSON report conforming to the [CycloneDX 1.6 specification](https://cyclonedx.org/specification/overview/).
- `json`: Use this to get as much information out of Grype as possible!
- `sarif`: Use this option to get a [SARIF](https://docs.oasis-open.org/sarif/sarif/v2.1.0/sarif-v2.1.0.html) report (Static Analysis Results Interchange Format)
- `template`: Lets the user specify the output format. See [&quot;Using templates&quot;](#using-templates) below.

### Using templates

Grype lets you define custom output formats, using [Go templates](https://golang.org/pkg/text/template/). Here&#039;s how it works:

- Define your format as a Go template, and save this template as a file.

- Set the output format to &quot;template&quot; (`-o template`).

- Specify the path to the template file (`-t ./path/to/custom.template`).

- Grype&#039;s template processing uses the same data models as the `json` output format ‚Äî so if you&#039;re wondering what data is available as you author a template, you can use the output from `grype &lt;image&gt; -o json` as a reference.

**Please note:** Templates can access information about the system they are running on, such as environment variables. You should never run untrusted templates.

There are several example templates in the [templates](https://github.com/anchore/grype/tree/main/templates) directory in the Grype source which can serve as a starting point for a custom output format. For example, [csv.tmpl](https://github.com/anchore/grype/blob/main/templates/csv.tmpl) produces a vulnerability report in CSV (comma separated value) format:

```text
&quot;Package&quot;,&quot;Version Installed&quot;,&quot;Vulnerability ID&quot;,&quot;Severity&quot;
&quot;coreutils&quot;,&quot;8.30-3ubuntu2&quot;,&quot;CVE-2016-2781&quot;,&quot;Low&quot;
&quot;libc-bin&quot;,&quot;2.31-0ubuntu9&quot;,&quot;CVE-2016-10228&quot;,&quot;Negligible&quot;
&quot;libc-bin&quot;,&quot;2.31-0ubuntu9&quot;,&quot;CVE-2020-6096&quot;,&quot;Low&quot;
...
```

You can also find the template for the default &quot;table&quot; output format in the same place.

Grype also includes a vast array of utility templating functions from [sprig](http://masterminds.github.io/sprig/) apart from the default golang [text/template](https://pkg.go.dev/text/template#hdr-Functions) to allow users to customize the output from Grype.

### Gating on severity of vulnerabilities

You can have Grype exit with an error if any vulnerabilities are reported at or above the specified severity level. This comes in handy when using Grype within a script or CI pipeline. To do this, use the `--fail-on &lt;severity&gt;` CLI flag.

For example, here&#039;s how you could trigger a CI pipeline failure if any vulnerabilities are found in the `ubuntu:latest` image with a severity of &quot;medium&quot; or higher:

```
grype ubuntu:latest --fail-on medium
```

**Note:** Grype returns exit code `2` on vulnerability errors.

### Specifying matches to ignore

If you&#039;re seeing Grype report **false positives** or any other vulnerability matches that you just don&#039;t want to see, you can tell Grype to **ignore** matches by specifying one or more _&quot;ignore rules&quot;_ in your Grype configuration file (e.g. `~/.grype.yaml`). This causes Grype not to report any vulnerability matches that meet the criteria specified by any of your ignore rules.

Each rule can specify any combination of the following criteria:

- vulnerability ID (e.g. `&quot;CVE-2008-4318&quot;`)
- include vulnerability aliases when matching on a vulnerability ID (set `&quot;include-aliases&quot;` to `true`)
- namespace (e.g. `&quot;nvd&quot;`)
- fix state (allowed valu

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[charmbracelet/crush]]></title>
            <link>https://github.com/charmbracelet/crush</link>
            <guid>https://github.com/charmbracelet/crush</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:43 GMT</pubDate>
            <description><![CDATA[The glamourous AI coding agent for your favourite terminal üíò]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/charmbracelet/crush">charmbracelet/crush</a></h1>
            <p>The glamourous AI coding agent for your favourite terminal üíò</p>
            <p>Language: Go</p>
            <p>Stars: 15,803</p>
            <p>Forks: 926</p>
            <p>Stars today: 52 stars today</p>
            <h2>README</h2><pre># Crush

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://stuff.charm.sh/crush/charm-crush.png&quot;&gt;&lt;img width=&quot;450&quot; alt=&quot;Charm Crush Logo&quot; src=&quot;https://github.com/user-attachments/assets/adc1a6f4-b284-4603-836c-59038caa2e8b&quot; /&gt;&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;https://github.com/charmbracelet/crush/releases&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/release/charmbracelet/crush&quot; alt=&quot;Latest Release&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/charmbracelet/crush/actions&quot;&gt;&lt;img src=&quot;https://github.com/charmbracelet/crush/actions/workflows/build.yml/badge.svg&quot; alt=&quot;Build Status&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;Your new coding bestie, now available in your favourite terminal.&lt;br /&gt;Your tools, your code, and your workflows, wired into your LLM of choice.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;‰Ω†ÁöÑÊñ∞ÁºñÁ®ã‰ºô‰º¥ÔºåÁé∞Âú®Â∞±Âú®‰Ω†ÊúÄÁà±ÁöÑÁªàÁ´Ø‰∏≠„ÄÇ&lt;br /&gt;‰Ω†ÁöÑÂ∑•ÂÖ∑„ÄÅ‰ª£Á†ÅÂíåÂ∑•‰ΩúÊµÅÔºåÈÉΩ‰∏éÊÇ®ÈÄâÊã©ÁöÑ LLM Ê®°ÂûãÁ¥ßÂØÜÁõ∏Ëøû„ÄÇ&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;800&quot; alt=&quot;Crush Demo&quot; src=&quot;https://github.com/user-attachments/assets/58280caf-851b-470a-b6f7-d5c4ea8a1968&quot; /&gt;&lt;/p&gt;

## Features

- **Multi-Model:** choose from a wide range of LLMs or add your own via OpenAI- or Anthropic-compatible APIs
- **Flexible:** switch LLMs mid-session while preserving context
- **Session-Based:** maintain multiple work sessions and contexts per project
- **LSP-Enhanced:** Crush uses LSPs for additional context, just like you do
- **Extensible:** add capabilities via MCPs (`http`, `stdio`, and `sse`)
- **Works Everywhere:** first-class support in every terminal on macOS, Linux, Windows (PowerShell and WSL), FreeBSD, OpenBSD, and NetBSD

## Installation

Use a package manager:

```bash
# Homebrew
brew install charmbracelet/tap/crush

# NPM
npm install -g @charmland/crush

# Arch Linux (btw)
yay -S crush-bin

# Nix
nix run github:numtide/nix-ai-tools#crush
```

Windows users:

```bash
# Winget
winget install charmbracelet.crush

# Scoop
scoop bucket add charm https://github.com/charmbracelet/scoop-bucket.git
scoop install crush
```

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Nix (NUR)&lt;/strong&gt;&lt;/summary&gt;

Crush is available via [NUR](https://github.com/nix-community/NUR) in `nur.repos.charmbracelet.crush`.

You can also try out Crush via `nix-shell`:

```bash
# Add the NUR channel.
nix-channel --add https://github.com/nix-community/NUR/archive/main.tar.gz nur
nix-channel --update

# Get Crush in a Nix shell.
nix-shell -p &#039;(import &lt;nur&gt; { pkgs = import &lt;nixpkgs&gt; {}; }).repos.charmbracelet.crush&#039;
```

### NixOS &amp; Home Manager Module Usage via NUR

Crush provides NixOS and Home Manager modules via NUR.
You can use these modules directly in your flake by importing them from NUR. Since it auto detects whether its a home manager or nixos context you can use the import the exact same way :)

```nix
{
  inputs = {
    nixpkgs.url = &quot;github:NixOS/nixpkgs/nixos-unstable&quot;;
    nur.url = &quot;github:nix-community/NUR&quot;;
  };

  outputs = { self, nixpkgs, nur, ... }: {
    nixosConfigurations.your-hostname = nixpkgs.lib.nixosSystem {
      system = &quot;x86_64-linux&quot;;
      modules = [
        nur.modules.nixos.default
        nur.repos.charmbracelet.modules.crush
        {
          programs.crush = {
            enable = true;
            settings = {
              providers = {
                openai = {
                  id = &quot;openai&quot;;
                  name = &quot;OpenAI&quot;;
                  base_url = &quot;https://api.openai.com/v1&quot;;
                  type = &quot;openai&quot;;
                  api_key = &quot;sk-fake123456789abcdef...&quot;;
                  models = [
                    {
                      id = &quot;gpt-4&quot;;
                      name = &quot;GPT-4&quot;;
                    }
                  ];
                };
              };
              lsp = {
                go = { command = &quot;gopls&quot;; enabled = true; };
                nix = { command = &quot;nil&quot;; enabled = true; };
              };
              options = {
                context_paths = [ &quot;/etc/nixos/configuration.nix&quot; ];
                tui = { compact_mode = true; };
                debug = false;
              };
            };
          };
        }
      ];
    };
  };
}
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Debian/Ubuntu&lt;/strong&gt;&lt;/summary&gt;

```bash
sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://repo.charm.sh/apt/gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/charm.gpg
echo &quot;deb [signed-by=/etc/apt/keyrings/charm.gpg] https://repo.charm.sh/apt/ * *&quot; | sudo tee /etc/apt/sources.list.d/charm.list
sudo apt update &amp;&amp; sudo apt install crush
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Fedora/RHEL&lt;/strong&gt;&lt;/summary&gt;

```bash
echo &#039;[charm]
name=Charm
baseurl=https://repo.charm.sh/yum/
enabled=1
gpgcheck=1
gpgkey=https://repo.charm.sh/yum/gpg.key&#039; | sudo tee /etc/yum.repos.d/charm.repo
sudo yum install crush
```

&lt;/details&gt;

Or, download it:

- [Packages][releases] are available in Debian and RPM formats
- [Binaries][releases] are available for Linux, macOS, Windows, FreeBSD, OpenBSD, and NetBSD

[releases]: https://github.com/charmbracelet/crush/releases

Or just install it with Go:

```
go install github.com/charmbracelet/crush@latest
```

&gt; [!WARNING]
&gt; Productivity may increase when using Crush and you may find yourself nerd
&gt; sniped when first using the application. If the symptoms persist, join the
&gt; [Discord][discord] and nerd snipe the rest of us.

## Getting Started

The quickest way to get started is to grab an API key for your preferred
provider such as Anthropic, OpenAI, Groq, or OpenRouter and just start
Crush. You&#039;ll be prompted to enter your API key.

That said, you can also set environment variables for preferred providers.

| Environment Variable        | Provider                                           |
| --------------------------- | -------------------------------------------------- |
| `ANTHROPIC_API_KEY`         | Anthropic                                          |
| `OPENAI_API_KEY`            | OpenAI                                             |
| `OPENROUTER_API_KEY`        | OpenRouter                                         |
| `GEMINI_API_KEY`            | Google Gemini                                      |
| `CEREBRAS_API_KEY`          | Cerebras                                           |
| `HF_TOKEN`                  | Huggingface Inference                              |
| `VERTEXAI_PROJECT`          | Google Cloud VertexAI (Gemini)                     |
| `VERTEXAI_LOCATION`         | Google Cloud VertexAI (Gemini)                     |
| `GROQ_API_KEY`              | Groq                                               |
| `AWS_ACCESS_KEY_ID`         | Amazon Bedrock (Claude)                               |
| `AWS_SECRET_ACCESS_KEY`     | Amazon Bedrock (Claude)                               |
| `AWS_REGION`                | Amazon Bedrock (Claude)                               |
| `AWS_PROFILE`               | Amazon Bedrock (Custom Profile)                       |
| `AWS_BEARER_TOKEN_BEDROCK`  | Amazon Bedrock                                        |
| `AZURE_OPENAI_API_ENDPOINT` | Azure OpenAI models                                |
| `AZURE_OPENAI_API_KEY`      | Azure OpenAI models (optional when using Entra ID) |
| `AZURE_OPENAI_API_VERSION`  | Azure OpenAI models                                |

### By the Way

Is there a provider you‚Äôd like to see in Crush? Is there an existing model that needs an update?

Crush‚Äôs default model listing is managed in [Catwalk](https://github.com/charmbracelet/catwalk), a community-supported, open source repository of Crush-compatible models, and you‚Äôre welcome to contribute.

&lt;a href=&quot;https://github.com/charmbracelet/catwalk&quot;&gt;&lt;img width=&quot;174&quot; height=&quot;174&quot; alt=&quot;Catwalk Badge&quot; src=&quot;https://github.com/user-attachments/assets/95b49515-fe82-4409-b10d-5beb0873787d&quot; /&gt;&lt;/a&gt;

## Configuration

Crush runs great with no configuration. That said, if you do need or want to
customize Crush, configuration can be added either local to the project itself,
or globally, with the following priority:

1. `.crush.json`
2. `crush.json`
3. `$HOME/.config/crush/crush.json`

Configuration itself is stored as a JSON object:

```json
{
  &quot;this-setting&quot;: { &quot;this&quot;: &quot;that&quot; },
  &quot;that-setting&quot;: [&quot;ceci&quot;, &quot;cela&quot;]
}
```

As an additional note, Crush also stores ephemeral data, such as application state, in one additional location:

```bash
# Unix
$HOME/.local/share/crush/crush.json

# Windows
%LOCALAPPDATA%\crush\crush.json
```

### LSPs

Crush can use LSPs for additional context to help inform its decisions, just
like you would. LSPs can be added manually like so:

```json
{
  &quot;$schema&quot;: &quot;https://charm.land/crush.json&quot;,
  &quot;lsp&quot;: {
    &quot;go&quot;: {
      &quot;command&quot;: &quot;gopls&quot;,
      &quot;env&quot;: {
        &quot;GOTOOLCHAIN&quot;: &quot;go1.24.5&quot;
      }
    },
    &quot;typescript&quot;: {
      &quot;command&quot;: &quot;typescript-language-server&quot;,
      &quot;args&quot;: [&quot;--stdio&quot;]
    },
    &quot;nix&quot;: {
      &quot;command&quot;: &quot;nil&quot;
    }
  }
}
```

### MCPs

Crush also supports Model Context Protocol (MCP) servers through three
transport types: `stdio` for command-line servers, `http` for HTTP endpoints,
and `sse` for Server-Sent Events. Environment variable expansion is supported
using `$(echo $VAR)` syntax.

```json
{
  &quot;$schema&quot;: &quot;https://charm.land/crush.json&quot;,
  &quot;mcp&quot;: {
    &quot;filesystem&quot;: {
      &quot;type&quot;: &quot;stdio&quot;,
      &quot;command&quot;: &quot;node&quot;,
      &quot;args&quot;: [&quot;/path/to/mcp-server.js&quot;],
      &quot;timeout&quot;: 120,
      &quot;disabled&quot;: false,
      &quot;disabled_tools&quot;: [&quot;some-tool-name&quot;],
      &quot;env&quot;: {
        &quot;NODE_ENV&quot;: &quot;production&quot;
      }
    },
    &quot;github&quot;: {
      &quot;type&quot;: &quot;http&quot;,
      &quot;url&quot;: &quot;https://api.githubcopilot.com/mcp/&quot;,
      &quot;timeout&quot;: 120,
      &quot;disabled&quot;: false,
      &quot;disabled_tools&quot;: [&quot;create_issue&quot;, &quot;create_pull_request&quot;],
      &quot;headers&quot;: {
        &quot;Authorization&quot;: &quot;Bearer $GH_PAT&quot;
      }
    },
    &quot;streaming-service&quot;: {
      &quot;type&quot;: &quot;sse&quot;,
      &quot;url&quot;: &quot;https://example.com/mcp/sse&quot;,
      &quot;timeout&quot;: 120,
      &quot;disabled&quot;: false,
      &quot;headers&quot;: {
        &quot;API-Key&quot;: &quot;$(echo $API_KEY)&quot;
      }
    }
  }
}
```

### Ignoring Files

Crush respects `.gitignore` files by default, but you can also create a
`.crushignore` file to specify additional files and directories that Crush
should ignore. This is useful for excluding files that you want in version
control but don&#039;t want Crush to consider when providing context.

The `.crushignore` file uses the same syntax as `.gitignore` and can be placed
in the root of your project or in subdirectories.

### Allowing Tools

By default, Crush will ask you for permission before running tool calls. If
you&#039;d like, you can allow tools to be executed without prompting you for
permissions. Use this with care.

```json
{
  &quot;$schema&quot;: &quot;https://charm.land/crush.json&quot;,
  &quot;permissions&quot;: {
    &quot;allowed_tools&quot;: [
      &quot;view&quot;,
      &quot;ls&quot;,
      &quot;grep&quot;,
      &quot;edit&quot;,
      &quot;mcp_context7_get-library-doc&quot;
    ]
  }
}
```

You can also skip all permission prompts entirely by running Crush with the
`--yolo` flag. Be very, very careful with this feature.

### Disabling Built-In Tools

If you&#039;d like to prevent Crush from using certain built-in tools entirely, you
can disable them via the `options.disabled_tools` list. Disabled tools are
completely hidden from the agent.

```json
{
  &quot;$schema&quot;: &quot;https://charm.land/crush.json&quot;,
  &quot;options&quot;: {
    &quot;disabled_tools&quot;: [
      &quot;bash&quot;,
      &quot;sourcegraph&quot;
    ]
  }
}
```

To disable tools from MCP servers, see the [MCP config section](#mcps).

### Initialization

When you initialize a project, Crush analyzes your codebase and creates
a context file that helps it work more effectively in future sessions.
By default, this file is named `AGENTS.md`, but you can customize the
name and location with the `initialize_as` option:

```json
{
  &quot;$schema&quot;: &quot;https://charm.land/crush.json&quot;,
  &quot;options&quot;: {
    &quot;initialize_as&quot;: &quot;AGENTS.md&quot;
  }
}
```

This is useful if you prefer a different naming convention or want to
place the file in a specific directory (e.g., `CRUSH.md` or
`docs/LLMs.md`). Crush will fill the file with project-specific context
like build commands, code patterns, and conventions it discovered during
initialization.

### Attribution Settings

By default, Crush adds attribution information to Git commits and pull requests
it creates. You can customize this behavior with the `attribution` option:

```json
{
  &quot;$schema&quot;: &quot;https://charm.land/crush.json&quot;,
  &quot;options&quot;: {
    &quot;attribution&quot;: {
      &quot;trailer_style&quot;: &quot;co-authored-by&quot;,
      &quot;generated_with&quot;: true
    }
  }
}
```

- `trailer_style`: Controls the attribution trailer added to commit messages
  (default: `assisted-by`)
	- `assisted-by`: Adds `Assisted-by: [Model Name] via Crush &lt;crush@charm.land&gt;`
	  (includes the model name)
	- `co-authored-by`: Adds `Co-Authored-By: Crush &lt;crush@charm.land&gt;`
	- `none`: No attribution trailer
- `generated_with`: When true (default), adds `üíò Generated with Crush` line to
  commit messages and PR descriptions

### Custom Providers

Crush supports custom provider configurations for both OpenAI-compatible and
Anthropic-compatible APIs.

&gt; [!NOTE]
&gt; Note that we support two &quot;types&quot; for OpenAI. Make sure to choose the right one
&gt; to ensure the best experience!
&gt; * `openai` should be used when proxying or routing requests through OpenAI.
&gt; * `openai-compat` should be used when using non-OpenAI providers that have OpenAI-compatible APIs.

#### OpenAI-Compatible APIs

Here‚Äôs an example configuration for Deepseek, which uses an OpenAI-compatible
API. Don&#039;t forget to set `DEEPSEEK_API_KEY` in your environment.

```json
{
  &quot;$schema&quot;: &quot;https://charm.land/crush.json&quot;,
  &quot;providers&quot;: {
    &quot;deepseek&quot;: {
      &quot;type&quot;: &quot;openai-compat&quot;,
      &quot;base_url&quot;: &quot;https://api.deepseek.com/v1&quot;,
      &quot;api_key&quot;: &quot;$DEEPSEEK_API_KEY&quot;,
      &quot;models&quot;: [
        {
          &quot;id&quot;: &quot;deepseek-chat&quot;,
          &quot;name&quot;: &quot;Deepseek V3&quot;,
          &quot;cost_per_1m_in&quot;: 0.27,
          &quot;cost_per_1m_out&quot;: 1.1,
          &quot;cost_per_1m_in_cached&quot;: 0.07,
          &quot;cost_per_1m_out_cached&quot;: 1.1,
          &quot;context_window&quot;: 64000,
          &quot;default_max_tokens&quot;: 5000
        }
      ]
    }
  }
}
```

#### Anthropic-Compatible APIs

Custom Anthropic-compatible providers follow this format:

```json
{
  &quot;$schema&quot;: &quot;https://charm.land/crush.json&quot;,
  &quot;providers&quot;: {
    &quot;custom-anthropic&quot;: {
      &quot;type&quot;: &quot;anthropic&quot;,
      &quot;base_url&quot;: &quot;https://api.anthropic.com/v1&quot;,
      &quot;api_key&quot;: &quot;$ANTHROPIC_API_KEY&quot;,
      &quot;extra_headers&quot;: {
        &quot;anthropic-version&quot;: &quot;2023-06-01&quot;
      },
      &quot;models&quot;: [
        {
          &quot;id&quot;: &quot;claude-sonnet-4-20250514&quot;,
          &quot;name&quot;: &quot;Claude Sonnet 4&quot;,
          &quot;cost_per_1m_in&quot;: 3,
          &quot;cost_per_1m_out&quot;: 15,
          &quot;cost_per_1m_in_cached&quot;: 3.75,
          &quot;cost_per_1m_out_cached&quot;: 0.3,
          &quot;context_window&quot;: 200000,
          &quot;default_max_tokens&quot;: 50000,
          &quot;can_reason&quot;: true,
          &quot;supports_attachments&quot;: true
        }
      ]
    }
  }
}
```

### Amazon Bedrock

Crush currently supports running Anthropic models through Bedrock, with caching disabled.

- A Bedrock provider will appear once you have AWS configured, i.e. `aws configure`
- Crush also expects the `AWS_REGION` or `AWS_DEFAULT_REGION` to be set
- To use a specific AWS profile set `AWS_PROFILE` in your environment, i.e. `AWS_PROFILE=myprofile crush`
- Alternatively to `aws configure`, you can also just set `AWS_BEARER_TOKEN_BEDROCK`

### Vertex AI Platform

Vertex AI will appear in the list of available providers when `VERTEXAI_PROJECT` and `VERTEXAI_LOCATION` are set. You will also need to be authenticated:

```bash
gcloud auth application-default login
```

To add specific models to the configuration, configure as such:

```json
{
  &quot;$schema&quot;: &quot;https://charm.land/crush.json&quot;,
  &quot;providers&quot;: {
    &quot;vertexai&quot;: {
      &quot;models&quot;: [
        {
          &quot;id&quot;: &quot;claude-sonnet-4@20250514&quot;,
          &quot;name&quot;: &quot;VertexAI Sonnet 4&quot;,
          &quot;cost_per_1m_in&quot;: 3,
          &quot;cost_per_1m_out&quot;: 15,
          &quot;cost_per_1m_in_cached&quot;: 3.75,
          &quot;cost_per_1m_out_cached&quot;: 0.3,
          &quot;context_window&quot;: 200000,
          &quot;default_max_tokens&quot;: 50000,
          &quot;can_reason&quot;: true,
          &quot;supports_attachments&quot;: true
        }
      ]
    }
  }
}
```

### Local Models

Local models can also be configured via OpenAI-compatible API. Here are two common examples:

#### Ollama

```json
{
  &quot;providers&quot;: {
    &quot;ollama&quot;: {
      &quot;name&quot;: &quot;Ollama&quot;,
      &quot;base_url&quot;: &quot;http://localhost:11434/v1/&quot;,
      &quot;type&quot;: &quot;openai-compat&quot;,
      &quot;models&quot;: [
        {
          &quot;name&quot;: &quot;Qwen 3 30B&quot;,
          &quot;id&quot;: &quot;qwen3:30b&quot;,
          &quot;context_window&quot;: 256000,
          &quot;default_max_tokens&quot;: 20000
        }
      ]
    }
  }
}
```

#### LM Studio

```json
{
  &quot;providers&quot;: {
    &quot;lmstudio&quot;: {
      &quot;name&quot;: &quot;LM Studio&quot;,
      &quot;base_url&quot;: &quot;http://localhost:1234/v1/&quot;,
      &quot;type&quot;: &quot;openai-compat&quot;,
      &quot;models&quot;: [
        {
          &quot;name&quot;: &quot;Qwen 3 30B&quot;,
          &quot;id&quot;: &quot;qwen/qwen3-30b-a3b-2507&quot;,
          &quot;context_window&quot;: 256000,
          &quot;default_max_tokens&quot;: 20000
        }
      ]
    }
  }
}
```

## Logging

Sometimes you need to look at logs. Luckily, Crush logs all sorts of
stuff. Logs are stored in `./.crush/logs/crush.log` relative to the project.

The CLI also contains some helper commands to make perusing recent logs easier:

```bash
# Print the last 1000 lines
crush logs

# Print the last 500 lines
crush logs --tail 500

# Follow logs in real time
crush logs --follow
```

Want more logging? Run `crush` with the `--debug` flag, or enable it in the
config:

```json
{
  &quot;$schema&quot;: &quot;https://charm.land/crush.json&quot;,
  &quot;options&quot;: {
    &quot;debug&quot;: true,
    &quot;debug_lsp&quot;: true
  }
}
```

## Provider Auto-Updates

By default, Crush automatically checks for the latest and greatest list of
providers and models from [Catwalk](https://github.com/charmbracelet/catwalk),
the open source Crush provider database. This means that when new providers and
models are available, or when model metadata changes, Crush automatically
updates your local configuration.

### Disabling automatic provider updates

For those with restricted internet access, or those who prefer to work in
air-gapped environments, this might not be want you want, and this feature can
be disabled.

To disable automatic provider updates, set `disable_provider_auto_update` into
your `crush.json` config:

```json
{
  &quot;$schema&quot;: &quot;https://charm.land/crush.json&quot;,
  &quot;options&quot;: {
    &quot;disable_provider_auto_update&quot;: true
  }
}
```

Or set the `CRUSH_DISABLE_PROVIDER_AUTO_UPDATE` environment variable:

```bash
export CRUSH_DISABLE_PROVIDER_AUTO_UPDATE=1
```

### Manually updating providers

Manually updating providers is possible with the `crush update-providers`
command:

```bash
# Update providers remotely from Catwalk.
crush update-providers

# Update providers from a custom Catwalk base URL.
crush update-providers https://example.com/

# Update providers from a local file.
crush update-providers /path/to/local-providers.json

# Reset providers to the embedded version, embedded at crush at build time.
crush update-providers embedded

# For more info:
crush update-providers --help
```

## Metrics

Crush records pseudonymous usage metrics (tied to a device-specific hash),
which maintainers rely on to inform development and support priorities. The
metrics include solely usage metadata; prompts and responses are NEVER
collected.

Details on exactly what‚Äôs collected are in the source code ([here](https://github.com/charmbracelet/crush/tree/main/internal/event)
and [here](https://github.com/charmbracelet/crush/blob/main/internal/llm/agent/event.go)).

You can opt out of metrics collection at any time by setting the environment
variable by setting the following in your environment:

```bash
export CRUSH_DISABLE_METRICS=1
```

Or by setting the following in your config:

```json
{
  &quot;options&quot;: {
    &quot;disable_metrics&quot;: true
  }
}
```

Crush also respects the [`DO_NOT_TRACK`](https://consoledonottrack.com)
convention which can be enabled via `export DO_NOT_TRACK=1`.

## Contributing

See the [contributing guide](https://github.com/charmbracelet/crush?tab=contributing-ov-file#contributing).

## Whatcha think?

We‚Äôd love to hear your tho

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[cloudnative-pg/cloudnative-pg]]></title>
            <link>https://github.com/cloudnative-pg/cloudnative-pg</link>
            <guid>https://github.com/cloudnative-pg/cloudnative-pg</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:42 GMT</pubDate>
            <description><![CDATA[CloudNativePG is a comprehensive platform designed to seamlessly manage PostgreSQL databases within Kubernetes environments, covering the entire operational lifecycle from initial deployment to ongoing maintenance]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/cloudnative-pg/cloudnative-pg">cloudnative-pg/cloudnative-pg</a></h1>
            <p>CloudNativePG is a comprehensive platform designed to seamlessly manage PostgreSQL databases within Kubernetes environments, covering the entire operational lifecycle from initial deployment to ongoing maintenance</p>
            <p>Language: Go</p>
            <p>Stars: 7,548</p>
            <p>Forks: 567</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre>[![CNCF Landscape](https://img.shields.io/badge/CNCF%20Landscape-5699C6)][cncf-landscape]
[![Latest Release](https://img.shields.io/github/v/release/cloudnative-pg/cloudnative-pg.svg)][latest-release]
[![GitHub License](https://img.shields.io/github/license/cloudnative-pg/cloudnative-pg)][license]
[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/9933/badge)][openssf]
[![OpenSSF Scorecard Badge][openssf-scorecard-badge]][openssf-socrecard-view]
[![Documentation][documentation-badge]][documentation]
[![Stack Overflow](https://img.shields.io/badge/stackoverflow-cloudnative--pg-blue?logo=stackoverflow&amp;logoColor=%23F48024&amp;link=https%3A%2F%2Fstackoverflow.com%2Fquestions%2Ftagged%2Fcloudnative-pg)][stackoverflow]
[![FOSSA Status][fossa-badge]][fossa]
[![CLOMonitor](https://img.shields.io/endpoint?url=https://clomonitor.io/api/projects/cncf/cloudnative-pg/badge)](https://clomonitor.io/projects/cncf/cloudnative-pg)
[![Artifact Hub](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/cloudnative-pg)](https://artifacthub.io/packages/search?repo=cloudnative-pg)

# Welcome to the CloudNativePG Project!

**CloudNativePG (CNPG)** is an open-source platform designed to seamlessly
manage [PostgreSQL](https://www.postgresql.org/) databases in Kubernetes
environments. It covers the entire operational lifecycle‚Äîfrom deployment to
ongoing maintenance‚Äîthrough its core component, the CloudNativePG operator.

## Table of Contents

- [Code of Conduct](CODE_OF_CONDUCT.md)
- [Governance Policies](https://github.com/cloudnative-pg/governance/blob/main/GOVERNANCE.md)
- [Contributing](CONTRIBUTING.md)
- [Adopters](ADOPTERS.md)
- [Commercial Support](https://cloudnative-pg.io/support/)
- [License](LICENSE)

## Getting Started

The best way to get started is the [Quickstart Guide](https://cloudnative-pg.io/documentation/current/quickstart/).

## Scope

### Mission

CloudNativePG aims to increase PostgreSQL adoption within Kubernetes by making
it an integral part of the development process and GitOps-driven CI/CD
automation.

### Core Principles &amp; Features

Designed by PostgreSQL experts for Kubernetes administrators, CloudNativePG
follows a Kubernetes-native approach to PostgreSQL primary/standby cluster
management. Instead of relying on external high-availability tools (like
Patroni, repmgr, or Stolon), it integrates directly with the Kubernetes API to
automate database operations that a skilled DBA would perform manually.

Key design decisions include:

- Direct integration with Kubernetes API: The PostgreSQL cluster‚Äôs status is
  available directly in the `Cluster` resource, allowing users to inspect it
  via the Kubernetes API.
- Operator pattern: The operator ensures that the desired PostgreSQL state is
  reconciled automatically, following Kubernetes best practices.
- Immutable application containers: Updates follow an immutable infrastructure
  model, as explained in
  [&quot;Why EDB Chose Immutable Application Containers&quot;](https://www.enterprisedb.com/blog/why-edb-chose-immutable-application-containers).

### How CloudNativePG Works

The operator continuously monitors and updates the PostgreSQL cluster state.
Examples of automated actions include:

- Failover management: If the primary instance fails, the operator elects a new
  primary, updates the cluster status, and orchestrates the transition.
- Scaling read replicas: When the number of desired replicas changes, the
  operator provisions or removes resources such as persistent volumes, secrets,
  and config maps while managing streaming replication.
- Service updates: Kubernetes remains the single source of truth, ensuring
  that PostgreSQL service endpoints are always up to date.
- Rolling updates: When an image is updated, the operator follows a rolling
  strategy‚Äîfirst updating replica pods before performing a controlled
  switchover for the primary.

CloudNativePG manages additional Kubernetes resources to enhance PostgreSQL
management, including: `Backup`, `ClusterImageCatalog`, `Database`,
`ImageCatalog`, `Pooler`, `Publication`, `ScheduledBackup`, and `Subscription`.

## Out of Scope

- **Kubernetes only:** CloudNativePG is dedicated to vanilla Kubernetes
  maintained by the [Cloud Native Computing Foundation
  (CNCF)](https://kubernetes.io/).
- **PostgreSQL only:** CloudNativePG is dedicated to vanilla PostgreSQL
  maintained by the [PostgreSQL Global Development Group
  (PGDG)](https://www.postgresql.org/about/).
- **No support for forks:** Features from PostgreSQL forks will only be
  considered if they can be integrated as extensions or pluggable frameworks.
- **Not a general-purpose database operator:** CloudNativePG does not support
  other databases (e.g., MariaDB).

CloudNativePG can be extended via the [CNPG-I plugin interface](https://github.com/cloudnative-pg/cnpg-i).

## Communications

- [Github Discussions](https://github.com/cloudnative-pg/cloudnative-pg/discussions)
- [Slack](https://cloud-native.slack.com/archives/C08MAUJ7NPM)
  (join the [CNCF Slack Workspace](https://communityinviter.com/apps/cloud-native/cncf)).
- [Twitter](https://twitter.com/CloudNativePg)
- [Mastodon](https://mastodon.social/@CloudNativePG)
- [Bluesky](https://bsky.app/profile/cloudnativepg.bsky.social)

## Resources

- [Roadmap](ROADMAP.md)
- [Website](https://cloudnative-pg.io)
- [FAQ](docs/src/faq.md)
- [Blog](https://cloudnative-pg.io/blog/)
- [CloudNativePG plugin Interface (CNPG-I)](https://github.com/cloudnative-pg/cnpg-i).

## Adopters

A list of publicly known users of the CloudNativePG operator is in [ADOPTERS.md](ADOPTERS.md).
Help us grow our community and CloudNativePG by adding yourself and your
organization to this list!

### CloudNativePG at KubeCon

- November 10, 2025, KubeCon North America 2025 in Atlanta: [&quot;Project Lightning Talk: CloudNativePG: Running Postgres The Kubernetes Way&quot;](https://www.youtube.com/watch?v=pYwYwehQX3U&amp;t=4s) - Gabriele Bartolini, EDB
- November 11, 2025, KubeCon North America 2025 in Atlanta: [&quot;Modern PostgreSQL Authorization With Keycloak: Cloud Native Identity Meets Database Security&quot;](https://www.youtube.com/watch?v=TYgPemq06fg) - Yoshiyuki Tabata, Hitachi, Ltd. &amp; Gabriele Bartolini, EDB
- November 13, 2025, KubeCon North America 2025 in Atlanta: [&quot;Quorum-Based Consistency for Cluster Changes With CloudNativePG Operator&quot;](https://www.youtube.com/watch?v=iQUOO3-JRK4&amp;list=PLj6h78yzYM2MLSW4tUDO2gs2pR5UpiD0C&amp;index=67) - Jeremy Schneider, GEICO Tech &amp; Gabriele Bartolini, EDB
- April 4, 2025, KubeCon Europe in London: [&quot;Consistent Volume Group Snapshots, Unraveling the Magic&quot;](https://sched.co/1tx8g) - Leonardo Cecchi (EDB) and Xing Yang (VMware)
- November 11, 2024, Cloud Native Rejekts NA 2024: [&quot;Maximising Microservice Databases with Kubernetes, Postgres, and CloudNativePG&quot;](https://www.youtube.com/watch?v=uBzl_stoxoc&amp;ab_channel=CloudNativeRejekts) - Gabriele Bartolini (EDB) and Leonardo Cecchi (EDB)
- March 21, 2024, KubeCon Europe 2024 in Paris: [&quot;Scaling Heights: Mastering Postgres Database Vertical Scalability with Kubernetes Storage Magic&quot;](https://kccnceu2024.sched.com/event/1YeM4/scaling-heights-mastering-postgres-database-vertical-scalability-with-kubernetes-storage-magic-gabriele-bartolini-edb-gari-singh-google) - Gari Singh, Google &amp; Gabriele Bartolini, EDB
- March 19, 2024, Data on Kubernetes Day at KubeCon Europe 2024 in Paris: [&quot;From Zero to Hero: Scaling Postgres in Kubernetes Using the Power of CloudNativePG&quot;](https://colocatedeventseu2024.sched.com/event/1YFha/from-zero-to-hero-scaling-postgres-in-kubernetes-using-the-power-of-cloudnativepg-gabriele-bartolini-edb) - Gabriele Bartolini, EDB
- November 7, 2023, KubeCon North America 2023 in Chicago: [&quot;Disaster Recovery with Very Large Postgres Databases (in Kubernetes)&quot;](https://kccncna2023.sched.com/event/1R2ml/disaster-recovery-with-very-large-postgres-databases-gabriele-bartolini-edb-michelle-au-google) - Michelle Au, Google &amp; Gabriele Bartolini, EDB
- October 27, 2022, KubeCon North America 2022 in Detroit: [&quot;Data On Kubernetes, Deploying And Running PostgreSQL And Patterns For Databases In a Kubernetes Cluster&quot;](https://kccncna2022.sched.com/event/182GB/data-on-kubernetes-deploying-and-running-postgresql-and-patterns-for-databases-in-a-kubernetes-cluster-chris-milsted-ondat-gabriele-bartolini-edb) - Chris Milsted, Ondat &amp; Gabriele Bartolini, EDB

### Useful links

- [&quot;Quorum-Based Consistency for Cluster Changes With CloudNativePG Operator&quot;](https://www.youtube.com/watch?v=sRF09UMAlsI) (webinar) - Jeremy Schneider, GEICO Tech &amp; Leonardo Cecchi, EDB
- [Data on Kubernetes (DoK) Community](https://dok.community/)
- [&quot;Cloud Neutral Postgres Databases with Kubernetes and CloudNativePG&quot; by Gabriele Bartolini](https://www.cncf.io/blog/2024/11/20/cloud-neutral-postgres-databases-with-kubernetes-and-cloudnativepg/) (November 2024)
- [&quot;How to migrate your PostgreSQL database in Kubernetes with ~0 downtime from anywhere&quot; by Gabriele Bartolini](https://gabrielebartolini.it/articles/2024/03/cloudnativepg-recipe-5-how-to-migrate-your-postgresql-database-in-kubernetes-with-~0-downtime-from-anywhere/) (March 2024)
- [&quot;Maximizing Microservice Databases with Kubernetes, Postgres, and CloudNativePG&quot; by Gabriele Bartolini](https://gabrielebartolini.it/articles/2024/02/maximizing-microservice-databases-with-kubernetes-postgres-and-cloudnativepg/) (February 2024)
- [&quot;Recommended Architectures for PostgreSQL in Kubernetes&quot; by Gabriele Bartolini](https://www.cncf.io/blog/2023/09/29/recommended-architectures-for-postgresql-in-kubernetes/) (September 2023)
- [&quot;The Current State of Major PostgreSQL Upgrades with CloudNativePG&quot; by Gabriele Bartolini](https://www.enterprisedb.com/blog/current-state-major-postgresql-upgrades-cloudnativepg-kubernetes) (August 2023)
- [&quot;The Rise of the Kubernetes Native Database&quot; by Jeff Carpenter](https://thenewstack.io/the-rise-of-the-kubernetes-native-database/) (December 2022)
- [&quot;Why Run Postgres in Kubernetes?&quot; by Gabriele Bartolini](https://cloudnativenow.com/kubecon-cnc-eu-2022/why-run-postgres-in-kubernetes/) (May 2022)
- [&quot;Shift-Left Security: The Path To PostgreSQL On Kubernetes&quot; by Gabriele Bartolini](https://www.tfir.io/shift-left-security-the-path-to-postgresql-on-kubernetes/) (April 2021)
- [&quot;Local Persistent Volumes and PostgreSQL usage in Kubernetes&quot; by Gabriele Bartolini](https://www.2ndquadrant.com/en/blog/local-persistent-volumes-and-postgresql-usage-in-kubernetes/) (June 2020)

---

&lt;p align=&quot;center&quot;&gt;
We are a &lt;a href=&quot;https://www.cncf.io/sandbox-projects/&quot;&gt;Cloud Native Computing Foundation Sandbox project&lt;/a&gt;.
&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot; align=&quot;center&quot;&gt;
      &lt;picture align=&quot;center&quot;&gt;
         &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/cncf/artwork/blob/main/other/cncf/horizontal/white/cncf-white.svg?raw=true&quot;&gt;
         &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/cncf/artwork/blob/main/other/cncf/horizontal/color/cncf-color.svg?raw=true&quot;&gt;
         &lt;img align=&quot;center&quot; src=&quot;https://github.com/cncf/artwork/blob/main/other/cncf/horizontal/color/cncf-color.svg?raw=true&quot; alt=&quot;CNCF logo&quot; width=&quot;50%&quot;/&gt;
      &lt;/picture&gt;
&lt;/p&gt;

---

&lt;p align=&quot;center&quot;&gt;
CloudNativePG was originally built and sponsored by &lt;a href=&quot;https://www.enterprisedb.com&quot;&gt;EDB&lt;/a&gt;.
&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot; align=&quot;center&quot;&gt;
      &lt;picture align=&quot;center&quot;&gt;
         &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/cloudnative-pg/.github/main/logo/edb_landscape_color_white.svg&quot;&gt;
         &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://raw.githubusercontent.com/cloudnative-pg/.github/main/logo/edb_landscape_color_grey.svg&quot;&gt;
         &lt;img align=&quot;center&quot; src=&quot;https://raw.githubusercontent.com/cloudnative-pg/.github/main/logo/edb_landscape_color_grey.svg&quot; alt=&quot;EDB logo&quot; width=&quot;25%&quot;/&gt;
      &lt;/picture&gt;
&lt;/p&gt;

---

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://www.postgresql.org/about/policies/trademarks/&quot;&gt;Postgres, PostgreSQL, and the Slonik Logo&lt;/a&gt;
are trademarks or registered trademarks of the PostgreSQL Community Association
of Canada, and used with their permission.
&lt;/p&gt;

---

[cncf-landscape]: https://landscape.cncf.io/?item=app-definition-and-development--database--cloudnativepg
[stackoverflow]: https://stackoverflow.com/questions/tagged/cloudnative-pg
[latest-release]: https://github.com/cloudnative-pg/cloudnative-pg/releases/latest
[documentation]: https://cloudnative-pg.io/documentation/current/
[license]: https://github.com/cloudnative-pg/cloudnative-pg?tab=Apache-2.0-1-ov-file#readme
[openssf]: https://www.bestpractices.dev/projects/9933
[openssf-scorecard-badge]: https://api.scorecard.dev/projects/github.com/cloudnative-pg/cloudnative-pg/badge
[openssf-socrecard-view]: https://scorecard.dev/viewer/?uri=github.com/cloudnative-pg/cloudnative-pg
[documentation-badge]: https://img.shields.io/badge/Documentation-white?logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAGN0lEQVR4nJRXXWwcVxU%2B8%2F%2BzP%2BPZtR2v7dqy07jUJUALNaiK6lZyUVVKWgGKaIv8QCMekBAVQlQICcEzVZFQVYFKQhASEBHlISJPCRJEshTFChgrIYHEiYMh69jetffHM7Mzc%2B9Bs7vjnTs7yZpZWbt37s%2F5zne%2Bc861CD0eXRkbHc3NfjeffvxNAGEAgULD2756v35%2B3qe1Nc4fnQVEXlA2LnOcXlCF8S%2B6vvVgq%2FL3M65X3e51PvfQCU4WJgZe%2B8GQ8fS7AKgjBB8KEHwjDXZSjkf0CREAaXM2eI9c65siqWxWl360Xl74ANHz%2Fy8AitxnTBfmz%2BhyYS4wGhwObQCIHSA0AigOMBzvOsXzd4pnjyL6NMmWEH8hi2b28Og3%2FqRJA0ewfQy0v1vGO2NovwPo%2FEU%2FwVgSU1PI%2BSu79v3lJAB8HM%2BTI%2FO%2FUUXzM4xHIe0xI4DdRqOAwnF%2F38ePPyzaDIDh%2FMxcWh462m08aojuGY97C0nrAEHg9BlF0fmeAPr0J15vbaKsp0BZQzEDEAlP9B209UIIVXUta%2FQEQHwxgxFjTc%2BRskAwrgVWmHtg22vMPJwLDqGUNJIAMHVAkGu3WdpZz6NAkgSXpINSycluV28er1a3rJ4M3F2%2F9AtCvXKycRrTQttrjINjxxxIL9jevxdaDHU%2FTBr6pL5ruzuLZubgUQBOY2hPij3GBUe7tBCMBRE2KrXVSz0BBI%2FtPVgtV%2F%2FxkZ5WSjI%2F%2BFIXC3sHJwgT4yFqrZFFTSlVrp3sGYLwcfxSmXCbS00j2Ms4K7qkOsFx6qdTuiHtG4AimfmM8NyvOvR2G48qXtZ2fsfrN7%2BqpcRyUp0glKiimDm4TwAcHBp%2B9WeA4ki0GMWNR9OVF8BZvn7xtI%2FF09H8jzLEgz6yLwCDuelnFXHkTZZOytCOEdqDOtGwsm%2BNj00fXt%2B6%2Bj4vcA7bwNrZwENmXwAKuZnvsNRThs5ozMPfPiHyoDF7xiduHcXb70A8dRFheHjiySQATBZk0nl9MHPkBEWUoEtYjyrPFNwGzfdlD37Zdu98KCv%2BMmD2BYpUCvcST39e0%2BS1Wr249FAAg7mPzWrS5NstEbE0xrsiA6QN1PfRFLnhr%2BspxVJTlY8Mw1DqNXeyCQFREEXz9cHB0QOev73QaNhOF4B%2B45PHFHFgDhJTqjuubJFqX1KQco7NTTuW8kq95k2G4eLEGzM7lfItnjNeTKcOfV%2FT8hOuV77A9IK0XjgMpCO0ZiuV3L%2F6njCFAOmucGB3OII5XgCXEJTDdZLElVbu3Vz0fWexvL30k0B6ggBACOmIUBAEUKX0dDTvW7RCYcdZPq6n%2FSsQnUO2RuyBRgQ9Rc5mMvJ6CNIj1nXfd9qWAsCkaZzJAk1L8UjVqY737dSjfCGrPHWqXL32Q0mB%2F2BXnke00WaEYv2aTzAbnuV5pcWkDGAAGJmhSafh6hjr%2BW2SVYHrP7bb%2BOdPW%2FUgflGlTM2gaK%2Ft7tp6%2BN6yixdN89DcIwGktIFPABfNbwoQqQWEUnDJzg1g0jDeK5p7Kp7nensXFI7uyAr%2FLyM7fYLnpa6LYScE8vDnot5hrKlslm%2BfE3nVxJgO4o3KcYu%2FF8XM8yFQ27n%2F65Te%2FzKl3Jhpjj6TCIDneRD5%2FItxr1vdkALw7p1qfeWPpjHxMtsXaPxu6FLc%2BrnbSB1r7fcrlr36nqwMzQfnplJDryQCGOh%2FbLjhcM%2FEvQ4Pdund9xRV5m1LfTXaF%2BK9gsLGB9nsgddcz8thM%2FarPzYM8%2FFazf9sMFaU%2Fi%2FwvNANwEhPvUGR8ozn7d%2BiDKXixtKpbHp81nV9E7puRy31ixKUbOe%2Fv3Ud891ghhDrL5Z975eaOvV%2BCNRp0Gfz%2BcJjDABdTwlpdfKbId0t5XYAcHz5D5ZVtWUp9%2Flog2L7PgVJqZx0HOE5Cqghemv1%2Bt%2FeGBmZ%2BdB2yNN72UEpnzXG32YADA186i3bIpPxMhuKrFK%2Fd77JUnbkKbYvRJlC8DzKSZK76Lq1he2dKy%2BZuSfesSz5a2xHDbLJ%2BJaqdv5H4EUY%2BzbG2m9HgN7mg81bfw4W1uu7AjvHaqDhqF%2FZ3Fq5XFy%2FcESSDsx5fvZ7wLEsNfXk%2BjlVHfpSCOB%2FAQAA%2F%2F8zd8orZc2N9AAAAABJRU5ErkJggg%3D%3D
[fossa-badge]: https://app.fossa.com/api/projects/git%2Bgithub.com%2Fcloudnative-pg%2Fcloudnative-pg.svg?type=small
[fossa]: https://app.fossa.com/projects/git%2Bgithub.com%2Fcloudnative-pg%2Fcloudnative-pg?ref=badge_small
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[open-telemetry/opentelemetry-go]]></title>
            <link>https://github.com/open-telemetry/opentelemetry-go</link>
            <guid>https://github.com/open-telemetry/opentelemetry-go</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:41 GMT</pubDate>
            <description><![CDATA[OpenTelemetry Go API and SDK]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/open-telemetry/opentelemetry-go">open-telemetry/opentelemetry-go</a></h1>
            <p>OpenTelemetry Go API and SDK</p>
            <p>Language: Go</p>
            <p>Stars: 6,196</p>
            <p>Forks: 1,235</p>
            <p>Stars today: 2 stars today</p>
            <h2>README</h2><pre># OpenTelemetry-Go

[![ci](https://github.com/open-telemetry/opentelemetry-go/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/open-telemetry/opentelemetry-go/actions/workflows/ci.yml)
[![codecov.io](https://codecov.io/gh/open-telemetry/opentelemetry-go/coverage.svg?branch=main)](https://app.codecov.io/gh/open-telemetry/opentelemetry-go?branch=main)
[![PkgGoDev](https://pkg.go.dev/badge/go.opentelemetry.io/otel)](https://pkg.go.dev/go.opentelemetry.io/otel)
[![Go Report Card](https://goreportcard.com/badge/go.opentelemetry.io/otel)](https://goreportcard.com/report/go.opentelemetry.io/otel)
[![OpenSSF Scorecard](https://api.scorecard.dev/projects/github.com/open-telemetry/opentelemetry-go/badge)](https://scorecard.dev/viewer/?uri=github.com/open-telemetry/opentelemetry-go)
[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/9996/badge)](https://www.bestpractices.dev/projects/9996)
[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/opentelemetry-go.svg)](https://issues.oss-fuzz.com/issues?q=project:opentelemetry-go)
[![FOSSA Status](https://app.fossa.com/api/projects/custom%2B162%2Fgithub.com%2Fopen-telemetry%2Fopentelemetry-go.svg?type=shield&amp;issueType=license)](https://app.fossa.com/projects/custom%2B162%2Fgithub.com%2Fopen-telemetry%2Fopentelemetry-go?ref=badge_shield&amp;issueType=license)
[![Slack](https://img.shields.io/badge/slack-@cncf/otel--go-brightgreen.svg?logo=slack)](https://cloud-native.slack.com/archives/C01NPAXACKT)

OpenTelemetry-Go is the [Go](https://golang.org/) implementation of [OpenTelemetry](https://opentelemetry.io/).
It provides a set of APIs to directly measure performance and behavior of your software and send this data to observability platforms.

## Project Status

| Signal  | Status             |
|---------|--------------------|
| Traces  | Stable             |
| Metrics | Stable             |
| Logs    | Beta[^1]           |

Progress and status specific to this repository is tracked in our
[project boards](https://github.com/open-telemetry/opentelemetry-go/projects)
and
[milestones](https://github.com/open-telemetry/opentelemetry-go/milestones).

Project versioning information and stability guarantees can be found in the
[versioning documentation](VERSIONING.md).

[^1]: https://github.com/orgs/open-telemetry/projects/43

### Compatibility

OpenTelemetry-Go ensures compatibility with the current supported versions of
the [Go language](https://golang.org/doc/devel/release#policy):

&gt; Each major Go release is supported until there are two newer major releases.
&gt; For example, Go 1.5 was supported until the Go 1.7 release, and Go 1.6 was supported until the Go 1.8 release.

For versions of Go that are no longer supported upstream, opentelemetry-go will
stop ensuring compatibility with these versions in the following manner:

- A minor release of opentelemetry-go will be made to add support for the new
  supported release of Go.
- The following minor release of opentelemetry-go will remove compatibility
  testing for the oldest (now archived upstream) version of Go. This, and
  future, releases of opentelemetry-go may include features only supported by
  the currently supported versions of Go.

Currently, this project supports the following environments.

| OS       | Go Version | Architecture |
|----------|------------|--------------|
| Ubuntu   | 1.25       | amd64        |
| Ubuntu   | 1.24       | amd64        |
| Ubuntu   | 1.25       | 386          |
| Ubuntu   | 1.24       | 386          |
| Ubuntu   | 1.25       | arm64        |
| Ubuntu   | 1.24       | arm64        |
| macOS    | 1.25       | amd64        |
| macOS    | 1.24       | amd64        |
| macOS    | 1.25       | arm64        |
| macOS    | 1.24       | arm64        |
| Windows  | 1.25       | amd64        |
| Windows  | 1.24       | amd64        |
| Windows  | 1.25       | 386          |
| Windows  | 1.24       | 386          |

While this project should work for other systems, no compatibility guarantees
are made for those systems currently.

## Getting Started

You can find a getting started guide on [opentelemetry.io](https://opentelemetry.io/docs/languages/go/getting-started/).

OpenTelemetry&#039;s goal is to provide a single set of APIs to capture distributed
traces and metrics from your application and send them to an observability
platform. This project allows you to do just that for applications written in
Go. There are two steps to this process: instrument your application, and
configure an exporter.

### Instrumentation

To start capturing distributed traces and metric events from your application
it first needs to be instrumented. The easiest way to do this is by using an
instrumentation library for your code. Be sure to check out [the officially
supported instrumentation
libraries](https://github.com/open-telemetry/opentelemetry-go-contrib/tree/main/instrumentation).

If you need to extend the telemetry an instrumentation library provides or want
to build your own instrumentation for your application directly you will need
to use the
[Go otel](https://pkg.go.dev/go.opentelemetry.io/otel)
package. The [examples](https://github.com/open-telemetry/opentelemetry-go-contrib/tree/main/examples)
are a good way to see some practical uses of this process.

### Export

Now that your application is instrumented to collect telemetry, it needs an
export pipeline to send that telemetry to an observability platform.

All officially supported exporters for the OpenTelemetry project are contained in the [exporters directory](./exporters).

| Exporter                              | Logs | Metrics | Traces |
|---------------------------------------|:----:|:-------:|:------:|
| [OTLP](./exporters/otlp/)             |  ‚úì   |    ‚úì    |   ‚úì    |
| [Prometheus](./exporters/prometheus/) |      |    ‚úì    |        |
| [stdout](./exporters/stdout/)         |  ‚úì   |    ‚úì    |   ‚úì    |
| [Zipkin](./exporters/zipkin/)         |      |         |   ‚úì    |

## Contributing

See the [contributing documentation](CONTRIBUTING.md).
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
    </channel>
</rss>