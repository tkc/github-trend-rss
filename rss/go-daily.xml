<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for go - Go Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for go.</description>
        <lastBuildDate>Thu, 25 Dec 2025 00:05:36 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[danielmiessler/Fabric]]></title>
            <link>https://github.com/danielmiessler/Fabric</link>
            <guid>https://github.com/danielmiessler/Fabric</guid>
            <pubDate>Thu, 25 Dec 2025 00:05:36 GMT</pubDate>
            <description><![CDATA[Fabric is an open-source framework for augmenting humans using AI. It provides a modular system for solving specific problems using a crowdsourced set of AI prompts that can be used anywhere.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/danielmiessler/Fabric">danielmiessler/Fabric</a></h1>
            <p>Fabric is an open-source framework for augmenting humans using AI. It provides a modular system for solving specific problems using a crowdsourced set of AI prompts that can be used anywhere.</p>
            <p>Language: Go</p>
            <p>Stars: 36,505</p>
            <p>Forks: 3,672</p>
            <p>Stars today: 137 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://go.warp.dev/fabric&quot; target=&quot;_blank&quot;&gt;
        &lt;sup&gt;Special thanks to:&lt;/sup&gt;
        &lt;br&gt;
        &lt;img alt=&quot;Warp sponsorship&quot; width=&quot;400&quot; src=&quot;https://raw.githubusercontent.com/warpdotdev/brand-assets/refs/heads/main/Github/Sponsor/Warp-Github-LG-02.png&quot;&gt;
        &lt;br&gt;
        &lt;h&gt;Warp, built for coding with multiple AI agents&lt;/b&gt;
        &lt;br&gt;
        &lt;sup&gt;Available for macOS, Linux and Windows&lt;/sup&gt;
    &lt;/a&gt;
&lt;/div&gt;

&lt;br&gt;

&lt;div align=&quot;center&quot;&gt;

&lt;img src=&quot;./docs/images/fabric-logo-gif.gif&quot; alt=&quot;fabriclogo&quot; width=&quot;400&quot; height=&quot;400&quot;/&gt;

# `fabric`

![Static Badge](https://img.shields.io/badge/mission-human_flourishing_via_AI_augmentation-purple)
&lt;br /&gt;
![GitHub top language](https://img.shields.io/github/languages/top/danielmiessler/fabric)
![GitHub last commit](https://img.shields.io/github/last-commit/danielmiessler/fabric)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/danielmiessler/fabric)

&lt;div align=&quot;center&quot;&gt;
&lt;h4&gt;&lt;code&gt;fabric&lt;/code&gt; is an open-source framework for augmenting humans using AI.&lt;/h4&gt;
&lt;/div&gt;

![Screenshot of fabric](./docs/images/fabric-summarize.png)

&lt;/div&gt;

[Updates](#updates) ‚Ä¢
[What and Why](#what-and-why) ‚Ä¢
[Philosophy](#philosophy) ‚Ä¢
[Installation](#installation) ‚Ä¢
[Usage](#usage) ‚Ä¢
[REST API](#rest-api-server) ‚Ä¢
[Examples](#examples) ‚Ä¢
[Just Use the Patterns](#just-use-the-patterns) ‚Ä¢
[Custom Patterns](#custom-patterns) ‚Ä¢
[Helper Apps](#helper-apps) ‚Ä¢
[Meta](#meta)

&lt;/div&gt;

## What and why

Since the start of modern AI in late 2022 we&#039;ve seen an **_extraordinary_** number of AI applications for accomplishing tasks. There are thousands of websites, chat-bots, mobile apps, and other interfaces for using all the different AI out there.

It&#039;s all really exciting and powerful, but _it&#039;s not easy to integrate this functionality into our lives._

&lt;div class=&quot;align center&quot;&gt;
&lt;h4&gt;In other words, AI doesn&#039;t have a capabilities problem‚Äîit has an &lt;em&gt;integration&lt;/em&gt; problem.&lt;/h4&gt;
&lt;/div&gt;

**Fabric was created to address this by creating and organizing the fundamental units of AI‚Äîthe prompts themselves!**

Fabric organizes prompts by real-world task, allowing people to create, collect, and organize their most important AI solutions in a single place for use in their favorite tools. And if you&#039;re command-line focused, you can use Fabric itself as the interface!

## Updates

&lt;details&gt;
&lt;summary&gt;Click to view recent updates&lt;/summary&gt;

Dear Users,

We&#039;ve been doing so many exciting things here at Fabric, I wanted to give a quick summary here to give you a sense of our development velocity!

Below are the **new features and capabilities** we&#039;ve added (newest first):

### Recent Major Features

- [v1.4.356](https://github.com/danielmiessler/fabric/releases/tag/v1.4.356) (Dec 22, 2025) ‚Äî **Complete Internationalization**: Full i18n support for setup prompts across all 10 languages with intelligent environment variable handling‚Äîmaking Fabric truly accessible worldwide while maintaining configuration consistency.
- [v1.4.350](https://github.com/danielmiessler/fabric/releases/tag/v1.4.350) (Dec 18, 2025) ‚Äî **Interactive API Documentation**: Adds Swagger/OpenAPI UI at `/swagger/index.html` with comprehensive REST API documentation, enhanced developer guides, and improved endpoint discoverability for easier integration.
- [v1.4.338](https://github.com/danielmiessler/fabric/releases/tag/v1.4.338) (Dec 4, 2025) ‚Äî Add Abacus vendor support for Chat-LLM
  models (see [RouteLLM APIs](https://abacus.ai/app/route-llm-apis)).
- [v1.4.337](https://github.com/danielmiessler/fabric/releases/tag/v1.4.337) (Dec 4, 2025) ‚Äî Add &quot;Z AI&quot; vendor support. See the [Z AI overview](https://docs.z.ai/guides/overview/overview) page for more details.
- [v1.4.334](https://github.com/danielmiessler/fabric/releases/tag/v1.4.334) (Nov 26, 2025) ‚Äî **Claude Opus 4.5**: Updates the Anthropic SDK to the latest and adds the new [Claude Opus 4.5](https://www.anthropic.com/news/claude-opus-4-5) to the available models.
- [v1.4.331](https://github.com/danielmiessler/fabric/releases/tag/v1.4.331) (Nov 23, 2025) ‚Äî **Support for GitHub Models**: Adds support for using GitHub Models.
- [v1.4.322](https://github.com/danielmiessler/fabric/releases/tag/v1.4.322) (Nov 5, 2025) ‚Äî **Interactive HTML Concept Maps and Claude Sonnet 4.5**: Adds `create_conceptmap` pattern for visual knowledge representation using Vis.js, introduces WELLNESS category with psychological analysis patterns, and upgrades to Claude Sonnet 4.5
- [v1.4.317](https://github.com/danielmiessler/fabric/releases/tag/v1.4.317) (Sep 21, 2025) ‚Äî **Portuguese Language Variants**: Adds BCP 47 locale normalization with support for Brazilian Portuguese (pt-BR) and European Portuguese (pt-PT) with intelligent fallback chains
- [v1.4.314](https://github.com/danielmiessler/fabric/releases/tag/v1.4.314) (Sep 17, 2025) ‚Äî **Azure OpenAI Migration**: Migrates to official `openai-go/azure` SDK with improved authentication and default API version support
- [v1.4.311](https://github.com/danielmiessler/fabric/releases/tag/v1.4.311) (Sep 13, 2025) ‚Äî **More internationalization support**: Adds de (German), fa (Persian / Farsi), fr (French), it (Italian),
  ja (Japanese), pt (Portuguese), zh (Chinese)
- [v1.4.309](https://github.com/danielmiessler/fabric/releases/tag/v1.4.309) (Sep 9, 2025) ‚Äî **Comprehensive internationalization support**: Includes English and Spanish locale files.
- [v1.4.303](https://github.com/danielmiessler/fabric/releases/tag/v1.4.303) (Aug 29, 2025) ‚Äî **New Binary Releases**: Linux ARM and Windows ARM targets. You can run Fabric on the Raspberry PI and on your Windows Surface!
- [v1.4.294](https://github.com/danielmiessler/fabric/releases/tag/v1.4.294) (Aug 20, 2025) ‚Äî **Venice AI Support**: Added the Venice AI provider. Venice is a Privacy-First, Open-Source AI provider. See their [&quot;About Venice&quot;](https://docs.venice.ai/overview/about-venice) page for details.
- [v1.4.291](https://github.com/danielmiessler/fabric/releases/tag/v1.4.291) (Aug 18, 2025) ‚Äî **Speech To Text**: Add OpenAI speech-to-text support with `--transcribe-file`, `--transcribe-model`, and `--split-media-file` flags.
- [v1.4.287](https://github.com/danielmiessler/fabric/releases/tag/v1.4.287) (Aug 16, 2025) ‚Äî **AI Reasoning**: Add Thinking to Gemini models and introduce `readme_updates` python script
- [v1.4.286](https://github.com/danielmiessler/fabric/releases/tag/v1.4.286) (Aug 14, 2025) ‚Äî **AI Reasoning**: Introduce Thinking Config Across Anthropic and OpenAI Providers
- [v1.4.285](https://github.com/danielmiessler/fabric/releases/tag/v1.4.285) (Aug 13, 2025) ‚Äî **Extended Context**: Enable One Million Token Context Beta Feature for Sonnet-4
- [v1.4.284](https://github.com/danielmiessler/fabric/releases/tag/v1.4.284) (Aug 12, 2025) ‚Äî **Easy Shell Completions Setup**: Introduce One-Liner Curl Install for Completions
- [v1.4.283](https://github.com/danielmiessler/fabric/releases/tag/v1.4.283) (Aug 12, 2025) ‚Äî **Model Management**: Add Vendor Selection Support for Models
- [v1.4.282](https://github.com/danielmiessler/fabric/releases/tag/v1.4.282) (Aug 11, 2025) ‚Äî **Enhanced Shell Completions**: Enhanced Shell Completions for Fabric CLI Binaries
- [v1.4.281](https://github.com/danielmiessler/fabric/releases/tag/v1.4.281) (Aug 11, 2025) ‚Äî **Gemini Search Tool**: Add Web Search Tool Support for Gemini Models
- [v1.4.278](https://github.com/danielmiessler/fabric/releases/tag/v1.4.278) (Aug 9, 2025) ‚Äî **Enhance YouTube Transcripts**: Enhance YouTube Support with Custom yt-dlp Arguments
- [v1.4.277](https://github.com/danielmiessler/fabric/releases/tag/v1.4.277) (Aug 8, 2025) ‚Äî **Desktop Notifications**: Add cross-platform desktop notifications to Fabric CLI
- [v1.4.274](https://github.com/danielmiessler/fabric/releases/tag/v1.4.274) (Aug 7, 2025) ‚Äî **Claude 4.1 Added**: Add Support for Claude Opus 4.1 Model
- [v1.4.271](https://github.com/danielmiessler/fabric/releases/tag/v1.4.271) (Jul 28, 2025) ‚Äî **AI Summarized Release Notes**: Enable AI summary updates for GitHub releases
- [v1.4.268](https://github.com/danielmiessler/fabric/releases/tag/v1.4.268) (Jul 26, 2025) ‚Äî **Gemini TTS Voice Selection**: add Gemini TTS voice selection and listing functionality
- [v1.4.267](https://github.com/danielmiessler/fabric/releases/tag/v1.4.267) (Jul 26, 2025) ‚Äî **Text-to-Speech**: Update Gemini Plugin to New SDK with TTS Support
- [v1.4.258](https://github.com/danielmiessler/fabric/releases/tag/v1.4.258) (Jul 17, 2025) ‚Äî **Onboarding Improved**: Add startup check to initialize config and .env file automatically
- [v1.4.257](https://github.com/danielmiessler/fabric/releases/tag/v1.4.257) (Jul 17, 2025) ‚Äî **OpenAI Routing Control**: Introduce CLI Flag to Disable OpenAI Responses API
- [v1.4.252](https://github.com/danielmiessler/fabric/releases/tag/v1.4.252) (Jul 16, 2025) ‚Äî **Hide Thinking Block**: Optional Hiding of Model Thinking Process with Configurable Tags
- [v1.4.246](https://github.com/danielmiessler/fabric/releases/tag/v1.4.246) (Jul 14, 2025) ‚Äî **Automatic ChangeLog Updates**: Add AI-powered changelog generation with high-performance Go tool and comprehensive caching
- [v1.4.245](https://github.com/danielmiessler/fabric/releases/tag/v1.4.245) (Jul 11, 2025) ‚Äî **Together AI**: Together AI Support with OpenAI Fallback Mechanism Added
- [v1.4.232](https://github.com/danielmiessler/fabric/releases/tag/v1.4.232) (Jul 6, 2025) ‚Äî **Add Custom**: Add Custom Patterns Directory Support
- [v1.4.231](https://github.com/danielmiessler/fabric/releases/tag/v1.4.231) (Jul 5, 2025) ‚Äî **OAuth Auto-Auth**: OAuth Authentication Support for Anthropic (Use your Max Subscription)
- [v1.4.230](https://github.com/danielmiessler/fabric/releases/tag/v1.4.230) (Jul 5, 2025) ‚Äî **Model Management**: Add advanced image generation parameters for OpenAI models with four new CLI flags
- [v1.4.227](https://github.com/danielmiessler/fabric/releases/tag/v1.4.227) (Jul 4, 2025) ‚Äî **Add Image**: Add Image Generation Support to Fabric
- [v1.4.226](https://github.com/danielmiessler/fabric/releases/tag/v1.4.226) (Jul 4, 2025) ‚Äî **Web Search**: OpenAI Plugin Now Supports Web Search Functionality
- [v1.4.225](https://github.com/danielmiessler/fabric/releases/tag/v1.4.225) (Jul 4, 2025) ‚Äî **Web Search**: Runtime Web Search Control via Command-Line `--search` Flag
- [v1.4.224](https://github.com/danielmiessler/fabric/releases/tag/v1.4.224) (Jul 1, 2025) ‚Äî **Add code_review**: Add code_review pattern and updates in Pattern_Descriptions
- [v1.4.222](https://github.com/danielmiessler/fabric/releases/tag/v1.4.222) (Jul 1, 2025) ‚Äî **OpenAI Plugin**: OpenAI Plugin Migrates to New Responses API
- [v1.4.218](https://github.com/danielmiessler/fabric/releases/tag/v1.4.218) (Jun 27, 2025) ‚Äî **Model Management**: Add Support for OpenAI Search and Research Model Variants
- [v1.4.217](https://github.com/danielmiessler/fabric/releases/tag/v1.4.217) (Jun 26, 2025) ‚Äî **New YouTube**: New YouTube Transcript Endpoint Added to REST API
- [v1.4.212](https://github.com/danielmiessler/fabric/releases/tag/v1.4.212) (Jun 23, 2025) ‚Äî **Add Langdock**: Add Langdock AI and enhance generic OpenAI compatible support
- [v1.4.211](https://github.com/danielmiessler/fabric/releases/tag/v1.4.211) (Jun 19, 2025) ‚Äî **REST API**: REST API and Web UI Now Support Dynamic Pattern Variables
- [v1.4.210](https://github.com/danielmiessler/fabric/releases/tag/v1.4.210) (Jun 18, 2025) ‚Äî **Add Citations**: Add Citation Support to Perplexity Response
- [v1.4.208](https://github.com/danielmiessler/fabric/releases/tag/v1.4.208) (Jun 17, 2025) ‚Äî **Add Perplexity**: Add Perplexity AI Provider with Token Limits Support
- [v1.4.203](https://github.com/danielmiessler/fabric/releases/tag/v1.4.203) (Jun 14, 2025) ‚Äî **Add Amazon Bedrock**: Add support for Amazon Bedrock

These features represent our commitment to making Fabric the most powerful and flexible AI augmentation framework available!

&lt;/details&gt;

## Intro videos

Keep in mind that many of these were recorded when Fabric was Python-based, so remember to use the current [install instructions](#installation) below.

- [Network Chuck](https://www.youtube.com/watch?v=UbDyjIIGaxQ)
- [David Bombal](https://www.youtube.com/watch?v=vF-MQmVxnCs)
- [My Own Intro to the Tool](https://www.youtube.com/watch?v=wPEyyigh10g)
- [More Fabric YouTube Videos](https://www.youtube.com/results?search_query=fabric+ai)

## Navigation

- [`fabric`](#fabric)
  - [What and why](#what-and-why)
  - [Updates](#updates)
    - [Recent Major Features](#recent-major-features)
  - [Intro videos](#intro-videos)
  - [Navigation](#navigation)
  - [Changelog](#changelog)
  - [Philosophy](#philosophy)
    - [Breaking problems into components](#breaking-problems-into-components)
    - [Too many prompts](#too-many-prompts)
  - [Installation](#installation)
    - [One-Line Install (Recommended)](#one-line-install-recommended)
    - [Manual Binary Downloads](#manual-binary-downloads)
    - [Using package managers](#using-package-managers)
      - [macOS (Homebrew)](#macos-homebrew)
      - [Arch Linux (AUR)](#arch-linux-aur)
      - [Windows](#windows)
    - [From Source](#from-source)
    - [Docker](#docker)
    - [Environment Variables](#environment-variables)
    - [Setup](#setup)
    - [Per-Pattern Model Mapping](#per-pattern-model-mapping)
    - [Add aliases for all patterns](#add-aliases-for-all-patterns)
      - [Save your files in markdown using aliases](#save-your-files-in-markdown-using-aliases)
    - [Migration](#migration)
    - [Upgrading](#upgrading)
    - [Shell Completions](#shell-completions)
      - [Quick install (no clone required)](#quick-install-no-clone-required)
      - [Zsh Completion](#zsh-completion)
      - [Bash Completion](#bash-completion)
      - [Fish Completion](#fish-completion)
  - [Usage](#usage)
    - [Debug Levels](#debug-levels)
    - [Extensions](#extensions)
  - [REST API Server](#rest-api-server)
  - [Our approach to prompting](#our-approach-to-prompting)
  - [Examples](#examples)
  - [Just use the Patterns](#just-use-the-patterns)
    - [Prompt Strategies](#prompt-strategies)
  - [Custom Patterns](#custom-patterns)
    - [Setting Up Custom Patterns](#setting-up-custom-patterns)
    - [Using Custom Patterns](#using-custom-patterns)
    - [How It Works](#how-it-works)
  - [Helper Apps](#helper-apps)
    - [`to_pdf`](#to_pdf)
    - [`to_pdf` Installation](#to_pdf-installation)
    - [`code_helper`](#code_helper)
  - [pbpaste](#pbpaste)
  - [Web Interface (Fabric Web App)](#web-interface-fabric-web-app)
  - [Meta](#meta)
    - [Primary contributors](#primary-contributors)
    - [Contributors](#contributors)

&lt;br /&gt;

## Changelog

Fabric is evolving rapidly.

Stay current with the latest features by reviewing the [CHANGELOG](./CHANGELOG.md) for all recent changes.

## Philosophy

&gt; AI isn&#039;t a thing; it&#039;s a _magnifier_ of a thing. And that thing is **human creativity**.

We believe the purpose of technology is to help humans flourish, so when we talk about AI we start with the **human** problems we want to solve.

### Breaking problems into components

Our approach is to break problems into individual pieces (see below) and then apply AI to them one at a time. See below for some examples.

&lt;img width=&quot;2078&quot; alt=&quot;augmented_challenges&quot; src=&quot;https://github.com/danielmiessler/fabric/assets/50654/31997394-85a9-40c2-879b-b347e4701f06&quot;&gt;

### Too many prompts

Prompts are good for this, but the biggest challenge I faced in 2023‚Äî‚Äîwhich still exists today‚Äîis **the sheer number of AI prompts out there**. We all have prompts that are useful, but it&#039;s hard to discover new ones, know if they are good or not, _and manage different versions of the ones we like_.

One of `fabric`&#039;s primary features is helping people collect and integrate prompts, which we call _Patterns_, into various parts of their lives.

Fabric has Patterns for all sorts of life and work activities, including:

- Extracting the most interesting parts of YouTube videos and podcasts
- Writing an essay in your own voice with just an idea as an input
- Summarizing opaque academic papers
- Creating perfectly matched AI art prompts for a piece of writing
- Rating the quality of content to see if you want to read/watch the whole thing
- Getting summaries of long, boring content
- Explaining code to you
- Turning bad documentation into usable documentation
- Creating social media posts from any content input
- And a million more‚Ä¶

## Installation

### One-Line Install (Recommended)

**Unix/Linux/macOS:**

```bash
curl -fsSL https://raw.githubusercontent.com/danielmiessler/fabric/main/scripts/installer/install.sh | bash
```

**Windows PowerShell:**

```powershell
iwr -useb https://raw.githubusercontent.com/danielmiessler/fabric/main/scripts/installer/install.ps1 | iex
```

&gt; See [scripts/installer/README.md](./scripts/installer/README.md) for custom installation options and troubleshooting.

### Manual Binary Downloads

The latest release binary archives and their expected SHA256 hashes can be found at &lt;https://github.com/danielmiessler/fabric/releases/latest&gt;

### Using package managers

**NOTE:** using Homebrew or the Arch Linux package managers makes `fabric` available as `fabric-ai`, so add
the following alias to your shell startup files to account for this:

```bash
alias fabric=&#039;fabric-ai&#039;
```

#### macOS (Homebrew)

`brew install fabric-ai`

#### Arch Linux (AUR)

`yay -S fabric-ai`

#### Windows

Use the official Microsoft supported `Winget` tool:

`winget install danielmiessler.Fabric`

### From Source

To install Fabric, [make sure Go is installed](https://go.dev/doc/install), and then run the following command.

```bash
# Install Fabric directly from the repo
go install github.com/danielmiessler/fabric/cmd/fabric@latest
```

### Docker

Run Fabric using pre-built Docker images:

```bash
# Use latest image from Docker Hub
docker run --rm -it kayvan/fabric:latest --version

# Use specific version from GHCR
docker run --rm -it ghcr.io/ksylvan/fabric:v1.4.305 --version

# Run setup (first time)
mkdir -p $HOME/.fabric-config
docker run --rm -it -v $HOME/.fabric-config:/root/.config/fabric kayvan/fabric:latest --setup

# Use Fabric with your patterns
docker run --rm -it -v $HOME/.fabric-config:/root/.config/fabric kayvan/fabric:latest -p summarize

# Run the REST API server (see REST API Server section)
docker run --rm -it -p 8080:8080 -v $HOME/.fabric-config:/root/.config/fabric kayvan/fabric:latest --serve
```

**Images available at:**

- Docker Hub: [kayvan/fabric](https://hub.docker.com/repository/docker/kayvan/fabric/general)
- GHCR: [ksylvan/fabric](https://github.com/ksylvan/fabric/pkgs/container/fabric)

See [scripts/docker/README.md](./scripts/docker/README.md) for building custom images and advanced configuration.

### Environment Variables

You may need to set some environment variables in your `~/.bashrc` on linux or `~/.zshrc` file on mac to be able to run the `fabric` command. Here is an example of what you can add:

For Intel based macs or linux

```bash
# Golang environment variables
export GOROOT=/usr/local/go
export GOPATH=$HOME/go

# Update PATH to include GOPATH and GOROOT binaries
export PATH=$GOPATH/bin:$GOROOT/bin:$HOME/.local/bin:$PATH
```

for Apple Silicon based macs

```bash
# Golang environment variables
export GOROOT=$(brew --prefix go)/libexec
export GOPATH=$HOME/go
export PATH=$GOPATH/bin:$GOROOT/bin:$HOME/.local/bin:$PATH
```

### Setup

Now run the following command

```bash
# Run the setup to set up your directories and keys
fabric --setup
```

If everything works you are good to go.

### Per-Pattern Model Mapping

 You can configure specific models for individual patterns using environment variables
 like `FABRIC_MODEL_PATTERN_NAME=vendor|model`

 This makes it easy to maintain these per-pattern model mappings in your shell startup files

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[etcd-io/etcd]]></title>
            <link>https://github.com/etcd-io/etcd</link>
            <guid>https://github.com/etcd-io/etcd</guid>
            <pubDate>Thu, 25 Dec 2025 00:05:35 GMT</pubDate>
            <description><![CDATA[Distributed reliable key-value store for the most critical data of a distributed system]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/etcd-io/etcd">etcd-io/etcd</a></h1>
            <p>Distributed reliable key-value store for the most critical data of a distributed system</p>
            <p>Language: Go</p>
            <p>Stars: 51,114</p>
            <p>Forks: 10,254</p>
            <p>Stars today: 23 stars today</p>
            <h2>README</h2><pre># etcd

[![Go Report Card](https://goreportcard.com/badge/github.com/etcd-io/etcd?style=flat-square)](https://goreportcard.com/report/github.com/etcd-io/etcd)
[![Coverage](https://codecov.io/gh/etcd-io/etcd/branch/main/graph/badge.svg)](https://app.codecov.io/gh/etcd-io/etcd/tree/main)
[![Tests](https://github.com/etcd-io/etcd/actions/workflows/tests.yaml/badge.svg)](https://github.com/etcd-io/etcd/actions/workflows/tests.yaml)
[![codeql-analysis](https://github.com/etcd-io/etcd/actions/workflows/codeql-analysis.yml/badge.svg)](https://github.com/etcd-io/etcd/actions/workflows/codeql-analysis.yml)
[![Docs](https://img.shields.io/badge/docs-latest-green.svg)](https://etcd.io/docs)
[![Godoc](http://img.shields.io/badge/go-documentation-blue.svg?style=flat-square)](https://godocs.io/go.etcd.io/etcd/v3)
[![Releases](https://img.shields.io/github/release/etcd-io/etcd/all.svg?style=flat-square)](https://github.com/etcd-io/etcd/releases)
[![LICENSE](https://img.shields.io/github/license/etcd-io/etcd.svg?style=flat-square)](https://github.com/etcd-io/etcd/blob/main/LICENSE)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/etcd-io/etcd/badge)](https://scorecard.dev/viewer/?uri=github.com/etcd-io/etcd)

**Note**: The `main` branch may be in an *unstable or even broken state* during development. For stable versions, see [releases][github-release].

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/cncf/artwork/9870640f123303a355611065195c43ac3f27aa19/projects/etcd/horizontal/white/etcd-horizontal-white.png&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;logos/etcd-horizontal-color.svg&quot;&gt;
  &lt;img alt=&quot;etcd logo&quot; src=&quot;logos/etcd-horizontal-color.svg&quot; width=269 /&gt;
&lt;/picture&gt;

etcd is a distributed reliable key-value store for the most critical data of a distributed system, with a focus on being:

* *Simple*: well-defined, user-facing API (gRPC)
* *Secure*: automatic TLS with optional client cert authentication
* *Fast*: benchmarked 10,000 writes/sec
* *Reliable*: properly distributed using Raft

etcd is written in Go and uses the [Raft][] consensus algorithm to manage a highly-available replicated log.

etcd is used [in production by many companies](./ADOPTERS.md), and the development team stands behind it in critical deployment scenarios, where etcd is frequently teamed with applications such as [Kubernetes][k8s], [locksmith][], [vulcand][], [Doorman][], and many others. Reliability is further ensured by rigorous [**robustness testing**](https://github.com/etcd-io/etcd/tree/main/tests/robustness).

See [etcdctl][etcdctl] for a simple command line client.

![etcd reliability is important](logos/etcd-xkcd-2347.png)

&lt;sub&gt;Original image credited to  xkcd.com/2347, alterations by Josh Berkus.&lt;/sub&gt;

[raft]: https://raft.github.io/
[k8s]: http://kubernetes.io/
[doorman]: https://github.com/youtube/doorman
[locksmith]: https://github.com/coreos/locksmith
[vulcand]: https://github.com/vulcand/vulcand
[etcdctl]: https://github.com/etcd-io/etcd/tree/main/etcdctl

## Documentation

The most common API documentation you&#039;ll need can be found here:

* [go.etcd.io/etcd/api/v3](https://godocs.io/go.etcd.io/etcd/api/v3)
* [go.etcd.io/etcd/client/pkg/v3](https://godocs.io/go.etcd.io/etcd/client/pkg/v3)
* [go.etcd.io/etcd/client/v3](https://godocs.io/go.etcd.io/etcd/client/v3)
* [go.etcd.io/etcd/etcdctl/v3](https://godocs.io/go.etcd.io/etcd/etcdctl/v3)
* [go.etcd.io/etcd/pkg/v3](https://godocs.io/go.etcd.io/etcd/pkg/v3)
* [go.etcd.io/etcd/raft/v3](https://godocs.io/go.etcd.io/etcd/raft/v3)
* [go.etcd.io/etcd/server/v3](https://godocs.io/go.etcd.io/etcd/server/v3)

## Maintainers

[Maintainers](OWNERS) strive to shape an inclusive open source project culture where users are heard and contributors feel respected and empowered. Maintainers aim to build productive relationships across different companies and disciplines. Read more about [Maintainers role and responsibilities](Documentation/contributor-guide/community-membership.md#maintainers).

## Getting started

### Getting etcd

The easiest way to get etcd is to use one of the pre-built release binaries which are available for OSX, Linux, Windows, and Docker on the [release page][github-release].

For more installation guides, please check out [play.etcd.io](http://play.etcd.io) and [operating etcd](https://etcd.io/docs/latest/op-guide).

[github-release]: https://github.com/etcd-io/etcd/releases

### Running etcd

First start a single-member cluster of etcd.

If etcd is installed using the [pre-built release binaries][github-release], run it from the installation location as below:

```bash
/tmp/etcd-download-test/etcd
```

The etcd command can be simply run as such if it is moved to the system path as below:

```bash
mv /tmp/etcd-download-test/etcd /usr/local/bin/
etcd
```

This will bring up etcd listening on port 2379 for client communication and on port 2380 for server-to-server communication.

Next, let&#039;s set a single key, and then retrieve it:

```bash
etcdctl put mykey &quot;this is awesome&quot;
etcdctl get mykey
```

etcd is now running and serving client requests. For more, please check out:

* [Interactive etcd playground](http://play.etcd.io)
* [Animated quick demo](https://etcd.io/docs/latest/demo)

### etcd TCP ports

The [official etcd ports][iana-ports] are 2379 for client requests, and 2380 for peer communication.

[iana-ports]: http://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.txt

### Running a local etcd cluster

First install [goreman](https://github.com/mattn/goreman), which manages Procfile-based applications.

Our [Procfile script](./Procfile) will set up a local example cluster. Start it with:

```bash
goreman start
```

This will bring up 3 etcd members `infra1`, `infra2` and `infra3` and optionally etcd `grpc-proxy`, which runs locally and composes a cluster.

Every cluster member and proxy accepts key value reads and key value writes.

Follow the comments in [Procfile script](./Procfile) to add a learner node to the cluster.

### Install etcd client v3

```bash
go get go.etcd.io/etcd/client/v3
```

### Next steps

Now it&#039;s time to dig into the full etcd API and other guides.

* Read the full [documentation].
* Review etcd [frequently asked questions].
* Explore the full gRPC [API].
* Set up a [multi-machine cluster][clustering].
* Learn the [config format, env variables and flags][configuration].
* Find [language bindings and tools][integrations].
* Use TLS to [secure an etcd cluster][security].
* [Tune etcd][tuning].

[documentation]: https://etcd.io/docs/latest
[api]: https://etcd.io/docs/latest/learning/api
[clustering]: https://etcd.io/docs/latest/op-guide/clustering
[configuration]: https://etcd.io/docs/latest/op-guide/configuration
[integrations]: https://etcd.io/docs/latest/integrations
[security]: https://etcd.io/docs/latest/op-guide/security
[tuning]: https://etcd.io/docs/latest/tuning

## Contact

* Email: [etcd-dev](https://groups.google.com/g/etcd-dev)
* Slack: [#sig-etcd](https://kubernetes.slack.com/archives/C3HD8ARJ5) channel on Kubernetes ([get an invite](http://slack.kubernetes.io/))
* [Community meetings](#community-meetings)

### Community meetings

etcd contributors and maintainers meet every week at `11:00` AM (USA Pacific) on Thursday and meetings alternate between community meetings and issue triage meetings. Meeting agendas are recorded in a [shared Google doc][shared-meeting-notes] and everyone is welcome to suggest additional topics or other agendas.

Issue triage meetings are aimed at getting through our backlog of PRs and Issues. Triage meetings are open to any contributor; you don&#039;t have to be a reviewer or approver to help out! They can also be a good way to get started contributing.

The meeting lead role is rotated for each meeting between etcd maintainers or sig-etcd leads and is recorded in a [shared Google sheet][shared-rotation-sheet].

Meeting recordings are uploaded to the official etcd [YouTube channel].

Get calendar invitations by joining [etcd-dev](https://groups.google.com/g/etcd-dev) mailing group.

Join the CNCF-funded Zoom channel: [zoom.us/my/cncfetcdproject](https://zoom.us/my/cncfetcdproject)

[shared-meeting-notes]: https://docs.google.com/document/d/16XEGyPBisZvmmoIHSZzv__LoyOeluC5a4x353CX0SIM/edit
[shared-rotation-sheet]: https://docs.google.com/spreadsheets/d/1jodHIO7Dk2VWTs1IRnfMFaRktS9IH8XRyifOnPdSY8I/edit
[YouTube channel]: https://www.youtube.com/@etcdio

## Contributing

See [CONTRIBUTING](CONTRIBUTING.md) for details on setting up your development environment, submitting patches and the contribution workflow.

Please refer to [community-membership.md](Documentation/contributor-guide/community-membership.md#member) for information on becoming an etcd project member.  We welcome and look forward to your contributions to the project!

Please also refer to [roadmap](Documentation/contributor-guide/roadmap.md) to get more details on the priorities for the next few major or minor releases.

## Reporting bugs

See [reporting bugs](https://github.com/etcd-io/etcd/blob/main/Documentation/contributor-guide/reporting_bugs.md) for details about reporting any issues. Before opening an issue please check it is not covered in our [frequently asked questions].

[frequently asked questions]: https://etcd.io/docs/latest/faq

## Reporting a security vulnerability

See [security disclosure and release process](security/README.md) for details on how to report a security vulnerability and how the etcd team manages it.

## Issue and PR management

See [issue triage guidelines](https://github.com/etcd-io/etcd/blob/main/Documentation/contributor-guide/triage_issues.md) for details on how issues are managed.

See [PR management](https://github.com/etcd-io/etcd/blob/main/Documentation/contributor-guide/triage_prs.md) for guidelines on how pull requests are managed.

## etcd Emeritus Maintainers

etcd [emeritus maintainers](OWNERS) dedicated a part of their career to etcd and reviewed code, triaged bugs and pushed the project forward over a substantial period of time. Their contribution is greatly appreciated.

### License

etcd is under the Apache 2.0 license. See the [LICENSE](LICENSE) file for details.
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[mudler/LocalAI]]></title>
            <link>https://github.com/mudler/LocalAI</link>
            <guid>https://github.com/mudler/LocalAI</guid>
            <pubDate>Thu, 25 Dec 2025 00:05:34 GMT</pubDate>
            <description><![CDATA[ü§ñ The free, Open Source alternative to OpenAI, Claude and others. Self-hosted and local-first. Drop-in replacement for OpenAI, running on consumer-grade hardware. No GPU required. Runs gguf, transformers, diffusers and many more. Features: Generate Text, MCP, Audio, Video, Images, Voice Cloning, Distributed, P2P and decentralized inference]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mudler/LocalAI">mudler/LocalAI</a></h1>
            <p>ü§ñ The free, Open Source alternative to OpenAI, Claude and others. Self-hosted and local-first. Drop-in replacement for OpenAI, running on consumer-grade hardware. No GPU required. Runs gguf, transformers, diffusers and many more. Features: Generate Text, MCP, Audio, Video, Images, Voice Cloning, Distributed, P2P and decentralized inference</p>
            <p>Language: Go</p>
            <p>Stars: 40,689</p>
            <p>Forks: 3,286</p>
            <p>Stars today: 49 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
  &lt;br&gt;
  &lt;img width=&quot;300&quot; src=&quot;./core/http/static/logo.png&quot;&gt; &lt;br&gt;
&lt;br&gt;
&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/go-skynet/LocalAI/fork&quot; target=&quot;blank&quot;&gt;
&lt;img src=&quot;https://img.shields.io/github/forks/go-skynet/LocalAI?style=for-the-badge&quot; alt=&quot;LocalAI forks&quot;/&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/go-skynet/LocalAI/stargazers&quot; target=&quot;blank&quot;&gt;
&lt;img src=&quot;https://img.shields.io/github/stars/go-skynet/LocalAI?style=for-the-badge&quot; alt=&quot;LocalAI stars&quot;/&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/go-skynet/LocalAI/pulls&quot; target=&quot;blank&quot;&gt;
&lt;img src=&quot;https://img.shields.io/github/issues-pr/go-skynet/LocalAI?style=for-the-badge&quot; alt=&quot;LocalAI pull-requests&quot;/&gt;
&lt;/a&gt;
&lt;a href=&#039;https://github.com/go-skynet/LocalAI/releases&#039;&gt;
&lt;img src=&#039;https://img.shields.io/github/release/go-skynet/LocalAI?&amp;label=Latest&amp;style=for-the-badge&#039;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://hub.docker.com/r/localai/localai&quot; target=&quot;blank&quot;&gt;
&lt;img src=&quot;https://img.shields.io/badge/dockerhub-images-important.svg?logo=Docker&quot; alt=&quot;LocalAI Docker hub&quot;/&gt;
&lt;/a&gt;
&lt;a href=&quot;https://quay.io/repository/go-skynet/local-ai?tab=tags&amp;tag=latest&quot; target=&quot;blank&quot;&gt;
&lt;img src=&quot;https://img.shields.io/badge/quay.io-images-important.svg?&quot; alt=&quot;LocalAI Quay.io&quot;/&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://twitter.com/LocalAI_API&quot; target=&quot;blank&quot;&gt;
&lt;img src=&quot;https://img.shields.io/badge/X-%23000000.svg?style=for-the-badge&amp;logo=X&amp;logoColor=white&amp;label=LocalAI_API&quot; alt=&quot;Follow LocalAI_API&quot;/&gt;
&lt;/a&gt;
&lt;a href=&quot;https://discord.gg/uJAeKSAGDy&quot; target=&quot;blank&quot;&gt;
&lt;img src=&quot;https://img.shields.io/badge/dynamic/json?color=blue&amp;label=Discord&amp;style=for-the-badge&amp;query=approximate_member_count&amp;url=https%3A%2F%2Fdiscordapp.com%2Fapi%2Finvites%2FuJAeKSAGDy%3Fwith_counts%3Dtrue&amp;logo=discord&quot; alt=&quot;Join LocalAI Discord Community&quot;/&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/5539&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/5539&quot; alt=&quot;mudler%2FLocalAI | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&gt; :bulb: Get help - [‚ùìFAQ](https://localai.io/faq/) [üí≠Discussions](https://github.com/go-skynet/LocalAI/discussions) [:speech_balloon: Discord](https://discord.gg/uJAeKSAGDy) [:book: Documentation website](https://localai.io/)
&gt;
&gt; [üíª Quickstart](https://localai.io/basics/getting_started/) [üñºÔ∏è Models](https://models.localai.io/) [üöÄ Roadmap](https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3Aroadmap) [üõ´ Examples](https://github.com/mudler/LocalAI-examples) Try on 
[![Telegram](https://img.shields.io/badge/Telegram-2CA5E0?style=for-the-badge&amp;logo=telegram&amp;logoColor=white)](https://t.me/localaiofficial_bot)

[![tests](https://github.com/go-skynet/LocalAI/actions/workflows/test.yml/badge.svg)](https://github.com/go-skynet/LocalAI/actions/workflows/test.yml)[![Build and Release](https://github.com/go-skynet/LocalAI/actions/workflows/release.yaml/badge.svg)](https://github.com/go-skynet/LocalAI/actions/workflows/release.yaml)[![build container images](https://github.com/go-skynet/LocalAI/actions/workflows/image.yml/badge.svg)](https://github.com/go-skynet/LocalAI/actions/workflows/image.yml)[![Bump dependencies](https://github.com/go-skynet/LocalAI/actions/workflows/bump_deps.yaml/badge.svg)](https://github.com/go-skynet/LocalAI/actions/workflows/bump_deps.yaml)[![Artifact Hub](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/localai)](https://artifacthub.io/packages/search?repo=localai)

**LocalAI** is the free, Open Source OpenAI alternative. LocalAI act as a drop-in replacement REST API that&#039;s compatible with OpenAI (Elevenlabs, Anthropic... ) API specifications for local AI inferencing. It allows you to run LLMs, generate images, audio (and not only) locally or on-prem with consumer grade hardware, supporting multiple model families. Does not require GPU. It is created and maintained by [Ettore Di Giacinto](https://github.com/mudler).


## üìöüÜï Local Stack Family

üÜï LocalAI is now part of a comprehensive suite of AI tools designed to work together:

&lt;table&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; valign=&quot;top&quot;&gt;
      &lt;a href=&quot;https://github.com/mudler/LocalAGI&quot;&gt;
        &lt;img src=&quot;https://raw.githubusercontent.com/mudler/LocalAGI/refs/heads/main/webui/react-ui/public/logo_2.png&quot; width=&quot;300&quot; alt=&quot;LocalAGI Logo&quot;&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td width=&quot;50%&quot; valign=&quot;top&quot;&gt;
      &lt;h3&gt;&lt;a href=&quot;https://github.com/mudler/LocalAGI&quot;&gt;LocalAGI&lt;/a&gt;&lt;/h3&gt;
      &lt;p&gt;A powerful Local AI agent management platform that serves as a drop-in replacement for OpenAI&#039;s Responses API, enhanced with advanced agentic capabilities.&lt;/p&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; valign=&quot;top&quot;&gt;
      &lt;a href=&quot;https://github.com/mudler/LocalRecall&quot;&gt;
        &lt;img src=&quot;https://raw.githubusercontent.com/mudler/LocalRecall/refs/heads/main/static/localrecall_horizontal.png&quot; width=&quot;300&quot; alt=&quot;LocalRecall Logo&quot;&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td width=&quot;50%&quot; valign=&quot;top&quot;&gt;
      &lt;h3&gt;&lt;a href=&quot;https://github.com/mudler/LocalRecall&quot;&gt;LocalRecall&lt;/a&gt;&lt;/h3&gt;
      &lt;p&gt;A REST-ful API and knowledge base management system that provides persistent memory and storage capabilities for AI agents.&lt;/p&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## Screenshots / Video

### Youtube video

&lt;h1 align=&quot;center&quot;&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=PDqYhB9nNHA&quot; target=&quot;_blank&quot;&gt; &lt;img width=&quot;300&quot; src=&quot;https://img.youtube.com/vi/PDqYhB9nNHA/0.jpg&quot;&gt; &lt;/a&gt;&lt;br&gt;
&lt;br&gt;
&lt;/h1&gt;


### Screenshots

| Talk Interface | Generate Audio |
| --- | --- |
| ![Screenshot 2025-03-31 at 12-01-36 LocalAI - Talk](./docs/assets/images/screenshots/screenshot_tts.png) | ![Screenshot 2025-03-31 at 12-01-29 LocalAI - Generate audio with voice-en-us-ryan-low](./docs/assets/images/screenshots/screenshot_tts.png) |

| Models Overview | Generate Images |
| --- | --- |
| ![Screenshot 2025-03-31 at 12-01-20 LocalAI - Models](./docs/assets/images/screenshots/screenshot_gallery.png) | ![Screenshot 2025-03-31 at 12-31-41 LocalAI - Generate images with flux 1-dev](./docs/assets/images/screenshots/screenshot_image.png) |

| Chat Interface | Home |
| --- | --- |
| ![Screenshot 2025-03-31 at 11-57-44 LocalAI - Chat with localai-functioncall-qwen2 5-7b-v0 5](./docs/assets/images/screenshots/screenshot_chat.png) | ![Screenshot 2025-03-31 at 11-57-23 LocalAI API - c2a39e3 (c2a39e3639227cfd94ffffe9f5691239acc275a8)](./docs/assets/images/screenshots/screenshot_home.png) |

| Login | Swarm |
| --- | --- |
|![Screenshot 2025-03-31 at 12-09-59 ](./docs/assets/images/screenshots/screenshot_login.png) | ![Screenshot 2025-03-31 at 12-10-39 LocalAI - P2P dashboard](./docs/assets/images/screenshots/screenshot_p2p.png) |

## üíª Quickstart

Run the installer script:

```bash
# Basic installation
curl https://localai.io/install.sh | sh
```

For more installation options, see [Installer Options](https://localai.io/installation/).

### macOS Download:

&lt;a href=&quot;https://github.com/mudler/LocalAI/releases/latest/download/LocalAI.dmg&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Download-macOS-blue?style=for-the-badge&amp;logo=apple&amp;logoColor=white&quot; alt=&quot;Download LocalAI for macOS&quot;/&gt;
&lt;/a&gt;

&gt; Note: the DMGs are not signed by Apple as quarantined. See https://github.com/mudler/LocalAI/issues/6268 for a workaround, fix is tracked here: https://github.com/mudler/LocalAI/issues/6244

Or run with docker:

&gt; **üí° Docker Run vs Docker Start**
&gt; 
&gt; - `docker run` creates and starts a new container. If a container with the same name already exists, this command will fail.
&gt; - `docker start` starts an existing container that was previously created with `docker run`.
&gt; 
&gt; If you&#039;ve already run LocalAI before and want to start it again, use: `docker start -i local-ai`

### CPU only image:

```bash
docker run -ti --name local-ai -p 8080:8080 localai/localai:latest
```

### NVIDIA GPU Images:

```bash
# CUDA 12.0
docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-gpu-nvidia-cuda-12

# CUDA 11.7
docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-gpu-nvidia-cuda-11

# NVIDIA Jetson (L4T) ARM64
docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-nvidia-l4t-arm64
```

### AMD GPU Images (ROCm):

```bash
docker run -ti --name local-ai -p 8080:8080 --device=/dev/kfd --device=/dev/dri --group-add=video localai/localai:latest-gpu-hipblas
```

### Intel GPU Images (oneAPI):

```bash
docker run -ti --name local-ai -p 8080:8080 --device=/dev/dri/card1 --device=/dev/dri/renderD128 localai/localai:latest-gpu-intel
```

### Vulkan GPU Images:

```bash
docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-gpu-vulkan
```

### AIO Images (pre-downloaded models):

```bash
# CPU version
docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-aio-cpu

# NVIDIA CUDA 12 version
docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-aio-gpu-nvidia-cuda-12

# NVIDIA CUDA 11 version
docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-aio-gpu-nvidia-cuda-11

# Intel GPU version
docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-aio-gpu-intel

# AMD GPU version
docker run -ti --name local-ai -p 8080:8080 --device=/dev/kfd --device=/dev/dri --group-add=video localai/localai:latest-aio-gpu-hipblas
```

For more information about the AIO images and pre-downloaded models, see [Container Documentation](https://localai.io/basics/container/).

To load models:

```bash
# From the model gallery (see available models with `local-ai models list`, in the WebUI from the model tab, or visiting https://models.localai.io)
local-ai run llama-3.2-1b-instruct:q4_k_m
# Start LocalAI with the phi-2 model directly from huggingface
local-ai run huggingface://TheBloke/phi-2-GGUF/phi-2.Q8_0.gguf
# Install and run a model from the Ollama OCI registry
local-ai run ollama://gemma:2b
# Run a model from a configuration file
local-ai run https://gist.githubusercontent.com/.../phi-2.yaml
# Install and run a model from a standard OCI registry (e.g., Docker Hub)
local-ai run oci://localai/phi-2:latest
```

&gt; ‚ö° **Automatic Backend Detection**: When you install models from the gallery or YAML files, LocalAI automatically detects your system&#039;s GPU capabilities (NVIDIA, AMD, Intel) and downloads the appropriate backend. For advanced configuration options, see [GPU Acceleration](https://localai.io/features/gpu-acceleration/#automatic-backend-detection).

For more information, see [üíª Getting started](https://localai.io/basics/getting_started/index.html), if you are interested in our roadmap items and future enhancements, you can see the [Issues labeled as Roadmap here](https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3Aroadmap)

## üì∞ Latest project news

- December 2025: [Dynamic Memory Resource reclaimer](https://github.com/mudler/LocalAI/pull/7583), [Automatic fitting of models to multiple GPUS(llama.cpp)](https://github.com/mudler/LocalAI/pull/7584), [Added Vibevoice backend](https://github.com/mudler/LocalAI/pull/7494)
- November 2025: Major improvements to the UX. Among these: [Import models via URL](https://github.com/mudler/LocalAI/pull/7245) and [Multiple chats and history](https://github.com/mudler/LocalAI/pull/7325)
- October 2025: üîå [Model Context Protocol (MCP)](https://localai.io/docs/features/mcp/) support added for agentic capabilities with external tools
- September 2025: New Launcher application for MacOS and Linux, extended support to many backends for Mac and Nvidia L4T devices. Models: Added MLX-Audio, WAN 2.2. WebUI improvements and Python-based backends now ships portable python environments.
- August 2025: MLX, MLX-VLM, Diffusers and llama.cpp are now supported on Mac M1/M2/M3+ chips ( with `development` suffix in the gallery ): https://github.com/mudler/LocalAI/pull/6049 https://github.com/mudler/LocalAI/pull/6119 https://github.com/mudler/LocalAI/pull/6121 https://github.com/mudler/LocalAI/pull/6060
- July/August 2025: üîç [Object Detection](https://localai.io/features/object-detection/) added to the API featuring [rf-detr](https://github.com/roboflow/rf-detr)
- July 2025: All backends migrated outside of the main binary. LocalAI is now more lightweight, small, and automatically downloads the required backend to run the model. [Read the release notes](https://github.com/mudler/LocalAI/releases/tag/v3.2.0)
- June 2025: [Backend management](https://github.com/mudler/LocalAI/pull/5607) has been added. Attention: extras images are going to be deprecated from the next release! Read [the backend management PR](https://github.com/mudler/LocalAI/pull/5607).
- May 2025: [Audio input](https://github.com/mudler/LocalAI/pull/5466) and [Reranking](https://github.com/mudler/LocalAI/pull/5396) in llama.cpp backend, [Realtime API](https://github.com/mudler/LocalAI/pull/5392),  Support to Gemma, SmollVLM, and more multimodal models (available in the gallery).
- May 2025: Important: image name changes [See release](https://github.com/mudler/LocalAI/releases/tag/v2.29.0)
- Apr 2025: Rebrand, WebUI enhancements
- Apr 2025: [LocalAGI](https://github.com/mudler/LocalAGI) and [LocalRecall](https://github.com/mudler/LocalRecall) join the LocalAI family stack.
- Apr 2025: WebUI overhaul, AIO images updates
- Feb 2025: Backend cleanup, Breaking changes, new backends (kokoro, OutelTTS, faster-whisper), Nvidia L4T images
- Jan 2025: LocalAI model release: https://huggingface.co/mudler/LocalAI-functioncall-phi-4-v0.3, SANA support in diffusers: https://github.com/mudler/LocalAI/pull/4603
- Dec 2024: stablediffusion.cpp backend (ggml) added ( https://github.com/mudler/LocalAI/pull/4289 )
- Nov 2024: Bark.cpp backend added ( https://github.com/mudler/LocalAI/pull/4287 )
- Nov 2024: Voice activity detection models (**VAD**) added to the API: https://github.com/mudler/LocalAI/pull/4204
- Oct 2024: examples moved to [LocalAI-examples](https://github.com/mudler/LocalAI-examples)
- Aug 2024:  üÜï FLUX-1, [P2P Explorer](https://explorer.localai.io)
- July 2024: üî•üî• üÜï P2P Dashboard, LocalAI Federated mode and AI Swarms: https://github.com/mudler/LocalAI/pull/2723. P2P Global community pools: https://github.com/mudler/LocalAI/issues/3113
- May 2024: üî•üî• Decentralized P2P llama.cpp:  https://github.com/mudler/LocalAI/pull/2343 (peer2peer llama.cpp!) üëâ Docs  https://localai.io/features/distribute/
- May 2024: üî•üî• Distributed inferencing: https://github.com/mudler/LocalAI/pull/2324
- April 2024: Reranker API: https://github.com/mudler/LocalAI/pull/2121

Roadmap items: [List of issues](https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3Aroadmap)

## üöÄ [Features](https://localai.io/features/)

- üß© [Backend Gallery](https://localai.io/backends/): Install/remove backends on the fly, powered by OCI images ‚Äî fully customizable and API-driven.
- üìñ [Text generation with GPTs](https://localai.io/features/text-generation/) (`llama.cpp`, `transformers`, `vllm` ... [:book: and more](https://localai.io/model-compatibility/index.html#model-compatibility-table))
- üó£ [Text to Audio](https://localai.io/features/text-to-audio/)
- üîà [Audio to Text](https://localai.io/features/audio-to-text/) (Audio transcription with `whisper.cpp`)
- üé® [Image generation](https://localai.io/features/image-generation)
- üî• [OpenAI-alike tools API](https://localai.io/features/openai-functions/) 
- üß† [Embeddings generation for vector databases](https://localai.io/features/embeddings/)
- ‚úçÔ∏è [Constrained grammars](https://localai.io/features/constrained_grammars/)
- üñºÔ∏è [Download Models directly from Huggingface ](https://localai.io/models/)
- ü•Ω [Vision API](https://localai.io/features/gpt-vision/)
- üîç [Object Detection](https://localai.io/features/object-detection/)
- üìà [Reranker API](https://localai.io/features/reranker/)
- üÜïüñß [P2P Inferencing](https://localai.io/features/distribute/)
- üÜïüîå [Model Context Protocol (MCP)](https://localai.io/docs/features/mcp/) - Agentic capabilities with external tools and [LocalAGI&#039;s Agentic capabilities](https://github.com/mudler/LocalAGI)
- üîä Voice activity detection (Silero-VAD support)
- üåç Integrated WebUI!

## üß© Supported Backends &amp; Acceleration

LocalAI supports a comprehensive range of AI backends with multiple acceleration options:

### Text Generation &amp; Language Models
| Backend | Description | Acceleration Support |
|---------|-------------|---------------------|
| **llama.cpp** | LLM inference in C/C++ | CUDA 11/12, ROCm, Intel SYCL, Vulkan, Metal, CPU |
| **vLLM** | Fast LLM inference with PagedAttention | CUDA 12, ROCm, Intel |
| **transformers** | HuggingFace transformers framework | CUDA 11/12, ROCm, Intel, CPU |
| **exllama2** | GPTQ inference library | CUDA 12 |
| **MLX** | Apple Silicon LLM inference | Metal (M1/M2/M3+) |
| **MLX-VLM** | Apple Silicon Vision-Language Models | Metal (M1/M2/M3+) |

### Audio &amp; Speech Processing
| Backend | Description | Acceleration Support |
|---------|-------------|---------------------|
| **whisper.cpp** | OpenAI Whisper in C/C++ | CUDA 12, ROCm, Intel SYCL, Vulkan, CPU |
| **faster-whisper** | Fast Whisper with CTranslate2 | CUDA 12, ROCm, Intel, CPU |
| **bark** | Text-to-audio generation | CUDA 12, ROCm, Intel |
| **bark-cpp** | C++ implementation of Bark | CUDA, Metal, CPU |
| **coqui** | Advanced TTS with 1100+ languages | CUDA 12, ROCm, Intel, CPU |
| **kokoro** | Lightweight TTS model | CUDA 12, ROCm, Intel, CPU |
| **chatterbox** | Production-grade TTS | CUDA 11/12, CPU |
| **piper** | Fast neural TTS system | CPU |
| **kitten-tts** | Kitten TTS models | CPU |
| **silero-vad** | Voice Activity Detection | CPU |
| **neutts** | Text-to-speech with voice cloning | CUDA 12, ROCm, CPU |

### Image &amp; Video Generation
| Backend | Description | Acceleration Support |
|---------|-------------|---------------------|
| **stablediffusion.cpp** | Stable Diffusion in C/C++ | CUDA 12, Intel SYCL, Vulkan, CPU |
| **diffusers** | HuggingFace diffusion models | CUDA 11/12, ROCm, Intel, Metal, CPU |

### Specialized AI Tasks
| Backend | Description | Acceleration Support |
|---------|-------------|---------------------|
| **rfdetr** | Real-time object detection | CUDA 12, Intel, CPU |
| **rerankers** | Document reranking API | CUDA 11/12, ROCm, Intel, CPU |
| **local-store** | Vector database | CPU |
| **huggingface** | HuggingFace API integration | API-based |

### Hardware Acceleration Matrix

| Acceleration Type | Supported Backends | Hardware Support |
|-------------------|-------------------|------------------|
| **NVIDIA CUDA 11** | llama.cpp, whisper, stablediffusion, diffusers, rerankers, bark, chatterbox | Nvidia hardware |
| **NVIDIA CUDA 12** | All CUDA-compatible backends | Nvidia hardware |
| **AMD ROCm** | llama.cpp, whisper, vllm, transformers, diffusers, rerankers, coqui, kokoro, bark, neutts | AMD Graphics |
| **Intel oneAPI** | llama.cpp, whisper, stablediffusion, vllm, transformers, diffusers, rfdetr, rerankers, exllama2, coqui, kokoro, bark | Intel Arc, Intel iGPUs |
| **Apple Metal** | llama.cpp, whisper, diffusers, MLX, MLX-VLM, bark-cpp | Apple M1/M2/M3+ |
| **Vulkan** | llama.cpp, whisper, stablediffusion | Cross-platform GPUs |
| **NVIDIA Jetson** | llama.cpp, whisper, stablediffusion, diffusers, rfdetr | ARM64 embedded AI |
| **CPU Optimized** | All backends | AVX/AVX2/AVX512, quantization support |

### üîó Community and integrations

Build and deploy custom containers:
- https://github.com/sozercan/aikit

WebUIs:
- https://github.com/Jirubizu/localai-admin
- https://github.com/go-skynet/LocalAI-frontend
- QA-Pilot(An interactive chat project that leverages LocalAI LLMs for rapid understanding and navigation of GitHub code repository) https://github.com/reid41/QA-Pilot

Agentic Libraries:
- https://github.com/mudler/cogito

MCPs:
- https://github.com/mudler/MCPs

Model galleries
- https://github.com/go-skynet/model-gallery

Voice:
- https://github.com/richiejp/VoxInput

Other:
- Helm chart https://github.com/go-skynet/h

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[rclone/rclone]]></title>
            <link>https://github.com/rclone/rclone</link>
            <guid>https://github.com/rclone/rclone</guid>
            <pubDate>Thu, 25 Dec 2025 00:05:33 GMT</pubDate>
            <description><![CDATA["rsync for cloud storage" - Google Drive, S3, Dropbox, Backblaze B2, One Drive, Swift, Hubic, Wasabi, Google Cloud Storage, Azure Blob, Azure Files, Yandex Files]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/rclone/rclone">rclone/rclone</a></h1>
            <p>"rsync for cloud storage" - Google Drive, S3, Dropbox, Backblaze B2, One Drive, Swift, Hubic, Wasabi, Google Cloud Storage, Azure Blob, Azure Files, Yandex Files</p>
            <p>Language: Go</p>
            <p>Stars: 54,604</p>
            <p>Forks: 4,827</p>
            <p>Stars today: 47 stars today</p>
            <h2>README</h2><pre>&lt;!-- markdownlint-disable-next-line first-line-heading no-inline-html --&gt;
[&lt;img src=&quot;https://rclone.org/img/logo_on_light__horizontal_color.svg&quot; width=&quot;50%&quot; alt=&quot;rclone logo&quot;&gt;](https://rclone.org/#gh-light-mode-only)
&lt;!-- markdownlint-disable-next-line no-inline-html --&gt;
[&lt;img src=&quot;https://rclone.org/img/logo_on_dark__horizontal_color.svg&quot; width=&quot;50%&quot; alt=&quot;rclone logo&quot;&gt;](https://rclone.org/#gh-dark-mode-only)

[Website](https://rclone.org) |
[Documentation](https://rclone.org/docs/) |
[Download](https://rclone.org/downloads/) |
[Contributing](CONTRIBUTING.md) |
[Changelog](https://rclone.org/changelog/) |
[Installation](https://rclone.org/install/) |
[Forum](https://forum.rclone.org/)

[![Build Status](https://github.com/rclone/rclone/workflows/build/badge.svg)](https://github.com/rclone/rclone/actions?query=workflow%3Abuild)
[![Go Report Card](https://goreportcard.com/badge/github.com/rclone/rclone)](https://goreportcard.com/report/github.com/rclone/rclone)
[![GoDoc](https://godoc.org/github.com/rclone/rclone?status.svg)](https://godoc.org/github.com/rclone/rclone)
[![Docker Pulls](https://img.shields.io/docker/pulls/rclone/rclone)](https://hub.docker.com/r/rclone/rclone)

# Rclone

Rclone *(&quot;rsync for cloud storage&quot;)* is a command-line program to sync files and
directories to and from different cloud storage providers.

## Storage providers

- 1Fichier [:page_facing_up:](https://rclone.org/fichier/)
- Akamai Netstorage [:page_facing_up:](https://rclone.org/netstorage/)
- Alibaba Cloud (Aliyun) Object Storage System (OSS) [:page_facing_up:](https://rclone.org/s3/#alibaba-oss)
- Amazon S3 [:page_facing_up:](https://rclone.org/s3/)
- ArvanCloud Object Storage (AOS) [:page_facing_up:](https://rclone.org/s3/#arvan-cloud-object-storage-aos)
- Backblaze B2 [:page_facing_up:](https://rclone.org/b2/)
- Box [:page_facing_up:](https://rclone.org/box/)
- Ceph [:page_facing_up:](https://rclone.org/s3/#ceph)
- China Mobile Ecloud Elastic Object Storage (EOS) [:page_facing_up:](https://rclone.org/s3/#china-mobile-ecloud-eos)
- Citrix ShareFile [:page_facing_up:](https://rclone.org/sharefile/)
- Cloudflare R2 [:page_facing_up:](https://rclone.org/s3/#cloudflare-r2)
- Cloudinary [:page_facing_up:](https://rclone.org/cloudinary/)
- Cubbit DS3 [:page_facing_up:](https://rclone.org/s3/#Cubbit)
- DigitalOcean Spaces [:page_facing_up:](https://rclone.org/s3/#digitalocean-spaces)
- Digi Storage [:page_facing_up:](https://rclone.org/koofr/#digi-storage)
- Dreamhost [:page_facing_up:](https://rclone.org/s3/#dreamhost)
- Dropbox [:page_facing_up:](https://rclone.org/dropbox/)
- Enterprise File Fabric [:page_facing_up:](https://rclone.org/filefabric/)
- Exaba [:page_facing_up:](https://rclone.org/s3/#exaba)
- Fastmail Files [:page_facing_up:](https://rclone.org/webdav/#fastmail-files)
- FileLu [:page_facing_up:](https://rclone.org/filelu/)
- Files.com [:page_facing_up:](https://rclone.org/filescom/)
- FlashBlade [:page_facing_up:](https://rclone.org/s3/#pure-storage-flashblade)
- FTP [:page_facing_up:](https://rclone.org/ftp/)
- GoFile [:page_facing_up:](https://rclone.org/gofile/)
- Google Cloud Storage [:page_facing_up:](https://rclone.org/googlecloudstorage/)
- Google Drive [:page_facing_up:](https://rclone.org/drive/)
- Google Photos [:page_facing_up:](https://rclone.org/googlephotos/)
- HDFS (Hadoop Distributed Filesystem) [:page_facing_up:](https://rclone.org/hdfs/)
- Hetzner Object Storage [:page_facing_up:](https://rclone.org/s3/#hetzner)
- Hetzner Storage Box [:page_facing_up:](https://rclone.org/sftp/#hetzner-storage-box)
- HiDrive [:page_facing_up:](https://rclone.org/hidrive/)
- HTTP [:page_facing_up:](https://rclone.org/http/)
- Huawei Cloud Object Storage Service(OBS) [:page_facing_up:](https://rclone.org/s3/#huawei-obs)
- iCloud Drive [:page_facing_up:](https://rclone.org/iclouddrive/)
- ImageKit [:page_facing_up:](https://rclone.org/imagekit/)
- Internet Archive [:page_facing_up:](https://rclone.org/internetarchive/)
- Jottacloud [:page_facing_up:](https://rclone.org/jottacloud/)
- IBM COS S3 [:page_facing_up:](https://rclone.org/s3/#ibm-cos-s3)
- Intercolo Object Storage [:page_facing_up:](https://rclone.org/s3/#intercolo)
- IONOS Cloud [:page_facing_up:](https://rclone.org/s3/#ionos)
- Koofr [:page_facing_up:](https://rclone.org/koofr/)
- Leviia Object Storage [:page_facing_up:](https://rclone.org/s3/#leviia)
- Liara Object Storage [:page_facing_up:](https://rclone.org/s3/#liara-object-storage)
- Linkbox [:page_facing_up:](https://rclone.org/linkbox)
- Linode Object Storage [:page_facing_up:](https://rclone.org/s3/#linode)
- Magalu Object Storage [:page_facing_up:](https://rclone.org/s3/#magalu)
- Mail.ru Cloud [:page_facing_up:](https://rclone.org/mailru/)
- Memset Memstore [:page_facing_up:](https://rclone.org/swift/)
- MEGA [:page_facing_up:](https://rclone.org/mega/)
- MEGA S4 Object Storage [:page_facing_up:](https://rclone.org/s3/#mega)
- Memory [:page_facing_up:](https://rclone.org/memory/)
- Microsoft Azure Blob Storage [:page_facing_up:](https://rclone.org/azureblob/)
- Microsoft Azure Files Storage [:page_facing_up:](https://rclone.org/azurefiles/)
- Microsoft OneDrive [:page_facing_up:](https://rclone.org/onedrive/)
- Minio [:page_facing_up:](https://rclone.org/s3/#minio)
- Nextcloud [:page_facing_up:](https://rclone.org/webdav/#nextcloud)
- Blomp Cloud Storage [:page_facing_up:](https://rclone.org/swift/)
- OpenDrive [:page_facing_up:](https://rclone.org/opendrive/)
- OpenStack Swift [:page_facing_up:](https://rclone.org/swift/)
- Oracle Cloud Storage [:page_facing_up:](https://rclone.org/swift/)
- Oracle Object Storage [:page_facing_up:](https://rclone.org/oracleobjectstorage/)
- Outscale [:page_facing_up:](https://rclone.org/s3/#outscale)
- OVHcloud Object Storage (Swift) [:page_facing_up:](https://rclone.org/swift/)
- OVHcloud Object Storage (S3-compatible) [:page_facing_up:](https://rclone.org/s3/#ovhcloud)
- ownCloud [:page_facing_up:](https://rclone.org/webdav/#owncloud)
- pCloud [:page_facing_up:](https://rclone.org/pcloud/)
- Petabox [:page_facing_up:](https://rclone.org/s3/#petabox)
- PikPak [:page_facing_up:](https://rclone.org/pikpak/)
- Pixeldrain [:page_facing_up:](https://rclone.org/pixeldrain/)
- premiumize.me [:page_facing_up:](https://rclone.org/premiumizeme/)
- put.io [:page_facing_up:](https://rclone.org/putio/)
- Proton Drive [:page_facing_up:](https://rclone.org/protondrive/)
- QingStor [:page_facing_up:](https://rclone.org/qingstor/)
- Qiniu Cloud Object Storage (Kodo) [:page_facing_up:](https://rclone.org/s3/#qiniu)
- Rabata Cloud Storage [:page_facing_up:](https://rclone.org/s3/#Rabata)
- Quatrix [:page_facing_up:](https://rclone.org/quatrix/)
- Rackspace Cloud Files [:page_facing_up:](https://rclone.org/swift/)
- RackCorp Object Storage [:page_facing_up:](https://rclone.org/s3/#RackCorp)
- rsync.net [:page_facing_up:](https://rclone.org/sftp/#rsync-net)
- Scaleway [:page_facing_up:](https://rclone.org/s3/#scaleway)
- Seafile [:page_facing_up:](https://rclone.org/seafile/)
- Seagate Lyve Cloud [:page_facing_up:](https://rclone.org/s3/#lyve)
- SeaweedFS [:page_facing_up:](https://rclone.org/s3/#seaweedfs)
- Selectel Object Storage [:page_facing_up:](https://rclone.org/s3/#selectel)
- Servercore Object Storage [:page_facing_up:](https://rclone.org/s3/#servercore)
- SFTP [:page_facing_up:](https://rclone.org/sftp/)
- Shade [:page_facing_up:](https://rclone.org/shade/)
- SMB / CIFS [:page_facing_up:](https://rclone.org/smb/)
- Spectra Logic [:page_facing_up:](https://rclone.org/s3/#spectralogic)
- StackPath [:page_facing_up:](https://rclone.org/s3/#stackpath)
- Storj [:page_facing_up:](https://rclone.org/storj/)
- SugarSync [:page_facing_up:](https://rclone.org/sugarsync/)
- Synology C2 Object Storage [:page_facing_up:](https://rclone.org/s3/#synology-c2)
- Tencent Cloud Object Storage (COS) [:page_facing_up:](https://rclone.org/s3/#tencent-cos)
- Uloz.to [:page_facing_up:](https://rclone.org/ulozto/)
- Wasabi [:page_facing_up:](https://rclone.org/s3/#wasabi)
- WebDAV [:page_facing_up:](https://rclone.org/webdav/)
- Yandex Disk [:page_facing_up:](https://rclone.org/yandex/)
- Zoho WorkDrive [:page_facing_up:](https://rclone.org/zoho/)
- Zata.ai [:page_facing_up:](https://rclone.org/s3/#Zata)
- The local filesystem [:page_facing_up:](https://rclone.org/local/)

Please see [the full list of all storage providers and their features](https://rclone.org/overview/)

### Virtual storage providers

These backends adapt or modify other storage providers

- Alias: rename existing remotes [:page_facing_up:](https://rclone.org/alias/)
- Archive: read archive files [:page_facing_up:](https://rclone.org/archive/)
- Cache: cache remotes (DEPRECATED) [:page_facing_up:](https://rclone.org/cache/)
- Chunker: split large files [:page_facing_up:](https://rclone.org/chunker/)
- Combine: combine multiple remotes into a directory tree [:page_facing_up:](https://rclone.org/combine/)
- Compress: compress files [:page_facing_up:](https://rclone.org/compress/)
- Crypt: encrypt files [:page_facing_up:](https://rclone.org/crypt/)
- Hasher: hash files [:page_facing_up:](https://rclone.org/hasher/)
- Union: join multiple remotes to work together [:page_facing_up:](https://rclone.org/union/)

## Features

- MD5/SHA-1 hashes checked at all times for file integrity
- Timestamps preserved on files
- Partial syncs supported on a whole file basis
- [Copy](https://rclone.org/commands/rclone_copy/) mode to just copy new/changed
  files
- [Sync](https://rclone.org/commands/rclone_sync/) (one way) mode to make a directory
  identical
- [Bisync](https://rclone.org/bisync/) (two way) to keep two directories in sync
  bidirectionally
- [Check](https://rclone.org/commands/rclone_check/) mode to check for file hash
  equality
- Can sync to and from network, e.g. two different cloud accounts
- Optional large file chunking ([Chunker](https://rclone.org/chunker/))
- Optional transparent compression ([Compress](https://rclone.org/compress/))
- Optional encryption ([Crypt](https://rclone.org/crypt/))
- Optional FUSE mount ([rclone mount](https://rclone.org/commands/rclone_mount/))
- Multi-threaded downloads to local disk
- Can [serve](https://rclone.org/commands/rclone_serve/) local or remote files
  over HTTP/WebDAV/FTP/SFTP/DLNA

## Installation &amp; documentation

Please see the [rclone website](https://rclone.org/) for:

- [Installation](https://rclone.org/install/)
- [Documentation &amp; configuration](https://rclone.org/docs/)
- [Changelog](https://rclone.org/changelog/)
- [FAQ](https://rclone.org/faq/)
- [Storage providers](https://rclone.org/overview/)
- [Forum](https://forum.rclone.org/)
- ...and more

## Downloads

- &lt;https://rclone.org/downloads/&gt;

## License

This is free software under the terms of the MIT license (check the
[COPYING file](/COPYING) included in this package).
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[siderolabs/talos]]></title>
            <link>https://github.com/siderolabs/talos</link>
            <guid>https://github.com/siderolabs/talos</guid>
            <pubDate>Thu, 25 Dec 2025 00:05:32 GMT</pubDate>
            <description><![CDATA[Talos Linux is a modern Linux distribution built for Kubernetes.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/siderolabs/talos">siderolabs/talos</a></h1>
            <p>Talos Linux is a modern Linux distribution built for Kubernetes.</p>
            <p>Language: Go</p>
            <p>Stars: 9,485</p>
            <p>Forks: 760</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre>&lt;!-- markdownlint-disable MD041 --&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;h1 align=&quot;center&quot;&gt;Talos Linux&lt;/h1&gt;
  &lt;p align=&quot;center&quot;&gt;A modern OS for Kubernetes.&lt;/p&gt;
  &lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://github.com/talos-systems/talos/releases/latest&quot;&gt;&lt;img alt=&quot;Release&quot; src=&quot;https://img.shields.io/github/release/talos-systems/talos.svg?logo=github&amp;logoColor=white&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/talos-systems/talos/releases/latest&quot;&gt;&lt;img alt=&quot;Pre-release&quot; src=&quot;https://img.shields.io/github/release-pre/talos-systems/talos.svg?label=pre-release&amp;logo=GitHub&amp;logoColor=white&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://www.bestpractices.dev/projects/7340&quot;&gt;&lt;img src=&quot;https://www.bestpractices.dev/projects/7340/badge&quot; alt=&quot;OpenSSF badge&quot;&gt;&lt;/a&gt;
  &lt;/p&gt;
&lt;/p&gt;

---

**Talos** is a modern OS for running Kubernetes: secure, immutable, and minimal.
Talos is fully open source, production-ready, and supported by the people at [Sidero Labs](https://www.SideroLabs.com/).
All system management is done via an API - there is no shell or interactive console.
Benefits include:

- **Security**: Talos reduces your attack surface: It&#039;s minimal, hardened, and immutable.
  All API access is secured with mutual TLS (mTLS) authentication.
- **Predictability**: Talos eliminates configuration drift, reduces unknown factors by employing immutable infrastructure ideology, and delivers atomic updates.
- **Evolvability**: Talos simplifies your architecture, increases your agility, and always delivers current stable Kubernetes and Linux versions.

## Documentation

For instructions on deploying and managing Talos, see the [Documentation](https://docs.siderolabs.com/talos).

## Community

- Support: Questions, bugs, feature requests [GitHub Discussions](https://github.com/siderolabs/talos/discussions)
- Slack: Join our [slack channel](https://slack.dev.talos-systems.io)
- Forum: [community](https://groups.google.com/a/SideroLabs.com/forum/#!forum/community)
- Twitter: [@SideroLabs](https://twitter.com/SideroLabs)
- Email: [info@SideroLabs.com](mailto:info@SideroLabs.com)

If you&#039;re interested in this project and would like to help in engineering efforts or have general usage questions, we are happy to have you!
We hold a monthly meeting that all audiences are welcome to attend.

We would appreciate your feedback so that we can make Talos even better!
To do so, you can take our [survey](https://docs.google.com/forms/d/1TUna5YTYGCKot68Y9YN_CLobY6z9JzLVCq1G7DoyNjA/edit).

### Office Hours

- When: Second Monday of every month at 16:30 UTC.
- Where: [Google Meet](https://meet.google.com/ivb-kjfm-jfc).

You can subscribe to this meeting by joining the community forum above.

&gt; Note: You can convert the meeting hours to your [local time](https://everytimezone.com/s/599e61d6).

## Contributing

Contributions are welcomed and appreciated!
See [Contributing](CONTRIBUTING.md) for our guidelines.

## License

&lt;a href=&quot;https://github.com/siderolabs/talos/blob/master/LICENSE&quot;&gt;
  &lt;img alt=&quot;GitHub&quot; src=&quot;https://img.shields.io/github/license/siderolabs/talos&quot;&gt;
&lt;/a&gt;

Some software we distribute is under the General Public License family of licenses or other licenses that require we provide you with the source code.
If you would like a copy of the source code for this software, please contact us via email: info at SideroLabs.com.
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[seaweedfs/seaweedfs]]></title>
            <link>https://github.com/seaweedfs/seaweedfs</link>
            <guid>https://github.com/seaweedfs/seaweedfs</guid>
            <pubDate>Thu, 25 Dec 2025 00:05:31 GMT</pubDate>
            <description><![CDATA[SeaweedFS is a fast distributed storage system for blobs, objects, files, and data lake, for billions of files! Blob store has O(1) disk seek, cloud tiering. Filer supports Cloud Drive, xDC replication, Kubernetes, POSIX FUSE mount, S3 API, S3 Gateway, Hadoop, WebDAV, encryption, Erasure Coding. Enterprise version is at seaweedfs.com.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/seaweedfs/seaweedfs">seaweedfs/seaweedfs</a></h1>
            <p>SeaweedFS is a fast distributed storage system for blobs, objects, files, and data lake, for billions of files! Blob store has O(1) disk seek, cloud tiering. Filer supports Cloud Drive, xDC replication, Kubernetes, POSIX FUSE mount, S3 API, S3 Gateway, Hadoop, WebDAV, encryption, Erasure Coding. Enterprise version is at seaweedfs.com.</p>
            <p>Language: Go</p>
            <p>Stars: 29,071</p>
            <p>Forks: 2,629</p>
            <p>Stars today: 33 stars today</p>
            <h2>README</h2><pre># SeaweedFS


[![Slack](https://img.shields.io/badge/slack-purple)](https://join.slack.com/t/seaweedfs/shared_invite/enQtMzI4MTMwMjU2MzA3LTEyYzZmZWYzOGQ3MDJlZWMzYmI0OTE4OTJiZjJjODBmMzUxNmYwODg0YjY3MTNlMjBmZDQ1NzQ5NDJhZWI2ZmY)
[![Twitter](https://img.shields.io/twitter/follow/seaweedfs.svg?style=social&amp;label=Follow)](https://twitter.com/intent/follow?screen_name=seaweedfs)
[![Build Status](https://img.shields.io/github/actions/workflow/status/seaweedfs/seaweedfs/go.yml)](https://github.com/seaweedfs/seaweedfs/actions/workflows/go.yml)
[![GoDoc](https://godoc.org/github.com/seaweedfs/seaweedfs/weed?status.svg)](https://godoc.org/github.com/seaweedfs/seaweedfs/weed)
[![Wiki](https://img.shields.io/badge/docs-wiki-blue.svg)](https://github.com/seaweedfs/seaweedfs/wiki)
[![Docker Pulls](https://img.shields.io/docker/pulls/chrislusf/seaweedfs?maxAge=4800)](https://hub.docker.com/r/chrislusf/seaweedfs/)
[![SeaweedFS on Maven Central](https://img.shields.io/maven-central/v/com.github.chrislusf/seaweedfs-client)](https://search.maven.org/search?q=g:com.github.chrislusf)
[![Artifact Hub](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/seaweedfs)](https://artifacthub.io/packages/search?repo=seaweedfs)

![SeaweedFS Logo](https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/note/seaweedfs.png)

&lt;h2 align=&quot;center&quot;&gt;&lt;a href=&quot;https://www.patreon.com/seaweedfs&quot;&gt;Sponsor SeaweedFS via Patreon&lt;/a&gt;&lt;/h2&gt;

SeaweedFS is an independent Apache-licensed open source project with its ongoing development made
possible entirely thanks to the support of these awesome [backers](https://github.com/seaweedfs/seaweedfs/blob/master/backers.md).
If you&#039;d like to grow SeaweedFS even stronger, please consider joining our
&lt;a href=&quot;https://www.patreon.com/seaweedfs&quot;&gt;sponsors on Patreon&lt;/a&gt;.

Your support will be really appreciated by me and other supporters!

&lt;!--
&lt;h4 align=&quot;center&quot;&gt;Platinum&lt;/h4&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;&quot; target=&quot;_blank&quot;&gt;
    Add your name or icon here
  &lt;/a&gt;
&lt;/p&gt;
--&gt;

### Gold Sponsors
[![nodion](https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/note/sponsor_nodion.png)](https://www.nodion.com)
[![piknik](https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/note/piknik.png)](https://www.piknik.com)
[![keepsec](https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/note/keepsec.png)](https://www.keepsec.ca)

---

- [Download Binaries for different platforms](https://github.com/seaweedfs/seaweedfs/releases/latest)
- [SeaweedFS on Slack](https://join.slack.com/t/seaweedfs/shared_invite/enQtMzI4MTMwMjU2MzA3LTEyYzZmZWYzOGQ3MDJlZWMzYmI0OTE4OTJiZjJjODBmMzUxNmYwODg0YjY3MTNlMjBmZDQ1NzQ5NDJhZWI2ZmY)
- [SeaweedFS on Twitter](https://twitter.com/SeaweedFS)
- [SeaweedFS on Telegram](https://t.me/Seaweedfs) 
- [SeaweedFS on Reddit](https://www.reddit.com/r/SeaweedFS/)
- [SeaweedFS Mailing List](https://groups.google.com/d/forum/seaweedfs)
- [Wiki Documentation](https://github.com/seaweedfs/seaweedfs/wiki)
- [SeaweedFS White Paper](https://github.com/seaweedfs/seaweedfs/wiki/SeaweedFS_Architecture.pdf)
- [SeaweedFS Introduction Slides 2025.5](https://docs.google.com/presentation/d/1tdkp45J01oRV68dIm4yoTXKJDof-EhainlA0LMXexQE/edit?usp=sharing)
- [SeaweedFS Introduction Slides 2021.5](https://docs.google.com/presentation/d/1DcxKWlINc-HNCjhYeERkpGXXm6nTCES8mi2W5G0Z4Ts/edit?usp=sharing)
- [SeaweedFS Introduction Slides 2019.3](https://www.slideshare.net/chrislusf/seaweedfs-introduction)

Table of Contents
=================

* [Quick Start](#quick-start)
    * [Quick Start for S3 API on Docker](#quick-start-for-s3-api-on-docker)
    * [Quick Start with Single Binary](#quick-start-with-single-binary)
    * [Quick Start SeaweedFS S3 on AWS](#quick-start-seaweedfs-s3-on-aws)
* [Introduction](#introduction)
* [Features](#features)
    * [Additional Features](#additional-features)
    * [Filer Features](#filer-features)
* [Example: Using Seaweed Object Store](#example-using-seaweed-object-store)
* [Architecture](#object-store-architecture)
* [Compared to Other File Systems](#compared-to-other-file-systems)
    * [Compared to HDFS](#compared-to-hdfs)
    * [Compared to GlusterFS, Ceph](#compared-to-glusterfs-ceph)
    * [Compared to GlusterFS](#compared-to-glusterfs)
    * [Compared to Ceph](#compared-to-ceph)
    * [Compared to Minio](#compared-to-minio)
* [Dev Plan](#dev-plan)
* [Installation Guide](#installation-guide)
* [Disk Related Topics](#disk-related-topics)
* [Benchmark](#benchmark)
* [Enterprise](#enterprise)
* [License](#license)

# Quick Start #

## Quick Start for S3 API on Docker ##

`docker run -p 8333:8333 chrislusf/seaweedfs server -s3`

## Quick Start with Single Binary ##
* Download the latest binary from https://github.com/seaweedfs/seaweedfs/releases and unzip a single binary file `weed` or `weed.exe`. Or run `go install github.com/seaweedfs/seaweedfs/weed@latest`.
* `export AWS_ACCESS_KEY_ID=admin ; export AWS_SECRET_ACCESS_KEY=key` as the admin credentials to access the object store.
* Run `weed server -dir=/some/data/dir -s3` to start one master, one volume server, one filer, and one S3 gateway.

Also, to increase capacity, just add more volume servers by running `weed volume -dir=&quot;/some/data/dir2&quot; -master=&quot;&lt;master_host&gt;:9333&quot; -port=8081` locally, or on a different machine, or on thousands of machines. That is it!

## Quick Start SeaweedFS S3 on AWS ##
* Setup fast production-ready [SeaweedFS S3 on AWS with cloudformation](https://aws.amazon.com/marketplace/pp/prodview-nzelz5gprlrjc)

# Introduction #

SeaweedFS is a simple and highly scalable distributed file system. There are two objectives:

1. to store billions of files!
2. to serve the files fast!

SeaweedFS started as an Object Store to handle small files efficiently. 
Instead of managing all file metadata in a central master, 
the central master only manages volumes on volume servers, 
and these volume servers manage files and their metadata. 
This relieves concurrency pressure from the central master and spreads file metadata into volume servers, 
allowing faster file access (O(1), usually just one disk read operation).

There is only 40 bytes of disk storage overhead for each file&#039;s metadata. 
It is so simple with O(1) disk reads that you are welcome to challenge the performance with your actual use cases.

SeaweedFS started by implementing [Facebook&#039;s Haystack design paper](http://www.usenix.org/event/osdi10/tech/full_papers/Beaver.pdf). 
Also, SeaweedFS implements erasure coding with ideas from 
[f4: Facebook‚Äôs Warm BLOB Storage System](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-muralidhar.pdf), and has a lot of similarities with [Facebook‚Äôs Tectonic Filesystem](https://www.usenix.org/system/files/fast21-pan.pdf)

On top of the object store, optional [Filer] can support directories and POSIX attributes. 
Filer is a separate linearly-scalable stateless server with customizable metadata stores, 
e.g., MySql, Postgres, Redis, Cassandra, HBase, Mongodb, Elastic Search, LevelDB, RocksDB, Sqlite, MemSql, TiDB, Etcd, CockroachDB, YDB, etc.

For any distributed key value stores, the large values can be offloaded to SeaweedFS. 
With the fast access speed and linearly scalable capacity, 
SeaweedFS can work as a distributed [Key-Large-Value store][KeyLargeValueStore].

SeaweedFS can transparently integrate with the cloud. 
With hot data on local cluster, and warm data on the cloud with O(1) access time, 
SeaweedFS can achieve both fast local access time and elastic cloud storage capacity.
What&#039;s more, the cloud storage access API cost is minimized. 
Faster and cheaper than direct cloud storage!

[Back to TOC](#table-of-contents)

# Features #
## Additional Features ##
* Can choose no replication or different replication levels, rack and data center aware.
* Automatic master servers failover - no single point of failure (SPOF).
* Automatic Gzip compression depending on file MIME type.
* Automatic compaction to reclaim disk space after deletion or update.
* [Automatic entry TTL expiration][VolumeServerTTL].
* Any server with some disk space can add to the total storage space.
* Adding/Removing servers does **not** cause any data re-balancing unless triggered by admin commands.
* Optional picture resizing.
* Support ETag, Accept-Range, Last-Modified, etc.
* Support in-memory/leveldb/readonly mode tuning for memory/performance balance.
* Support rebalancing the writable and readonly volumes.
* [Customizable Multiple Storage Tiers][TieredStorage]: Customizable storage disk types to balance performance and cost.
* [Transparent cloud integration][CloudTier]: unlimited capacity via tiered cloud storage for warm data.
* [Erasure Coding for warm storage][ErasureCoding]  Rack-Aware 10.4 erasure coding reduces storage cost and increases availability.

[Back to TOC](#table-of-contents)

## Filer Features ##
* [Filer server][Filer] provides &quot;normal&quot; directories and files via HTTP.
* [File TTL][FilerTTL] automatically expires file metadata and actual file data.
* [Mount filer][Mount] reads and writes files directly as a local directory via FUSE.
* [Filer Store Replication][FilerStoreReplication] enables HA for filer meta data stores.
* [Active-Active Replication][ActiveActiveAsyncReplication] enables asynchronous one-way or two-way cross cluster continuous replication.
* [Amazon S3 compatible API][AmazonS3API] accesses files with S3 tooling.
* [Hadoop Compatible File System][Hadoop] accesses files from Hadoop/Spark/Flink/etc or even runs HBase.
* [Async Replication To Cloud][BackupToCloud] has extremely fast local access and backups to Amazon S3, Google Cloud Storage, Azure, BackBlaze.
* [WebDAV] accesses as a mapped drive on Mac and Windows, or from mobile devices.
* [AES256-GCM Encrypted Storage][FilerDataEncryption] safely stores the encrypted data.
* [Super Large Files][SuperLargeFiles] stores large or super large files in tens of TB.
* [Cloud Drive][CloudDrive] mounts cloud storage to local cluster, cached for fast read and write with asynchronous write back.
* [Gateway to Remote Object Store][GatewayToRemoteObjectStore] mirrors bucket operations to remote object storage, in addition to [Cloud Drive][CloudDrive]

## Kubernetes ##
* [Kubernetes CSI Driver][SeaweedFsCsiDriver] A Container Storage Interface (CSI) Driver. [![Docker Pulls](https://img.shields.io/docker/pulls/chrislusf/seaweedfs-csi-driver.svg?maxAge=4800)](https://hub.docker.com/r/chrislusf/seaweedfs-csi-driver/)
* [SeaweedFS Operator](https://github.com/seaweedfs/seaweedfs-operator)

[Filer]: https://github.com/seaweedfs/seaweedfs/wiki/Directories-and-Files
[SuperLargeFiles]: https://github.com/seaweedfs/seaweedfs/wiki/Data-Structure-for-Large-Files
[Mount]: https://github.com/seaweedfs/seaweedfs/wiki/FUSE-Mount
[AmazonS3API]: https://github.com/seaweedfs/seaweedfs/wiki/Amazon-S3-API
[BackupToCloud]: https://github.com/seaweedfs/seaweedfs/wiki/Async-Replication-to-Cloud
[Hadoop]: https://github.com/seaweedfs/seaweedfs/wiki/Hadoop-Compatible-File-System
[WebDAV]: https://github.com/seaweedfs/seaweedfs/wiki/WebDAV
[ErasureCoding]: https://github.com/seaweedfs/seaweedfs/wiki/Erasure-coding-for-warm-storage
[TieredStorage]: https://github.com/seaweedfs/seaweedfs/wiki/Tiered-Storage
[CloudTier]: https://github.com/seaweedfs/seaweedfs/wiki/Cloud-Tier
[FilerDataEncryption]: https://github.com/seaweedfs/seaweedfs/wiki/Filer-Data-Encryption
[FilerTTL]: https://github.com/seaweedfs/seaweedfs/wiki/Filer-Stores
[VolumeServerTTL]: https://github.com/seaweedfs/seaweedfs/wiki/Store-file-with-a-Time-To-Live
[SeaweedFsCsiDriver]: https://github.com/seaweedfs/seaweedfs-csi-driver
[ActiveActiveAsyncReplication]: https://github.com/seaweedfs/seaweedfs/wiki/Filer-Active-Active-cross-cluster-continuous-synchronization
[FilerStoreReplication]: https://github.com/seaweedfs/seaweedfs/wiki/Filer-Store-Replication
[KeyLargeValueStore]: https://github.com/seaweedfs/seaweedfs/wiki/Filer-as-a-Key-Large-Value-Store
[CloudDrive]: https://github.com/seaweedfs/seaweedfs/wiki/Cloud-Drive-Architecture
[GatewayToRemoteObjectStore]: https://github.com/seaweedfs/seaweedfs/wiki/Gateway-to-Remote-Object-Storage


[Back to TOC](#table-of-contents)

## Example: Using Seaweed Object Store ##

By default, the master node runs on port 9333, and the volume nodes run on port 8080.
Let&#039;s start one master node, and two volume nodes on port 8080 and 8081. Ideally, they should be started from different machines. We&#039;ll use localhost as an example.

SeaweedFS uses HTTP REST operations to read, write, and delete. The responses are in JSON or JSONP format.

### Start Master Server ###

```
&gt; ./weed master
```

### Start Volume Servers ###

```
&gt; weed volume -dir=&quot;/tmp/data1&quot; -max=5  -master=&quot;localhost:9333&quot; -port=8080 &amp;
&gt; weed volume -dir=&quot;/tmp/data2&quot; -max=10 -master=&quot;localhost:9333&quot; -port=8081 &amp;
```

### Write File ###

To upload a file: first, send a HTTP POST, PUT, or GET request to `/dir/assign` to get an `fid` and a volume server URL:

```
&gt; curl http://localhost:9333/dir/assign
{&quot;count&quot;:1,&quot;fid&quot;:&quot;3,01637037d6&quot;,&quot;url&quot;:&quot;127.0.0.1:8080&quot;,&quot;publicUrl&quot;:&quot;localhost:8080&quot;}
```

Second, to store the file content, send a HTTP multi-part POST request to `url + &#039;/&#039; + fid` from the response:

```
&gt; curl -F file=@/home/chris/myphoto.jpg http://127.0.0.1:8080/3,01637037d6
{&quot;name&quot;:&quot;myphoto.jpg&quot;,&quot;size&quot;:43234,&quot;eTag&quot;:&quot;1cc0118e&quot;}
```

To update, send another POST request with updated file content.

For deletion, send an HTTP DELETE request to the same `url + &#039;/&#039; + fid` URL:

```
&gt; curl -X DELETE http://127.0.0.1:8080/3,01637037d6
```

### Save File Id ###

Now, you can save the `fid`, 3,01637037d6 in this case, to a database field.

The number 3 at the start represents a volume id. After the comma, it&#039;s one file key, 01, and a file cookie, 637037d6.

The volume id is an unsigned 32-bit integer. The file key is an unsigned 64-bit integer. The file cookie is an unsigned 32-bit integer, used to prevent URL guessing.

The file key and file cookie are both coded in hex. You can store the &lt;volume id, file key, file cookie&gt; tuple in your own format, or simply store the `fid` as a string.

If stored as a string, in theory, you would need 8+1+16+8=33 bytes. A char(33) would be enough, if not more than enough, since most uses will not need 2^32 volumes.

If space is really a concern, you can store the file id in your own format. You would need one 4-byte integer for volume id, 8-byte long number for file key, and a 4-byte integer for the file cookie. So 16 bytes are more than enough.

### Read File ###

Here is an example of how to render the URL.

First look up the volume server&#039;s URLs by the file&#039;s volumeId:

```
&gt; curl http://localhost:9333/dir/lookup?volumeId=3
{&quot;volumeId&quot;:&quot;3&quot;,&quot;locations&quot;:[{&quot;publicUrl&quot;:&quot;localhost:8080&quot;,&quot;url&quot;:&quot;localhost:8080&quot;}]}
```

Since (usually) there are not too many volume servers, and volumes don&#039;t move often, you can cache the results most of the time. Depending on the replication type, one volume can have multiple replica locations. Just randomly pick one location to read.

Now you can take the public URL, render the URL or directly read from the volume server via URL:

```
 http://localhost:8080/3,01637037d6.jpg
```

Notice we add a file extension &quot;.jpg&quot; here. It&#039;s optional and just one way for the client to specify the file content type.

If you want a nicer URL, you can use one of these alternative URL formats:

```
 http://localhost:8080/3/01637037d6/my_preferred_name.jpg
 http://localhost:8080/3/01637037d6.jpg
 http://localhost:8080/3,01637037d6.jpg
 http://localhost:8080/3/01637037d6
 http://localhost:8080/3,01637037d6
```

If you want to get a scaled version of an image, you can add some params:

```
http://localhost:8080/3/01637037d6.jpg?height=200&amp;width=200
http://localhost:8080/3/01637037d6.jpg?height=200&amp;width=200&amp;mode=fit
http://localhost:8080/3/01637037d6.jpg?height=200&amp;width=200&amp;mode=fill
```

### Rack-Aware and Data Center-Aware Replication ###

SeaweedFS applies the replication strategy at a volume level. So, when you are getting a file id, you can specify the replication strategy. For example:

```
curl http://localhost:9333/dir/assign?replication=001
```

The replication parameter options are:

```
000: no replication
001: replicate once on the same rack
010: replicate once on a different rack, but same data center
100: replicate once on a different data center
200: replicate twice on two different data center
110: replicate once on a different rack, and once on a different data center
```

More details about replication can be found [on the wiki][Replication].

[Replication]: https://github.com/seaweedfs/seaweedfs/wiki/Replication

You can also set the default replication strategy when starting the master server.

### Allocate File Key on Specific Data Center ###

Volume servers can be started with a specific data center name:

```
 weed volume -dir=/tmp/1 -port=8080 -dataCenter=dc1
 weed volume -dir=/tmp/2 -port=8081 -dataCenter=dc2
```

When requesting a file key, an optional &quot;dataCenter&quot; parameter can limit the assigned volume to the specific data center. For example, this specifies that the assigned volume should be limited to &#039;dc1&#039;:

```
 http://localhost:9333/dir/assign?dataCenter=dc1
```

### Other Features ###
  * [No Single Point of Failure][feat-1]
  * [Insert with your own keys][feat-2]
  * [Chunking large files][feat-3]
  * [Collection as a Simple Name Space][feat-4]

[feat-1]: https://github.com/seaweedfs/seaweedfs/wiki/Failover-Master-Server
[feat-2]: https://github.com/seaweedfs/seaweedfs/wiki/Optimization#insert-with-your-own-keys
[feat-3]: https://github.com/seaweedfs/seaweedfs/wiki/Optimization#upload-large-files
[feat-4]: https://github.com/seaweedfs/seaweedfs/wiki/Optimization#collection-as-a-simple-name-space

[Back to TOC](#table-of-contents)

## Object Store Architecture ##

Usually distributed file systems split each file into chunks, a central master keeps a mapping of filenames, chunk indices to chunk handles, and also which chunks each chunk server has.

The main drawback is that the central master can&#039;t handle many small files efficiently, and since all read requests need to go through the chunk master, so it might not scale well for many concurrent users.

Instead of managing chunks, SeaweedFS manages data volumes in the master server. Each data volume is 32GB in size, and can hold a lot of files. And each storage node can have many data volumes. So the master node only needs to store the metadata about the volumes, which is a fairly small amount of data and is generally stable.

The actual file metadata is stored in each volume on volume servers. Since each volume server only manages metadata of files on its own disk, with only 16 bytes for each file, all file access can read file metadata just from memory and only needs one disk operation to actually read file data.

For comparison, consider that an xfs inode structure in Linux is 536 bytes.

### Master Server and Volume Server ###

The architecture is fairly simple. The actual data is stored in volumes on storage nodes. One volume server can have multiple volumes, and can both support read and write access with basic authentication.

All volumes are managed by a master server. The master server contains the volume id to volume server mapping. This is fairly static information, and can be easily cached.

On each write request, the master server also generates a file key, which is a growing 64-bit unsigned integer. Since write requests are not generally as frequent as read requests, one master server should be able to handle the concurrency well.

### Write and Read files ###

When a client sends a write request, the master server returns (volume id, file key, file cookie, volume node URL) for the file. The client then contacts the volume node and POSTs the file content.

When a client needs to read a file based on (volume id, file key, file cookie), it asks the master server by the volume id for the 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[XTLS/Xray-core]]></title>
            <link>https://github.com/XTLS/Xray-core</link>
            <guid>https://github.com/XTLS/Xray-core</guid>
            <pubDate>Thu, 25 Dec 2025 00:05:30 GMT</pubDate>
            <description><![CDATA[Xray, Penetrates Everything. Also the best v2ray-core. Where the magic happens. An open platform for various uses.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/XTLS/Xray-core">XTLS/Xray-core</a></h1>
            <p>Xray, Penetrates Everything. Also the best v2ray-core. Where the magic happens. An open platform for various uses.</p>
            <p>Language: Go</p>
            <p>Stars: 33,645</p>
            <p>Forks: 4,822</p>
            <p>Stars today: 28 stars today</p>
            <h2>README</h2><pre># Project X

[Project X](https://github.com/XTLS) originates from XTLS protocol, providing a set of network tools such as [Xray-core](https://github.com/XTLS/Xray-core) and [REALITY](https://github.com/XTLS/REALITY).

[README](https://github.com/XTLS/Xray-core#readme) is open, so feel free to submit your project [here](https://github.com/XTLS/Xray-core/pulls).

## Sponsors

[![Remnawave](https://github.com/user-attachments/assets/a22d34ae-01ee-441c-843a-85356748ed1e)](https://docs.rw)

[![Happ](https://github.com/user-attachments/assets/14055dab-e8bb-48bd-89e8-962709e4098e)](https://happ.su)

[**Sponsor Xray-core**](https://github.com/XTLS/Xray-core/issues/3668)

## Donation &amp; NFTs

### [Collect a Project X NFT to support the development of Project X!](https://opensea.io/item/ethereum/0x5ee362866001613093361eb8569d59c4141b76d1/1)

[&lt;img alt=&quot;Project X NFT&quot; width=&quot;150px&quot; src=&quot;https://raw2.seadn.io/ethereum/0x5ee362866001613093361eb8569d59c4141b76d1/7fa9ce900fb39b44226348db330e32/8b7fa9ce900fb39b44226348db330e32.svg&quot; /&gt;](https://opensea.io/item/ethereum/0x5ee362866001613093361eb8569d59c4141b76d1/1)

- **TRX(Tron)/USDT/USDC: `TNrDh5VSfwd4RPrwsohr6poyNTfFefNYan`**
- **TON: `UQApeV-u2gm43aC1uP76xAC1m6vCylstaN1gpfBmre_5IyTH`**
- **BTC: `1JpqcziZZuqv3QQJhZGNGBVdCBrGgkL6cT`**
- **XMR: `4ABHQZ3yJZkBnLoqiKvb3f8eqUnX4iMPb6wdant5ZLGQELctcerceSGEfJnoCk6nnyRZm73wrwSgvZ2WmjYLng6R7sR67nq`**
- **SOL/USDT/USDC: `3x5NuXHzB5APG6vRinPZcsUv5ukWUY1tBGRSJiEJWtZa`**
- **ETH/USDT/USDC: `0xDc3Fe44F0f25D13CACb1C4896CD0D321df3146Ee`**
- **Project X NFT: https://opensea.io/item/ethereum/0x5ee362866001613093361eb8569d59c4141b76d1/1**
- **VLESS NFT: https://opensea.io/collection/vless**
- **REALITY NFT: https://opensea.io/item/ethereum/0x5ee362866001613093361eb8569d59c4141b76d1/2**
- **Related links: [VLESS Post-Quantum Encryption](https://github.com/XTLS/Xray-core/pull/5067), [XHTTP: Beyond REALITY](https://github.com/XTLS/Xray-core/discussions/4113), [Announcement of NFTs by Project X](https://github.com/XTLS/Xray-core/discussions/3633)**

## License

[Mozilla Public License Version 2.0](https://github.com/XTLS/Xray-core/blob/main/LICENSE)

## Documentation

[Project X Official Website](https://xtls.github.io)

## Telegram

[Project X](https://t.me/projectXray)

[Project X Channel](https://t.me/projectXtls)

[Project VLESS](https://t.me/projectVless) (–†—É—Å—Å–∫–∏–π)

[Project XHTTP](https://t.me/projectXhttp) (Persian)

## Installation

- Linux Script
  - [XTLS/Xray-install](https://github.com/XTLS/Xray-install) (**Official**)
  - [tempest](https://github.com/team-cloudchaser/tempest) (supports [`systemd`](https://systemd.io) and [OpenRC](https://github.com/OpenRC/openrc); Linux-only)
- Docker
  - [ghcr.io/xtls/xray-core](https://ghcr.io/xtls/xray-core) (**Official**)
  - [teddysun/xray](https://hub.docker.com/r/teddysun/xray)
  - [wulabing/xray_docker](https://github.com/wulabing/xray_docker)
- Web Panel - **WARNING: Please DO NOT USE plain HTTP panels like 3X-UI**, as they are believed to be bribed by Iran GFW for supporting plain HTTP by default and refused to change (https://github.com/XTLS/Xray-core/pull/3884#issuecomment-2439595331), which has already put many users&#039; data security in danger in the past few years. **If you are already using 3X-UI, please switch to the following panels, which are verified to support HTTPS and SSH port forwarding only:**
  - [Remnawave](https://github.com/remnawave/panel)
  - [X-Panel](https://github.com/xeefei/X-Panel)
  - [PasarGuard](https://github.com/PasarGuard/panel)
  - [Marzban](https://github.com/Gozargah/Marzban)
  - [Xray-UI](https://github.com/qist/xray-ui)
  - [Hiddify](https://github.com/hiddify/Hiddify-Manager)
- One Click
  - [Xray-REALITY](https://github.com/zxcvos/Xray-script), [xray-reality](https://github.com/sajjaddg/xray-reality), [reality-ezpz](https://github.com/aleskxyz/reality-ezpz)
  - [Xray_bash_onekey](https://github.com/hello-yunshu/Xray_bash_onekey), [XTool](https://github.com/LordPenguin666/XTool), [VPainLess](https://github.com/vpainless/vpainless)
  - [v2ray-agent](https://github.com/mack-a/v2ray-agent), [Xray_onekey](https://github.com/wulabing/Xray_onekey), [ProxySU](https://github.com/proxysu/ProxySU)
- Magisk
  - [Xray4Magisk](https://github.com/Asterisk4Magisk/Xray4Magisk)
  - [Xray_For_Magisk](https://github.com/E7KMbb/Xray_For_Magisk)
- Homebrew
  - `brew install xray`

## Usage

- Example
  - [VLESS-XTLS-uTLS-REALITY](https://github.com/XTLS/REALITY#readme)
  - [VLESS-TCP-XTLS-Vision](https://github.com/XTLS/Xray-examples/tree/main/VLESS-TCP-XTLS-Vision)
  - [All-in-One-fallbacks-Nginx](https://github.com/XTLS/Xray-examples/tree/main/All-in-One-fallbacks-Nginx)
- Xray-examples
  - [XTLS/Xray-examples](https://github.com/XTLS/Xray-examples)
  - [chika0801/Xray-examples](https://github.com/chika0801/Xray-examples)
  - [lxhao61/integrated-examples](https://github.com/lxhao61/integrated-examples)
- Tutorial
  - [XTLS Vision](https://github.com/chika0801/Xray-install)
  - [REALITY (English)](https://cscot.pages.dev/2023/03/02/Xray-REALITY-tutorial/)
  - [XTLS-Iran-Reality (English)](https://github.com/SasukeFreestyle/XTLS-Iran-Reality)
  - [Xray REALITY with &#039;steal oneself&#039; (English)](https://computerscot.github.io/vless-xtls-utls-reality-steal-oneself.html)
  - [Xray with WireGuard inbound (English)](https://g800.pages.dev/wireguard)

## GUI Clients

- OpenWrt
  - [PassWall](https://github.com/xiaorouji/openwrt-passwall), [PassWall 2](https://github.com/xiaorouji/openwrt-passwall2)
  - [ShadowSocksR Plus+](https://github.com/fw876/helloworld)
  - [luci-app-xray](https://github.com/yichya/luci-app-xray) ([openwrt-xray](https://github.com/yichya/openwrt-xray))
- Asuswrt-Merlin
  - [XRAYUI](https://github.com/DanielLavrushin/asuswrt-merlin-xrayui)
- Windows
  - [v2rayN](https://github.com/2dust/v2rayN)
  - [Furious](https://github.com/LorenEteval/Furious)
  - [Invisible Man - Xray](https://github.com/InvisibleManVPN/InvisibleMan-XRayClient)
  - [AnyPortal](https://github.com/AnyPortal/AnyPortal)
- Android
  - [v2rayNG](https://github.com/2dust/v2rayNG)
  - [X-flutter](https://github.com/XTLS/X-flutter)
  - [SaeedDev94/Xray](https://github.com/SaeedDev94/Xray)
  - [SimpleXray](https://github.com/lhear/SimpleXray)
  - [AnyPortal](https://github.com/AnyPortal/AnyPortal)
- iOS &amp; macOS arm64 &amp; tvOS
  - [Happ](https://apps.apple.com/app/happ-proxy-utility/id6504287215) ([tvOS](https://apps.apple.com/us/app/happ-proxy-utility-for-tv/id6748297274))
  - [Streisand](https://apps.apple.com/app/streisand/id6450534064)
  - [OneXray](https://github.com/OneXray/OneXray)
- macOS arm64 &amp; x64
  - [Happ](https://apps.apple.com/app/happ-proxy-utility/id6504287215)
  - [V2rayU](https://github.com/yanue/V2rayU)
  - [V2RayXS](https://github.com/tzmax/V2RayXS)
  - [Furious](https://github.com/LorenEteval/Furious)
  - [OneXray](https://github.com/OneXray/OneXray)
  - [GoXRay](https://github.com/goxray/desktop)
  - [AnyPortal](https://github.com/AnyPortal/AnyPortal)
  - [v2rayN](https://github.com/2dust/v2rayN)
- Linux
  - [v2rayA](https://github.com/v2rayA/v2rayA)
  - [Furious](https://github.com/LorenEteval/Furious)
  - [GorzRay](https://github.com/ketetefid/GorzRay)
  - [GoXRay](https://github.com/goxray/desktop)
  - [AnyPortal](https://github.com/AnyPortal/AnyPortal)
  - [v2rayN](https://github.com/2dust/v2rayN)

## Others that support VLESS, XTLS, REALITY, XUDP, PLUX...

- iOS &amp; macOS arm64 &amp; tvOS
  - [Shadowrocket](https://apps.apple.com/app/shadowrocket/id932747118)
  - [Loon](https://apps.apple.com/us/app/loon/id1373567447)
- Xray Tools
  - [xray-knife](https://github.com/lilendian0x00/xray-knife)
  - [xray-checker](https://github.com/kutovoys/xray-checker)
- Xray Wrapper
  - [XTLS/libXray](https://github.com/XTLS/libXray)
  - [xtls-sdk](https://github.com/remnawave/xtls-sdk)
  - [xtlsapi](https://github.com/hiddify/xtlsapi)
  - [AndroidLibXrayLite](https://github.com/2dust/AndroidLibXrayLite)
  - [Xray-core-python](https://github.com/LorenEteval/Xray-core-python)
  - [xray-api](https://github.com/XVGuardian/xray-api)
- [XrayR](https://github.com/XrayR-project/XrayR)
  - [XrayR-release](https://github.com/XrayR-project/XrayR-release)
  - [XrayR-V2Board](https://github.com/missuo/XrayR-V2Board)
- Cores
  - [Amnezia VPN](https://github.com/amnezia-vpn)
  - [mihomo](https://github.com/MetaCubeX/mihomo)
  - [sing-box](https://github.com/SagerNet/sing-box)

## Contributing

[Code of Conduct](https://github.com/XTLS/Xray-core/blob/main/CODE_OF_CONDUCT.md)

[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/XTLS/Xray-core)

## Credits

- [Xray-core v1.0.0](https://github.com/XTLS/Xray-core/releases/tag/v1.0.0) was forked from [v2fly-core 9a03cc5](https://github.com/v2fly/v2ray-core/commit/9a03cc5c98d04cc28320fcee26dbc236b3291256), and we have made &amp; accumulated a huge number of enhancements over time, check [the release notes for each version](https://github.com/XTLS/Xray-core/releases).
- For third-party projects used in [Xray-core](https://github.com/XTLS/Xray-core), check your local or [the latest go.mod](https://github.com/XTLS/Xray-core/blob/main/go.mod).

## One-line Compilation

### Windows (PowerShell)

```powershell
$env:CGO_ENABLED=0
go build -o xray.exe -trimpath -buildvcs=false -ldflags=&quot;-s -w -buildid=&quot; -v ./main
```

### Linux / macOS

```bash
CGO_ENABLED=0 go build -o xray -trimpath -buildvcs=false -ldflags=&quot;-s -w -buildid=&quot; -v ./main
```

### Reproducible Releases

Make sure that you are using the same Go version, and remember to set the git commit id (7 bytes):

```bash
CGO_ENABLED=0 go build -o xray -trimpath -buildvcs=false -gcflags=&quot;all=-l=4&quot; -ldflags=&quot;-X github.com/xtls/xray-core/core.build=REPLACE -s -w -buildid=&quot; -v ./main
```

If you are compiling a 32-bit MIPS/MIPSLE target, use this command instead:

```bash
CGO_ENABLED=0 go build -o xray -trimpath -buildvcs=false -gcflags=&quot;-l=4&quot; -ldflags=&quot;-X github.com/xtls/xray-core/core.build=REPLACE -s -w -buildid=&quot; -v ./main
```

## Stargazers over time

[![Stargazers over time](https://starchart.cc/XTLS/Xray-core.svg)](https://starchart.cc/XTLS/Xray-core)
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[charmbracelet/glow]]></title>
            <link>https://github.com/charmbracelet/glow</link>
            <guid>https://github.com/charmbracelet/glow</guid>
            <pubDate>Thu, 25 Dec 2025 00:05:29 GMT</pubDate>
            <description><![CDATA[Render markdown on the CLI, with pizzazz! üíÖüèª]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/charmbracelet/glow">charmbracelet/glow</a></h1>
            <p>Render markdown on the CLI, with pizzazz! üíÖüèª</p>
            <p>Language: Go</p>
            <p>Stars: 21,963</p>
            <p>Forks: 537</p>
            <p>Stars today: 22 stars today</p>
            <h2>README</h2><pre># Glow

Render markdown on the CLI, with _pizzazz_!

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://stuff.charm.sh/glow/glow-banner-github.gif&quot; alt=&quot;Glow Logo&quot;&gt;
    &lt;a href=&quot;https://github.com/charmbracelet/glow/releases&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/release/charmbracelet/glow.svg&quot; alt=&quot;Latest Release&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://pkg.go.dev/github.com/charmbracelet/glow?tab=doc&quot;&gt;&lt;img src=&quot;https://godoc.org/github.com/golang/gddo?status.svg&quot; alt=&quot;GoDoc&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/charmbracelet/glow/actions&quot;&gt;&lt;img src=&quot;https://github.com/charmbracelet/glow/workflows/build/badge.svg&quot; alt=&quot;Build Status&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://goreportcard.com/report/github.com/charmbracelet/glow&quot;&gt;&lt;img src=&quot;https://goreportcard.com/badge/charmbracelet/glow&quot; alt=&quot;Go ReportCard&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/c2246366-f84b-4847-b431-32a61ca07b74&quot; width=&quot;800&quot; alt=&quot;Glow UI Demo&quot;&gt;
&lt;/p&gt;

## What is it?

Glow is a terminal based markdown reader designed from the ground up to bring
out the beauty‚Äîand power‚Äîof the CLI.

Use it to discover markdown files, read documentation directly on the command
line. Glow will find local markdown files in subdirectories or a local
Git repository.

## Installation

### Package Manager

```bash
# macOS or Linux
brew install glow
```

```bash
# macOS (with MacPorts)
sudo port install glow
```

```bash
# Arch Linux (btw)
pacman -S glow
```

```bash
# Void Linux
xbps-install -S glow
```

```bash
# Nix shell
nix-shell -p glow --command glow
```

```bash
# FreeBSD
pkg install glow
```

```bash
# Solus
eopkg install glow
```

```bash
# Windows (with Chocolatey, Scoop, or Winget)
choco install glow
scoop install glow
winget install charmbracelet.glow
```

```bash
# Android (with termux)
pkg install glow
```

```bash
# Ubuntu (Snapcraft)
sudo snap install glow
```

```bash
# Debian/Ubuntu
sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://repo.charm.sh/apt/gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/charm.gpg
echo &quot;deb [signed-by=/etc/apt/keyrings/charm.gpg] https://repo.charm.sh/apt/ * *&quot; | sudo tee /etc/apt/sources.list.d/charm.list
sudo apt update &amp;&amp; sudo apt install glow
```

```bash
# Fedora/RHEL
echo &#039;[charm]
name=Charm
baseurl=https://repo.charm.sh/yum/
enabled=1
gpgcheck=1
gpgkey=https://repo.charm.sh/yum/gpg.key&#039; | sudo tee /etc/yum.repos.d/charm.repo
sudo yum install glow
```

Or download a binary from the [releases][releases] page. MacOS, Linux, Windows,
FreeBSD and OpenBSD binaries are available, as well as Debian, RPM, and Alpine
packages. ARM builds are also available for macOS, Linux, FreeBSD and OpenBSD.

### Go

Or just install it with `go`:

```bash
go install github.com/charmbracelet/glow/v2@latest
```

### Build (requires Go 1.21+)

```bash
git clone https://github.com/charmbracelet/glow.git
cd glow
go build
```

[releases]: https://github.com/charmbracelet/glow/releases

## The TUI

Simply run `glow` without arguments to start the textual user interface and
browse local. Glow will find local markdown files in the
current directory and below or, if you‚Äôre in a Git repository, Glow will search
the repo.

Markdown files can be read with Glow&#039;s high-performance pager. Most of the
keystrokes you know from `less` are the same, but you can press `?` to list
the hotkeys.

## The CLI

In addition to a TUI, Glow has a CLI for working with Markdown. To format a
document use a markdown source as the primary argument:

```bash
# Read from file
glow README.md

# Read from stdin
echo &quot;[Glow](https://github.com/charmbracelet/glow)&quot; | glow -

# Fetch README from GitHub / GitLab
glow github.com/charmbracelet/glow

# Fetch markdown from HTTP
glow https://host.tld/file.md
```

### Word Wrapping

The `-w` flag lets you set a maximum width at which the output will be wrapped:

```bash
glow -w 60
```

### Paging

CLI output can be displayed in your preferred pager with the `-p` flag. This defaults
to the ANSI-aware `less -r` if `$PAGER` is not explicitly set.

### Styles

You can choose a style with the `-s` flag. When no flag is provided `glow` tries
to detect your terminal&#039;s current background color and automatically picks
either the `dark` or the `light` style for you.

```bash
glow -s [dark|light]
```

Alternatively you can also supply a custom JSON stylesheet:

```bash
glow -s mystyle.json
```

For additional usage details see:

```bash
glow --help
```

Check out the [Glamour Style Section](https://github.com/charmbracelet/glamour/blob/master/styles/gallery/README.md)
to find more styles. Or [make your own](https://github.com/charmbracelet/glamour/tree/master/styles)!

## The Config File

If you find yourself supplying the same flags to `glow` all the time, it&#039;s
probably a good idea to create a config file. Run `glow config`, which will open
it in your favorite $EDITOR. Alternatively you can manually put a file named
`glow.yml` in the default config path of you platform. If you&#039;re not sure where
that is, please refer to `glow --help`.

Here&#039;s an example config:

```yaml
# style name or JSON path (default &quot;auto&quot;)
style: &quot;light&quot;
# mouse wheel support (TUI-mode only)
mouse: true
# use pager to display markdown
pager: true
# at which column should we word wrap?
width: 80
# show all files, including hidden and ignored.
all: false
# show line numbers (TUI-mode only)
showLineNumbers: false
# preserve newlines in the output
preserveNewLines: false
```

## Contributing

See [contributing][contribute].

[contribute]: https://github.com/charmbracelet/glow/contribute

## Feedback

We‚Äôd love to hear your thoughts on this project. Feel free to drop us a note!

- [Twitter](https://twitter.com/charmcli)
- [The Fediverse](https://mastodon.social/@charmcli)
- [Discord](https://charm.sh/chat)

## License

[MIT](https://github.com/charmbracelet/glow/raw/master/LICENSE)

---

Part of [Charm](https://charm.sh).

&lt;a href=&quot;https://charm.sh/&quot;&gt;&lt;img alt=&quot;The Charm logo&quot; src=&quot;https://stuff.charm.sh/charm-badge.jpg&quot; width=&quot;400&quot;&gt;&lt;/a&gt;

CharmÁÉ≠Áà±ÂºÄÊ∫ê ‚Ä¢ Charm loves open source
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[istio/istio]]></title>
            <link>https://github.com/istio/istio</link>
            <guid>https://github.com/istio/istio</guid>
            <pubDate>Thu, 25 Dec 2025 00:05:28 GMT</pubDate>
            <description><![CDATA[Connect, secure, control, and observe services.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/istio/istio">istio/istio</a></h1>
            <p>Connect, secure, control, and observe services.</p>
            <p>Language: Go</p>
            <p>Stars: 37,734</p>
            <p>Forks: 8,192</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre># Istio

[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1395/badge)](https://bestpractices.coreinfrastructure.org/projects/1395)
[![Go Report Card](https://goreportcard.com/badge/github.com/istio/istio)](https://goreportcard.com/report/github.com/istio/istio)
[![GoDoc](https://godoc.org/istio.io/istio?status.svg)](https://godoc.org/istio.io/istio)

&lt;a href=&quot;https://istio.io/&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/cncf/artwork/refs/heads/main/projects/istio/icon/color/istio-icon-color.svg&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/istio/istio/raw/master/logo/istio-bluelogo-whitebackground-unframed.svg&quot;&gt;
      &lt;img title=&quot;Istio&quot; height=&quot;100&quot; width=&quot;100&quot; alt=&quot;Istio logo&quot; src=&quot;https://github.com/istio/istio/raw/master/logo/istio-bluelogo-whitebackground-unframed.svg&quot;&gt;
    &lt;/picture&gt;
&lt;/a&gt;

---

Istio is an open source service mesh that layers transparently onto existing distributed applications. Istio‚Äôs powerful features provide a uniform and more efficient way to secure, connect, and monitor services. Istio is the path to load balancing, service-to-service authentication, and monitoring ‚Äì with few or no service code changes.

- For in-depth information about how to use Istio, visit [istio.io](https://istio.io)
- To ask questions and get assistance from our community, visit [GitHub Discussions](https://github.com/istio/istio/discussions)
- To learn how to participate in our overall community, visit [our community page](https://istio.io/about/community)

In this README:

- [Introduction](#introduction)
- [Repositories](#repositories)
- [Issue management](#issue-management)

In addition, here are some other documents you may wish to read:

- [Istio Community](https://github.com/istio/community#istio-community) - describes how to get involved and contribute to the Istio project
- [Istio Developer&#039;s Guide](https://github.com/istio/istio/wiki/Preparing-for-Development) - explains how to set up and use an Istio development environment
- [Project Conventions](https://github.com/istio/istio/wiki/Development-Conventions) - describes the conventions we use within the code base
- [Creating Fast and Lean Code](https://github.com/istio/istio/wiki/Writing-Fast-and-Lean-Code) - performance-oriented advice and guidelines for the code base

You&#039;ll find many other useful documents on our [Wiki](https://github.com/istio/istio/wiki).

## Introduction

[Istio](https://istio.io/latest/docs/concepts/what-is-istio/) is an open platform for providing a uniform way to [integrate
microservices](https://istio.io/latest/docs/examples/microservices-istio/), manage [traffic flow](https://istio.io/latest/docs/concepts/traffic-management/) across microservices, enforce policies
and aggregate telemetry data. Istio&#039;s control plane provides an abstraction
layer over the underlying cluster management platform, such as Kubernetes.

Istio is composed of these components:

- **Envoy** - Sidecar proxies per microservice to handle ingress/egress traffic
   between services in the cluster and from a service to external
   services. The proxies form a _secure microservice mesh_ providing a rich
   set of functions like discovery, rich layer-7 routing, circuit breakers,
   policy enforcement and telemetry recording/reporting
   functions.

  &gt; Note: The service mesh is not an overlay network. It
  &gt; simplifies and enhances how microservices in an application talk to each
  &gt; other over the network provided by the underlying platform.

* **Ztunnel** - A lightweight data plane proxy written in Rust,
    used in Ambient mesh mode to provide secure connectivity and observability for workloads without sidecar proxies.

- **Istiod** - The Istio control plane. It provides service discovery, configuration and certificate management.

## Repositories

The Istio project is divided across a few GitHub repositories:

- [istio/api](https://github.com/istio/api). This repository defines
component-level APIs and common configuration formats for the Istio platform.

- [istio/community](https://github.com/istio/community). This repository contains
information on the Istio community, including the various documents that govern
the Istio open source project.

- [istio/istio](README.md). This is the main code repository. It hosts Istio&#039;s
core components, install artifacts, and sample programs. It includes:

    - [istioctl](istioctl/). This directory contains code for the
[_istioctl_](https://istio.io/latest/docs/reference/commands/istioctl/) command line utility.

    - [pilot](pilot/). This directory
contains platform-specific code to populate the
[abstract service model](https://istio.io/docs/concepts/traffic-management/#pilot), dynamically reconfigure the proxies
when the application topology changes, as well as translate
[routing rules](https://istio.io/latest/docs/reference/config/networking/) into proxy specific configuration.

    - [security](security/). This directory contains [security](https://istio.io/latest/docs/concepts/security/) related code.

- [istio/proxy](https://github.com/istio/proxy). The Istio proxy contains
extensions to the [Envoy proxy](https://github.com/envoyproxy/envoy) (in the form of
Envoy filters) that support authentication, authorization, and telemetry collection.

- [istio/ztunnel](https://github.com/istio/ztunnel). The repository contains the Rust implementation of the ztunnel
component of Ambient mesh.

- [istio/client-go](https://github.com/istio/client-go). This repository defines
  auto-generated Kubernetes clients for interacting with Istio resources programmatically.

&gt; [!NOTE]
&gt; Only the `istio/api` and `istio/client-go` repositories expose stable interfaces intended for direct usage as libraries.

## Issue management

We use GitHub to track all of our bugs and feature requests. Each issue we track has a variety of metadata:

- **Epic**. An epic represents a feature area for Istio as a whole. Epics are fairly broad in scope and are basically product-level things.
Each issue is ultimately part of an epic.

- **Milestone**. Each issue is assigned a milestone. This is 0.1, 0.2, ..., or &#039;Nebulous Future&#039;. The milestone indicates when we
think the issue should get addressed.

- **Priority**. Each issue has a priority which is represented by the column in the [Prioritization](https://github.com/orgs/istio/projects/6) project. Priority can be one of
P0, P1, P2, or &gt;P2. The priority indicates how important it is to address the issue within the milestone. P0 says that the
milestone cannot be considered achieved if the issue isn&#039;t resolved.

---

&lt;div align=&quot;center&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/cncf/artwork/refs/heads/main/other/cncf/horizontal/color-whitetext/cncf-color-whitetext.svg&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://raw.githubusercontent.com/cncf/artwork/master/other/cncf/horizontal/color/cncf-color.svg&quot;&gt;
      &lt;img width=&quot;300&quot; alt=&quot;Cloud Native Computing Foundation logo&quot; src=&quot;https://raw.githubusercontent.com/cncf/artwork/refs/heads/main/other/cncf/horizontal/color-whitetext/cncf-color-whitetext.svg&quot;&gt;
    &lt;/picture&gt;
    &lt;p&gt;Istio is a &lt;a href=&quot;https://cncf.io&quot;&gt;Cloud Native Computing Foundation&lt;/a&gt; project.&lt;/p&gt;
&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[memodb-io/Acontext]]></title>
            <link>https://github.com/memodb-io/Acontext</link>
            <guid>https://github.com/memodb-io/Acontext</guid>
            <pubDate>Thu, 25 Dec 2025 00:05:27 GMT</pubDate>
            <description><![CDATA[Context Data Platform for Agents. Join the community‚ù§Ô∏è: https://discord.acontext.io]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/memodb-io/Acontext">memodb-io/Acontext</a></h1>
            <p>Context Data Platform for Agents. Join the community‚ù§Ô∏è: https://discord.acontext.io</p>
            <p>Language: Go</p>
            <p>Stars: 2,095</p>
            <p>Forks: 186</p>
            <p>Stars today: 37 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://discord.acontext.io&quot;&gt;
      &lt;img alt=&quot;Show Acontext header banner&quot; src=&quot;./assets/Acontext-header-banner.png&quot;&gt;
  &lt;/a&gt;
  &lt;p&gt;
    &lt;h4&gt;Context Data Platform for Building Cloud-native AI Agents&lt;/h4&gt;
  &lt;/p&gt;
  &lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://pypi.org/project/acontext/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/acontext.svg&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://www.npmjs.com/package/@acontext/acontext&quot;&gt;&lt;img src=&quot;https://img.shields.io/npm/v/@acontext/acontext.svg?logo=npm&amp;logoColor=fff&amp;style=flat&amp;labelColor=2C2C2C&amp;color=28CF8D&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/memodb-io/acontext/actions/workflows/core-test.yaml&quot;&gt;&lt;img src=&quot;https://github.com/memodb-io/acontext/actions/workflows/core-test.yaml/badge.svg&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/memodb-io/acontext/actions/workflows/api-test.yaml&quot;&gt;&lt;img src=&quot;https://github.com/memodb-io/acontext/actions/workflows/api-test.yaml/badge.svg&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/memodb-io/acontext/actions/workflows/cli-test.yaml&quot;&gt;&lt;img src=&quot;https://github.com/memodb-io/acontext/actions/workflows/cli-test.yaml/badge.svg&quot;&gt;&lt;/a&gt;
  &lt;/p&gt;
  &lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://x.com/acontext_io&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/acontext_io?style=social&quot; alt=&quot;Twitter Follow&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://discord.acontext.io&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/dynamic/json?label=Acontext&amp;style=flat&amp;query=approximate_member_count&amp;url=https%3A%2F%2Fdiscord.com%2Fapi%2Fv10%2Finvites%2FSG9xJcqVBu%3Fwith_counts%3Dtrue&amp;logo=discord&amp;logoColor=white&amp;suffix=+members&amp;color=36393f&amp;labelColor=5765F2&quot; alt=&quot;Acontext Discord&quot;&gt;&lt;/a&gt;
  &lt;/p&gt;
  &lt;div align=&quot;center&quot;&gt;
    &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
    &lt;a href=&quot;./readme/de/README.md&quot;&gt;Deutsch&lt;/a&gt; | 
    &lt;a href=&quot;./readme/es/README.md&quot;&gt;Espa√±ol&lt;/a&gt; | 
    &lt;a href=&quot;./readme/fr/README.md&quot;&gt;Fran√ßais&lt;/a&gt; | 
    &lt;a href=&quot;./readme/ja/README.md&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt; | 
    &lt;a href=&quot;./readme/ko/README.md&quot;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | 
    &lt;a href=&quot;./readme/pt/README.md&quot;&gt;Portugu√™s&lt;/a&gt; | 
    &lt;a href=&quot;./readme/ru/README.md&quot;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | 
    &lt;a href=&quot;./readme/zh/README.md&quot;&gt;‰∏≠Êñá&lt;/a&gt;
  &lt;/div&gt;
  &lt;br/&gt;
&lt;/div&gt;

Acontext can help you:

- **Build a scalable agent with better context engineering**
- **Observe your agent success rate**
- **Improve your agent with self-learning for each user**



&lt;div align=&quot;center&quot;&gt;
    &lt;picture&gt;
      &lt;img alt=&quot;Acontext Learning&quot; src=&quot;./assets/acontext-components.jpg&quot; width=&quot;100%&quot;&gt;
    &lt;/picture&gt;
  &lt;p&gt;Context Data Platform that Store, Observe and Learn&lt;/p&gt;
&lt;/div&gt;


# üí° Core Features

- Context Engineering
  - [Session](https://docs.acontext.io/store/messages/multi-provider): Multi-modal LLM Message Storage
  - [Disk](https://docs.acontext.io/store/disk): Filesystem for artifacts
  - [Context Editing](https://docs.acontext.io/store/editing) - Context Engineering in one call

&lt;div align=&quot;center&quot;&gt;
    &lt;picture&gt;
      &lt;img alt=&quot;Acontext Learning&quot; src=&quot;./assets/acontext-context-engineering.png&quot; width=&quot;80%&quot;&gt;
    &lt;/picture&gt;
  &lt;p&gt;Context Engineering in Acontext&lt;/p&gt;
&lt;/div&gt;

- **Observe agent tasks and user feedback**
  - [Task Agent](https://docs.acontext.io/observe/agent_tasks) - Background agent that collects task&#039;s status, progress and preferences
- **Enable agent self-learning**
  - [Experience Agent](https://docs.acontext.io/learn/advance/experience-agent) - Background agents that distill, save and search skills for each user.
- **View everything in one [dashboard](https://docs.acontext.io/observe/dashboard)**

&lt;div align=&quot;center&quot;&gt;
    &lt;picture&gt;
      &lt;img alt=&quot;Dashboard&quot; src=&quot;./docs/images/dashboard/BI.png&quot; width=&quot;80%&quot;&gt;
    &lt;/picture&gt;
  &lt;p&gt;Dashboard of Agent Success Rate and Other Metrics&lt;/p&gt;
&lt;/div&gt;



# üèóÔ∏è How it works?

&lt;details&gt;
&lt;summary&gt;click to open&lt;/summary&gt;

```mermaid
graph TB
    subgraph &quot;Client Layer&quot;
        PY[&quot;pip install acontext&quot;]
        TS[&quot;npm i @acontext/acontext&quot;]
    end
    
    subgraph &quot;Acontext Backend&quot;
      subgraph &quot; &quot;
          API[&quot;API&lt;br/&gt;localhost:8029&quot;]
          CORE[&quot;Core&quot;]
          API --&gt;|FastAPI &amp; MQ| CORE
      end
      
      subgraph &quot; &quot;
          Infrastructure[&quot;Infrastructures&quot;]
          PG[&quot;PostgreSQL&quot;]
          S3[&quot;S3&quot;]
          REDIS[&quot;Redis&quot;]
          MQ[&quot;RabbitMQ&quot;]
      end
    end
    
    subgraph &quot;Dashboard&quot;
        UI[&quot;Web Dashboard&lt;br/&gt;localhost:3000&quot;]
    end
    
    PY --&gt;|RESTFUL API| API
    TS --&gt;|RESTFUL API| API
    UI --&gt;|RESTFUL API| API
    API --&gt; Infrastructure
    CORE --&gt; Infrastructure

    Infrastructure --&gt; PG
    Infrastructure --&gt; S3
    Infrastructure --&gt; REDIS
    Infrastructure --&gt; MQ
    
    
    style PY fill:#3776ab,stroke:#fff,stroke-width:2px,color:#fff
    style TS fill:#3178c6,stroke:#fff,stroke-width:2px,color:#fff
    style API fill:#00add8,stroke:#fff,stroke-width:2px,color:#fff
    style CORE fill:#ffd43b,stroke:#333,stroke-width:2px,color:#333
    style UI fill:#000,stroke:#fff,stroke-width:2px,color:#fff
    style PG fill:#336791,stroke:#fff,stroke-width:2px,color:#fff
    style S3 fill:#ff9900,stroke:#fff,stroke-width:2px,color:#fff
    style REDIS fill:#dc382d,stroke:#fff,stroke-width:2px,color:#fff
    style MQ fill:#ff6600,stroke:#fff,stroke-width:2px,color:#fff
```

## How They Work Together

```txt
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ User ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ Your Agent ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   Session    ‚îÇ    ‚îÇ Artifact Disk ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ                  ‚îÇ # if enable
                  ‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ         ‚îÇ Observed Tasks  ‚îÇ
                  ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ                  ‚îÇ # if enable
                  ‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ         ‚îÇ   Learn Skills  ‚îÇ
                  ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      Search skills
```



## Data Structures

&lt;details&gt;
&lt;summary&gt;üìñ Task Structure&lt;/summary&gt;

```json
{
  &quot;task_description&quot;: &quot;Star https://github.com/memodb-io/Acontext&quot;,
  &quot;progresses&quot;: [
    &quot;I have navigated to Acontext repo&quot;,
    &quot;Tried to Star but a pop-up required me to login&quot;,
    ...
  ],
  &quot;user_preferences&quot;: [
    &quot;user wants to use outlook email to login&quot;
  ]
}
```
&lt;/details&gt;



&lt;details&gt;
&lt;summary&gt;üìñ Skill Structure&lt;/summary&gt;


```json
{
    &quot;use_when&quot;: &quot;star a repo on github.com&quot;,
    &quot;preferences&quot;: &quot;use user&#039;s outlook account&quot;,
    &quot;tool_sops&quot;: [
        {&quot;tool_name&quot;: &quot;goto&quot;, &quot;action&quot;: &quot;goto github.com&quot;},
        {&quot;tool_name&quot;: &quot;click&quot;, &quot;action&quot;: &quot;find login button if any. login first&quot;},
        ...
    ]
}
```

&lt;/details&gt;



&lt;details&gt;
&lt;summary&gt;üìñ Space Structure&lt;/summary&gt;

```txt
/
‚îî‚îÄ‚îÄ github/ (folder)
    ‚îî‚îÄ‚îÄ GTM (page)
        ‚îú‚îÄ‚îÄ find_trending_repos (sop)
        ‚îî‚îÄ‚îÄ find_contributor_emails (sop)
    ‚îî‚îÄ‚îÄ basic_ops (page)
        ‚îú‚îÄ‚îÄ create_repo (sop)
        ‚îî‚îÄ‚îÄ delete_repo (sop)
    ...
```
&lt;/details&gt;

&lt;/details&gt;





# üöÄ Start the Backend Locally

We have an `acontext-cli` to help you do quick proof-of-concept. Download it first in your terminal:

```bash
curl -fsSL https://install.acontext.io | sh
```

You should have [docker](https://www.docker.com/get-started/) installed and an OpenAI API Key to start an Acontext backend on your computer:

```bash
mkdir acontext_server &amp;&amp; cd acontext_server
acontext docker up
```

&gt; [üìñ local setup](https://docs.acontext.io/local#start-acontext-server-locally) We recommend `gpt-4.1` as the LLM model

`acontext docker up` will create/use  `.env` and `config.yaml` for Acontext, and create a `db` folder to persist data.



Once it&#039;s done, you can access the following endpoints:

- Acontext API Base URL: http://localhost:8029/api/v1
- Acontext Dashboard: http://localhost:3000/







# üßê Use Acontext to build Agent

Download end-to-end scripts with `acontext`:

**Python**

```bash
acontext create my-proj --template-path &quot;python/openai-basic&quot;
```

&gt; More examples on Python:
&gt;
&gt; - `python/openai-agent-basic`: self-learning agent in openai agent sdk.
&gt; - `python/agno-basic`: self-learning agent in agno frameworkd.
&gt; - `python/openai-agent-artifacts`: agent that can edit and download artifacts.

**Typescript**

```bash
acontext create my-proj --template-path &quot;typescript/openai-basic&quot;
```

&gt; More examples on Typescript:
&gt;
&gt; - `typescript/vercel-ai-basic`: self-learning agent in @vercel/ai-sdk



&gt; [!NOTE]
&gt;
&gt; Check our example repo for more templates: [Acontext-Examples](https://github.com/memodb-io/Acontext-Examples).
&gt;
&gt; We&#039;re cooking more full-stack Agent Applications! [Tell us what you want!](https://discord.acontext.io)



## Step-by-step Quickstart

&lt;details&gt;
&lt;summary&gt;click to open&lt;/summary&gt;


We&#039;re maintaining Python [![pypi](https://img.shields.io/pypi/v/acontext.svg)](https://pypi.org/project/acontext/) and Typescript [![npm](https://img.shields.io/npm/v/@acontext/acontext.svg?logo=npm&amp;logoColor=fff&amp;style=flat&amp;labelColor=2C2C2C&amp;color=28CF8D)](https://www.npmjs.com/package/@acontext/acontext) SDKs. The snippets below are using Python.

## Install SDKs

```
pip install acontext # for Python
npm i @acontext/acontext # for Typescript
```



## Initialize Client

```python
from acontext import AcontextClient

client = AcontextClient(
    base_url=&quot;http://localhost:8029/api/v1&quot;,
    api_key=&quot;sk-ac-your-root-api-bearer-token&quot;
)
client.ping()

# yes, the default api_key is sk-ac-your-root-api-bearer-token
```

&gt; [üìñ async client doc](https://docs.acontext.io/settings/core)



## Store

Acontext can manage agent sessions and artifacts.

### Save Messages [üìñ](https://docs.acontext.io/api-reference/session/store-message-to-session)

Acontext offers persistent storage for message data. When you call `session.store_message`, Acontext will persist the message and start to monitor this session:

&lt;details&gt;
&lt;summary&gt;Code Snippet&lt;/summary&gt;

```python
session = client.sessions.create()

messages = [
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;I need to write a landing page of iPhone 15 pro max&quot;},
    {
        &quot;role&quot;: &quot;assistant&quot;,
        &quot;content&quot;: &quot;Sure, my plan is below:\n1. Search for the latest news about iPhone 15 pro max\n2. Init Next.js project for the landing page\n3. Deploy the landing page to the website&quot;,
    }
]

# Save messages
for msg in messages:
    client.sessions.store_message(session_id=session.id, blob=msg, format=&quot;openai&quot;)
```

&gt; [üìñ](https://docs.acontext.io/store/messages/multi-modal) We also support multi-modal message storage and anthropic SDK.


&lt;/details&gt;

### Load Messages [üìñ](https://docs.acontext.io/api-reference/session/get-messages-from-session)

Obtain your session messages using `sessions.get_messages`

&lt;details&gt;
&lt;summary&gt;Code Snippet&lt;/summary&gt;

```python
r = client.sessions.get_messages(session.id)
new_msg = r.items

new_msg.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;How are you doing?&quot;})
r = openai_client.chat.completions.create(model=&quot;gpt-4.1&quot;, messages=new_msg)
print(r.choices[0].message.content)
client.sessions.store_message(session_id=session.id, blob=r.choices[0].message)
```

&lt;/details&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;picture&gt;
      &lt;img alt=&quot;Session&quot; src=&quot;./docs/images/dashboard/message_viewer.png&quot; width=&quot;100%&quot;&gt;
    &lt;/picture&gt;
  &lt;p&gt;You can view sessions in your local Dashboard&lt;/p&gt;
&lt;/div&gt;


### Artifacts [üìñ](https://docs.acontext.io/store/disk)

Create a disk for your agent to store and read artifacts using file paths:

&lt;details&gt;
&lt;summary&gt;Code Snippet&lt;/summary&gt;

```python
from acontext import FileUpload

disk = client.disks.create()

file = FileUpload(
    filename=&quot;todo.md&quot;,
    content=b&quot;# Sprint Plan\n\n## Goals\n- Complete user authentication\n- Fix critical bugs&quot;
)
artifact = client.disks.artifacts.upsert(
    disk.id,
    file=file,
    file_path=&quot;/todo/&quot;
)


print(client.disks.artifacts.list(
    disk.id,
    path=&quot;/todo/&quot;
))

result = client.disks.artifacts.get(
    disk.id,
    file_path=&quot;/todo/&quot;,
    filename=&quot;todo.md&quot;,
    with_public_url=True,
    with_content=True
)
print(f&quot;‚úì File content: {result.content.raw}&quot;)
print(f&quot;‚úì Download URL: {result.public_url}&quot;)        
```
&lt;/details&gt;



&lt;div align=&quot;center&quot;&gt;
    &lt;picture&gt;
      &lt;img alt=&quot;Artifacts&quot; src=&quot;./docs/images/dashboard/artifact_viewer.png&quot; width=&quot;100%&quot;&gt;
    &lt;/picture&gt;
  &lt;p&gt;You can view artifacts in your local Dashboard&lt;/p&gt;
&lt;/div&gt;



## Observe [üìñ](https://docs.acontext.io/observe)

For every session, Acontext will **automatically** launch a background agent to track the task progress and user feedback. **It&#039;s like a background TODO agent**. Acontext will use it to observe your daily agent success rate.

You can use the SDK to retrieve the current state of the agent session, for Context Engineering like Reduction and Compression. 

&lt;details&gt;
&lt;summary&gt;Full Script&lt;/summary&gt;

```python
from acontext import AcontextClient

# Initialize client
client = AcontextClient(
    base_url=&quot;http://localhost:8029/api/v1&quot;, api_key=&quot;sk-ac-your-root-api-bearer-token&quot;
)

# Create a project and session
session = client.sessions.create()

# Conversation messages
messages = [
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;I need to write a landing page of iPhone 15 pro max&quot;},
    {
        &quot;role&quot;: &quot;assistant&quot;,
        &quot;content&quot;: &quot;Sure, my plan is below:\n1. Search for the latest news about iPhone 15 pro max\n2. Init Next.js project for the landing page\n3. Deploy the landing page to the website&quot;,
    },
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: &quot;That sounds good. Let&#039;s first collect the message and report to me before any landing page coding.&quot;,
    },
    {
        &quot;role&quot;: &quot;assistant&quot;,
        &quot;content&quot;: &quot;Sure, I will first collect the message then report to you before any landing page coding.&quot;,
      	&quot;tool_calls&quot;: [
            {
                &quot;id&quot;: &quot;call_001&quot;,
                &quot;type&quot;: &quot;function&quot;,
                &quot;function&quot;: {
                    &quot;name&quot;: &quot;search_news&quot;,
                    &quot;arguments&quot;: &quot;{\&quot;query\&quot;: \&quot;iPhone news\&quot;}&quot;
                }
            }
        ]
    },
]

# Store messages in a loop
for msg in messages:
    client.sessions.store_message(session_id=session.id, blob=msg, format=&quot;openai&quot;)

# Wait for task extraction to complete
client.sessions.flush(session.id)

# Display extracted tasks
tasks_response = client.sessions.get_tasks(session.id)
print(tasks_response)
for task in tasks_response.items:
    print(f&quot;\nTask #{task.order}:&quot;)
    print(f&quot;  ID: {task.id}&quot;)
    print(f&quot;  Title: {task.data.task_description}&quot;)
    print(f&quot;  Status: {task.status}&quot;)

    # Show progress updates if available
    if task.data.progresses:
        print(f&quot;  Progress updates: {len(task.data.progresses)}&quot;)
        for progress in task.data.progresses:
            print(f&quot;    - {progress}&quot;)

    # Show user preferences if available
    if task.data.user_preferences:
        print(&quot;  User preferences:&quot;)
        for pref in task.data.user_preferences:
            print(f&quot;    - {pref}&quot;)

```
&gt; `flush` is a blocking call, it will wait for the task extraction to complete.
&gt; You don&#039;t need to call it in production, Acontext has a [buffer mechanism](https://docs.acontext.io/observe/buffer) to ensure the task extraction is completed right on time.

&lt;/details&gt;

Example Task Return:

```txt
Task #1:
  Title: Search for the latest news about iPhone 15 Pro Max and report findings to the user before any landing page coding.
  Status: success
  Progress updates: 2
    - I confirmed that the first step will be reporting before moving on to landing page development.
    - I have already collected all the iPhone 15 pro max info and reported to the user, waiting for approval for next step.
  User preferences:
    - user expects a report on latest news about iPhone 15 pro max before any coding work on the landing page.

Task #2:
  Title: Initialize a Next.js project for the iPhone 15 Pro Max landing page.
  Status: pending

Task #3:
  Title: Deploy the completed landing page to the website.
  Status: pending
```



You can view the session tasks&#039; statuses in the Dashboard:

&lt;div align=&quot;center&quot;&gt;
    &lt;picture&gt;
      &lt;img alt=&quot;Acontext Learning&quot; src=&quot;./docs/images/dashboard/session_task_viewer.png&quot; width=&quot;100%&quot;&gt;
    &lt;/picture&gt;
  &lt;p&gt;A Task Demo&lt;/p&gt;
&lt;/div&gt;



## Self-learning

Acontext can gather a bunch of sessions and learn skills (SOPs) on how to call tools for certain tasks.

### Learn Skills to a `Space` [üìñ](https://docs.acontext.io/learn/skill-space)

&lt;div align=&quot;center&quot;&gt;
    &lt;picture&gt;
      &lt;img alt=&quot;A Space Demo&quot; src=&quot;./assets/acontext_dataflow.png&quot; width=&quot;100%&quot;&gt;
    &lt;/picture&gt;
  &lt;p&gt;How self-learning works?&lt;/p&gt;
&lt;/div&gt;

A `Space` can store skills, and memories in a Notion-like system. You first need to connect a session to `Space` to enable the learning process:

```python
# Step 1: Create a Space for skill learning
space = client.spaces.create()
print(f&quot;Created Space: {space.id}&quot;)

# Step 2: Create a session attached to the space
session = client.sessions.create(space_id=space.id)

# ... push the agent working context
```

The learning happens in the background and is not real-time (delay around 10-30s). 

What Acontext will do in the background:

```mermaid
graph LR
    A[Task Completed] --&gt; B[Task Extraction]
    B --&gt; C{Space Connected?}
    C --&gt;|Yes| D[Queue for Learning]
    C --&gt;|No| E[Skip Learning]
    D --&gt; F[Extract SOP]
    F --&gt; G{Hard Enough?}
    G --&gt;|No - Too Simple| H[Skip Learning]
    G --&gt;|Yes - Complex| I[Store as Skill Block]
    I --&gt; J[Available for Future Sessions]
```

Eventually, SOP blocks with tool-call pattern will be saved to `Space`. You can view every `Space` in the Dashboard:

&lt;div align=&quot;center&quot;&gt;
    &lt;picture&gt;
      &lt;img alt=&quot;A Space Demo&quot; src=&quot;./docs/images/dashboard/skill_viewer.png&quot; width=&quot;100%&quot;&gt;
    &lt;/picture&gt;
  &lt;p&gt;A Space Demo&lt;/p&gt;
&lt;/div&gt;




### Search Skills from a `Space` [üìñ](https://docs.acontext.io/learn/search-skills)

To search skills from a `Space` and use them in the next session:

```python
result = client.spaces.experience_search(
    space_id=space.id,
    query=&quot;I need to implement authentication&quot;,
  	mode=&quot;fast&quot;
)
```

Acontext supports `fast` and `agentic` modes for search. The former uses embeddings to match skills. The latter uses an Experience Agent to explore the entire `Space` and tries to cover every skill needed.

The return is a list of sop blocks, which look like below:

```json
{
    &quot;use_when&quot;: &quot;star a github repo&quot;,
    &quot;preferences&quot;: &quot;use personal account. star but not fork&quot;,
    &quot;tool_sops&quot;: [
        {&quot;tool_name&quot;: &quot;goto&quot;, &quot;action&quot;: &quot;goto the user given github repo url&quot;},
        {&quot;tool_name&quot;: &quot;click&quot;, &quot;action&quot;: &quot;find login button if any, and start to login first&quot;},
        ...
    ]
}
```

&lt;/details&gt;







# üîç Document

To understand what Acontext can do better, please view [our docs](https://docs.acontext.io/)



# ‚ù§Ô∏è Stay Updated

Star Acontext on Github to support and receive instant notifications 

![click_star](./assets/star_acontext.gif)



# ü§ù Stay Together

Join the community for support and discussions:

-   [Discuss with Builders on Acontext Discord](https://discord.acontext.io) üëª 
-  [Follow Acontext on X](https://x.com/acontext_io) ùïè 



# üåü Contributing

- Check our [roadmap.md](./ROADMAP.md) first.
- Read [contributing.md](./CONTRIBUTING.md)



# üìë LICENSE

This project is currently licensed under [Apache License 2.0](LICENSE).



# ü•á Badges

![Made with Acontext](./assets/badge-made-with-acontext.svg) ![Made with Acontext (dark)](./assets/badge-made-with-acontext-dark.svg)

```md
[![Made with Acontext](https://assets.memodb.io/Acontext/badge-made-with-acontext.svg)](https://acontext.io)

[![Made with Acontext](https://assets.memodb.io/Acontext/badge-made-with-acontext-dark.svg)](https://acontext.io)
```</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[spf13/viper]]></title>
            <link>https://github.com/spf13/viper</link>
            <guid>https://github.com/spf13/viper</guid>
            <pubDate>Thu, 25 Dec 2025 00:05:26 GMT</pubDate>
            <description><![CDATA[Go configuration with fangs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/spf13/viper">spf13/viper</a></h1>
            <p>Go configuration with fangs</p>
            <p>Language: Go</p>
            <p>Stars: 29,734</p>
            <p>Forks: 2,090</p>
            <p>Stars today: 5 stars today</p>
            <h2>README</h2><pre>&gt; ## Viper v2 Feedback
&gt;
&gt; Viper is heading towards v2 and we would love to hear what _**you**_ would
&gt; like to see in it. Share your thoughts here:
&gt; https://forms.gle/R6faU74qPRPAzchZ9
&gt;
&gt; **Thank you!**

![viper logo](https://github.com/user-attachments/assets/acae9193-2974-41f3-808d-2d433f5ada5e)

[![Mentioned in Awesome Go](https://awesome.re/mentioned-badge-flat.svg)](https://github.com/avelino/awesome-go#configuration)
[![run on repl.it](https://repl.it/badge/github/sagikazarmark/Viper-example)](https://repl.it/@sagikazarmark/Viper-example#main.go)

[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/spf13/viper/ci.yaml?branch=master&amp;style=flat-square)](https://github.com/spf13/viper/actions?query=workflow%3ACI)
[![Join the chat at https://gitter.im/spf13/viper](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/spf13/viper?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge)
[![Go Report Card](https://goreportcard.com/badge/github.com/spf13/viper?style=flat-square)](https://goreportcard.com/report/github.com/spf13/viper)
![Go Version](https://img.shields.io/badge/go%20version-%3E=1.23-61CFDD.svg?style=flat-square)
[![PkgGoDev](https://pkg.go.dev/badge/mod/github.com/spf13/viper)](https://pkg.go.dev/mod/github.com/spf13/viper)

**Go configuration with fangs!**

Many Go projects are built using Viper including:

* [Hugo](http://gohugo.io)
* [EMC RexRay](http://rexray.readthedocs.org/en/stable/)
* [Imgur‚Äôs Incus](https://github.com/Imgur/incus)
* [Nanobox](https://github.com/nanobox-io/nanobox)/[Nanopack](https://github.com/nanopack)
* [Docker Notary](https://github.com/docker/Notary)
* [BloomApi](https://www.bloomapi.com/)
* [doctl](https://github.com/digitalocean/doctl)
* [Clairctl](https://github.com/jgsqware/clairctl)
* [Mercure](https://mercure.rocks)
* [Meshery](https://github.com/meshery/meshery)
* [Bearer](https://github.com/bearer/bearer)
* [Coder](https://github.com/coder/coder)
* [Vitess](https://vitess.io/)


## Install

```shell
go get github.com/spf13/viper
```

&gt; **NOTE** Viper uses [Go Modules](https://go.dev/wiki/Modules) to manage dependencies.


## Why use Viper?

Viper is a complete configuration solution for Go applications including
[12-Factor apps](https://12factor.net/#the_twelve_factors). It is designed to
work within any application, and can handle all types of configuration needs
and formats. It supports:

* setting defaults
* setting explicit values
* reading config files
* dynamic discovery of config files across multiple locations
* reading from environment variables
* reading from remote systems (e.g. Etcd or Consul)
* reading from command line flags
* reading from buffers
* live watching and updating configuration
* aliasing configuration keys for easy refactoring

Viper can be thought of as a registry for all of your applications&#039;
configuration needs.


## Putting Values in Viper

Viper can read from multiple configuration sources and merges them together
into one set of configuration keys and values.

Viper uses the following precedence for merging:

 * explicit call to `Set`
 * flags
 * environment variables
 * config files
 * external key/value stores
 * defaults

&gt; **NOTE** Viper configuration keys are case insensitive.

### Reading Config Files

Viper requires minimal configuration to load config files. Viper currently supports:

* JSON
* TOML
* YAML
* INI
* envfile
* Java Propeties

A single Viper instance only supports a single configuration file, but multiple
paths may be searched for one.

Here is an example of how to use Viper to search for and read a configuration
file. At least one path should be provided where a configuration file is
expected.

```go
// Name of the config file without an extension (Viper will intuit the type
// from an extension on the actual file)
viper.SetConfigName(&quot;config&quot;)

// Add search paths to find the file
viper.AddConfigPath(&quot;/etc/appname/&quot;)
viper.AddConfigPath(&quot;$HOME/.appname&quot;)
viper.AddConfigPath(&quot;.&quot;)

// Find and read the config file
err := viper.ReadInConfig()

// Handle errors
if err != nil {
	panic(fmt.Errorf(&quot;fatal error config file: %w&quot;, err))
}
```

You can handle the specific case where no config file is found.

```go
var fileLookupError viper.FileLookupError
if err := viper.ReadInConfig(); err != nil {
    if errors.As(err, &amp;fileLookupError) {
        // Indicates an explicitly set config file is not found (such as with
        // using `viper.SetConfigFile`) or that no config file was found in
        // any search path (such as when using `viper.AddConfigPath`)
    } else {
        // Config file was found but another error was produced
    }
}

// Config file found and successfully parsed
```

&gt; **NOTE (since 1.6)** You can also have a file without an extension and
&gt; specify the format programmatically, which is useful for files that naturally
&gt; have no extension (e.g., `.bashrc`).

### Writing Config Files

At times you may want to store all configuration modifications made during run
time.

```go
// Writes current config to the path set by `AddConfigPath` and `SetConfigName`
viper.WriteConfig()
viper.SafeWriteConfig() // Like the above, but will error if the config file exists

// Writes current config to a specific place
viper.WriteConfigAs(&quot;/path/to/my/.config&quot;)

// Will error since it has already been written
viper.SafeWriteConfigAs(&quot;/path/to/my/.config&quot;)

viper.SafeWriteConfigAs(&quot;/path/to/my/.other_config&quot;)
```

As a rule of the thumb, methods prefixed with `Safe` won&#039;t overwrite any
existing file, while other methods will.

### Watching and Re-reading Config Files

Gone are the days of needing to restart a server to have a config take
effect--Viper powered applications can read an update to a config file while
running and not miss a beat.

It&#039;s also possible to provide a function for Viper to run each time a change
occurs.

```go
// All config paths must be defined prior to calling `WatchConfig()`
viper.AddConfigPath(&quot;$HOME/.appname&quot;)

viper.OnConfigChange(func(e fsnotify.Event) {
	fmt.Println(&quot;Config file changed:&quot;, e.Name)
})

viper.WatchConfig()
```

### Reading Config from `io.Reader`

Viper predefines many configuration sources but you can also implement your own
required configuration source.

```go
viper.SetConfigType(&quot;yaml&quot;)

var yamlExample = []byte(`
hacker: true
hobbies:
- skateboarding
- snowboarding
- go
name: steve
`)

viper.ReadConfig(bytes.NewBuffer(yamlExample))

viper.Get(&quot;name&quot;) // &quot;steve&quot;
```

### Setting Defaults

A good configuration system will support default values, which are used if a
key hasn&#039;t been set in some other way.

Examples:

```go
viper.SetDefault(&quot;ContentDir&quot;, &quot;content&quot;)
viper.SetDefault(&quot;LayoutDir&quot;, &quot;layouts&quot;)
viper.SetDefault(&quot;Taxonomies&quot;, map[string]string{&quot;tag&quot;: &quot;tags&quot;, &quot;category&quot;: &quot;categories&quot;})
```

### Setting Overrides

Viper allows explict setting of configuration, such as from your own
application logic.

```go
viper.Set(&quot;verbose&quot;, true)
viper.Set(&quot;host.port&quot;, 5899) // Set an embedded key
```

### Registering and Using Aliases

Aliases permit a single value to be referenced by multiple keys

```go
viper.RegisterAlias(&quot;loud&quot;, &quot;Verbose&quot;)

viper.Set(&quot;verbose&quot;, true) // Same result as next line
viper.Set(&quot;loud&quot;, true)    // Same result as prior line

viper.GetBool(&quot;loud&quot;)    // true
viper.GetBool(&quot;verbose&quot;) // true
```

### Working with Environment Variables

Viper has full support for environment variables.

&gt; **NOTE** Unlike other configuration sources, environment variables are case
&gt; sensitive.

```go
// Tells Viper to use this prefix when reading environment variables
viper.SetEnvPrefix(&quot;spf&quot;)

// Viper will look for &quot;SPF_ID&quot;, automatically uppercasing the prefix and key
viper.BindEnv(&quot;id&quot;)

// Alternatively, we can search for any environment variable prefixed and load
// them in
viper.AutomaticEnv()

os.Setenv(&quot;SPF_ID&quot;, &quot;13&quot;)

id := viper.Get(&quot;id&quot;) // 13
```

* By default, empty environment variables are considered unset and will fall back to
  the next configuration source, unless `AllowEmptyEnv` is used.
* Viper does not &quot;cache&quot; environment variables--the value will be read each
  time it is accessed.
* `SetEnvKeyReplacer` and `EnvKeyReplacer` allow you to rewrite environment
  variable keys, which is useful to merge SCREAMING_SNAKE_CASE environment
  variables with kebab-cased configuration values from other sources.

### Working with Flags

Viper has the ability to bind to flags. Specifically, Viper supports
[pflag](https://github.com/spf13/pflag/) as used in the
[Cobra](https://github.com/spf13/cobra) library.

Like environment variables, the value is not set when the binding method is
called, but when it is accessed.

For individual flags, the `BindPFlag` method provides this functionality.

```go
serverCmd.Flags().Int(&quot;port&quot;, 1138, &quot;Port to run Application server on&quot;)

viper.BindPFlag(&quot;port&quot;, serverCmd.Flags().Lookup(&quot;port&quot;))
```

You can also bind an existing set of pflags.

```go
pflag.Int(&quot;flagname&quot;, 1234, &quot;help message for flagname&quot;)
pflag.Parse()

viper.BindPFlags(pflag.CommandLine)

i := viper.GetInt(&quot;flagname&quot;) // Retrieve values from viper instead of pflag
```

The standard library [flag](https://golang.org/pkg/flag/) package is not
directly supported, but may be parsed through pflag.

```go
package main

import (
	&quot;flag&quot;

	&quot;github.com/spf13/pflag&quot;
)

func main() {
	// Using standard library &quot;flag&quot; package
	flag.Int(&quot;flagname&quot;, 1234, &quot;help message for flagname&quot;)

    // Pass standard library flags to pflag
	pflag.CommandLine.AddGoFlagSet(flag.CommandLine)
	pflag.Parse()

    // Viper takes over
	viper.BindPFlags(pflag.CommandLine)
}
```

Use of pflag may be avoided entirely by implementing the `FlagValue` and
`FlagValueSet` interfaces.

```go
// Implementing FlagValue

type myFlag struct {}
func (f myFlag) HasChanged() bool { return false }
func (f myFlag) Name() string { return &quot;my-flag-name&quot; }
func (f myFlag) ValueString() string { return &quot;my-flag-value&quot; }
func (f myFlag) ValueType() string { return &quot;string&quot; }

viper.BindFlagValue(&quot;my-flag-name&quot;, myFlag{})

// Implementing FlagValueSet

type myFlagSet struct {
	flags []myFlag
}
func (f myFlagSet) VisitAll(fn func(FlagValue)) {
	for _, flag := range flags {
		fn(flag)
	}
}

fSet := myFlagSet{
	flags: []myFlag{myFlag{}, myFlag{}},
}
viper.BindFlagValues(&quot;my-flags&quot;, fSet)
```

### Remote Key/Value Store Support

To enable remote support in Viper, do a blank import of the `viper/remote`
package.

```go
import _ &quot;github.com/spf13/viper/remote&quot;
```

Viper supports the following remote key/value stores. Examples for each are
provided below.

* Etcd and Etcd3
* Consul
* Firestore
* NATS

Viper will read a config string retrieved from a path in a key/value store.

Viper supports multiple hosts separated by `;`. For example:
`http://127.0.0.1:4001;http://127.0.0.1:4002`.

#### Encryption

Viper uses [crypt](https://github.com/sagikazarmark/crypt) to retrieve
configuration from the key/value store, which means that you can store your
configuration values encrypted and have them automatically decrypted if you
have the correct GPG keyring. Encryption is optional.

Crypt has a command-line helper that you can use to put configurations in your
key/value store.

```bash
$ go get github.com/sagikazarmark/crypt/bin/crypt
$ crypt set -plaintext /config/hugo.json /Users/hugo/settings/config.json
$ crypt get -plaintext /config/hugo.json
```

See the Crypt documentation for examples of how to set encrypted values, or
how to use Consul.

### Remote Key/Value Store Examples (Unencrypted)

#### etcd

```go
viper.AddRemoteProvider(&quot;etcd&quot;, &quot;http://127.0.0.1:4001&quot;,&quot;/config/hugo.json&quot;)
viper.SetConfigType(&quot;json&quot;) // because there is no file extension in a stream of bytes, supported extensions are &quot;json&quot;, &quot;toml&quot;, &quot;yaml&quot;, &quot;yml&quot;, &quot;properties&quot;, &quot;props&quot;, &quot;prop&quot;, &quot;env&quot;, &quot;dotenv&quot;
err := viper.ReadRemoteConfig()
```

#### etcd3

```go
viper.AddRemoteProvider(&quot;etcd3&quot;, &quot;http://127.0.0.1:4001&quot;,&quot;/config/hugo.json&quot;)
viper.SetConfigType(&quot;json&quot;) // because there is no file extension in a stream of bytes, supported extensions are &quot;json&quot;, &quot;toml&quot;, &quot;yaml&quot;, &quot;yml&quot;, &quot;properties&quot;, &quot;props&quot;, &quot;prop&quot;, &quot;env&quot;, &quot;dotenv&quot;
err := viper.ReadRemoteConfig()
```

#### Consul

Given a Consul key `MY_CONSUL_KEY` with the value:

```json
{
    &quot;port&quot;: 8080,
    &quot;hostname&quot;: &quot;myhostname.com&quot;
}
```

```go
viper.AddRemoteProvider(&quot;consul&quot;, &quot;localhost:8500&quot;, &quot;MY_CONSUL_KEY&quot;)
viper.SetConfigType(&quot;json&quot;) // Need to explicitly set this to json
err := viper.ReadRemoteConfig()

fmt.Println(viper.Get(&quot;port&quot;)) // 8080
```

#### Firestore

```go
viper.AddRemoteProvider(&quot;firestore&quot;, &quot;google-cloud-project-id&quot;, &quot;collection/document&quot;)
viper.SetConfigType(&quot;json&quot;) // Config&#039;s format: &quot;json&quot;, &quot;toml&quot;, &quot;yaml&quot;, &quot;yml&quot;
err := viper.ReadRemoteConfig()
```

Of course, you&#039;re allowed to use `SecureRemoteProvider` also.

#### NATS

```go
viper.AddRemoteProvider(&quot;nats&quot;, &quot;nats://127.0.0.1:4222&quot;, &quot;myapp.config&quot;)
viper.SetConfigType(&quot;json&quot;)
err := viper.ReadRemoteConfig()
```

### Remote Key/Value Store Examples (Encrypted)

```go
viper.AddSecureRemoteProvider(&quot;etcd&quot;,&quot;http://127.0.0.1:4001&quot;,&quot;/config/hugo.json&quot;,&quot;/etc/secrets/mykeyring.gpg&quot;)
viper.SetConfigType(&quot;json&quot;) // because there is no file extension in a stream of bytes,  supported extensions are &quot;json&quot;, &quot;toml&quot;, &quot;yaml&quot;, &quot;yml&quot;, &quot;properties&quot;, &quot;props&quot;, &quot;prop&quot;, &quot;env&quot;, &quot;dotenv&quot;
err := viper.ReadRemoteConfig()
```

### Watching Key/Value Store Changes

```go
// Alternatively, you can create a new viper instance
var runtime_viper = viper.New()

runtime_viper.AddRemoteProvider(&quot;etcd&quot;, &quot;http://127.0.0.1:4001&quot;, &quot;/config/hugo.yml&quot;)
runtime_viper.SetConfigType(&quot;yaml&quot;) // because there is no file extension in a stream of bytes, supported extensions are &quot;json&quot;, &quot;toml&quot;, &quot;yaml&quot;, &quot;yml&quot;, &quot;properties&quot;, &quot;props&quot;, &quot;prop&quot;, &quot;env&quot;, &quot;dotenv&quot;

// Read from remote config the first time
err := runtime_viper.ReadRemoteConfig()

// Unmarshal config
runtime_viper.Unmarshal(&amp;runtime_conf)

// Open a goroutine to watch remote changes forever
go func(){
	for {
		time.Sleep(time.Second * 5) // delay after each request

		// Currently, only tested with Etcd support
		err := runtime_viper.WatchRemoteConfig()
		if err != nil {
			log.Errorf(&quot;unable to read remote config: %v&quot;, err)
			continue
		}

		// Unmarshal new config into our runtime config struct
		runtime_viper.Unmarshal(&amp;runtime_conf)
	}
}()
```

## Getting Values From Viper

The simplest way to retrieve configuration values from Viper is to use `Get*`
functions. `Get` will return an any type, but specific types may be retrieved
with `Get&lt;Type&gt;` functions.

Note that each `Get*` function will return a zero value if it‚Äôs key is not
found. To check if a key exists, use the `IsSet` method.

Nested keys use `.` as a delimiter and numbers for array indexes. Given the
following configuration:

```jsonc
{
    &quot;datastore&quot;: {
        &quot;metric&quot;: {
            &quot;host&quot;: &quot;127.0.0.1&quot;,
            &quot;ports&quot;: [
                5799,
                6029
            ]
        }
    }
}

```

```go
GetString(&quot;datastore.metric.host&quot;) // &quot;127.0.0.1&quot;
GetInt(&quot;host.ports.1&quot;) // 6029
```

&gt; **NOTE** Viper _does not_ deep merge configuration values. Complex values
&gt; that are overridden will be entirely replaced.

If there exists a key that matches the delimited key path, its value will be
returned instead.

```jsonc
{
    &quot;datastore.metric.host&quot;: &quot;0.0.0.0&quot;,
    &quot;datastore&quot;: {
        &quot;metric&quot;: {
            &quot;host&quot;: &quot;127.0.0.1&quot;
        }
    }
}
```

```go
GetString(&quot;datastore.metric.host&quot;) // &quot;0.0.0.0&quot;
```

### Configuration Subsets

It&#039;s often useful to extract a subset of configuration (e.g., when developing a
reusable module which should accept specific sections of configuration).

```yaml
cache:
  cache1:
    item-size: 64
    max-items: 100
  cache2:
    item-size: 80
    max-items: 200
```

```go
func NewCache(v *Viper) *Cache {
	return &amp;Cache{
		ItemSize: v.GetInt(&quot;item-size&quot;),
		MaxItems: v.GetInt(&quot;max-items&quot;),
	}
}

cache1Config := viper.Sub(&quot;cache.cache1&quot;)

if cache1Config == nil {
    // Sub returns nil if the key cannot be found
	panic(&quot;cache configuration not found&quot;)
}

cache1 := NewCache(cache1Config)
```

### Unmarshaling

You also have the option of unmarshaling configuration to a struct, map, etc.,
using `Unmarshal*` methods.

```go
type config struct {
	Port int
	Name string
	PathMap string `mapstructure:&quot;path_map&quot;`
}

var C config

err := viper.Unmarshal(&amp;C)
if err != nil {
	t.Fatalf(&quot;unable to decode into struct, %v&quot;, err)
}
```

If you want to unmarshal configuration where the keys themselves contain `.`
(the default key delimiter), you can change the delimiter.

```go
v := viper.NewWithOptions(viper.KeyDelimiter(&quot;::&quot;))

v.SetDefault(&quot;chart::values&quot;, map[string]any{
	&quot;ingress&quot;: map[string]any{
		&quot;annotations&quot;: map[string]any{
			&quot;traefik.frontend.rule.type&quot;:                 &quot;PathPrefix&quot;,
			&quot;traefik.ingress.kubernetes.io/ssl-redirect&quot;: &quot;true&quot;,
		},
	},
})

type config struct {
	Chart struct{
		Values map[string]any
	}
}

var C config

v.Unmarshal(&amp;C)
```

Viper also supports unmarshaling into embedded structs.

```go
/*
Example config:

module:
    enabled: true
    token: 89h3f98hbwf987h3f98wenf89ehf
*/
type config struct {
	Module struct {
		Enabled bool

		moduleConfig `mapstructure:&quot;,squash&quot;`
	}
}

type moduleConfig struct {
	Token string
}

var C config

err := viper.Unmarshal(&amp;C)
if err != nil {
	t.Fatalf(&quot;unable to decode into struct, %v&quot;, err)
}
```

Viper uses
[github.com/go-viper/mapstructure](https://github.com/go-viper/mapstructure)
under the hood for unmarshaling values which uses `mapstructure` tags, by
default.

### Marshalling to String

You may need to marshal all the settings held in Viper into a string. You can
use your favorite format&#039;s marshaller with the config returned by
`AllSettings`.

```go
import (
	yaml &quot;go.yaml.in/yaml/v3&quot;
)

func yamlStringSettings() string {
	c := viper.AllSettings()
	bs, err := yaml.Marshal(c)
	if err != nil {
		log.Fatalf(&quot;unable to marshal config to YAML: %v&quot;, err)
	}
	return string(bs)
}
```

### Decoding Custom Formats

A frequently requested feature is adding more value formats and decoders (for
example; parsing character delimited strings into slices. This is already
available in Viper using mapstructure decode hooks.

Read more in [this blog
post](https://sagikazarmark.hu/blog/decoding-custom-formats-with-viper/).


## FAQ

### Why is it called ‚ÄúViper‚Äù?

Viper is designed to be a
[companion](http://en.wikipedia.org/wiki/Viper_(G.I._Joe)) to
[Cobra](https://github.com/spf13/cobra). While both can operate completely
independently, together they make a powerful pair to handle much of your
application foundation needs.

### I found a bug or want a feature, should I file an issue or a PR?

Yes, but there are two things to be aware of.

1.  The Viper project is currently prioritizing backwards compatibility and
    stability over features.
2.  Features may be deferred until Viper 2 forms.

### Can multiple Viper instances be used?

**tl;dr:** Yes.

Each will have its own unique configuration and can read from a different
configuration source. All of the functions that the Viper package supports are
mirrored as methods on a Viper instance.

```go
x := viper.New()
y := viper.New()

x.SetDefault(&quot;ContentDir&quot;, &quot;content&quot;)
y.SetDefault(&quot;ContentDir&quot;, &quot;foobar&quot;)
```

### Should Viper be a global singleton or passed around?

The best practice is to initialize a Viper instance and pass that around when
necessary.

Viper comes with a global instance (singleton) out of the box. Although it
makes setting up configuration easy, using it is generally discouraged as it
makes testing harder and can lead to unexpected behavior.

The global instance may be deprecated in the future. See
[#1855](https://github.com/spf13/viper/issues/1855) for more details.

### Does Viper support case sensitive keys?

**tl;dr:** No.

Viper merges configuration from various sources, many of which are either case
insensitive or use different casing than other sourc

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[grpc/grpc-go]]></title>
            <link>https://github.com/grpc/grpc-go</link>
            <guid>https://github.com/grpc/grpc-go</guid>
            <pubDate>Thu, 25 Dec 2025 00:05:25 GMT</pubDate>
            <description><![CDATA[The Go language implementation of gRPC. HTTP/2 based RPC]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/grpc/grpc-go">grpc/grpc-go</a></h1>
            <p>The Go language implementation of gRPC. HTTP/2 based RPC</p>
            <p>Language: Go</p>
            <p>Stars: 22,677</p>
            <p>Forks: 4,628</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre># gRPC-Go

[![GoDoc](https://pkg.go.dev/badge/google.golang.org/grpc)][API]
[![GoReportCard](https://goreportcard.com/badge/grpc/grpc-go)](https://goreportcard.com/report/github.com/grpc/grpc-go)
[![codecov](https://codecov.io/gh/grpc/grpc-go/graph/badge.svg)](https://codecov.io/gh/grpc/grpc-go)

The [Go][] implementation of [gRPC][]: A high performance, open source, general
RPC framework that puts mobile and HTTP/2 first. For more information see the
[Go gRPC docs][], or jump directly into the [quick start][].

## Prerequisites

- **[Go][]**: any one of the **two latest major** [releases][go-releases].

## Installation

Simply add the following import to your code, and then `go [build|run|test]`
will automatically fetch the necessary dependencies:


```go
import &quot;google.golang.org/grpc&quot;
```

&gt; **Note:** If you are trying to access `grpc-go` from **China**, see the
&gt; [FAQ](#FAQ) below.

## Learn more

- [Go gRPC docs][], which include a [quick start][] and [API
  reference][API] among other resources
- [Low-level technical docs](Documentation) from this repository
- [Performance benchmark][]
- [Examples](examples)
- [Contribution guidelines](CONTRIBUTING.md)

## FAQ

### I/O Timeout Errors

The `golang.org` domain may be blocked from some countries. `go get` usually
produces an error like the following when this happens:

```console
$ go get -u google.golang.org/grpc
package google.golang.org/grpc: unrecognized import path &quot;google.golang.org/grpc&quot; (https fetch: Get https://google.golang.org/grpc?go-get=1: dial tcp 216.239.37.1:443: i/o timeout)
```

To build Go code, there are several options:

- Set up a VPN and access google.golang.org through that.

- With Go module support: it is possible to use the `replace` feature of `go
  mod` to create aliases for golang.org packages.  In your project&#039;s directory:

  ```sh
  go mod edit -replace=google.golang.org/grpc=github.com/grpc/grpc-go@latest
  go mod tidy
  go mod vendor
  go build -mod=vendor
  ```

  Again, this will need to be done for all transitive dependencies hosted on
  golang.org as well. For details, refer to [golang/go issue
  #28652](https://github.com/golang/go/issues/28652).

### Compiling error, undefined: grpc.SupportPackageIsVersion

Please update to the latest version of gRPC-Go using
`go get google.golang.org/grpc`.

### How to turn on logging

The default logger is controlled by environment variables. Turn everything on
like this:

```console
$ export GRPC_GO_LOG_VERBOSITY_LEVEL=99
$ export GRPC_GO_LOG_SEVERITY_LEVEL=info
```

### The RPC failed with error `&quot;code = Unavailable desc = transport is closing&quot;`

This error means the connection the RPC is using was closed, and there are many
possible reasons, including:
 1. mis-configured transport credentials, connection failed on handshaking
 1. bytes disrupted, possibly by a proxy in between
 1. server shutdown
 1. Keepalive parameters caused connection shutdown, for example if you have
    configured your server to terminate connections regularly to [trigger DNS
    lookups](https://github.com/grpc/grpc-go/issues/3170#issuecomment-552517779).
    If this is the case, you may want to increase your
    [MaxConnectionAgeGrace](https://pkg.go.dev/google.golang.org/grpc/keepalive?tab=doc#ServerParameters),
    to allow longer RPC calls to finish.

It can be tricky to debug this because the error happens on the client side but
the root cause of the connection being closed is on the server side. Turn on
logging on __both client and server__, and see if there are any transport
errors.

[API]: https://pkg.go.dev/google.golang.org/grpc
[Go]: https://golang.org
[Go module]: https://github.com/golang/go/wiki/Modules
[gRPC]: https://grpc.io
[Go gRPC docs]: https://grpc.io/docs/languages/go
[Performance benchmark]: https://performance-dot-grpc-testing.appspot.com/explore?dashboard=5180705743044608
[quick start]: https://grpc.io/docs/languages/go/quickstart
[go-releases]: https://golang.org/doc/devel/release.html
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[go-rod/rod]]></title>
            <link>https://github.com/go-rod/rod</link>
            <guid>https://github.com/go-rod/rod</guid>
            <pubDate>Thu, 25 Dec 2025 00:05:24 GMT</pubDate>
            <description><![CDATA[A Chrome DevTools Protocol driver for web automation and scraping.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/go-rod/rod">go-rod/rod</a></h1>
            <p>A Chrome DevTools Protocol driver for web automation and scraping.</p>
            <p>Language: Go</p>
            <p>Stars: 6,505</p>
            <p>Forks: 432</p>
            <p>Stars today: 8 stars today</p>
            <h2>README</h2><pre># Overview

[![Go Reference](https://pkg.go.dev/badge/github.com/go-rod/rod.svg)](https://pkg.go.dev/github.com/go-rod/rod)
[![Discord Chat](https://img.shields.io/discord/719933559456006165.svg)][discord room]

## [Documentation](https://go-rod.github.io/) | [API reference](https://pkg.go.dev/github.com/go-rod/rod?tab=doc) | [FAQ](https://go-rod.github.io/#/faq/README)

Rod is a high-level driver directly based on [DevTools Protocol](https://chromedevtools.github.io/devtools-protocol).
It&#039;s designed for web automation and scraping for both high-level and low-level use, senior developers can use the low-level packages and functions to easily
customize or build up their own version of Rod, the high-level functions are just examples to build a default version of Rod.

[‰∏≠Êñá API ÊñáÊ°£](https://pkg.go.dev/github.com/go-rod/go-rod-chinese)

## Features

- Chained context design, intuitive to timeout or cancel the long-running task
- Auto-wait elements to be ready
- Debugging friendly, auto input tracing, remote monitoring headless browser
- Thread-safe for all operations
- Automatically find or download [browser](lib/launcher)
- High-level helpers like WaitStable, WaitRequestIdle, HijackRequests, WaitDownload, etc
- Two-step WaitEvent design, never miss an event ([how it works](https://github.com/ysmood/goob))
- Correctly handles nested iframes or shadow DOMs
- No zombie browser process after the crash ([how it works](https://github.com/ysmood/leakless))
- [CI](https://github.com/go-rod/rod/actions) enforced 100% test coverage

## Examples

Please check the [examples_test.go](examples_test.go) file first, then check the [examples](lib/examples) folder.

For more detailed examples, please search the unit tests.
Such as the usage of method `HandleAuth`, you can search all the `*_test.go` files that contain `HandleAuth`,
for example, use GitHub online [search in repository](https://github.com/go-rod/rod/search?q=HandleAuth&amp;unscoped_q=HandleAuth).
You can also search the GitHub [issues](https://github.com/go-rod/rod/issues) or [discussions](https://github.com/go-rod/rod/discussions),
a lot of usage examples are recorded there.

[Here is a comparison](lib/examples/compare-chromedp) of the examples between rod and Chromedp.

If you have questions, please raise an [issues](https://github.com/go-rod/rod/issues)/[discussions](https://github.com/go-rod/rod/discussions) or join the [chat room][discord room].

## Sponsors

Rod is sponsored by many organizations and individuals, thank you for your support!

Please contact [yad@ysmood.org](mailto:yad@ysmood.org) if you want to be listed here.

&lt;!-- markdownlint-disable MD033 --&gt;

&lt;table style=&quot;border-collapse: collapse&quot;&gt;
  &lt;tr&gt;
    &lt;td&gt;
      &lt;p&gt;Browser testing via&lt;/p&gt;
      &lt;a href=&quot;https://www.lambdatest.com/&quot; target=&quot;_blank&quot;&gt;
        &lt;img
          src=&quot;https://www.lambdatest.com/blue-logo.png&quot;
          alt=&quot;LambdaTest Logo&quot;
          width=&quot;250&quot;
          height=&quot;45&quot;
        /&gt;
      &lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;!-- markdownlint-enable MD033 --&gt;

## Join us

Your help is more than welcome! Even just open an issue to ask a question may greatly help others.

Please read [How To Ask Questions The Smart Way](http://www.catb.org/~esr/faqs/smart-questions.html) before you ask questions.

If you want to contribute please read the [Contributor Guide](.github/CONTRIBUTING.md).

[discord room]: https://discord.gg/CpevuvY
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[hashicorp/consul]]></title>
            <link>https://github.com/hashicorp/consul</link>
            <guid>https://github.com/hashicorp/consul</guid>
            <pubDate>Thu, 25 Dec 2025 00:05:23 GMT</pubDate>
            <description><![CDATA[Consul is a distributed, highly available, and data center aware solution to connect and configure applications across dynamic, distributed infrastructure.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hashicorp/consul">hashicorp/consul</a></h1>
            <p>Consul is a distributed, highly available, and data center aware solution to connect and configure applications across dynamic, distributed infrastructure.</p>
            <p>Language: Go</p>
            <p>Stars: 29,616</p>
            <p>Forks: 4,553</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre>&lt;h1&gt;
  &lt;img src=&quot;./website/public/img/logo.svg&quot; align=&quot;left&quot; height=&quot;46px&quot; alt=&quot;Consul logo&quot;/&gt;
  &lt;span&gt;Consul&lt;/span&gt;
&lt;/h1&gt;

[![License: BUSL-1.1](https://img.shields.io/badge/License-BUSL--1.1-yellow.svg)](LICENSE)
[![Docker Pulls](https://img.shields.io/docker/pulls/hashicorp/consul.svg)](https://hub.docker.com/r/hashicorp/consul)
[![Go Report Card](https://goreportcard.com/badge/github.com/hashicorp/consul)](https://goreportcard.com/report/github.com/hashicorp/consul)

Consul is a distributed, highly available, and data center aware solution to connect and configure applications across dynamic, distributed infrastructure.

* Documentation and Tutorials: [https://developer.hashicorp.com/consul]
* Forum: [Discuss](https://discuss.hashicorp.com/c/consul)

Consul provides several key features:

* **Multi-Datacenter** - Consul is built to be datacenter aware, and can
  support any number of regions without complex configuration.

* **Service Mesh** - Consul Service Mesh enables secure service-to-service
  communication with automatic TLS encryption and identity-based authorization. Applications
  can use sidecar proxies in a service mesh configuration to establish TLS
  connections for inbound and outbound connections with Transparent Proxy.

* **API Gateway** - Consul API Gateway manages access to services within Consul Service Mesh, 
  allow users to define traffic and authorization policies to services deployed within the mesh.  

* **Service Discovery** - Consul makes it simple for services to register
  themselves and to discover other services via a DNS or HTTP interface.
  External services such as SaaS providers can be registered as well.

* **Health Checking** - Health Checking enables Consul to quickly alert
  operators about any issues in a cluster. The integration with service
  discovery prevents routing traffic to unhealthy hosts and enables service
  level circuit breakers.

* **Dynamic App Configuration** - An HTTP API that allows users to store indexed objects within Consul,
  for storing configuration parameters and application metadata.

Consul runs on Linux, macOS, FreeBSD, Solaris, and Windows and includes an
optional [browser based UI](https://demo.consul.io). A commercial version
called [Consul Enterprise](https://developer.hashicorp.com/consul/docs/enterprise) is also
available.

**Please note**: We take Consul&#039;s security and our users&#039; trust very seriously. If you
believe you have found a security issue in Consul, please [responsibly disclose](https://www.hashicorp.com/security#vulnerability-reporting)
by contacting us at security@hashicorp.com.

## Quick Start

A few quick start guides are available on the Consul website:

* **Standalone binary install:** https://learn.hashicorp.com/collections/consul/get-started-vms
* **Minikube install:** https://learn.hashicorp.com/tutorials/consul/kubernetes-minikube
* **Kind install:** https://learn.hashicorp.com/tutorials/consul/kubernetes-kind
* **Kubernetes install:** https://learn.hashicorp.com/tutorials/consul/kubernetes-deployment-guide
* **Deploy HCP Consul:** https://learn.hashicorp.com/tutorials/consul/hcp-gs-deploy 

## Documentation

Full, comprehensive documentation is available on the Consul website: https://developer.hashicorp.com/consul/docs

## Contributing

Thank you for your interest in contributing! Please refer to [CONTRIBUTING.md](https://github.com/hashicorp/consul/blob/main/.github/CONTRIBUTING.md)
for guidance. For contributions specifically to the browser based UI, please
refer to the UI&#039;s [README.md](https://github.com/hashicorp/consul/blob/main/ui/packages/consul-ui/README.md)
for guidance.
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[ollama/ollama]]></title>
            <link>https://github.com/ollama/ollama</link>
            <guid>https://github.com/ollama/ollama</guid>
            <pubDate>Thu, 25 Dec 2025 00:05:22 GMT</pubDate>
            <description><![CDATA[Get up and running with OpenAI gpt-oss, DeepSeek-R1, Gemma 3 and other models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ollama/ollama">ollama/ollama</a></h1>
            <p>Get up and running with OpenAI gpt-oss, DeepSeek-R1, Gemma 3 and other models.</p>
            <p>Language: Go</p>
            <p>Stars: 158,186</p>
            <p>Forks: 14,001</p>
            <p>Stars today: 57 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
¬† &lt;a href=&quot;https://ollama.com&quot;&gt;
    &lt;img alt=&quot;ollama&quot; width=&quot;240&quot; src=&quot;https://github.com/ollama/ollama/assets/3325447/0d0b44e2-8f4a-4e99-9b52-a5c1c741c8f7&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

# Ollama

Get up and running with large language models.

### macOS

[Download](https://ollama.com/download/Ollama.dmg)

### Windows

[Download](https://ollama.com/download/OllamaSetup.exe)

### Linux

```shell
curl -fsSL https://ollama.com/install.sh | sh
```

[Manual install instructions](https://docs.ollama.com/linux#manual-install)

### Docker

The official [Ollama Docker image](https://hub.docker.com/r/ollama/ollama) `ollama/ollama` is available on Docker Hub.

### Libraries

- [ollama-python](https://github.com/ollama/ollama-python)
- [ollama-js](https://github.com/ollama/ollama-js)

### Community

- [Discord](https://discord.gg/ollama)
- [Reddit](https://reddit.com/r/ollama)

## Quickstart

To run and chat with [Gemma 3](https://ollama.com/library/gemma3):

```shell
ollama run gemma3
```

## Model library

Ollama supports a list of models available on [ollama.com/library](https://ollama.com/library &#039;ollama model library&#039;)

Here are some example models that can be downloaded:

| Model              | Parameters | Size  | Download                         |
| ------------------ | ---------- | ----- | -------------------------------- |
| Gemma 3            | 1B         | 815MB | `ollama run gemma3:1b`           |
| Gemma 3            | 4B         | 3.3GB | `ollama run gemma3`              |
| Gemma 3            | 12B        | 8.1GB | `ollama run gemma3:12b`          |
| Gemma 3            | 27B        | 17GB  | `ollama run gemma3:27b`          |
| QwQ                | 32B        | 20GB  | `ollama run qwq`                 |
| DeepSeek-R1        | 7B         | 4.7GB | `ollama run deepseek-r1`         |
| DeepSeek-R1        | 671B       | 404GB | `ollama run deepseek-r1:671b`    |
| Llama 4            | 109B       | 67GB  | `ollama run llama4:scout`        |
| Llama 4            | 400B       | 245GB | `ollama run llama4:maverick`     |
| Llama 3.3          | 70B        | 43GB  | `ollama run llama3.3`            |
| Llama 3.2          | 3B         | 2.0GB | `ollama run llama3.2`            |
| Llama 3.2          | 1B         | 1.3GB | `ollama run llama3.2:1b`         |
| Llama 3.2 Vision   | 11B        | 7.9GB | `ollama run llama3.2-vision`     |
| Llama 3.2 Vision   | 90B        | 55GB  | `ollama run llama3.2-vision:90b` |
| Llama 3.1          | 8B         | 4.7GB | `ollama run llama3.1`            |
| Llama 3.1          | 405B       | 231GB | `ollama run llama3.1:405b`       |
| Phi 4              | 14B        | 9.1GB | `ollama run phi4`                |
| Phi 4 Mini         | 3.8B       | 2.5GB | `ollama run phi4-mini`           |
| Mistral            | 7B         | 4.1GB | `ollama run mistral`             |
| Moondream 2        | 1.4B       | 829MB | `ollama run moondream`           |
| Neural Chat        | 7B         | 4.1GB | `ollama run neural-chat`         |
| Starling           | 7B         | 4.1GB | `ollama run starling-lm`         |
| Code Llama         | 7B         | 3.8GB | `ollama run codellama`           |
| Llama 2 Uncensored | 7B         | 3.8GB | `ollama run llama2-uncensored`   |
| LLaVA              | 7B         | 4.5GB | `ollama run llava`               |
| Granite-3.3         | 8B         | 4.9GB | `ollama run granite3.3`          |

&gt; [!NOTE]
&gt; You should have at least 8 GB of RAM available to run the 7B models, 16 GB to run the 13B models, and 32 GB to run the 33B models.

## Customize a model

### Import from GGUF

Ollama supports importing GGUF models in the Modelfile:

1. Create a file named `Modelfile`, with a `FROM` instruction with the local filepath to the model you want to import.

   ```
   FROM ./vicuna-33b.Q4_0.gguf
   ```

2. Create the model in Ollama

   ```shell
   ollama create example -f Modelfile
   ```

3. Run the model

   ```shell
   ollama run example
   ```

### Import from Safetensors

See the [guide](https://docs.ollama.com/import) on importing models for more information.

### Customize a prompt

Models from the Ollama library can be customized with a prompt. For example, to customize the `llama3.2` model:

```shell
ollama pull llama3.2
```

Create a `Modelfile`:

```
FROM llama3.2

# set the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 1

# set the system message
SYSTEM &quot;&quot;&quot;
You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.
&quot;&quot;&quot;
```

Next, create and run the model:

```
ollama create mario -f ./Modelfile
ollama run mario
&gt;&gt;&gt; hi
Hello! It&#039;s your friend Mario.
```

For more information on working with a Modelfile, see the [Modelfile](https://docs.ollama.com/modelfile) documentation.

## CLI Reference

### Create a model

`ollama create` is used to create a model from a Modelfile.

```shell
ollama create mymodel -f ./Modelfile
```

### Pull a model

```shell
ollama pull llama3.2
```

&gt; This command can also be used to update a local model. Only the diff will be pulled.

### Remove a model

```shell
ollama rm llama3.2
```

### Copy a model

```shell
ollama cp llama3.2 my-model
```

### Multiline input

For multiline input, you can wrap text with `&quot;&quot;&quot;`:

```
&gt;&gt;&gt; &quot;&quot;&quot;Hello,
... world!
... &quot;&quot;&quot;
I&#039;m a basic program that prints the famous &quot;Hello, world!&quot; message to the console.
```

### Multimodal models

```
ollama run llava &quot;What&#039;s in this image? /Users/jmorgan/Desktop/smile.png&quot;
```

&gt; **Output**: The image features a yellow smiley face, which is likely the central focus of the picture.

### Pass the prompt as an argument

```shell
ollama run llama3.2 &quot;Summarize this file: $(cat README.md)&quot;
```

&gt; **Output**: Ollama is a lightweight, extensible framework for building and running language models on the local machine. It provides a simple API for creating, running, and managing models, as well as a library of pre-built models that can be easily used in a variety of applications.

### Show model information

```shell
ollama show llama3.2
```

### List models on your computer

```shell
ollama list
```

### List which models are currently loaded

```shell
ollama ps
```

### Stop a model which is currently running

```shell
ollama stop llama3.2
```

### Generate embeddings from the CLI

```shell
ollama run embeddinggemma &quot;Your text to embed&quot;
```

You can also pipe text for scripted workflows:

```shell
echo &quot;Your text to embed&quot; | ollama run embeddinggemma
```

### Start Ollama

`ollama serve` is used when you want to start ollama without running the desktop application.

## Building

See the [developer guide](https://github.com/ollama/ollama/blob/main/docs/development.md)

### Running local builds

Next, start the server:

```shell
./ollama serve
```

Finally, in a separate shell, run a model:

```shell
./ollama run llama3.2
```

## REST API

Ollama has a REST API for running and managing models.

### Generate a response

```shell
curl http://localhost:11434/api/generate -d &#039;{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;prompt&quot;:&quot;Why is the sky blue?&quot;
}&#039;
```

### Chat with a model

```shell
curl http://localhost:11434/api/chat -d &#039;{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;messages&quot;: [
    { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;why is the sky blue?&quot; }
  ]
}&#039;
```

See the [API documentation](./docs/api.md) for all endpoints.

## Community Integrations

### Web &amp; Desktop

- [Open WebUI](https://github.com/open-webui/open-webui)
- [SwiftChat (macOS with ReactNative)](https://github.com/aws-samples/swift-chat)
- [Enchanted (macOS native)](https://github.com/AugustDev/enchanted)
- [Hollama](https://github.com/fmaclen/hollama)
- [Lollms WebUI (Single user)](https://github.com/ParisNeo/lollms-webui)
- [Lollms (Multi users)](https://github.com/ParisNeo/lollms)
- [LibreChat](https://github.com/danny-avila/LibreChat)
- [Bionic GPT](https://github.com/bionic-gpt/bionic-gpt)
- [HTML UI](https://github.com/rtcfirefly/ollama-ui)
- [AI-UI](https://github.com/bajahaw/ai-ui)
- [Saddle](https://github.com/jikkuatwork/saddle)
- [TagSpaces](https://www.tagspaces.org) (A platform for file-based apps, [utilizing Ollama](https://docs.tagspaces.org/ai/) for the generation of tags and descriptions)
- [Chatbot UI](https://github.com/ivanfioravanti/chatbot-ollama)
- [Chatbot UI v2](https://github.com/mckaywrigley/chatbot-ui)
- [Typescript UI](https://github.com/ollama-interface/Ollama-Gui?tab=readme-ov-file)
- [Minimalistic React UI for Ollama Models](https://github.com/richawo/minimal-llm-ui)
- [Ollamac](https://github.com/kevinhermawan/Ollamac)
- [big-AGI](https://github.com/enricoros/big-AGI)
- [Cheshire Cat assistant framework](https://github.com/cheshire-cat-ai/core)
- [Amica](https://github.com/semperai/amica)
- [chatd](https://github.com/BruceMacD/chatd)
- [Ollama-SwiftUI](https://github.com/kghandour/Ollama-SwiftUI)
- [Dify.AI](https://github.com/langgenius/dify)
- [MindMac](https://mindmac.app)
- [NextJS Web Interface for Ollama](https://github.com/jakobhoeg/nextjs-ollama-llm-ui)
- [Msty](https://msty.app)
- [Chatbox](https://github.com/Bin-Huang/Chatbox)
- [WinForm Ollama Copilot](https://github.com/tgraupmann/WinForm_Ollama_Copilot)
- [NextChat](https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web) with [Get Started Doc](https://docs.nextchat.dev/models/ollama)
- [Alpaca WebUI](https://github.com/mmo80/alpaca-webui)
- [OllamaGUI](https://github.com/enoch1118/ollamaGUI)
- [OpenAOE](https://github.com/InternLM/OpenAOE)
- [Odin Runes](https://github.com/leonid20000/OdinRunes)
- [LLM-X](https://github.com/mrdjohnson/llm-x) (Progressive Web App)
- [AnythingLLM (Docker + MacOs/Windows/Linux native app)](https://github.com/Mintplex-Labs/anything-llm)
- [Ollama Basic Chat: Uses HyperDiv Reactive UI](https://github.com/rapidarchitect/ollama_basic_chat)
- [Ollama-chats RPG](https://github.com/drazdra/ollama-chats)
- [IntelliBar](https://intellibar.app/) (AI-powered assistant for macOS)
- [Jirapt](https://github.com/AliAhmedNada/jirapt) (Jira Integration to generate issues, tasks, epics)
- [ojira](https://github.com/AliAhmedNada/ojira) (Jira chrome plugin to easily generate descriptions for tasks)
- [QA-Pilot](https://github.com/reid41/QA-Pilot) (Interactive chat tool that can leverage Ollama models for rapid understanding and navigation of GitHub code repositories)
- [ChatOllama](https://github.com/sugarforever/chat-ollama) (Open Source Chatbot based on Ollama with Knowledge Bases)
- [CRAG Ollama Chat](https://github.com/Nagi-ovo/CRAG-Ollama-Chat) (Simple Web Search with Corrective RAG)
- [RAGFlow](https://github.com/infiniflow/ragflow) (Open-source Retrieval-Augmented Generation engine based on deep document understanding)
- [StreamDeploy](https://github.com/StreamDeploy-DevRel/streamdeploy-llm-app-scaffold) (LLM Application Scaffold)
- [chat](https://github.com/swuecho/chat) (chat web app for teams)
- [Lobe Chat](https://github.com/lobehub/lobe-chat) with [Integrating Doc](https://lobehub.com/docs/self-hosting/examples/ollama)
- [Ollama RAG Chatbot](https://github.com/datvodinh/rag-chatbot.git) (Local Chat with multiple PDFs using Ollama and RAG)
- [BrainSoup](https://www.nurgo-software.com/products/brainsoup) (Flexible native client with RAG &amp; multi-agent automation)
- [macai](https://github.com/Renset/macai) (macOS client for Ollama, ChatGPT, and other compatible API back-ends)
- [RWKV-Runner](https://github.com/josStorer/RWKV-Runner) (RWKV offline LLM deployment tool, also usable as a client for ChatGPT and Ollama)
- [Ollama Grid Search](https://github.com/dezoito/ollama-grid-search) (app to evaluate and compare models)
- [Olpaka](https://github.com/Otacon/olpaka) (User-friendly Flutter Web App for Ollama)
- [Casibase](https://casibase.org) (An open source AI knowledge base and dialogue system combining the latest RAG, SSO, ollama support, and multiple large language models.)
- [OllamaSpring](https://github.com/CrazyNeil/OllamaSpring) (Ollama Client for macOS)
- [LLocal.in](https://github.com/kartikm7/llocal) (Easy to use Electron Desktop Client for Ollama)
- [Shinkai Desktop](https://github.com/dcSpark/shinkai-apps) (Two click install Local AI using Ollama + Files + RAG)
- [AiLama](https://github.com/zeyoyt/ailama) (A Discord User App that allows you to interact with Ollama anywhere in Discord)
- [Ollama with Google Mesop](https://github.com/rapidarchitect/ollama_mesop/) (Mesop Chat Client implementation with Ollama)
- [R2R](https://github.com/SciPhi-AI/R2R) (Open-source RAG engine)
- [Ollama-Kis](https://github.com/elearningshow/ollama-kis) (A simple easy-to-use GUI with sample custom LLM for Drivers Education)
- [OpenGPA](https://opengpa.org) (Open-source offline-first Enterprise Agentic Application)
- [Painting Droid](https://github.com/mateuszmigas/painting-droid) (Painting app with AI integrations)
- [Kerlig AI](https://www.kerlig.com/) (AI writing assistant for macOS)
- [AI Studio](https://github.com/MindWorkAI/AI-Studio)
- [Sidellama](https://github.com/gyopak/sidellama) (browser-based LLM client)
- [LLMStack](https://github.com/trypromptly/LLMStack) (No-code multi-agent framework to build LLM agents and workflows)
- [BoltAI for Mac](https://boltai.com) (AI Chat Client for Mac)
- [Harbor](https://github.com/av/harbor) (Containerized LLM Toolkit with Ollama as default backend)
- [PyGPT](https://github.com/szczyglis-dev/py-gpt) (AI desktop assistant for Linux, Windows, and Mac)
- [Alpaca](https://github.com/Jeffser/Alpaca) (An Ollama client application for Linux and macOS made with GTK4 and Adwaita)
- [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT/blob/master/docs/content/platform/ollama.md) (AutoGPT Ollama integration)
- [Go-CREW](https://www.jonathanhecl.com/go-crew/) (Powerful Offline RAG in Golang)
- [PartCAD](https://github.com/openvmp/partcad/) (CAD model generation with OpenSCAD and CadQuery)
- [Ollama4j Web UI](https://github.com/ollama4j/ollama4j-web-ui) - Java-based Web UI for Ollama built with Vaadin, Spring Boot, and Ollama4j
- [PyOllaMx](https://github.com/kspviswa/pyOllaMx) - macOS application capable of chatting with both Ollama and Apple MLX models.
- [Cline](https://github.com/cline/cline) - Formerly known as Claude Dev is a VS Code extension for multi-file/whole-repo coding
- [Void](https://github.com/voideditor/void) (Open source AI code editor and Cursor alternative)
- [Cherry Studio](https://github.com/kangfenmao/cherry-studio) (Desktop client with Ollama support)
- [ConfiChat](https://github.com/1runeberg/confichat) (Lightweight, standalone, multi-platform, and privacy-focused LLM chat interface with optional encryption)
- [Archyve](https://github.com/nickthecook/archyve) (RAG-enabling document library)
- [crewAI with Mesop](https://github.com/rapidarchitect/ollama-crew-mesop) (Mesop Web Interface to run crewAI with Ollama)
- [Tkinter-based client](https://github.com/chyok/ollama-gui) (Python tkinter-based Client for Ollama)
- [LLMChat](https://github.com/trendy-design/llmchat) (Privacy focused, 100% local, intuitive all-in-one chat interface)
- [Local Multimodal AI Chat](https://github.com/Leon-Sander/Local-Multimodal-AI-Chat) (Ollama-based LLM Chat with support for multiple features, including PDF RAG, voice chat, image-based interactions, and integration with OpenAI.)
- [ARGO](https://github.com/xark-argo/argo) (Locally download and run Ollama and Huggingface models with RAG and deep research on Mac/Windows/Linux)
- [OrionChat](https://github.com/EliasPereirah/OrionChat) - OrionChat is a web interface for chatting with different AI providers
- [G1](https://github.com/bklieger-groq/g1) (Prototype of using prompting strategies to improve the LLM&#039;s reasoning through o1-like reasoning chains.)
- [Web management](https://github.com/lemonit-eric-mao/ollama-web-management) (Web management page)
- [Promptery](https://github.com/promptery/promptery) (desktop client for Ollama.)
- [Ollama App](https://github.com/JHubi1/ollama-app) (Modern and easy-to-use multi-platform client for Ollama)
- [chat-ollama](https://github.com/annilq/chat-ollama) (a React Native client for Ollama)
- [SpaceLlama](https://github.com/tcsenpai/spacellama) (Firefox and Chrome extension to quickly summarize web pages with ollama in a sidebar)
- [YouLama](https://github.com/tcsenpai/youlama) (Webapp to quickly summarize any YouTube video, supporting Invidious as well)
- [DualMind](https://github.com/tcsenpai/dualmind) (Experimental app allowing two models to talk to each other in the terminal or in a web interface)
- [ollamarama-matrix](https://github.com/h1ddenpr0cess20/ollamarama-matrix) (Ollama chatbot for the Matrix chat protocol)
- [ollama-chat-app](https://github.com/anan1213095357/ollama-chat-app) (Flutter-based chat app)
- [Perfect Memory AI](https://www.perfectmemory.ai/) (Productivity AI assists personalized by what you have seen on your screen, heard, and said in the meetings)
- [Hexabot](https://github.com/hexastack/hexabot) (A conversational AI builder)
- [Reddit Rate](https://github.com/rapidarchitect/reddit_analyzer) (Search and Rate Reddit topics with a weighted summation)
- [OpenTalkGpt](https://github.com/adarshM84/OpenTalkGpt) (Chrome Extension to manage open-source models supported by Ollama, create custom models, and chat with models from a user-friendly UI)
- [VT](https://github.com/vinhnx/vt.ai) (A minimal multimodal AI chat app, with dynamic conversation routing. Supports local models via Ollama)
- [Nosia](https://github.com/nosia-ai/nosia) (Easy to install and use RAG platform based on Ollama)
- [Witsy](https://github.com/nbonamy/witsy) (An AI Desktop application available for Mac/Windows/Linux)
- [Abbey](https://github.com/US-Artificial-Intelligence/abbey) (A configurable AI interface server with notebooks, document storage, and YouTube support)
- [Minima](https://github.com/dmayboroda/minima) (RAG with on-premises or fully local workflow)
- [aidful-ollama-model-delete](https://github.com/AidfulAI/aidful-ollama-model-delete) (User interface for simplified model cleanup)
- [Perplexica](https://github.com/ItzCrazyKns/Perplexica) (An AI-powered search engine &amp; an open-source alternative to Perplexity AI)
- [Ollama Chat WebUI for Docker ](https://github.com/oslook/ollama-webui) (Support for local docker deployment, lightweight ollama webui)
- [AI Toolkit for Visual Studio Code](https://aka.ms/ai-tooklit/ollama-docs) (Microsoft-official VS Code extension to chat, test, evaluate models with Ollama support, and use them in your AI applications.)
- [MinimalNextOllamaChat](https://github.com/anilkay/MinimalNextOllamaChat) (Minimal Web UI for Chat and Model Control)
- [Chipper](https://github.com/TilmanGriesel/chipper) AI interface for tinkerers (Ollama, Haystack RAG, Python)
- [ChibiChat](https://github.com/CosmicEventHorizon/ChibiChat) (Kotlin-based Android app to chat with Ollama and Koboldcpp API endpoints)
- [LocalLLM](https://github.com/qusaismael/localllm) (Minimal Web-App to run ollama models on it with a GUI)
- [Ollamazing](https://github.com/buiducnhat/ollamazing) (Web extension to run Ollama models)
- [OpenDeepResearcher-via-searxng](https://github.com/benhaotang/OpenDeepResearcher-via-searxng) (A Deep Research equivalent endpoint with Ollama support for running locally)
- [AntSK](https://github.com/AIDotNet/AntSK) (Out-of-the-box &amp; Adaptable RAG Chatbot)
- [MaxKB](https://github.com/1Panel-dev/MaxKB/) (Ready-to-use &amp; flexible RAG Chatbot)
- [yla](https://github.com/danielekp/yla) (Web interface to freely interact with your customized models)
- [LangBot](https://github.com/RockChinQ/LangBot) (LLM-based instant messaging bots platform, with Agents, RAG features, supports multiple platforms)
- [1Panel](https://github.com/1Panel-dev/1Panel/) (Web-based Linux Server Management Tool)
- [AstrBot](https://github.com/Soulter/AstrBot/) (User-friendly LLM-based multi-platform chatbot with a WebUI, supporting RAG, LLM agents, and plugins integration)
- [Reins](https://github.com/ibrahimcetin/reins) (Easily tweak parameters, customize system prompts per chat, and enhanc

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[wavetermdev/waveterm]]></title>
            <link>https://github.com/wavetermdev/waveterm</link>
            <guid>https://github.com/wavetermdev/waveterm</guid>
            <pubDate>Thu, 25 Dec 2025 00:05:21 GMT</pubDate>
            <description><![CDATA[An open-source, cross-platform terminal for seamless workflows]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/wavetermdev/waveterm">wavetermdev/waveterm</a></h1>
            <p>An open-source, cross-platform terminal for seamless workflows</p>
            <p>Language: Go</p>
            <p>Stars: 15,665</p>
            <p>Forks: 666</p>
            <p>Stars today: 61 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.waveterm.dev&quot;&gt;
	&lt;picture&gt;
		&lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;./assets/wave-dark.png&quot;&gt;
		&lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;./assets/wave-light.png&quot;&gt;
		&lt;img alt=&quot;Wave Terminal Logo&quot; src=&quot;./assets/wave-light.png&quot; width=&quot;240&quot;&gt;
	&lt;/picture&gt;
  &lt;/a&gt;
  &lt;br/&gt;
&lt;/p&gt;

# Wave Terminal

[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Fwavetermdev%2Fwaveterm.svg?type=shield)](https://app.fossa.com/projects/git%2Bgithub.com%2Fwavetermdev%2Fwaveterm?ref=badge_shield)

Wave is an open-source terminal that combines traditional terminal features with graphical capabilities like file previews, web browsing, and AI assistance. It runs on MacOS, Linux, and Windows.

Modern development involves constantly switching between terminals and browsers - checking documentation, previewing files, monitoring systems, and using AI tools. Wave brings these graphical tools directly into the terminal, letting you control them from the command line. This means you can stay in your terminal workflow while still having access to the visual interfaces you need.

![WaveTerm Screenshot](./assets/wave-screenshot.webp)

## Key Features

- Flexible drag &amp; drop interface to organize terminal blocks, editors, web browsers, and AI assistants
- Built-in editor for seamlessly editing remote files with syntax highlighting and modern editor features
- Rich file preview system for remote files (markdown, images, video, PDFs, CSVs, directories)
- Quick full-screen toggle for any block - expand terminals, editors, and previews for better visibility, then instantly return to multi-block view
- Wave AI - Context-aware terminal assistant that reads your terminal output, analyzes widgets, and performs file operations
- AI chat widget with support for multiple models (OpenAI, Claude, Azure, Perplexity, Ollama)
- Command Blocks for isolating and monitoring individual commands with auto-close options
- One-click remote connections with full terminal and file system access
- Secure secret storage using native system backends - store API keys and credentials locally, access them across SSH sessions
- Rich customization including tab themes, terminal styles, and background images
- Powerful `wsh` command system for managing your workspace from the CLI and sharing data between terminal sessions
- Connected file management with `wsh file` - seamlessly copy and sync files between local, remote SSH hosts, Wave filesystem, and S3

## Wave AI

Wave AI is your context-aware terminal assistant with access to your workspace:

- **Terminal Context**: Reads terminal output and scrollback for debugging and analysis
- **File Operations**: Read, write, and edit files with automatic backups and user approval
- **CLI Integration**: Use `wsh ai` to pipe output or attach files directly from the command line
- **Free Beta**: Included AI credits while we refine the experience
- **Coming Soon**: Command execution (with approval), local model support, and alternate AI providers (BYOK)

Learn more in our [Wave AI documentation](https://docs.waveterm.dev/waveai).

## Installation

Wave Terminal works on macOS, Linux, and Windows.

Platform-specific installation instructions can be found [here](https://docs.waveterm.dev/gettingstarted).

You can also install Wave Terminal directly from: [www.waveterm.dev/download](https://www.waveterm.dev/download).

### Minimum requirements

Wave Terminal runs on the following platforms:

- macOS 11 or later (arm64, x64)
- Windows 10 1809 or later (x64)
- Linux based on glibc-2.28 or later (Debian 10, RHEL 8, Ubuntu 20.04, etc.) (arm64, x64)

The WSH helper runs on the following platforms:

- macOS 11 or later (arm64, x64)
- Windows 10 or later (arm64, x64)
- Linux Kernel 2.6.32 or later (x64), Linux Kernel 3.1 or later (arm64)

## Roadmap

Wave is constantly improving! Our roadmap will be continuously updated with our goals for each release. You can find it [here](./ROADMAP.md).

Want to provide input to our future releases? Connect with us on [Discord](https://discord.gg/XfvZ334gwU) or open a [Feature Request](https://github.com/wavetermdev/waveterm/issues/new/choose)!

## Links

- Homepage &amp;mdash; https://www.waveterm.dev
- Download Page &amp;mdash; https://www.waveterm.dev/download
- Documentation &amp;mdash; https://docs.waveterm.dev
- Legacy Documentation &amp;mdash; https://legacydocs.waveterm.dev
- Blog &amp;mdash; https://blog.waveterm.dev
- X &amp;mdash; https://x.com/wavetermdev
- Discord Community &amp;mdash; https://discord.gg/XfvZ334gwU

## Building from Source

See [Building Wave Terminal](BUILD.md).

## Contributing

Wave uses GitHub Issues for issue tracking.

Find more information in our [Contributions Guide](CONTRIBUTING.md), which includes:

- [Ways to contribute](CONTRIBUTING.md#contributing-to-wave-terminal)
- [Contribution guidelines](CONTRIBUTING.md#before-you-start)

## License

Wave Terminal is licensed under the Apache-2.0 License. For more information on our dependencies, see [here](./ACKNOWLEDGEMENTS.md).
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[ethereum/go-ethereum]]></title>
            <link>https://github.com/ethereum/go-ethereum</link>
            <guid>https://github.com/ethereum/go-ethereum</guid>
            <pubDate>Thu, 25 Dec 2025 00:05:20 GMT</pubDate>
            <description><![CDATA[Go implementation of the Ethereum protocol]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ethereum/go-ethereum">ethereum/go-ethereum</a></h1>
            <p>Go implementation of the Ethereum protocol</p>
            <p>Language: Go</p>
            <p>Stars: 50,545</p>
            <p>Forks: 21,667</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre>## Go Ethereum

Golang execution layer implementation of the Ethereum protocol.

[![API Reference](
https://pkg.go.dev/badge/github.com/ethereum/go-ethereum
)](https://pkg.go.dev/github.com/ethereum/go-ethereum?tab=doc)
[![Go Report Card](https://goreportcard.com/badge/github.com/ethereum/go-ethereum)](https://goreportcard.com/report/github.com/ethereum/go-ethereum)
[![Travis](https://app.travis-ci.com/ethereum/go-ethereum.svg?branch=master)](https://app.travis-ci.com/github/ethereum/go-ethereum)
[![Discord](https://img.shields.io/badge/discord-join%20chat-blue.svg)](https://discord.gg/nthXNEv)
[![Twitter](https://img.shields.io/twitter/follow/go_ethereum)](https://x.com/go_ethereum)

Automated builds are available for stable releases and the unstable master branch. Binary
archives are published at https://geth.ethereum.org/downloads/.

## Building the source

For prerequisites and detailed build instructions please read the [Installation Instructions](https://geth.ethereum.org/docs/getting-started/installing-geth).

Building `geth` requires both a Go (version 1.23 or later) and a C compiler. You can install
them using your favourite package manager. Once the dependencies are installed, run

```shell
make geth
```

or, to build the full suite of utilities:

```shell
make all
```

## Executables

The go-ethereum project comes with several wrappers/executables found in the `cmd`
directory.

|  Command   | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| :--------: | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **`geth`** | Our main Ethereum CLI client. It is the entry point into the Ethereum network (main-, test- or private net), capable of running as a full node (default), archive node (retaining all historical state) or a light node (retrieving data live). It can be used by other processes as a gateway into the Ethereum network via JSON RPC endpoints exposed on top of HTTP, WebSocket and/or IPC transports. `geth --help` and the [CLI page](https://geth.ethereum.org/docs/fundamentals/command-line-options) for command line options. |
|   `clef`   | Stand-alone signing tool, which can be used as a backend signer for `geth`.                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
|  `devp2p`  | Utilities to interact with nodes on the networking layer, without running a full blockchain.                                                                                                                                                                                                                                                                                                                                                                                                                                       |
|  `abigen`  | Source code generator to convert Ethereum contract definitions into easy-to-use, compile-time type-safe Go packages. It operates on plain [Ethereum contract ABIs](https://docs.soliditylang.org/en/develop/abi-spec.html) with expanded functionality if the contract bytecode is also available. However, it also accepts Solidity source files, making development much more streamlined. Please see our [Native DApps](https://geth.ethereum.org/docs/developers/dapp-developer/native-bindings) page for details.                                  |
|   `evm`    | Developer utility version of the EVM (Ethereum Virtual Machine) that is capable of running bytecode snippets within a configurable environment and execution mode. Its purpose is to allow isolated, fine-grained debugging of EVM opcodes (e.g. `evm --code 60ff60ff --debug run`).                                                                                                                                                                                                                                               |
| `rlpdump`  | Developer utility tool to convert binary RLP ([Recursive Length Prefix](https://ethereum.org/en/developers/docs/data-structures-and-encoding/rlp)) dumps (data encoding used by the Ethereum protocol both network as well as consensus wise) to user-friendlier hierarchical representation (e.g. `rlpdump --hex CE0183FFFFFFC4C304050583616263`).                                                                                                                                                                                |

## Running `geth`

Going through all the possible command line flags is out of scope here (please consult our
[CLI Wiki page](https://geth.ethereum.org/docs/fundamentals/command-line-options)),
but we&#039;ve enumerated a few common parameter combos to get you up to speed quickly
on how you can run your own `geth` instance.

### Hardware Requirements

Minimum:

* CPU with 4+ cores
* 8GB RAM
* 1TB free storage space to sync the Mainnet
* 8 MBit/sec download Internet service

Recommended:

* Fast CPU with 8+ cores
* 16GB+ RAM
* High-performance SSD with at least 1TB of free space
* 25+ MBit/sec download Internet service

### Full node on the main Ethereum network

By far the most common scenario is people wanting to simply interact with the Ethereum
network: create accounts; transfer funds; deploy and interact with contracts. For this
particular use case, the user doesn&#039;t care about years-old historical data, so we can
sync quickly to the current state of the network. To do so:

```shell
$ geth console
```

This command will:
 * Start `geth` in snap sync mode (default, can be changed with the `--syncmode` flag),
   causing it to download more data in exchange for avoiding processing the entire history
   of the Ethereum network, which is very CPU intensive.
 * Start the built-in interactive [JavaScript console](https://geth.ethereum.org/docs/interacting-with-geth/javascript-console),
   (via the trailing `console` subcommand) through which you can interact using [`web3` methods](https://github.com/ChainSafe/web3.js/blob/0.20.7/DOCUMENTATION.md) 
   (note: the `web3` version bundled within `geth` is very old, and not up to date with official docs),
   as well as `geth`&#039;s own [management APIs](https://geth.ethereum.org/docs/interacting-with-geth/rpc).
   This tool is optional and if you leave it out you can always attach it to an already running
   `geth` instance with `geth attach`.

### A Full node on the Holesky test network

Transitioning towards developers, if you&#039;d like to play around with creating Ethereum
contracts, you almost certainly would like to do that without any real money involved until
you get the hang of the entire system. In other words, instead of attaching to the main
network, you want to join the **test** network with your node, which is fully equivalent to
the main network, but with play-Ether only.

```shell
$ geth --holesky console
```

The `console` subcommand has the same meaning as above and is equally
useful on the testnet too.

Specifying the `--holesky` flag, however, will reconfigure your `geth` instance a bit:

 * Instead of connecting to the main Ethereum network, the client will connect to the Holesky 
   test network, which uses different P2P bootnodes, different network IDs and genesis
   states.
 * Instead of using the default data directory (`~/.ethereum` on Linux for example), `geth`
   will nest itself one level deeper into a `holesky` subfolder (`~/.ethereum/holesky` on
   Linux). Note, on OSX and Linux this also means that attaching to a running testnet node
   requires the use of a custom endpoint since `geth attach` will try to attach to a
   production node endpoint by default, e.g.,
   `geth attach &lt;datadir&gt;/holesky/geth.ipc`. Windows users are not affected by
   this.

*Note: Although some internal protective measures prevent transactions from
crossing over between the main network and test network, you should always
use separate accounts for play and real money. Unless you manually move
accounts, `geth` will by default correctly separate the two networks and will not make any
accounts available between them.*

### Configuration

As an alternative to passing the numerous flags to the `geth` binary, you can also pass a
configuration file via:

```shell
$ geth --config /path/to/your_config.toml
```

To get an idea of how the file should look like you can use the `dumpconfig` subcommand to
export your existing configuration:

```shell
$ geth --your-favourite-flags dumpconfig
```

#### Docker quick start

One of the quickest ways to get Ethereum up and running on your machine is by using
Docker:

```shell
docker run -d --name ethereum-node -v /Users/alice/ethereum:/root \
           -p 8545:8545 -p 30303:30303 \
           ethereum/client-go
```

This will start `geth` in snap-sync mode with a DB memory allowance of 1GB, as the
above command does.  It will also create a persistent volume in your home directory for
saving your blockchain as well as map the default ports. There is also an `alpine` tag
available for a slim version of the image.

Do not forget `--http.addr 0.0.0.0`, if you want to access RPC from other containers
and/or hosts. By default, `geth` binds to the local interface and RPC endpoints are not
accessible from the outside.

### Programmatically interfacing `geth` nodes

As a developer, sooner rather than later you&#039;ll want to start interacting with `geth` and the
Ethereum network via your own programs and not manually through the console. To aid
this, `geth` has built-in support for a JSON-RPC based APIs ([standard APIs](https://ethereum.org/en/developers/docs/apis/json-rpc/)
and [`geth` specific APIs](https://geth.ethereum.org/docs/interacting-with-geth/rpc)).
These can be exposed via HTTP, WebSockets and IPC (UNIX sockets on UNIX based
platforms, and named pipes on Windows).

The IPC interface is enabled by default and exposes all the APIs supported by `geth`,
whereas the HTTP and WS interfaces need to manually be enabled and only expose a
subset of APIs due to security reasons. These can be turned on/off and configured as
you&#039;d expect.

HTTP based JSON-RPC API options:

  * `--http` Enable the HTTP-RPC server
  * `--http.addr` HTTP-RPC server listening interface (default: `localhost`)
  * `--http.port` HTTP-RPC server listening port (default: `8545`)
  * `--http.api` API&#039;s offered over the HTTP-RPC interface (default: `eth,net,web3`)
  * `--http.corsdomain` Comma separated list of domains from which to accept cross-origin requests (browser enforced)
  * `--ws` Enable the WS-RPC server
  * `--ws.addr` WS-RPC server listening interface (default: `localhost`)
  * `--ws.port` WS-RPC server listening port (default: `8546`)
  * `--ws.api` API&#039;s offered over the WS-RPC interface (default: `eth,net,web3`)
  * `--ws.origins` Origins from which to accept WebSocket requests
  * `--ipcdisable` Disable the IPC-RPC server
  * `--ipcpath` Filename for IPC socket/pipe within the datadir (explicit paths escape it)

You&#039;ll need to use your own programming environments&#039; capabilities (libraries, tools, etc) to
connect via HTTP, WS or IPC to a `geth` node configured with the above flags and you&#039;ll
need to speak [JSON-RPC](https://www.jsonrpc.org/specification) on all transports. You
can reuse the same connection for multiple requests!

**Note: Please understand the security implications of opening up an HTTP/WS based
transport before doing so! Hackers on the internet are actively trying to subvert
Ethereum nodes with exposed APIs! Further, all browser tabs can access locally
running web servers, so malicious web pages could try to subvert locally available
APIs!**

### Operating a private network

Maintaining your own private network is more involved as a lot of configurations taken for
granted in the official networks need to be manually set up.

Unfortunately since [the Merge](https://ethereum.org/en/roadmap/merge/) it is no longer possible
to easily set up a network of geth nodes without also setting up a corresponding beacon chain.

There are three different solutions depending on your use case:

  * If you are looking for a simple way to test smart contracts from go in your CI, you can use the [Simulated Backend](https://geth.ethereum.org/docs/developers/dapp-developer/native-bindings#blockchain-simulator).
  * If you want a convenient single node environment for testing, you can use our [Dev Mode](https://geth.ethereum.org/docs/developers/dapp-developer/dev-mode).
  * If you are looking for a multiple node test network, you can set one up quite easily with [Kurtosis](https://geth.ethereum.org/docs/fundamentals/kurtosis).

## Contribution

Thank you for considering helping out with the source code! We welcome contributions
from anyone on the internet, and are grateful for even the smallest of fixes!

If you&#039;d like to contribute to go-ethereum, please fork, fix, commit and send a pull request
for the maintainers to review and merge into the main code base. If you wish to submit
more complex changes though, please check up with the core devs first on [our Discord Server](https://discord.gg/invite/nthXNEv)
to ensure those changes are in line with the general philosophy of the project and/or get
some early feedback which can make both your efforts much lighter as well as our review
and merge procedures quick and simple.

Please make sure your contributions adhere to our coding guidelines:

 * Code must adhere to the official Go [formatting](https://golang.org/doc/effective_go.html#formatting)
   guidelines (i.e. uses [gofmt](https://golang.org/cmd/gofmt/)).
 * Code must be documented adhering to the official Go [commentary](https://golang.org/doc/effective_go.html#commentary)
   guidelines.
 * Pull requests need to be based on and opened against the `master` branch.
 * Commit messages should be prefixed with the package(s) they modify.
   * E.g. &quot;eth, rpc: make trace configs optional&quot;

Please see the [Developers&#039; Guide](https://geth.ethereum.org/docs/developers/geth-developer/dev-guide)
for more details on configuring your environment, managing project dependencies, and
testing procedures.

### Contributing to geth.ethereum.org

For contributions to the [go-ethereum website](https://geth.ethereum.org), please checkout and raise pull requests against the `website` branch.
For more detailed instructions please see the `website` branch [README](https://github.com/ethereum/go-ethereum/tree/website#readme) or the 
[contributing](https://geth.ethereum.org/docs/developers/geth-developer/contributing) page of the website.

## License

The go-ethereum library (i.e. all code outside of the `cmd` directory) is licensed under the
[GNU Lesser General Public License v3.0](https://www.gnu.org/licenses/lgpl-3.0.en.html),
also included in our repository in the `COPYING.LESSER` file.

The go-ethereum binaries (i.e. all code inside of the `cmd` directory) are licensed under the
[GNU General Public License v3.0](https://www.gnu.org/licenses/gpl-3.0.en.html), also
included in our repository in the `COPYING` file.
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[sirupsen/logrus]]></title>
            <link>https://github.com/sirupsen/logrus</link>
            <guid>https://github.com/sirupsen/logrus</guid>
            <pubDate>Thu, 25 Dec 2025 00:05:19 GMT</pubDate>
            <description><![CDATA[Structured, pluggable logging for Go.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sirupsen/logrus">sirupsen/logrus</a></h1>
            <p>Structured, pluggable logging for Go.</p>
            <p>Language: Go</p>
            <p>Stars: 25,636</p>
            <p>Forks: 2,280</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre># Logrus &lt;img src=&quot;http://i.imgur.com/hTeVwmJ.png&quot; width=&quot;40&quot; height=&quot;40&quot; alt=&quot;:walrus:&quot; class=&quot;emoji&quot; title=&quot;:walrus:&quot;/&gt; [![Build Status](https://github.com/sirupsen/logrus/workflows/CI/badge.svg)](https://github.com/sirupsen/logrus/actions?query=workflow%3ACI) [![Go Reference](https://pkg.go.dev/badge/github.com/sirupsen/logrus.svg)](https://pkg.go.dev/github.com/sirupsen/logrus)

Logrus is a structured logger for Go (golang), completely API compatible with
the standard library logger.

**Logrus is in maintenance-mode.** We will not be introducing new features. It&#039;s
simply too hard to do in a way that won&#039;t break many people&#039;s projects, which is
the last thing you want from your Logging library (again...).

This does not mean Logrus is dead. Logrus will continue to be maintained for
security, (backwards compatible) bug fixes, and performance (where we are
limited by the interface).

I believe Logrus&#039; biggest contribution is to have played a part in today&#039;s
widespread use of structured logging in Golang. There doesn&#039;t seem to be a
reason to do a major, breaking iteration into Logrus V2, since the fantastic Go
community has built those independently. Many fantastic alternatives have sprung
up. Logrus would look like those, had it been re-designed with what we know
about structured logging in Go today. Check out, for example,
[Zerolog][zerolog], [Zap][zap], and [Apex][apex].

[zerolog]: https://github.com/rs/zerolog
[zap]: https://github.com/uber-go/zap
[apex]: https://github.com/apex/log

**Seeing weird case-sensitive problems?** It&#039;s in the past been possible to
import Logrus as both upper- and lower-case. Due to the Go package environment,
this caused issues in the community and we needed a standard. Some environments
experienced problems with the upper-case variant, so the lower-case was decided.
Everything using `logrus` will need to use the lower-case:
`github.com/sirupsen/logrus`. Any package that isn&#039;t, should be changed.

To fix Glide, see [these
comments](https://github.com/sirupsen/logrus/issues/553#issuecomment-306591437).
For an in-depth explanation of the casing issue, see [this
comment](https://github.com/sirupsen/logrus/issues/570#issuecomment-313933276).

Nicely color-coded in development (when a TTY is attached, otherwise just
plain text):

![Colored](http://i.imgur.com/PY7qMwd.png)

With `logrus.SetFormatter(&amp;logrus.JSONFormatter{})`, for easy parsing by logstash
or Splunk:

```text
{&quot;animal&quot;:&quot;walrus&quot;,&quot;level&quot;:&quot;info&quot;,&quot;msg&quot;:&quot;A group of walrus emerges from the
ocean&quot;,&quot;size&quot;:10,&quot;time&quot;:&quot;2014-03-10 19:57:38.562264131 -0400 EDT&quot;}

{&quot;level&quot;:&quot;warning&quot;,&quot;msg&quot;:&quot;The group&#039;s number increased tremendously!&quot;,
&quot;number&quot;:122,&quot;omg&quot;:true,&quot;time&quot;:&quot;2014-03-10 19:57:38.562471297 -0400 EDT&quot;}

{&quot;animal&quot;:&quot;walrus&quot;,&quot;level&quot;:&quot;info&quot;,&quot;msg&quot;:&quot;A giant walrus appears!&quot;,
&quot;size&quot;:10,&quot;time&quot;:&quot;2014-03-10 19:57:38.562500591 -0400 EDT&quot;}

{&quot;animal&quot;:&quot;walrus&quot;,&quot;level&quot;:&quot;info&quot;,&quot;msg&quot;:&quot;Tremendously sized cow enters the ocean.&quot;,
&quot;size&quot;:9,&quot;time&quot;:&quot;2014-03-10 19:57:38.562527896 -0400 EDT&quot;}

{&quot;level&quot;:&quot;fatal&quot;,&quot;msg&quot;:&quot;The ice breaks!&quot;,&quot;number&quot;:100,&quot;omg&quot;:true,
&quot;time&quot;:&quot;2014-03-10 19:57:38.562543128 -0400 EDT&quot;}
```

With the default `logrus.SetFormatter(&amp;logrus.TextFormatter{})` when a TTY is not
attached, the output is compatible with the
[logfmt](https://pkg.go.dev/github.com/kr/logfmt) format:

```text
time=&quot;2015-03-26T01:27:38-04:00&quot; level=debug msg=&quot;Started observing beach&quot; animal=walrus number=8
time=&quot;2015-03-26T01:27:38-04:00&quot; level=info msg=&quot;A group of walrus emerges from the ocean&quot; animal=walrus size=10
time=&quot;2015-03-26T01:27:38-04:00&quot; level=warning msg=&quot;The group&#039;s number increased tremendously!&quot; number=122 omg=true
time=&quot;2015-03-26T01:27:38-04:00&quot; level=debug msg=&quot;Temperature changes&quot; temperature=-4
time=&quot;2015-03-26T01:27:38-04:00&quot; level=panic msg=&quot;It&#039;s over 9000!&quot; animal=orca size=9009
time=&quot;2015-03-26T01:27:38-04:00&quot; level=fatal msg=&quot;The ice breaks!&quot; err=&amp;{0x2082280c0 map[animal:orca size:9009] 2015-03-26 01:27:38.441574009 -0400 EDT panic It&#039;s over 9000!} number=100 omg=true
```
To ensure this behaviour even if a TTY is attached, set your formatter as follows:

```go
logrus.SetFormatter(&amp;logrus.TextFormatter{
    DisableColors: true,
    FullTimestamp: true,
})
```

#### Logging Method Name

If you wish to add the calling method as a field, instruct the logger via:

```go
logrus.SetReportCaller(true)
```
This adds the caller as &#039;method&#039; like so:

```json
{&quot;animal&quot;:&quot;penguin&quot;,&quot;level&quot;:&quot;fatal&quot;,&quot;method&quot;:&quot;github.com/sirupsen/arcticcreatures.migrate&quot;,&quot;msg&quot;:&quot;a penguin swims by&quot;,
&quot;time&quot;:&quot;2014-03-10 19:57:38.562543129 -0400 EDT&quot;}
```

```text
time=&quot;2015-03-26T01:27:38-04:00&quot; level=fatal method=github.com/sirupsen/arcticcreatures.migrate msg=&quot;a penguin swims by&quot; animal=penguin
```
Note that this does add measurable overhead - the cost will depend on the version of Go, but is
between 20 and 40% in recent tests with 1.6 and 1.7.  You can validate this in your
environment via benchmarks:

```bash
go test -bench=.*CallerTracing
```

#### Case-sensitivity

The organization&#039;s name was changed to lower-case--and this will not be changed
back. If you are getting import conflicts due to case sensitivity, please use
the lower-case import: `github.com/sirupsen/logrus`.

#### Example

The simplest way to use Logrus is simply the package-level exported logger:

```go
package main

import &quot;github.com/sirupsen/logrus&quot;

func main() {
  logrus.WithFields(logrus.Fields{
    &quot;animal&quot;: &quot;walrus&quot;,
  }).Info(&quot;A walrus appears&quot;)
}
```

Note that it&#039;s completely api-compatible with the stdlib logger, so you can
replace your `log` imports everywhere with `log &quot;github.com/sirupsen/logrus&quot;`
and you&#039;ll now have the flexibility of Logrus. You can customize it all you
want:

```go
package main

import (
  &quot;os&quot;

  log &quot;github.com/sirupsen/logrus&quot;
)

func init() {
  // Log as JSON instead of the default ASCII formatter.
  log.SetFormatter(&amp;log.JSONFormatter{})

  // Output to stdout instead of the default stderr
  // Can be any io.Writer, see below for File example
  log.SetOutput(os.Stdout)

  // Only log the warning severity or above.
  log.SetLevel(log.WarnLevel)
}

func main() {
  log.WithFields(log.Fields{
    &quot;animal&quot;: &quot;walrus&quot;,
    &quot;size&quot;:   10,
  }).Info(&quot;A group of walrus emerges from the ocean&quot;)

  log.WithFields(log.Fields{
    &quot;omg&quot;:    true,
    &quot;number&quot;: 122,
  }).Warn(&quot;The group&#039;s number increased tremendously!&quot;)

  log.WithFields(log.Fields{
    &quot;omg&quot;:    true,
    &quot;number&quot;: 100,
  }).Fatal(&quot;The ice breaks!&quot;)

  // A common pattern is to re-use fields between logging statements by re-using
  // the logrus.Entry returned from WithFields()
  contextLogger := log.WithFields(log.Fields{
    &quot;common&quot;: &quot;this is a common field&quot;,
    &quot;other&quot;: &quot;I also should be logged always&quot;,
  })

  contextLogger.Info(&quot;I&#039;ll be logged with common and other field&quot;)
  contextLogger.Info(&quot;Me too&quot;)
}
```

For more advanced usage such as logging to multiple locations from the same
application, you can also create an instance of the `logrus` Logger:

```go
package main

import (
  &quot;os&quot;

  &quot;github.com/sirupsen/logrus&quot;
)

// Create a new instance of the logger. You can have any number of instances.
var logger = logrus.New()

func main() {
  // The API for setting attributes is a little different than the package level
  // exported logger. See Godoc. 
  logger.Out = os.Stdout

  // You could set this to any `io.Writer` such as a file
  // file, err := os.OpenFile(&quot;logrus.log&quot;, os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0666)
  // if err == nil {
  //  logger.Out = file
  // } else {
  //  logger.Info(&quot;Failed to log to file, using default stderr&quot;)
  // }

  logger.WithFields(logrus.Fields{
    &quot;animal&quot;: &quot;walrus&quot;,
    &quot;size&quot;:   10,
  }).Info(&quot;A group of walrus emerges from the ocean&quot;)
}
```

#### Fields

Logrus encourages careful, structured logging through logging fields instead of
long, unparseable error messages. For example, instead of: `logrus.Fatalf(&quot;Failed
to send event %s to topic %s with key %d&quot;)`, you should log the much more
discoverable:

```go
logrus.WithFields(logrus.Fields{
  &quot;event&quot;: event,
  &quot;topic&quot;: topic,
  &quot;key&quot;: key,
}).Fatal(&quot;Failed to send event&quot;)
```

We&#039;ve found this API forces you to think about logging in a way that produces
much more useful logging messages. We&#039;ve been in countless situations where just
a single added field to a log statement that was already there would&#039;ve saved us
hours. The `WithFields` call is optional.

In general, with Logrus using any of the `printf`-family functions should be
seen as a hint you should add a field, however, you can still use the
`printf`-family functions with Logrus.

#### Default Fields

Often it&#039;s helpful to have fields _always_ attached to log statements in an
application or parts of one. For example, you may want to always log the
`request_id` and `user_ip` in the context of a request. Instead of writing
`logger.WithFields(logrus.Fields{&quot;request_id&quot;: request_id, &quot;user_ip&quot;: user_ip})` on
every line, you can create a `logrus.Entry` to pass around instead:

```go
requestLogger := logger.WithFields(logrus.Fields{&quot;request_id&quot;: request_id, &quot;user_ip&quot;: user_ip})
requestLogger.Info(&quot;something happened on that request&quot;) // will log request_id and user_ip
requestLogger.Warn(&quot;something not great happened&quot;)
```

#### Hooks

You can add hooks for logging levels. For example to send errors to an exception
tracking service on `Error`, `Fatal` and `Panic`, info to StatsD or log to
multiple places simultaneously, e.g. syslog.

Logrus comes with [built-in hooks](hooks/). Add those, or your custom hook, in
`init`:

```go
package main

import (
  &quot;log/syslog&quot;

  &quot;github.com/sirupsen/logrus&quot;
  airbrake &quot;gopkg.in/gemnasium/logrus-airbrake-hook.v2&quot;
  logrus_syslog &quot;github.com/sirupsen/logrus/hooks/syslog&quot;
)

func init() {

  // Use the Airbrake hook to report errors that have Error severity or above to
  // an exception tracker. You can create custom hooks, see the Hooks section.
  logrus.AddHook(airbrake.NewHook(123, &quot;xyz&quot;, &quot;production&quot;))

  hook, err := logrus_syslog.NewSyslogHook(&quot;udp&quot;, &quot;localhost:514&quot;, syslog.LOG_INFO, &quot;&quot;)
  if err != nil {
    logrus.Error(&quot;Unable to connect to local syslog daemon&quot;)
  } else {
    logrus.AddHook(hook)
  }
}
```
Note: Syslog hooks also support connecting to local syslog (Ex. &quot;/dev/log&quot; or &quot;/var/run/syslog&quot; or &quot;/var/run/log&quot;). For the detail, please check the [syslog hook README](hooks/syslog/README.md).

A list of currently known service hooks can be found in this wiki [page](https://github.com/sirupsen/logrus/wiki/Hooks)


#### Level logging

Logrus has seven logging levels: Trace, Debug, Info, Warning, Error, Fatal and Panic.

```go
logrus.Trace(&quot;Something very low level.&quot;)
logrus.Debug(&quot;Useful debugging information.&quot;)
logrus.Info(&quot;Something noteworthy happened!&quot;)
logrus.Warn(&quot;You should probably take a look at this.&quot;)
logrus.Error(&quot;Something failed but I&#039;m not quitting.&quot;)
// Calls os.Exit(1) after logging
logrus.Fatal(&quot;Bye.&quot;)
// Calls panic() after logging
logrus.Panic(&quot;I&#039;m bailing.&quot;)
```

You can set the logging level on a `Logger`, then it will only log entries with
that severity or anything above it:

```go
// Will log anything that is info or above (warn, error, fatal, panic). Default.
logrus.SetLevel(logrus.InfoLevel)
```

It may be useful to set `logrus.Level = logrus.DebugLevel` in a debug or verbose
environment if your application has that.

Note: If you want different log levels for global (`logrus.SetLevel(...)`) and syslog logging, please check the [syslog hook README](hooks/syslog/README.md#different-log-levels-for-local-and-remote-logging).

#### Entries

Besides the fields added with `WithField` or `WithFields` some fields are
automatically added to all logging events:

1. `time`. The timestamp when the entry was created.
2. `msg`. The logging message passed to `{Info,Warn,Error,Fatal,Panic}` after
   the `AddFields` call. E.g. `Failed to send event.`
3. `level`. The logging level. E.g. `info`.

#### Environments

Logrus has no notion of environment.

If you wish for hooks and formatters to only be used in specific environments,
you should handle that yourself. For example, if your application has a global
variable `Environment`, which is a string representation of the environment you
could do:

```go
import (
  &quot;github.com/sirupsen/logrus&quot;
)

func init() {
  // do something here to set environment depending on an environment variable
  // or command-line flag
  if Environment == &quot;production&quot; {
    logrus.SetFormatter(&amp;logrus.JSONFormatter{})
  } else {
    // The TextFormatter is default, you don&#039;t actually have to do this.
    logrus.SetFormatter(&amp;logrus.TextFormatter{})
  }
}
```

This configuration is how `logrus` was intended to be used, but JSON in
production is mostly only useful if you do log aggregation with tools like
Splunk or Logstash.

#### Formatters

The built-in logging formatters are:

* `logrus.TextFormatter`. Logs the event in colors if stdout is a tty, otherwise
  without colors.
  * *Note:* to force colored output when there is no TTY, set the `ForceColors`
    field to `true`.  To force no colored output even if there is a TTY  set the
    `DisableColors` field to `true`. For Windows, see
    [github.com/mattn/go-colorable](https://github.com/mattn/go-colorable).
  * When colors are enabled, levels are truncated to 4 characters by default. To disable
    truncation set the `DisableLevelTruncation` field to `true`.
  * When outputting to a TTY, it&#039;s often helpful to visually scan down a column where all the levels are the same width. Setting the `PadLevelText` field to `true` enables this behavior, by adding padding to the level text.
  * All options are listed in the [generated docs](https://pkg.go.dev/github.com/sirupsen/logrus#TextFormatter).
* `logrus.JSONFormatter`. Logs fields as JSON.
  * All options are listed in the [generated docs](https://pkg.go.dev/github.com/sirupsen/logrus#JSONFormatter).

Third-party logging formatters:

* [`FluentdFormatter`](https://github.com/joonix/log). Formats entries that can be parsed by Kubernetes and Google Container Engine.
* [`GELF`](https://github.com/fabienm/go-logrus-formatters). Formats entries so they comply to Graylog&#039;s [GELF 1.1 specification](http://docs.graylog.org/en/2.4/pages/gelf.html).
* [`logstash`](https://github.com/bshuster-repo/logrus-logstash-hook). Logs fields as [Logstash](http://logstash.net) Events.
* [`prefixed`](https://github.com/x-cray/logrus-prefixed-formatter). Displays log entry source along with alternative layout.
* [`zalgo`](https://github.com/aybabtme/logzalgo). Invoking the Power of Zalgo.
* [`nested-logrus-formatter`](https://github.com/antonfisher/nested-logrus-formatter). Converts logrus fields to a nested structure.
* [`powerful-logrus-formatter`](https://github.com/zput/zxcTool). get fileName, log&#039;s line number and the latest function&#039;s name when print log; Save log to files.
* [`caption-json-formatter`](https://github.com/nolleh/caption_json_formatter). logrus&#039;s message json formatter with human-readable caption added.

You can define your formatter by implementing the `Formatter` interface,
requiring a `Format` method. `Format` takes an `*Entry`. `entry.Data` is a
`Fields` type (`map[string]interface{}`) with all your fields as well as the
default ones (see Entries section above):

```go
type MyJSONFormatter struct{}

logrus.SetFormatter(new(MyJSONFormatter))

func (f *MyJSONFormatter) Format(entry *Entry) ([]byte, error) {
  // Note this doesn&#039;t include Time, Level and Message which are available on
  // the Entry. Consult `godoc` on information about those fields or read the
  // source of the official loggers.
  serialized, err := json.Marshal(entry.Data)
    if err != nil {
      return nil, fmt.Errorf(&quot;Failed to marshal fields to JSON, %w&quot;, err)
    }
  return append(serialized, &#039;\n&#039;), nil
}
```

#### Logger as an `io.Writer`

Logrus can be transformed into an `io.Writer`. That writer is the end of an `io.Pipe` and it is your responsibility to close it.

```go
w := logger.Writer()
defer w.Close()

srv := http.Server{
    // create a stdlib log.Logger that writes to
    // logrus.Logger.
    ErrorLog: log.New(w, &quot;&quot;, 0),
}
```

Each line written to that writer will be printed the usual way, using formatters
and hooks. The level for those entries is `info`.

This means that we can override the standard library logger easily:

```go
logger := logrus.New()
logger.Formatter = &amp;logrus.JSONFormatter{}

// Use logrus for standard log output
// Note that `log` here references stdlib&#039;s log
// Not logrus imported under the name `log`.
log.SetOutput(logger.Writer())
```

#### Rotation

Log rotation is not provided with Logrus. Log rotation should be done by an
external program (like `logrotate(8)`) that can compress and delete old log
entries. It should not be a feature of the application-level logger.

#### Tools

| Tool | Description |
| ---- | ----------- |
|[Logrus Mate](https://github.com/gogap/logrus_mate)|Logrus mate is a tool for Logrus to manage loggers, you can initial logger&#039;s level, hook and formatter by config file, the logger will be generated with different configs in different environments.|
|[Logrus Viper Helper](https://github.com/heirko/go-contrib/tree/master/logrusHelper)|An Helper around Logrus to wrap with spf13/Viper to load configuration with fangs! And to simplify Logrus configuration use some behavior of [Logrus Mate](https://github.com/gogap/logrus_mate). [sample](https://github.com/heirko/iris-contrib/blob/master/middleware/logrus-logger/example) |

#### Testing

Logrus has a built-in facility for asserting the presence of log messages. This is implemented through the `test` hook and provides:

* decorators for existing logger (`test.NewLocal` and `test.NewGlobal`) which basically just adds the `test` hook
* a test logger (`test.NewNullLogger`) that just records log messages (and does not output any):

```go
import(
  &quot;testing&quot;

  &quot;github.com/sirupsen/logrus&quot;
  &quot;github.com/sirupsen/logrus/hooks/test&quot;
  &quot;github.com/stretchr/testify/assert&quot;
)

func TestSomething(t*testing.T){
  logger, hook := test.NewNullLogger()
  logger.Error(&quot;Helloerror&quot;)

  assert.Equal(t, 1, len(hook.Entries))
  assert.Equal(t, logrus.ErrorLevel, hook.LastEntry().Level)
  assert.Equal(t, &quot;Helloerror&quot;, hook.LastEntry().Message)

  hook.Reset()
  assert.Nil(t, hook.LastEntry())
}
```

#### Fatal handlers

Logrus can register one or more functions that will be called when any `fatal`
level message is logged. The registered handlers will be executed before
logrus performs an `os.Exit(1)`. This behavior may be helpful if callers need
to gracefully shut down. Unlike a `panic(&quot;Something went wrong...&quot;)` call which can be intercepted with a deferred `recover` a call to `os.Exit(1)` can not be intercepted.

```go
// ...
handler := func() {
  // gracefully shut down something...
}
logrus.RegisterExitHandler(handler)
// ...
```

#### Thread safety

By default, Logger is protected by a mutex for concurrent writes. The mutex is held when calling hooks and writing logs.
If you are sure such locking is not needed, you can call logger.SetNoLock() to disable the locking.

Situations when locking is not needed include:

* You have no hooks registered, or hooks calling is already thread-safe.

* Writing to logger.Out is already thread-safe, for example:

  1) logger.Out is protected by locks.

  2) logger.Out is an os.File handler opened with `O_APPEND` flag, and every write is smaller than 4k. (This allows multi-thread/multi-process writing)

     (Refer to http://www.notthewizard.com/2014/06/17/are-files-appends-really-atomic/)
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[docker/cli]]></title>
            <link>https://github.com/docker/cli</link>
            <guid>https://github.com/docker/cli</guid>
            <pubDate>Thu, 25 Dec 2025 00:05:18 GMT</pubDate>
            <description><![CDATA[The Docker CLI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/docker/cli">docker/cli</a></h1>
            <p>The Docker CLI</p>
            <p>Language: Go</p>
            <p>Stars: 5,624</p>
            <p>Forks: 2,073</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre># Docker CLI

[![PkgGoDev](https://pkg.go.dev/badge/github.com/docker/cli)](https://pkg.go.dev/github.com/docker/cli)
[![Build Status](https://img.shields.io/github/actions/workflow/status/docker/cli/build.yml?branch=master&amp;label=build&amp;logo=github)](https://github.com/docker/cli/actions?query=workflow%3Abuild)
[![Test Status](https://img.shields.io/github/actions/workflow/status/docker/cli/test.yml?branch=master&amp;label=test&amp;logo=github)](https://github.com/docker/cli/actions?query=workflow%3Atest)
[![Go Report Card](https://goreportcard.com/badge/github.com/docker/cli)](https://goreportcard.com/report/github.com/docker/cli)
[![OpenSSF Scorecard](https://api.scorecard.dev/projects/github.com/docker/cli/badge)](https://scorecard.dev/viewer/?uri=github.com/docker/cli)
[![Codecov](https://img.shields.io/codecov/c/github/docker/cli?logo=codecov)](https://codecov.io/gh/docker/cli)

## About

This repository is the home of the Docker CLI.

## Development

`docker/cli` is developed using Docker.

Build CLI from source:

```shell
docker buildx bake
```

Build binaries for all supported platforms:

```shell
docker buildx bake cross
```

Build for a specific platform:

```shell
docker buildx bake --set binary.platform=linux/arm64 
```

Build dynamic binary for glibc or musl:

```shell
USE_GLIBC=1 docker buildx bake dynbinary 
```

Run all linting:

```shell
docker buildx bake lint shellcheck
```

Run test:

```shell
docker buildx bake test
```

List all the available targets:

```shell
make help
```

### In-container development environment

Start an interactive development environment:

```shell
make -f docker.Makefile shell
```

## Legal

*Brought to you courtesy of our legal counsel. For more context,
see the [NOTICE](https://github.com/docker/cli/blob/master/NOTICE) document in this repo.*

Use and transfer of Docker may be subject to certain restrictions by the
United States and other governments.

It is your responsibility to ensure that your use and/or transfer does not
violate applicable laws.

For more information, see https://www.bis.doc.gov

## Licensing

docker/cli is licensed under the Apache License, Version 2.0. See
[LICENSE](https://github.com/docker/docker/blob/master/LICENSE) for the full
license text.
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[go-sql-driver/mysql]]></title>
            <link>https://github.com/go-sql-driver/mysql</link>
            <guid>https://github.com/go-sql-driver/mysql</guid>
            <pubDate>Thu, 25 Dec 2025 00:05:17 GMT</pubDate>
            <description><![CDATA[Go MySQL Driver is a MySQL driver for Go's (golang) database/sql package]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/go-sql-driver/mysql">go-sql-driver/mysql</a></h1>
            <p>Go MySQL Driver is a MySQL driver for Go's (golang) database/sql package</p>
            <p>Language: Go</p>
            <p>Stars: 15,357</p>
            <p>Forks: 2,341</p>
            <p>Stars today: 5 stars today</p>
            <h2>README</h2><pre># Go-MySQL-Driver

[![DeepWiki](https://img.shields.io/badge/DeepWiki-go--sql--driver%2Fmysql-blue.svg?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAyCAYAAAAnWDnqAAAAAXNSR0IArs4c6QAAA05JREFUaEPtmUtyEzEQhtWTQyQLHNak2AB7ZnyXZMEjXMGeK/AIi+QuHrMnbChYY7MIh8g01fJoopFb0uhhEqqcbWTp06/uv1saEDv4O3n3dV60RfP947Mm9/SQc0ICFQgzfc4CYZoTPAswgSJCCUJUnAAoRHOAUOcATwbmVLWdGoH//PB8mnKqScAhsD0kYP3j/Yt5LPQe2KvcXmGvRHcDnpxfL2zOYJ1mFwrryWTz0advv1Ut4CJgf5uhDuDj5eUcAUoahrdY/56ebRWeraTjMt/00Sh3UDtjgHtQNHwcRGOC98BJEAEymycmYcWwOprTgcB6VZ5JK5TAJ+fXGLBm3FDAmn6oPPjR4rKCAoJCal2eAiQp2x0vxTPB3ALO2CRkwmDy5WohzBDwSEFKRwPbknEggCPB/imwrycgxX2NzoMCHhPkDwqYMr9tRcP5qNrMZHkVnOjRMWwLCcr8ohBVb1OMjxLwGCvjTikrsBOiA6fNyCrm8V1rP93iVPpwaE+gO0SsWmPiXB+jikdf6SizrT5qKasx5j8ABbHpFTx+vFXp9EnYQmLx02h1QTTrl6eDqxLnGjporxl3NL3agEvXdT0WmEost648sQOYAeJS9Q7bfUVoMGnjo4AZdUMQku50McDcMWcBPvr0SzbTAFDfvJqwLzgxwATnCgnp4wDl6Aa+Ax283gghmj+vj7feE2KBBRMW3FzOpLOADl0Isb5587h/U4gGvkt5v60Z1VLG8BhYjbzRwyQZemwAd6cCR5/XFWLYZRIMpX39AR0tjaGGiGzLVyhse5C9RKC6ai42ppWPKiBagOvaYk8lO7DajerabOZP46Lby5wKjw1HCRx7p9sVMOWGzb/vA1hwiWc6jm3MvQDTogQkiqIhJV0nBQBTU+3okKCFDy9WwferkHjtxib7t3xIUQtHxnIwtx4mpg26/HfwVNVDb4oI9RHmx5WGelRVlrtiw43zboCLaxv46AZeB3IlTkwouebTr1y2NjSpHz68WNFjHvupy3q8TFn3Hos2IAk4Ju5dCo8B3wP7VPr/FGaKiG+T+v+TQqIrOqMTL1VdWV1DdmcbO8KXBz6esmYWYKPwDL5b5FA1a0hwapHiom0r/cKaoqr+27/XcrS5UwSMbQAAAABJRU5ErkJggg==)](https://deepwiki.com/go-sql-driver/mysql)


A MySQL-Driver for Go&#039;s [database/sql](https://golang.org/pkg/database/sql/) package

![Go-MySQL-Driver logo](https://raw.github.com/wiki/go-sql-driver/mysql/gomysql_m.png &quot;Golang Gopher holding the MySQL Dolphin&quot;)

---------------------------------------
  * [Features](#features)
  * [Requirements](#requirements)
  * [Installation](#installation)
  * [Usage](#usage)
    * [DSN (Data Source Name)](#dsn-data-source-name)
      * [Password](#password)
      * [Protocol](#protocol)
      * [Address](#address)
      * [Parameters](#parameters)
      * [Examples](#examples)
    * [Connection pool and timeouts](#connection-pool-and-timeouts)
    * [context.Context Support](#contextcontext-support)
    * [ColumnType Support](#columntype-support)
    * [LOAD DATA LOCAL INFILE support](#load-data-local-infile-support)
    * [time.Time support](#timetime-support)
    * [Unicode support](#unicode-support)
  * [Testing / Development](#testing--development)
  * [License](#license)

---------------------------------------

## Features
  * Lightweight and [fast](https://github.com/go-sql-driver/sql-benchmark &quot;golang MySQL-Driver performance&quot;)
  * Native Go implementation. No C-bindings, just pure Go
  * Connections over TCP/IPv4, TCP/IPv6, Unix domain sockets or [custom protocols](https://godoc.org/github.com/go-sql-driver/mysql#DialFunc)
  * Automatic handling of broken connections
  * Automatic Connection Pooling *(by database/sql package)*
  * Supports queries larger than 16MB
  * Full [`sql.RawBytes`](https://golang.org/pkg/database/sql/#RawBytes) support.
  * Intelligent `LONG DATA` handling in prepared statements
  * Secure `LOAD DATA LOCAL INFILE` support with file allowlisting and `io.Reader` support
  * Optional `time.Time` parsing
  * Optional placeholder interpolation
  * Supports zlib compression.

## Requirements

* Go 1.22 or higher. We aim to support the 3 latest versions of Go.
* MySQL (5.7+) and MariaDB (10.5+) are supported.
* [TiDB](https://github.com/pingcap/tidb) is supported by PingCAP.
  * Do not ask questions about TiDB in our issue tracker or forum.
  * [Document](https://docs.pingcap.com/tidb/v6.1/dev-guide-sample-application-golang)
  * [Forum](https://ask.pingcap.com/)
* go-mysql would work with Percona Server, Google CloudSQL or Sphinx (2.2.3+).
  * Maintainers won&#039;t support them. Do not expect issues are investigated and resolved by maintainers.
  * Investigate issues yourself and please send a pull request to fix it.

---------------------------------------

## Installation
Simple install the package to your [$GOPATH](https://github.com/golang/go/wiki/GOPATH &quot;GOPATH&quot;) with the [go tool](https://golang.org/cmd/go/ &quot;go command&quot;) from shell:
```bash
go get -u github.com/go-sql-driver/mysql
```
Make sure [Git is installed](https://git-scm.com/downloads) on your machine and in your system&#039;s `PATH`.

## Usage
_Go MySQL Driver_ is an implementation of Go&#039;s `database/sql/driver` interface. You only need to import the driver and can use the full [`database/sql`](https://golang.org/pkg/database/sql/) API then.

Use `mysql` as `driverName` and a valid [DSN](#dsn-data-source-name)  as `dataSourceName`:

```go
import (
	&quot;database/sql&quot;
	&quot;time&quot;

	_ &quot;github.com/go-sql-driver/mysql&quot;
)

// ...

db, err := sql.Open(&quot;mysql&quot;, &quot;user:password@/dbname&quot;)
if err != nil {
	panic(err)
}
// See &quot;Important settings&quot; section.
db.SetConnMaxLifetime(time.Minute * 3)
db.SetMaxOpenConns(10)
db.SetMaxIdleConns(10)
```

[Examples are available in our Wiki](https://github.com/go-sql-driver/mysql/wiki/Examples &quot;Go-MySQL-Driver Examples&quot;).

### Important settings

`db.SetConnMaxLifetime()` is required to ensure connections are closed by the driver safely before connection is closed by MySQL server, OS, or other middlewares. Since some middlewares close idle connections by 5 minutes, we recommend timeout shorter than 5 minutes. This setting helps load balancing and changing system variables too.

`db.SetMaxOpenConns()` is highly recommended to limit the number of connection used by the application. There is no recommended limit number because it depends on application and MySQL server.

`db.SetMaxIdleConns()` is recommended to be set same to `db.SetMaxOpenConns()`. When it is smaller than `SetMaxOpenConns()`, connections can be opened and closed much more frequently than you expect. Idle connections can be closed by the `db.SetConnMaxLifetime()`. If you want to close idle connections more rapidly, you can use `db.SetConnMaxIdleTime()` since Go 1.15.


### DSN (Data Source Name)

The Data Source Name has a common format, like e.g. [PEAR DB](http://pear.php.net/manual/en/package.database.db.intro-dsn.php) uses it, but without type-prefix (optional parts marked by squared brackets):
```
[username[:password]@][protocol[(address)]]/dbname[?param1=value1&amp;...&amp;paramN=valueN]
```

A DSN in its fullest form:
```
username:password@protocol(address)/dbname?param=value
```

Except for the databasename, all values are optional. So the minimal DSN is:
```
/dbname
```

If you do not want to preselect a database, leave `dbname` empty:
```
/
```
This has the same effect as an empty DSN string:
```

```

`dbname` is escaped by [PathEscape()](https://pkg.go.dev/net/url#PathEscape) since v1.8.0. If your database name is `dbname/withslash`, it becomes:

```
/dbname%2Fwithslash
```

Alternatively, [Config.FormatDSN](https://godoc.org/github.com/go-sql-driver/mysql#Config.FormatDSN) can be used to create a DSN string by filling a struct.

#### Password
Passwords can consist of any character. Escaping is **not** necessary.

#### Protocol
See [net.Dial](https://golang.org/pkg/net/#Dial) for more information which networks are available.
In general you should use a Unix domain socket if available and TCP otherwise for best performance.

#### Address
For TCP and UDP networks, addresses have the form `host[:port]`.
If `port` is omitted, the default port will be used.
If `host` is a literal IPv6 address, it must be enclosed in square brackets.
The functions [net.JoinHostPort](https://golang.org/pkg/net/#JoinHostPort) and [net.SplitHostPort](https://golang.org/pkg/net/#SplitHostPort) manipulate addresses in this form.

For Unix domain sockets the address is the absolute path to the MySQL-Server-socket, e.g. `/var/run/mysqld/mysqld.sock` or `/tmp/mysql.sock`.

#### Parameters
*Parameters are case-sensitive!*

Notice that any of `true`, `TRUE`, `True` or `1` is accepted to stand for a true boolean value. Not surprisingly, false can be specified as any of: `false`, `FALSE`, `False` or `0`.

##### `allowAllFiles`

```
Type:           bool
Valid Values:   true, false
Default:        false
```

`allowAllFiles=true` disables the file allowlist for `LOAD DATA LOCAL INFILE` and allows *all* files.
[*Might be insecure!*](https://dev.mysql.com/doc/refman/8.0/en/load-data.html#load-data-local)

##### `allowCleartextPasswords`

```
Type:           bool
Valid Values:   true, false
Default:        false
```

`allowCleartextPasswords=true` allows using the [cleartext client side plugin](https://dev.mysql.com/doc/en/cleartext-pluggable-authentication.html) if required by an account, such as one defined with the [PAM authentication plugin](http://dev.mysql.com/doc/en/pam-authentication-plugin.html). Sending passwords in clear text may be a security problem in some configurations. To avoid problems if there is any possibility that the password would be intercepted, clients should connect to MySQL Server using a method that protects the password. Possibilities include [TLS / SSL](#tls), IPsec, or a private network.


##### `allowFallbackToPlaintext`

```
Type:           bool
Valid Values:   true, false
Default:        false
```

`allowFallbackToPlaintext=true` acts like a `--ssl-mode=PREFERRED` MySQL client as described in [Command Options for Connecting to the Server](https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-mode)

##### `allowNativePasswords`

```
Type:           bool
Valid Values:   true, false
Default:        true
```
`allowNativePasswords=false` disallows the usage of MySQL native password method.

##### `allowOldPasswords`

```
Type:           bool
Valid Values:   true, false
Default:        false
```
`allowOldPasswords=true` allows the usage of the insecure old password method. This should be avoided, but is necessary in some cases. See also [the old_passwords wiki page](https://github.com/go-sql-driver/mysql/wiki/old_passwords).

##### `charset`

```
Type:           string
Valid Values:   &lt;name&gt;
Default:        none
```

Sets the charset used for client-server interaction (`&quot;SET NAMES &lt;value&gt;&quot;`). If multiple charsets are set (separated by a comma), the following charset is used if setting the charset fails. This enables for example support for `utf8mb4` ([introduced in MySQL 5.5.3](http://dev.mysql.com/doc/refman/5.5/en/charset-unicode-utf8mb4.html)) with fallback to `utf8` for older servers (`charset=utf8mb4,utf8`).

See also [Unicode Support](#unicode-support).

##### `checkConnLiveness`

```
Type:           bool
Valid Values:   true, false
Default:        true
```

On supported platforms connections retrieved from the connection pool are checked for liveness before using them. If the check fails, the respective connection is marked as bad and the query retried with another connection.
`checkConnLiveness=false` disables this liveness check of connections.

##### `collation`

```
Type:           string
Valid Values:   &lt;name&gt;
Default:        utf8mb4_general_ci
```

Sets the collation used for client-server interaction on connection. In contrast to `charset`, `collation` does not issue additional queries. If the specified collation is unavailable on the target server, the connection will fail.

A list of valid charsets for a server is retrievable with `SHOW COLLATION`.

The default collation (`utf8mb4_general_ci`) is supported from MySQL 5.5.  You should use an older collation (e.g. `utf8_general_ci`) for older MySQL.

Collations for charset &quot;ucs2&quot;, &quot;utf16&quot;, &quot;utf16le&quot;, and &quot;utf32&quot; can not be used ([ref](https://dev.mysql.com/doc/refman/5.7/en/charset-connection.html#charset-connection-impermissible-client-charset)).

See also [Unicode Support](#unicode-support).

##### `clientFoundRows`

```
Type:           bool
Valid Values:   true, false
Default:        false
```

`clientFoundRows=true` causes an UPDATE to return the number of matching rows instead of the number of rows changed.

##### `columnsWithAlias`

```
Type:           bool
Valid Values:   true, false
Default:        false
```

When `columnsWithAlias` is true, calls to `sql.Rows.Columns()` will return the table alias and the column name separated by a dot. For example:

```
SELECT u.id FROM users as u
```

will return `u.id` instead of just `id` if `columnsWithAlias=true`.

##### `compress`

```
Type:           bool
Valid Values:   true, false
Default:        false
```

Toggles zlib compression. false by default.

##### `interpolateParams`

```
Type:           bool
Valid Values:   true, false
Default:        false
```

If `interpolateParams` is true, placeholders (`?`) in calls to `db.Query()` and `db.Exec()` are interpolated into a single query string with given parameters. This reduces the number of roundtrips, since the driver has to prepare a statement, execute it with given parameters and close the statement again with `interpolateParams=false`.

*This can not be used together with the multibyte encodings BIG5, CP932, GB2312, GBK or SJIS. These are rejected as they may [introduce a SQL injection vulnerability](http://stackoverflow.com/a/12118602/3430118)!*

##### `loc`

```
Type:           string
Valid Values:   &lt;escaped name&gt;
Default:        UTC
```

Sets the location for time.Time values (when using `parseTime=true`). *&quot;Local&quot;* sets the system&#039;s location. See [time.LoadLocation](https://golang.org/pkg/time/#LoadLocation) for details.

Note that this sets the location for time.Time values but does not change MySQL&#039;s [time_zone setting](https://dev.mysql.com/doc/refman/5.5/en/time-zone-support.html). For that see the [time_zone system variable](#system-variables), which can also be set as a DSN parameter.

Please keep in mind, that param values must be [url.QueryEscape](https://golang.org/pkg/net/url/#QueryEscape)&#039;ed. Alternatively you can manually replace the `/` with `%2F`. For example `US/Pacific` would be `loc=US%2FPacific`.

##### `timeTruncate`

```
Type:           duration
Default:        0
```

[Truncate time values](https://pkg.go.dev/time#Duration.Truncate) to the specified duration. The value must be a decimal number with a unit suffix (*&quot;ms&quot;*, *&quot;s&quot;*, *&quot;m&quot;*, *&quot;h&quot;*), such as *&quot;30s&quot;*, *&quot;0.5m&quot;* or *&quot;1m30s&quot;*.

##### `maxAllowedPacket`
```
Type:          decimal number
Default:       64*1024*1024
```

Max packet size allowed in bytes. The default value is 64 MiB and should be adjusted to match the server settings. `maxAllowedPacket=0` can be used to automatically fetch the `max_allowed_packet` variable from server *on every connection*.

##### `multiStatements`

```
Type:           bool
Valid Values:   true, false
Default:        false
```

Allow multiple statements in one query. This can be used to bach multiple queries. Use [Rows.NextResultSet()](https://pkg.go.dev/database/sql#Rows.NextResultSet) to get result of the second and subsequent queries.

When `multiStatements` is used, `?` parameters must only be used in the first statement. [interpolateParams](#interpolateparams) can be used to avoid this limitation unless prepared statement is used explicitly.

It&#039;s possible to access the last inserted ID and number of affected rows for multiple statements by using `sql.Conn.Raw()` and the `mysql.Result`. For example:

```go
conn, _ := db.Conn(ctx)
conn.Raw(func(conn any) error {
  ex := conn.(driver.Execer)
  res, err := ex.Exec(`
  UPDATE point SET x = 1 WHERE y = 2;
  UPDATE point SET x = 2 WHERE y = 3;
  `, nil)
  // Both slices have 2 elements.
  log.Print(res.(mysql.Result).AllRowsAffected())
  log.Print(res.(mysql.Result).AllLastInsertIds())
})
```

##### `parseTime`

```
Type:           bool
Valid Values:   true, false
Default:        false
```

`parseTime=true` changes the output type of `DATE` and `DATETIME` values to `time.Time` instead of `[]byte` / `string`
The date or datetime like `0000-00-00 00:00:00` is converted into zero value of `time.Time`.


##### `readTimeout`

```
Type:           duration
Default:        0
```

I/O read timeout. The value must be a decimal number with a unit suffix (*&quot;ms&quot;*, *&quot;s&quot;*, *&quot;m&quot;*, *&quot;h&quot;*), such as *&quot;30s&quot;*, *&quot;0.5m&quot;* or *&quot;1m30s&quot;*.

##### `rejectReadOnly`

```
Type:           bool
Valid Values:   true, false
Default:        false
```


`rejectReadOnly=true` causes the driver to reject read-only connections. This
is for a possible race condition during an automatic failover, where the mysql
client gets connected to a read-only replica after the failover.

Note that this should be a fairly rare case, as an automatic failover normally
happens when the primary is down, and the race condition shouldn&#039;t happen
unless it comes back up online as soon as the failover is kicked off. On the
other hand, when this happens, a MySQL application can get stuck on a
read-only connection until restarted. It is however fairly easy to reproduce,
for example, using a manual failover on AWS Aurora&#039;s MySQL-compatible cluster.

If you are not relying on read-only transactions to reject writes that aren&#039;t
supposed to happen, setting this on some MySQL providers (such as AWS Aurora)
is safer for failovers.

Note that ERROR 1290 can be returned for a `read-only` server and this option will
cause a retry for that error. However the same error number is used for some
other cases. You should ensure your application will never cause an ERROR 1290
except for `read-only` mode when enabling this option.


##### `serverPubKey`

```
Type:           string
Valid Values:   &lt;name&gt;
Default:        none
```

Server public keys can be registered with [`mysql.RegisterServerPubKey`](https://godoc.org/github.com/go-sql-driver/mysql#RegisterServerPubKey), which can then be used by the assigned name in the DSN.
Public keys are used to transmit encrypted data, e.g. for authentication.
If the server&#039;s public key is known, it should be set manually to avoid expensive and potentially insecure transmissions of the public key from the server to the client each time it is required.


##### `timeout`

```
Type:           duration
Default:        OS default
```

Timeout for establishing connections, aka dial timeout. The value must be a decimal number with a unit suffix (*&quot;ms&quot;*, *&quot;s&quot;*, *&quot;m&quot;*, *&quot;h&quot;*), such as *&quot;30s&quot;*, *&quot;0.5m&quot;* or *&quot;1m30s&quot;*.


##### `tls`

```
Type:           bool / string
Valid Values:   true, false, skip-verify, preferred, &lt;name&gt;
Default:        false
```

`tls=true` enables TLS / SSL encrypted connection to the server. Use `skip-verify` if you want to use a self-signed or invalid certificate (server side) or use `preferred` to use TLS only when advertised by the server. This is similar to `skip-verify`, but additionally allows a fallback to a connection which is not encrypted. Neither `skip-verify` nor `preferred` add any reliable security. You can use a custom TLS config after registering it with [`mysql.RegisterTLSConfig`](https://godoc.org/github.com/go-sql-driver/mysql#RegisterTLSConfig).


##### `writeTimeout`

```
Type:           duration
Default:        0
```

I/O write timeout. The value must be a decimal number with a unit suffix (*&quot;ms&quot;*, *&quot;s&quot;*, *&quot;m&quot;*, *&quot;h&quot;*), such as *&quot;30s&quot;*, *&quot;0.5m&quot;* or *&quot;1m30s&quot;*.

##### `connectionAttributes`

```
Type:           comma-delimited string of user-defined &quot;key:value&quot; pairs
Valid Values:   (&lt;name1&gt;:&lt;value1&gt;,&lt;name2&gt;:&lt;value2&gt;,...)
Default:        none
```

[Connection attributes](https://dev.mysql.com/doc/refman/8.0/en/performance-schema-connection-attribute-tables.html) are key-value pairs that application programs can pass to the server at connect time.

##### System Variables

Any other parameters are interpreted as system variables:
  * `&lt;boolean_var&gt;=&lt;value&gt;`: `SET &lt;boolean_var&gt;=&lt;value&gt;`
  * `&lt;enum_var&gt;=&lt;value&gt;`: `SET &lt;enum_var&gt;=&lt;value&gt;`
  * `&lt;string_var&gt;=%27&lt;value&gt;%27`: `SET &lt;string_var&gt;=&#039;&lt;value&gt;&#039;`

Rules:
* The values for string variables must be quoted with `&#039;`.
* The values must also be [url.QueryEscape](http://golang.org/pkg/net/url/#QueryEscape)&#039;ed!
 (which implies values of string variables must be wrapped with `%27`).

Examples:
  * `autocommit=1`: `SET autocommit=1`
  * [`time_zone=%27Europe%2FParis%27`](https://dev.mysql.com/doc/refman/5.5/

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[cloudwego/eino]]></title>
            <link>https://github.com/cloudwego/eino</link>
            <guid>https://github.com/cloudwego/eino</guid>
            <pubDate>Thu, 25 Dec 2025 00:05:16 GMT</pubDate>
            <description><![CDATA[The ultimate LLM/AI application development framework in Golang.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/cloudwego/eino">cloudwego/eino</a></h1>
            <p>The ultimate LLM/AI application development framework in Golang.</p>
            <p>Language: Go</p>
            <p>Stars: 8,765</p>
            <p>Forks: 682</p>
            <p>Stars today: 21 stars today</p>
            <h2>README</h2><pre># Eino

![coverage](https://raw.githubusercontent.com/cloudwego/eino/badges/.badges/main/coverage.svg)
[![Release](https://img.shields.io/github/v/release/cloudwego/eino)](https://github.com/cloudwego/eino/releases)
[![WebSite](https://img.shields.io/website?up_message=cloudwego&amp;url=https%3A%2F%2Fwww.cloudwego.io%2F)](https://www.cloudwego.io/)
[![License](https://img.shields.io/github/license/cloudwego/eino)](https://github.com/cloudwego/eino/blob/main/LICENSE)
[![Go Report Card](https://goreportcard.com/badge/github.com/cloudwego/eino)](https://goreportcard.com/report/github.com/cloudwego/eino)
[![OpenIssue](https://img.shields.io/github/issues/cloudwego/eino)](https://github.com/cloudwego/kitex/eino)
[![ClosedIssue](https://img.shields.io/github/issues-closed/cloudwego/eino)](https://github.com/cloudwego/eino/issues?q=is%3Aissue+is%3Aclosed)
![Stars](https://img.shields.io/github/stars/cloudwego/eino)
![Forks](https://img.shields.io/github/forks/cloudwego/eino)

English | [‰∏≠Êñá](README.zh_CN.md)

# Overview

**Eino[&#039;aino]** (pronounced similarly to &quot;I know&quot;) aims to be the ultimate LLM application development framework in Golang. Drawing inspirations from many excellent LLM application development frameworks in the open-source community such as LangChain &amp; LlamaIndex, etc., as well as learning from cutting-edge research and real world applications, Eino offers an LLM application development framework that emphasizes on simplicity, scalability, reliability and effectiveness that better aligns with Golang programming conventions.

What Eino provides are:
- a carefully curated list of **component** abstractions and implementations that can be easily reused and combined to build LLM applications
- a powerful **composition** framework that does the heavy lifting of strong type checking, stream processing, concurrency management, aspect injection, option assignment, etc. for the user.
- a set of meticulously designed **API** that obsesses on simplicity and clarity.
- an ever-growing collection of best practices in the form of bundled **flows** and **examples**.
- a useful set of tools that covers the entire development cycle, from visualized development and debugging to online tracing and evaluation.

With the above arsenal, Eino can standardize, simplify, and improve efficiency at different stages of the AI application development cycle:
![](.github/static/img/eino/eino_concept.jpeg)

# A quick walkthrough

Use a component directly:
```Go
model, _ := openai.NewChatModel(ctx, config) // create an invokable LLM instance
message, _ := model.Generate(ctx, []*Message{
    SystemMessage(&quot;you are a helpful assistant.&quot;),
    UserMessage(&quot;what does the future AI App look like?&quot;)})
```

Of course, you can do that, Eino provides lots of useful components to use out of the box. But you can do more by using orchestration, for three reasons:
- orchestration encapsulates common patterns of LLM application.
- orchestration solves the difficult problem of processing stream response by the LLM.
- orchestration handles type safety, concurrency management, aspect injection and option assignment for you.

Eino provides three set of APIs for orchestration

| API      | Characteristics and usage                                             |
| -------- |-----------------------------------------------------------------------|
| Chain    | Simple chained directed graph that can only go forward.               |
| Graph    | Cyclic or Acyclic directed graph. Powerful and flexible.              |
| Workflow | Acyclic graph that supports data mapping at struct field level. |

Let&#039;s create a simple chain: a ChatTemplate followed by a ChatModel.

![](.github/static/img/eino/simple_chain.png)

```Go
chain, _ := NewChain[map[string]any, *Message]().
           AppendChatTemplate(prompt).
           AppendChatModel(model).
           Compile(ctx)

chain.Invoke(ctx, map[string]any{&quot;query&quot;: &quot;what&#039;s your name?&quot;})
```

Now let&#039;s create a graph that uses a ChatModel to generate answer or tool calls, then uses a ToolsNode to execute those tools if needed.

![](.github/static/img/eino/tool_call_graph.png)

```Go
graph := NewGraph[map[string]any, *schema.Message]()

_ = graph.AddChatTemplateNode(&quot;node_template&quot;, chatTpl)
_ = graph.AddChatModelNode(&quot;node_model&quot;, chatModel)
_ = graph.AddToolsNode(&quot;node_tools&quot;, toolsNode)
_ = graph.AddLambdaNode(&quot;node_converter&quot;, takeOne)

_ = graph.AddEdge(START, &quot;node_template&quot;)
_ = graph.AddEdge(&quot;node_template&quot;, &quot;node_model&quot;)
_ = graph.AddBranch(&quot;node_model&quot;, branch)
_ = graph.AddEdge(&quot;node_tools&quot;, &quot;node_converter&quot;)
_ = graph.AddEdge(&quot;node_converter&quot;, END)

compiledGraph, err := graph.Compile(ctx)
if err != nil {
return err
}
out, err := compiledGraph.Invoke(ctx, map[string]any{&quot;query&quot;:&quot;Beijing&#039;s weather this weekend&quot;})
```

Now let&#039;s create a workflow that flexibly maps input &amp; output at the field level:

![](.github/static/img/eino/simple_workflow.png)

```Go
type Input1 struct {
    Input string
}

type Output1 struct {
    Output string
}

type Input2 struct {
    Role schema.RoleType
}

type Output2 struct {
    Output string
}

type Input3 struct {
    Query string
    MetaData string
}

var (
    ctx context.Context
    m model.BaseChatModel
    lambda1 func(context.Context, Input1) (Output1, error)
    lambda2 func(context.Context, Input2) (Output2, error)
    lambda3 func(context.Context, Input3) (*schema.Message, error)
)

wf := NewWorkflow[[]*schema.Message, *schema.Message]()
wf.AddChatModelNode(&quot;model&quot;, m).AddInput(START)
wf.AddLambdaNode(&quot;lambda1&quot;, InvokableLambda(lambda1)).
    AddInput(&quot;model&quot;, MapFields(&quot;Content&quot;, &quot;Input&quot;))
wf.AddLambdaNode(&quot;lambda2&quot;, InvokableLambda(lambda2)).
    AddInput(&quot;model&quot;, MapFields(&quot;Role&quot;, &quot;Role&quot;))
wf.AddLambdaNode(&quot;lambda3&quot;, InvokableLambda(lambda3)).
    AddInput(&quot;lambda1&quot;, MapFields(&quot;Output&quot;, &quot;Query&quot;)).
    AddInput(&quot;lambda2&quot;, MapFields(&quot;Output&quot;, &quot;MetaData&quot;))
wf.End().AddInput(&quot;lambda3&quot;)
runnable, err := wf.Compile(ctx)
if err != nil {
    return err
}
our, err := runnable.Invoke(ctx, []*schema.Message{
    schema.UserMessage(&quot;kick start this workflow!&quot;),
})
```

Now let&#039;s create a &#039;ReAct&#039; agent: A ChatModel binds to Tools. It receives input Messages and decides independently whether to call the Tool or output the final result. The execution result of the Tool will again become the input Message for the ChatModel and serve as the context for the next round of independent judgment.

![](.github/static/img/eino/react.png)

We provide a complete implementation for ReAct Agent out of the box in the `flow` package. Check out the code here: [flow/agent/react](https://github.com/cloudwego/eino/blob/main/flow/agent/react/react.go)

Our implementation of ReAct Agent uses Eino&#039;s **graph orchestration** exclusively, which provides the following benefits out of the box:
- Type checking: it makes sure the two nodes&#039; input and output types match at compile time.
- Stream processing: concatenates message stream before passing to chatModel and toolsNode if needed, and copies the stream into callback handlers.
- Concurrency management: the shared state can be safely read and written because the StatePreHandler is concurrency safe.
- Aspect injection: injects callback aspects before and after the execution of ChatModel if the specified ChatModel implementation hasn&#039;t injected itself.
- Option assignment: call options are assigned either globally, to specific component type or to specific node.

For example, you could easily extend the compiled graph with callbacks:
```Go
handler := NewHandlerBuilder().
  OnStartFn(
    func(ctx context.Context, info *RunInfo, input CallbackInput) context.Context) {
        log.Infof(&quot;onStart, runInfo: %v, input: %v&quot;, info, input)
    }).
  OnEndFn(
    func(ctx context.Context, info *RunInfo, output CallbackOutput) context.Context) {
        log.Infof(&quot;onEnd, runInfo: %v, out: %v&quot;, info, output)
    }).
  Build()
  
compiledGraph.Invoke(ctx, input, WithCallbacks(handler))
```

or you could easily assign options to different nodes:
```Go
// assign to All nodes
compiledGraph.Invoke(ctx, input, WithCallbacks(handler))

// assign only to ChatModel nodes
compiledGraph.Invoke(ctx, input, WithChatModelOption(WithTemperature(0.5))

// assign only to node_1
compiledGraph.Invoke(ctx, input, WithCallbacks(handler).DesignateNode(&quot;node_1&quot;))
```

# Key Features

## Rich Components

- Encapsulates common building blocks into **component abstractions**, each have multiple **component implementations** that are ready to be used out of the box.
    - component abstractions such as ChatModel, Tool, ChatTemplate, Retriever, Document Loader, Lambda, etc.
    - each component type has an interface of its own: defined Input &amp; Output Type, defined Option type, and streaming paradigms that make sense.
    - implementations are transparent. Abstractions are all you care about when orchestrating components together.

- Implementations can be nested and captures complex business logic.
    - ReAct Agent, MultiQueryRetriever, Host MultiAgent, etc. They consist of multiple components and non-trivial business logic.
    - They are still transparent from the outside. A MultiQueryRetriever can be used anywhere that accepts a Retriever.

## Powerful Orchestration

- Data flows from Retriever / Document Loaders / ChatTemplate to ChatModel, then flows to Tools and parsed as Final Answer. This directed, controlled flow of data through multiple components can be implemented through **graph orchestration**.
- Component instances are graph nodes, and edges are data flow channels.
- Graph orchestration is powerful and flexible enough to implement complex business logic:
  - type checking, stream processing, concurrency management, aspect injection and option assignment are handled by the framework.
  - branch out execution at runtime, read and write global state, or do field level data mapping using workflow(currently in alpha stage).


## Complete Stream Processing

- Stream processing is important because ChatModel outputs chunks of messages in real time as it generates them. It&#039;s especially important with orchestration because more components need to handle streaming data.
- Eino automatically **concatenates** stream chunks for downstream nodes that only accepts non-stream input, such as ToolsNode.
- Eino automatically **boxes** non stream into stream when stream is needed during graph execution.  
- Eino automatically **merges** multiple streams as they converge into a single downward node.
- Eino automatically **copies** stream as they fan out to different downward node, or is passed to callback handlers.
- Orchestration elements such as **branch** and **state handlers** are also stream aware.
- With these streaming processing abilities, the streaming paradigms of components themselves become transparent to the user. 
- A compiled Graph can run with 4 different streaming paradigms:

| Streaming Paradigm | Explanation                                                                 |
| ------------------ | --------------------------------------------------------------------------- |
| Invoke             | Accepts non-stream type I and returns non-stream type O                     |
| Stream             | Accepts non-stream type I and returns stream type StreamReader[O]           |
| Collect            | Accepts stream type StreamReader[I] and returns non-stream type O           |
| Transform          | Accepts stream type StreamReader[I] and returns stream type StreamReader[O] |

## Highly Extensible Aspects (Callbacks)

- Aspects handle cross-cutting concerns such as logging, tracing, metrics, etc., as well as exposing internal details of component implementations.
- Five aspects are supported: **OnStart, OnEnd, OnError, OnStartWithStreamInput, OnEndWithStreamOutput**.
- Developers can easily create custom callback handlers, add them during graph run via options, and they will be invoked during graph run.
- Graph can also inject aspects to those component implementations that do not support callbacks on their own.

# Eino Framework Structure

![](.github/static/img/eino/eino_framework.jpeg)

The Eino framework consists of several parts:

- Eino(this repo): Contains Eino&#039;s type definitions, streaming mechanism, component abstractions, orchestration capabilities, aspect mechanisms, etc.

- [EinoExt](https://github.com/cloudwego/eino-ext): Component implementations, callback handlers implementations, component usage examples, and various tools such as evaluators, prompt optimizers.

- [Eino Devops](https://github.com/cloudwego/eino-ext/tree/main/devops): visualized developing, visualized debugging
  etc.

- [EinoExamples](https://github.com/cloudwego/eino-examples) is the repo containing example applications and best practices for Eino.

## Detailed Documentation

For learning and using Eino, we provide a comprehensive Eino User Manual to help you quickly understand the concepts in Eino and master the skills of developing AI applications based on Eino. Start exploring through the [Eino User Manual](https://www.cloudwego.io/zh/docs/eino/) now!

For a quick introduction to building AI applications with Eino, we recommend starting with [Eino: Quick Start](https://www.cloudwego.io/zh/docs/eino/quick_start/)

## Dependencies
- Go 1.18 and above.

## Security

If you discover a potential security issue in this project, or think you may
have discovered a security issue, we ask that you notify Bytedance Security via our [security center](https://security.bytedance.com/src) or [vulnerability reporting email](sec@bytedance.com).

Please do **not** create a public GitHub issue.

## Contact US
- How to become a member: [COMMUNITY MEMBERSHIP](https://github.com/cloudwego/community/blob/main/COMMUNITY_MEMBERSHIP.md)
- Issues: [Issues](https://github.com/cloudwego/eino/issues)
- Lark: Scan the QR code below with [Register Feishu](https://www.feishu.cn/en/) to join our CloudWeGo/eino user group.

&amp;ensp;&amp;ensp;&amp;ensp; &lt;img src=&quot;.github/static/img/eino/lark_group_zh.png&quot; alt=&quot;LarkGroup&quot; width=&quot;200&quot;/&gt;

## License

This project is licensed under the [Apache-2.0 License](LICENSE-APACHE).
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[projectdiscovery/nuclei]]></title>
            <link>https://github.com/projectdiscovery/nuclei</link>
            <guid>https://github.com/projectdiscovery/nuclei</guid>
            <pubDate>Thu, 25 Dec 2025 00:05:15 GMT</pubDate>
            <description><![CDATA[Nuclei is a fast, customizable vulnerability scanner powered by the global security community and built on a simple YAML-based DSL, enabling collaboration to tackle trending vulnerabilities on the internet. It helps you find vulnerabilities in your applications, APIs, networks, DNS, and cloud configurations.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/projectdiscovery/nuclei">projectdiscovery/nuclei</a></h1>
            <p>Nuclei is a fast, customizable vulnerability scanner powered by the global security community and built on a simple YAML-based DSL, enabling collaboration to tackle trending vulnerabilities on the internet. It helps you find vulnerabilities in your applications, APIs, networks, DNS, and cloud configurations.</p>
            <p>Language: Go</p>
            <p>Stars: 26,246</p>
            <p>Forks: 3,021</p>
            <p>Stars today: 28 stars today</p>
            <h2>README</h2><pre>![nuclei](/static/nuclei-cover-image.png)

&lt;div align=&quot;center&quot;&gt;
  
  &lt;a href=&quot;https://github.com/projectdiscovery/nuclei/blob/main/README.md&quot;&gt;`English`&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://github.com/projectdiscovery/nuclei/blob/main/README_CN.md&quot;&gt;`‰∏≠Êñá`&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://github.com/projectdiscovery/nuclei/blob/main/README_KR.md&quot;&gt;`Korean`&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://github.com/projectdiscovery/nuclei/blob/main/README_ID.md&quot;&gt;`Indonesia`&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://github.com/projectdiscovery/nuclei/blob/main/README_ES.md&quot;&gt;`Spanish`&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://github.com/projectdiscovery/nuclei/blob/main/README_JP.md&quot;&gt;`Êó•Êú¨Ë™û`&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://github.com/projectdiscovery/nuclei/blob/main/README_PT-BR.md&quot;&gt;`Portuguese`&lt;/a&gt;
  
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;

&lt;a href=&quot;https://docs.projectdiscovery.io/tools/nuclei/overview?utm_source=github&amp;utm_medium=web&amp;utm_campaign=nuclei_readme&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Documentation-%23000000.svg?style=for-the-badge&amp;logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiNmZmZmZmYiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIiBjbGFzcz0ibHVjaWRlIGx1Y2lkZS1ib29rLW9wZW4iPjxwYXRoIGQ9Ik0xMiA3djE0Ii8+PHBhdGggZD0iTTMgMThhMSAxIDAgMCAxLTEtMVY0YTEgMSAwIDAgMSAxLTFoNWE0IDQgMCAwIDEgNCA0IDQgNCAwIDAgMSA0LTRoNWExIDEgMCAwIDEgMSAxdjEzYTEgMSAwIDAgMS0xIDFoLTZhMyAzIDAgMCAwLTMgMyAzIDMgMCAwIDAtMy0zeiIvPjwvc3ZnPg==&amp;logoColor=white&quot;&gt;&lt;/a&gt;
&amp;nbsp;&amp;nbsp;
&lt;a href=&quot;https://github.com/projectdiscovery/nuclei-templates&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Templates Library-%23000000.svg?style=for-the-badge&amp;logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgaGVpZ2h0PSIxNiIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiNmZmZmZmYiIHN0cm9rZS13aWR0aD0iMS41IiBzdHJva2UtbGluZWNhcD0icm91bmQiIHN0cm9rZS1saW5lam9pbj0icm91bmQiIGNsYXNzPSJsdWNpZGUgbHVjaWRlLXNoaWVsZCI+PHBhdGggZD0iTTIwIDEzYzAgNS0zLjUgNy41LTcuNjYgOC45NWExIDEgMCAwIDEtLjY3LS4wMUM3LjUgMjAuNSA0IDE4IDQgMTNWNmExIDEgMCAwIDEgMS0xYzIgMCA0LjUtMS4yIDYuMjQtMi43MmExLjE3IDEuMTcgMCAwIDEgMS41MiAwQzE0LjUxIDMuODEgMTcgNSAxOSA1YTEgMSAwIDAgMSAxIDF6Ii8+PC9zdmc+&amp;logoColor=white&quot;&gt;&lt;/a&gt;
&amp;nbsp;&amp;nbsp;
&lt;a href=&quot;https://discord.gg/projectdiscovery?utm_source=github&amp;utm_medium=web&amp;utm_campaign=nuclei_readme&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-%235865F2.svg?style=for-the-badge&amp;logo=discord&amp;logoColor=white&quot;&gt;&lt;/a&gt;

&lt;hr&gt;

&lt;/p&gt;

&lt;br&gt;

**Nuclei is a modern, high-performance vulnerability scanner that leverages simple YAML-based templates. It empowers you to design custom vulnerability detection scenarios that mimic real-world conditions, leading to zero false positives.**

- Simple YAML format for creating and customizing vulnerability templates.
- Contributed by thousands of security professionals to tackle trending vulnerabilities.
- Reduce false positives by simulating real-world steps to verify a vulnerability.
- Ultra-fast parallel scan processing and request clustering.
- Integrate into CI/CD pipelines for vulnerability detection and regression testing.
- Supports multiple protocols like TCP, DNS, HTTP, SSL, WHOIS, JavaScript, Code and more.
- Integrate with Jira, Splunk, GitHub, Elastic, GitLab.

&lt;br&gt;
&lt;br&gt;

## Table of Contents

- [**`Get Started`**](#get-started)
  - [_`1. Nuclei CLI`_](#1-nuclei-cli)
  - [_`2. Pro and Enterprise Editions`_](#2-pro-and-enterprise-editions)
- [**`Documentation`**](#documentation)
  - [_`Command Line Flags`_](#command-line-flags)
  - [_`Single target scan`_](#single-target-scan)
  - [_`Scanning multiple targets`_](#scanning-multiple-targets)
  - [_`Network scan`_](#network-scan)
  - [_`Scanning with your custom template`_](#scanning-with-your-custom-template)
  - [_`Connect Nuclei to ProjectDiscovery_`_](#connect-nuclei-to-projectdiscovery)
- [**`Nuclei Templates, Community and Rewards`**](#nuclei-templates-community-and-rewards-) üíé
- [**`Our Mission`**](#our-mission)
- [**`Contributors`**](#contributors-heart) ‚ù§
- [**`License`**](#license)

&lt;br&gt;
&lt;br&gt;

## Get Started

### **1. Nuclei CLI**

_Install Nuclei on your machine. Get started by following the installation guide [**`here`**](https://docs.projectdiscovery.io/tools/nuclei/install?utm_source=github&amp;utm_medium=web&amp;utm_campaign=nuclei_readme). Additionally, We provide [**`a free cloud tier`**](https://cloud.projectdiscovery.io/sign-up) and comes with a generous monthly free limits:_

- Store and visualize your vulnerability findings
- Write and manage your nuclei templates
- Access latest nuclei templates
- Discover and store your targets

&gt; [!Important]
&gt; |**This project is in active development**. Expect breaking changes with releases. Review the release changelog before updating.|
&gt; |:--------------------------------|
&gt; | This project is primarily built to be used as a standalone CLI tool. **Running nuclei as a service may pose security risks.** It&#039;s recommended to use with caution and additional security measures. |

&lt;br&gt;

### **2. Pro and Enterprise Editions**

_For security teams and enterprises, we provide a cloud-hosted service built on top of Nuclei OSS, fine-tuned to help you continuously run vulnerability scans at scale with your team and existing workflows:_

- 50x faster scans
- Large scale scanning with high accuracy
- Integrations with cloud services (AWS, GCP, Azure, Cloudflare, Fastly, Terraform, Kubernetes)
- Jira, Slack, Linear, APIs and Webhooks
- Executive and compliance reporting
- Plus: Real-time scanning, SAML SSO, SOC 2 compliant platform (with EU and US hosting options), shared team workspaces, and more
- We&#039;re constantly [**`adding new features`**](https://feedback.projectdiscovery.io/changelog)!
- **Ideal for:** Pentesters, security teams, and enterprises

[**`Sign up to Pro`**](https://projectdiscovery.io/pricing?utm_source=github&amp;utm_medium=web&amp;utm_campaign=nuclei_readme) or [**`Talk to our team`**](https://projectdiscovery.io/request-demo?utm_source=github&amp;utm_medium=web&amp;utm_campaign=nuclei_readme) if you have large organization and complex requirements.

&lt;br&gt;
&lt;br&gt;

## Documentation

Browse the full Nuclei [**`documentation here`**](https://docs.projectdiscovery.io/tools/nuclei/running). If you‚Äôre new to Nuclei, check out our [**`foundational YouTube series`**](https://www.youtube.com/playlist?list=PLZRbR9aMzTTpItEdeNSulo8bYsvil80Rl).

&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://www.youtube.com/watch?v=b5qMyQvL1ZA&amp;list=PLZRbR9aMzTTpItEdeNSulo8bYsvil80Rl&amp;utm_source=github&amp;utm_medium=web&amp;utm_campaign=nuclei_readme&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;/static/nuclei-getting-started.png&quot; width=&quot;350px&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=nFXygQdtjyw&amp;utm_source=github&amp;utm_medium=web&amp;utm_campaign=nuclei_readme&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;/static/nuclei-write-your-first-template.png&quot; width=&quot;350px&quot;&gt;&lt;/a&gt;

&lt;/div&gt;

&lt;br&gt;

### Installation

`nuclei` requires **go &gt;= 1.24.2** to install successfully. Run the following command to get the repo:

```sh
go install -v github.com/projectdiscovery/nuclei/v3/cmd/nuclei@latest
```

To learn more about installing nuclei, see `https://docs.projectdiscovery.io/tools/nuclei/install`.

### Command Line Flags

To display all the flags for the tool:

```sh
nuclei -h
```

&lt;details&gt;
  &lt;summary&gt;Expand full help flags&lt;/summary&gt;

```yaml
Nuclei is a fast, template based vulnerability scanner focusing
on extensive configurability, massive extensibility and ease of use.

Usage:
  ./nuclei [flags]

Flags:
TARGET:
   -u, -target string[]          target URLs/hosts to scan
   -l, -list string              path to file containing a list of target URLs/hosts to scan (one per line)
   -eh, -exclude-hosts string[]  hosts to exclude to scan from the input list (ip, cidr, hostname)
   -resume string                resume scan from and save to specified file (clustering will be disabled)
   -sa, -scan-all-ips            scan all the IP&#039;s associated with dns record
   -iv, -ip-version string[]     IP version to scan of hostname (4,6) - (default 4)

TARGET-FORMAT:
   -im, -input-mode string        mode of input file (list, burp, jsonl, yaml, openapi, swagger) (default &quot;list&quot;)
   -ro, -required-only            use only required fields in input format when generating requests
   -sfv, -skip-format-validation  skip format validation (like missing vars) when parsing input file

TEMPLATES:
   -nt, -new-templates                    run only new templates added in latest nuclei-templates release
   -ntv, -new-templates-version string[]  run new templates added in specific version
   -as, -automatic-scan                   automatic web scan using wappalyzer technology detection to tags mapping
   -t, -templates string[]                list of template or template directory to run (comma-separated, file)
   -turl, -template-url string[]          template url or list containing template urls to run (comma-separated, file)
   -ai, -prompt string                    generate and run template using ai prompt
   -w, -workflows string[]                list of workflow or workflow directory to run (comma-separated, file)
   -wurl, -workflow-url string[]          workflow url or list containing workflow urls to run (comma-separated, file)
   -validate                              validate the passed templates to nuclei
   -nss, -no-strict-syntax                disable strict syntax check on templates
   -td, -template-display                 displays the templates content
   -tl                                    list all templates matching current filters
   -tgl                                   list all available tags
   -sign                                  signs the templates with the private key defined in NUCLEI_SIGNATURE_PRIVATE_KEY env variable
   -code                                  enable loading code protocol-based templates
   -dut, -disable-unsigned-templates      disable running unsigned templates or templates with mismatched signature
   -esc, -enable-self-contained           enable loading self-contained templates
   -egm, -enable-global-matchers          enable loading global matchers templates
   -file                                  enable loading file templates

FILTERING:
   -a, -author string[]               templates to run based on authors (comma-separated, file)
   -tags string[]                     templates to run based on tags (comma-separated, file)
   -etags, -exclude-tags string[]     templates to exclude based on tags (comma-separated, file)
   -itags, -include-tags string[]     tags to be executed even if they are excluded either by default or configuration
   -id, -template-id string[]         templates to run based on template ids (comma-separated, file, allow-wildcard)
   -eid, -exclude-id string[]         templates to exclude based on template ids (comma-separated, file)
   -it, -include-templates string[]   path to template file or directory to be executed even if they are excluded either by default or configuration
   -et, -exclude-templates string[]   path to template file or directory to exclude (comma-separated, file)
   -em, -exclude-matchers string[]    template matchers to exclude in result
   -s, -severity value[]              templates to run based on severity. Possible values: info, low, medium, high, critical, unknown
   -es, -exclude-severity value[]     templates to exclude based on severity. Possible values: info, low, medium, high, critical, unknown
   -pt, -type value[]                 templates to run based on protocol type. Possible values: dns, file, http, headless, tcp, workflow, ssl, websocket, whois, code, javascript
   -ept, -exclude-type value[]        templates to exclude based on protocol type. Possible values: dns, file, http, headless, tcp, workflow, ssl, websocket, whois, code, javascript
   -tc, -template-condition string[]  templates to run based on expression condition

OUTPUT:
   -o, -output string            output file to write found issues/vulnerabilities
   -sresp, -store-resp           store all request/response passed through nuclei to output directory
   -srd, -store-resp-dir string  store all request/response passed through nuclei to custom directory (default &quot;output&quot;)
   -silent                       display findings only
   -nc, -no-color                disable output content coloring (ANSI escape codes)
   -j, -jsonl                    write output in JSONL(ines) format
   -irr, -include-rr -omit-raw   include request/response pairs in the JSON, JSONL, and Markdown outputs (for findings only) [DEPRECATED use -omit-raw] (default true)
   -or, -omit-raw                omit request/response pairs in the JSON, JSONL, and Markdown outputs (for findings only)
   -ot, -omit-template           omit encoded template in the JSON, JSONL output
   -nm, -no-meta                 disable printing result metadata in cli output
   -ts, -timestamp               enables printing timestamp in cli output
   -rdb, -report-db string       nuclei reporting database (always use this to persist report data)
   -ms, -matcher-status          display match failure status
   -me, -markdown-export string  directory to export results in markdown format
   -se, -sarif-export string     file to export results in SARIF format
   -je, -json-export string      file to export results in JSON format
   -jle, -jsonl-export string    file to export results in JSONL(ine) format
   -rd, -redact string[]         redact given list of keys from query parameter, request header and body

CONFIGURATIONS:
   -config string                        path to the nuclei configuration file
   -tp, -profile string                  template profile config file to run
   -tpl, -profile-list                   list community template profiles
   -fr, -follow-redirects                enable following redirects for http templates
   -fhr, -follow-host-redirects          follow redirects on the same host
   -mr, -max-redirects int               max number of redirects to follow for http templates (default 10)
   -dr, -disable-redirects               disable redirects for http templates
   -rc, -report-config string            nuclei reporting module configuration file
   -H, -header string[]                  custom header/cookie to include in all http request in header:value format (cli, file)
   -V, -var value                        custom vars in key=value format
   -r, -resolvers string                 file containing resolver list for nuclei
   -sr, -system-resolvers                use system DNS resolving as error fallback
   -dc, -disable-clustering              disable clustering of requests
   -passive                              enable passive HTTP response processing mode
   -fh2, -force-http2                    force http2 connection on requests
   -ev, -env-vars                        enable environment variables to be used in template
   -cc, -client-cert string              client certificate file (PEM-encoded) used for authenticating against scanned hosts
   -ck, -client-key string               client key file (PEM-encoded) used for authenticating against scanned hosts
   -ca, -client-ca string                client certificate authority file (PEM-encoded) used for authenticating against scanned hosts
   -sml, -show-match-line                show match lines for file templates, works with extractors only
   -ztls                                 use ztls library with autofallback to standard one for tls13 [Deprecated] autofallback to ztls is enabled by default
   -sni string                           tls sni hostname to use (default: input domain name)
   -dka, -dialer-keep-alive value        keep-alive duration for network requests.
   -lfa, -allow-local-file-access        allows file (payload) access anywhere on the system
   -lna, -restrict-local-network-access  blocks connections to the local / private network
   -i, -interface string                 network interface to use for network scan
   -at, -attack-type string              type of payload combinations to perform (batteringram,pitchfork,clusterbomb)
   -sip, -source-ip string               source ip address to use for network scan
   -rsr, -response-size-read int         max response size to read in bytes
   -rss, -response-size-save int         max response size to read in bytes (default 1048576)
   -reset                                reset removes all nuclei configuration and data files (including nuclei-templates)
   -tlsi, -tls-impersonate               enable experimental client hello (ja3) tls randomization
   -hae, -http-api-endpoint string       experimental http api endpoint

INTERACTSH:
   -iserver, -interactsh-server string  interactsh server url for self-hosted instance (default: oast.pro,oast.live,oast.site,oast.online,oast.fun,oast.me)
   -itoken, -interactsh-token string    authentication token for self-hosted interactsh server
   -interactions-cache-size int         number of requests to keep in the interactions cache (default 5000)
   -interactions-eviction int           number of seconds to wait before evicting requests from cache (default 60)
   -interactions-poll-duration int      number of seconds to wait before each interaction poll request (default 5)
   -interactions-cooldown-period int    extra time for interaction polling before exiting (default 5)
   -ni, -no-interactsh                  disable interactsh server for OAST testing, exclude OAST based templates

FUZZING:
   -ft, -fuzzing-type string           overrides fuzzing type set in template (replace, prefix, postfix, infix)
   -fm, -fuzzing-mode string           overrides fuzzing mode set in template (multiple, single)
   -fuzz                               enable loading fuzzing templates (Deprecated: use -dast instead)
   -dast                               enable / run dast (fuzz) nuclei templates
   -dts, -dast-server                  enable dast server mode (live fuzzing)
   -dtr, -dast-report                  write dast scan report to file
   -dtst, -dast-server-token string    dast server token (optional)
   -dtsa, -dast-server-address string  dast server address (default &quot;localhost:9055&quot;)
   -dfp, -display-fuzz-points          display fuzz points in the output for debugging
   -fuzz-param-frequency int           frequency of uninteresting parameters for fuzzing before skipping (default 10)
   -fa, -fuzz-aggression string        fuzzing aggression level controls payload count for fuzz (low, medium, high) (default &quot;low&quot;)
   -cs, -fuzz-scope string[]           in scope url regex to be followed by fuzzer
   -cos, -fuzz-out-scope string[]      out of scope url regex to be excluded by fuzzer

UNCOVER:
   -uc, -uncover                  enable uncover engine
   -uq, -uncover-query string[]   uncover search query
   -ue, -uncover-engine string[]  uncover search engine (shodan,censys,fofa,shodan-idb,quake,hunter,zoomeye,netlas,criminalip,publicwww,hunterhow,google) (default shodan)
   -uf, -uncover-field string     uncover fields to return (ip,port,host) (default &quot;ip:port&quot;)
   -ul, -uncover-limit int        uncover results to return (default 100)
   -ur, -uncover-ratelimit int    override ratelimit of engines with unknown ratelimit (default 60 req/min) (default 60)

RATE-LIMIT:
   -rl, -rate-limit int               maximum number of requests to send per second (default 150)
   -rld, -rate-limit-duration value   maximum number of requests to send per second (default 1s)
   -rlm, -rate-limit-minute int       maximum number of requests to send per minute (DEPRECATED)
   -bs, -bulk-size int                maximum number of hosts to be analyzed in parallel per template (default 25)
   -c, -concurrency int               maximum number of templates to be executed in parallel (default 25)
   -hbs, -headless-bulk-size int      maximum number of headless hosts to be analyzed in parallel per template (default 10)
   -headc, -headless-concurrency int  maximum number of headless templates to be executed in parallel 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[kserve/kserve]]></title>
            <link>https://github.com/kserve/kserve</link>
            <guid>https://github.com/kserve/kserve</guid>
            <pubDate>Thu, 25 Dec 2025 00:05:14 GMT</pubDate>
            <description><![CDATA[Standardized Distributed Generative and Predictive AI Inference Platform for Scalable, Multi-Framework Deployment on Kubernetes]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/kserve/kserve">kserve/kserve</a></h1>
            <p>Standardized Distributed Generative and Predictive AI Inference Platform for Scalable, Multi-Framework Deployment on Kubernetes</p>
            <p>Language: Go</p>
            <p>Stars: 4,938</p>
            <p>Forks: 1,325</p>
            <p>Stars today: 5 stars today</p>
            <h2>README</h2><pre># KServe
[![go.dev reference](https://img.shields.io/badge/go.dev-reference-007d9c?logo=go&amp;logoColor=white)](https://pkg.go.dev/github.com/kserve/kserve)
[![Coverage Status](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/andyi2it/5174bd748ac63a6e4803afea902e9810/raw/coverage.json)](https://github.com/kserve/kserve/actions/workflows/go.yml)
[![Go Report Card](https://goreportcard.com/badge/github.com/kserve/kserve)](https://goreportcard.com/report/github.com/kserve/kserve)
[![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/6643/badge)](https://bestpractices.coreinfrastructure.org/projects/6643)
[![Releases](https://img.shields.io/github/release-pre/kserve/kserve.svg?sort=semver)](https://github.com/kserve/kserve/releases)
[![LICENSE](https://img.shields.io/github/license/kserve/kserve.svg)](https://github.com/kserve/kserve/blob/master/LICENSE)
[![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&amp;style=social)](https://github.com/kserve/community/blob/main/README.md#questions-and-issues)
[![Gurubase](https://img.shields.io/badge/Gurubase-Ask%20KServe%20Guru-006BFF)](https://gurubase.io/g/kserve)

KServe is a standardized distributed generative and predictive AI inference platform for scalable, multi-framework deployment on Kubernetes.

KServe is being [used by many organizations](https://kserve.github.io/website/docs/community/adopters) and is a [Cloud Native Computing Foundation (CNCF)](https://www.cncf.io/) incubating project.

For more details, visit the [KServe website](https://kserve.github.io/website/).

![KServe](/docs/diagrams/kserve_new.png)

### Why KServe?

Single platform that unifies Generative and Predictive AI inference on Kubernetes. Simple enough for quick deployments, yet powerful enough to handle enterprise-scale AI workloads with advanced features.

### Features

**Generative AI**
  * üß† **LLM-Optimized**: OpenAI-compatible inference protocol for seamless integration with large language models
  * üöÖ **GPU Acceleration**: High-performance serving with GPU support and optimized memory management for large models
  * üíæ **Model Caching**: Intelligent model caching to reduce loading times and improve response latency for frequently used models
  * üóÇÔ∏è **KV Cache Offloading**: Advanced memory management with KV cache offloading to CPU/disk for handling longer sequences efficiently
  * üìà **Autoscaling**: Request-based autoscaling capabilities optimized for generative workload patterns
  * üîß **Hugging Face Ready**: Native support for Hugging Face models with streamlined deployment workflows

**Predictive AI**
  * üßÆ **Multi-Framework**: Support for TensorFlow, PyTorch, scikit-learn, XGBoost, ONNX, and more
  * üîÄ **Intelligent Routing**: Seamless request routing between predictor, transformer, and explainer components with automatic traffic management
  * üîÑ **Advanced Deployments**: Canary rollouts, inference pipelines, and ensembles with InferenceGraph
  * ‚ö° **Autoscaling**: Request-based autoscaling with scale-to-zero for predictive workloads
  * üîç **Model Explainability**: Built-in support for model explanations and feature attribution to understand prediction reasoning
  * üìä **Advanced Monitoring**: Enables payload logging, outlier detection, adversarial detection, and drift detection
  * üí∞ **Cost Efficient**: Scale-to-zero on expensive resources when not in use, reducing infrastructure costs

### Learn More
To learn more about KServe, how to use various supported features, and how to participate in the KServe community, 
please follow the [KServe website documentation](https://kserve.github.io/website). 
Additionally, we have compiled a list of [presentations and demos](https://kserve.github.io/website/docs/community/presentations) to dive through various details.

### :hammer_and_wrench: Installation

#### Standalone Installation
- **[Standard Kubernetes Installation](https://kserve.github.io/website/docs/admin-guide/overview#raw-kubernetes-deployment)**: Compared to Serverless Installation, this is a more **lightweight** installation. However, this option does not support canary deployment and request based autoscaling with scale-to-zero.
- **[Knative Installation](https://kserve.github.io/website/docs/admin-guide/overview#serverless-deployment)**: KServe by default installs Knative for **serverless deployment** for InferenceService.
- **[ModelMesh Installation](https://kserve.github.io/website/docs/admin-guide/overview#modelmesh-deployment)**: You can optionally install ModelMesh to enable **high-scale**, **high-density** and **frequently-changing model serving** use cases. 
- **[Quick Installation](https://kserve.github.io/website/docs/getting-started/quickstart-guide)**: Install KServe on your local machine.

#### Kubeflow Installation
KServe is an important addon component of Kubeflow, please learn more from the [Kubeflow KServe documentation](https://www.kubeflow.org/docs/external-add-ons/kserve/kserve). Check out the following guides for running [on AWS](https://awslabs.github.io/kubeflow-manifests/main/docs/component-guides/kserve) or [on OpenShift Container Platform](https://github.com/kserve/kserve/blob/master/docs/OPENSHIFT_GUIDE.md).

### :flight_departure: [Create your first InferenceService](https://kserve.github.io/website/docs/getting-started/genai-first-isvc)

### :bulb: [Roadmap](./ROADMAP.md)

### :blue_book: [InferenceService API Reference](https://kserve.github.io/website/docs/reference/crd-api)

### :toolbox: [Developer Guide](https://kserve.github.io/website/docs/developer-guide)

### :writing_hand: [Contributor Guide](https://kserve.github.io/website/docs/developer-guide/contribution)

### :handshake: [Adopters](https://kserve.github.io/website/docs/community/adopters)

### Star History

[![Star History Chart](https://api.star-history.com/svg?repos=kserve/kserve&amp;type=Date)](https://www.star-history.com/#kserve/kserve&amp;Date)
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
    </channel>
</rss>