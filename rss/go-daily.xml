<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for go - Go Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for go.</description>
        <lastBuildDate>Wed, 05 Nov 2025 00:51:39 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[mudler/LocalAI]]></title>
            <link>https://github.com/mudler/LocalAI</link>
            <guid>https://github.com/mudler/LocalAI</guid>
            <pubDate>Wed, 05 Nov 2025 00:51:39 GMT</pubDate>
            <description><![CDATA[ü§ñ The free, Open Source alternative to OpenAI, Claude and others. Self-hosted and local-first. Drop-in replacement for OpenAI, running on consumer-grade hardware. No GPU required. Runs gguf, transformers, diffusers and many more. Features: Generate Text, Audio, Video, Images, Voice Cloning, Distributed, P2P and decentralized inference]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mudler/LocalAI">mudler/LocalAI</a></h1>
            <p>ü§ñ The free, Open Source alternative to OpenAI, Claude and others. Self-hosted and local-first. Drop-in replacement for OpenAI, running on consumer-grade hardware. No GPU required. Runs gguf, transformers, diffusers and many more. Features: Generate Text, Audio, Video, Images, Voice Cloning, Distributed, P2P and decentralized inference</p>
            <p>Language: Go</p>
            <p>Stars: 36,918</p>
            <p>Forks: 2,918</p>
            <p>Stars today: 333 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
  &lt;br&gt;
  &lt;img width=&quot;300&quot; src=&quot;./core/http/static/logo.png&quot;&gt; &lt;br&gt;
&lt;br&gt;
&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/go-skynet/LocalAI/fork&quot; target=&quot;blank&quot;&gt;
&lt;img src=&quot;https://img.shields.io/github/forks/go-skynet/LocalAI?style=for-the-badge&quot; alt=&quot;LocalAI forks&quot;/&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/go-skynet/LocalAI/stargazers&quot; target=&quot;blank&quot;&gt;
&lt;img src=&quot;https://img.shields.io/github/stars/go-skynet/LocalAI?style=for-the-badge&quot; alt=&quot;LocalAI stars&quot;/&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/go-skynet/LocalAI/pulls&quot; target=&quot;blank&quot;&gt;
&lt;img src=&quot;https://img.shields.io/github/issues-pr/go-skynet/LocalAI?style=for-the-badge&quot; alt=&quot;LocalAI pull-requests&quot;/&gt;
&lt;/a&gt;
&lt;a href=&#039;https://github.com/go-skynet/LocalAI/releases&#039;&gt;
&lt;img src=&#039;https://img.shields.io/github/release/go-skynet/LocalAI?&amp;label=Latest&amp;style=for-the-badge&#039;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://hub.docker.com/r/localai/localai&quot; target=&quot;blank&quot;&gt;
&lt;img src=&quot;https://img.shields.io/badge/dockerhub-images-important.svg?logo=Docker&quot; alt=&quot;LocalAI Docker hub&quot;/&gt;
&lt;/a&gt;
&lt;a href=&quot;https://quay.io/repository/go-skynet/local-ai?tab=tags&amp;tag=latest&quot; target=&quot;blank&quot;&gt;
&lt;img src=&quot;https://img.shields.io/badge/quay.io-images-important.svg?&quot; alt=&quot;LocalAI Quay.io&quot;/&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://twitter.com/LocalAI_API&quot; target=&quot;blank&quot;&gt;
&lt;img src=&quot;https://img.shields.io/badge/X-%23000000.svg?style=for-the-badge&amp;logo=X&amp;logoColor=white&amp;label=LocalAI_API&quot; alt=&quot;Follow LocalAI_API&quot;/&gt;
&lt;/a&gt;
&lt;a href=&quot;https://discord.gg/uJAeKSAGDy&quot; target=&quot;blank&quot;&gt;
&lt;img src=&quot;https://dcbadge.vercel.app/api/server/uJAeKSAGDy?style=flat-square&amp;theme=default-inverted&quot; alt=&quot;Join LocalAI Discord Community&quot;/&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/5539&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/5539&quot; alt=&quot;mudler%2FLocalAI | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&gt; :bulb: Get help - [‚ùìFAQ](https://localai.io/faq/) [üí≠Discussions](https://github.com/go-skynet/LocalAI/discussions) [:speech_balloon: Discord](https://discord.gg/uJAeKSAGDy) [:book: Documentation website](https://localai.io/)
&gt;
&gt; [üíª Quickstart](https://localai.io/basics/getting_started/) [üñºÔ∏è Models](https://models.localai.io/) [üöÄ Roadmap](https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3Aroadmap) [üåç Explorer](https://explorer.localai.io) [üõ´ Examples](https://github.com/mudler/LocalAI-examples) Try on 
[![Telegram](https://img.shields.io/badge/Telegram-2CA5E0?style=for-the-badge&amp;logo=telegram&amp;logoColor=white)](https://t.me/localaiofficial_bot)

[![tests](https://github.com/go-skynet/LocalAI/actions/workflows/test.yml/badge.svg)](https://github.com/go-skynet/LocalAI/actions/workflows/test.yml)[![Build and Release](https://github.com/go-skynet/LocalAI/actions/workflows/release.yaml/badge.svg)](https://github.com/go-skynet/LocalAI/actions/workflows/release.yaml)[![build container images](https://github.com/go-skynet/LocalAI/actions/workflows/image.yml/badge.svg)](https://github.com/go-skynet/LocalAI/actions/workflows/image.yml)[![Bump dependencies](https://github.com/go-skynet/LocalAI/actions/workflows/bump_deps.yaml/badge.svg)](https://github.com/go-skynet/LocalAI/actions/workflows/bump_deps.yaml)[![Artifact Hub](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/localai)](https://artifacthub.io/packages/search?repo=localai)

**LocalAI** is the free, Open Source OpenAI alternative. LocalAI act as a drop-in replacement REST API that&#039;s compatible with OpenAI (Elevenlabs, Anthropic... ) API specifications for local AI inferencing. It allows you to run LLMs, generate images, audio (and not only) locally or on-prem with consumer grade hardware, supporting multiple model families. Does not require GPU. It is created and maintained by [Ettore Di Giacinto](https://github.com/mudler).


## üìöüÜï Local Stack Family

üÜï LocalAI is now part of a comprehensive suite of AI tools designed to work together:

&lt;table&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; valign=&quot;top&quot;&gt;
      &lt;a href=&quot;https://github.com/mudler/LocalAGI&quot;&gt;
        &lt;img src=&quot;https://raw.githubusercontent.com/mudler/LocalAGI/refs/heads/main/webui/react-ui/public/logo_2.png&quot; width=&quot;300&quot; alt=&quot;LocalAGI Logo&quot;&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td width=&quot;50%&quot; valign=&quot;top&quot;&gt;
      &lt;h3&gt;&lt;a href=&quot;https://github.com/mudler/LocalAGI&quot;&gt;LocalAGI&lt;/a&gt;&lt;/h3&gt;
      &lt;p&gt;A powerful Local AI agent management platform that serves as a drop-in replacement for OpenAI&#039;s Responses API, enhanced with advanced agentic capabilities.&lt;/p&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot; valign=&quot;top&quot;&gt;
      &lt;a href=&quot;https://github.com/mudler/LocalRecall&quot;&gt;
        &lt;img src=&quot;https://raw.githubusercontent.com/mudler/LocalRecall/refs/heads/main/static/localrecall_horizontal.png&quot; width=&quot;300&quot; alt=&quot;LocalRecall Logo&quot;&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td width=&quot;50%&quot; valign=&quot;top&quot;&gt;
      &lt;h3&gt;&lt;a href=&quot;https://github.com/mudler/LocalRecall&quot;&gt;LocalRecall&lt;/a&gt;&lt;/h3&gt;
      &lt;p&gt;A REST-ful API and knowledge base management system that provides persistent memory and storage capabilities for AI agents.&lt;/p&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## Screenshots


| Talk Interface | Generate Audio |
| --- | --- |
| ![Screenshot 2025-03-31 at 12-01-36 LocalAI - Talk](./docs/assets/images/screenshots/screenshot_tts.png) | ![Screenshot 2025-03-31 at 12-01-29 LocalAI - Generate audio with voice-en-us-ryan-low](./docs/assets/images/screenshots/screenshot_tts.png) |

| Models Overview | Generate Images |
| --- | --- |
| ![Screenshot 2025-03-31 at 12-01-20 LocalAI - Models](./docs/assets/images/screenshots/screenshot_gallery.png) | ![Screenshot 2025-03-31 at 12-31-41 LocalAI - Generate images with flux 1-dev](./docs/assets/images/screenshots/screenshot_image.png) |

| Chat Interface | Home |
| --- | --- |
| ![Screenshot 2025-03-31 at 11-57-44 LocalAI - Chat with localai-functioncall-qwen2 5-7b-v0 5](./docs/assets/images/screenshots/screenshot_chat.png) | ![Screenshot 2025-03-31 at 11-57-23 LocalAI API - c2a39e3 (c2a39e3639227cfd94ffffe9f5691239acc275a8)](./docs/assets/images/screenshots/screenshot_home.png) |

| Login | Swarm |
| --- | --- |
|![Screenshot 2025-03-31 at 12-09-59 ](./docs/assets/images/screenshots/screenshot_login.png) | ![Screenshot 2025-03-31 at 12-10-39 LocalAI - P2P dashboard](./docs/assets/images/screenshots/screenshot_p2p.png) |

## üíª Quickstart

Run the installer script:

```bash
# Basic installation
curl https://localai.io/install.sh | sh
```

For more installation options, see [Installer Options](https://localai.io/docs/advanced/installer/).

### macOS Download:

&lt;a href=&quot;https://github.com/mudler/LocalAI/releases/latest/download/LocalAI.dmg&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Download-macOS-blue?style=for-the-badge&amp;logo=apple&amp;logoColor=white&quot; alt=&quot;Download LocalAI for macOS&quot;/&gt;
&lt;/a&gt;

&gt; Note: the DMGs are not signed by Apple as quarantined. See https://github.com/mudler/LocalAI/issues/6268 for a workaround, fix is tracked here: https://github.com/mudler/LocalAI/issues/6244

Or run with docker:

&gt; **üí° Docker Run vs Docker Start**
&gt; 
&gt; - `docker run` creates and starts a new container. If a container with the same name already exists, this command will fail.
&gt; - `docker start` starts an existing container that was previously created with `docker run`.
&gt; 
&gt; If you&#039;ve already run LocalAI before and want to start it again, use: `docker start -i local-ai`

### CPU only image:

```bash
docker run -ti --name local-ai -p 8080:8080 localai/localai:latest
```

### NVIDIA GPU Images:

```bash
# CUDA 12.0
docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-gpu-nvidia-cuda-12

# CUDA 11.7
docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-gpu-nvidia-cuda-11

# NVIDIA Jetson (L4T) ARM64
docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-nvidia-l4t-arm64
```

### AMD GPU Images (ROCm):

```bash
docker run -ti --name local-ai -p 8080:8080 --device=/dev/kfd --device=/dev/dri --group-add=video localai/localai:latest-gpu-hipblas
```

### Intel GPU Images (oneAPI):

```bash
docker run -ti --name local-ai -p 8080:8080 --device=/dev/dri/card1 --device=/dev/dri/renderD128 localai/localai:latest-gpu-intel
```

### Vulkan GPU Images:

```bash
docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-gpu-vulkan
```

### AIO Images (pre-downloaded models):

```bash
# CPU version
docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-aio-cpu

# NVIDIA CUDA 12 version
docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-aio-gpu-nvidia-cuda-12

# NVIDIA CUDA 11 version
docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-aio-gpu-nvidia-cuda-11

# Intel GPU version
docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-aio-gpu-intel

# AMD GPU version
docker run -ti --name local-ai -p 8080:8080 --device=/dev/kfd --device=/dev/dri --group-add=video localai/localai:latest-aio-gpu-hipblas
```

For more information about the AIO images and pre-downloaded models, see [Container Documentation](https://localai.io/basics/container/).

To load models:

```bash
# From the model gallery (see available models with `local-ai models list`, in the WebUI from the model tab, or visiting https://models.localai.io)
local-ai run llama-3.2-1b-instruct:q4_k_m
# Start LocalAI with the phi-2 model directly from huggingface
local-ai run huggingface://TheBloke/phi-2-GGUF/phi-2.Q8_0.gguf
# Install and run a model from the Ollama OCI registry
local-ai run ollama://gemma:2b
# Run a model from a configuration file
local-ai run https://gist.githubusercontent.com/.../phi-2.yaml
# Install and run a model from a standard OCI registry (e.g., Docker Hub)
local-ai run oci://localai/phi-2:latest
```

&gt; ‚ö° **Automatic Backend Detection**: When you install models from the gallery or YAML files, LocalAI automatically detects your system&#039;s GPU capabilities (NVIDIA, AMD, Intel) and downloads the appropriate backend. For advanced configuration options, see [GPU Acceleration](https://localai.io/features/gpu-acceleration/#automatic-backend-detection).

For more information, see [üíª Getting started](https://localai.io/basics/getting_started/index.html)

## üì∞ Latest project news

- October 2025: üîå [Model Context Protocol (MCP)](https://localai.io/docs/features/mcp/) support added for agentic capabilities with external tools
- September 2025: New Launcher application for MacOS and Linux, extended support to many backends for Mac and Nvidia L4T devices. Models: Added MLX-Audio, WAN 2.2. WebUI improvements and Python-based backends now ships portable python environments.
- August 2025: MLX, MLX-VLM, Diffusers and llama.cpp are now supported on Mac M1/M2/M3+ chips ( with `development` suffix in the gallery ): https://github.com/mudler/LocalAI/pull/6049 https://github.com/mudler/LocalAI/pull/6119 https://github.com/mudler/LocalAI/pull/6121 https://github.com/mudler/LocalAI/pull/6060
- July/August 2025: üîç [Object Detection](https://localai.io/features/object-detection/) added to the API featuring [rf-detr](https://github.com/roboflow/rf-detr)
- July 2025: All backends migrated outside of the main binary. LocalAI is now more lightweight, small, and automatically downloads the required backend to run the model. [Read the release notes](https://github.com/mudler/LocalAI/releases/tag/v3.2.0)
- June 2025: [Backend management](https://github.com/mudler/LocalAI/pull/5607) has been added. Attention: extras images are going to be deprecated from the next release! Read [the backend management PR](https://github.com/mudler/LocalAI/pull/5607).
- May 2025: [Audio input](https://github.com/mudler/LocalAI/pull/5466) and [Reranking](https://github.com/mudler/LocalAI/pull/5396) in llama.cpp backend, [Realtime API](https://github.com/mudler/LocalAI/pull/5392),  Support to Gemma, SmollVLM, and more multimodal models (available in the gallery).
- May 2025: Important: image name changes [See release](https://github.com/mudler/LocalAI/releases/tag/v2.29.0)
- Apr 2025: Rebrand, WebUI enhancements
- Apr 2025: [LocalAGI](https://github.com/mudler/LocalAGI) and [LocalRecall](https://github.com/mudler/LocalRecall) join the LocalAI family stack.
- Apr 2025: WebUI overhaul, AIO images updates
- Feb 2025: Backend cleanup, Breaking changes, new backends (kokoro, OutelTTS, faster-whisper), Nvidia L4T images
- Jan 2025: LocalAI model release: https://huggingface.co/mudler/LocalAI-functioncall-phi-4-v0.3, SANA support in diffusers: https://github.com/mudler/LocalAI/pull/4603
- Dec 2024: stablediffusion.cpp backend (ggml) added ( https://github.com/mudler/LocalAI/pull/4289 )
- Nov 2024: Bark.cpp backend added ( https://github.com/mudler/LocalAI/pull/4287 )
- Nov 2024: Voice activity detection models (**VAD**) added to the API: https://github.com/mudler/LocalAI/pull/4204
- Oct 2024: examples moved to [LocalAI-examples](https://github.com/mudler/LocalAI-examples)
- Aug 2024:  üÜï FLUX-1, [P2P Explorer](https://explorer.localai.io)
- July 2024: üî•üî• üÜï P2P Dashboard, LocalAI Federated mode and AI Swarms: https://github.com/mudler/LocalAI/pull/2723. P2P Global community pools: https://github.com/mudler/LocalAI/issues/3113
- May 2024: üî•üî• Decentralized P2P llama.cpp:  https://github.com/mudler/LocalAI/pull/2343 (peer2peer llama.cpp!) üëâ Docs  https://localai.io/features/distribute/
- May 2024: üî•üî• Distributed inferencing: https://github.com/mudler/LocalAI/pull/2324
- April 2024: Reranker API: https://github.com/mudler/LocalAI/pull/2121

Roadmap items: [List of issues](https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3Aroadmap)

## üöÄ [Features](https://localai.io/features/)

- üß© [Backend Gallery](https://localai.io/backends/): Install/remove backends on the fly, powered by OCI images ‚Äî fully customizable and API-driven.
- üìñ [Text generation with GPTs](https://localai.io/features/text-generation/) (`llama.cpp`, `transformers`, `vllm` ... [:book: and more](https://localai.io/model-compatibility/index.html#model-compatibility-table))
- üó£ [Text to Audio](https://localai.io/features/text-to-audio/)
- üîà [Audio to Text](https://localai.io/features/audio-to-text/) (Audio transcription with `whisper.cpp`)
- üé® [Image generation](https://localai.io/features/image-generation)
- üî• [OpenAI-alike tools API](https://localai.io/features/openai-functions/) 
- üß† [Embeddings generation for vector databases](https://localai.io/features/embeddings/)
- ‚úçÔ∏è [Constrained grammars](https://localai.io/features/constrained_grammars/)
- üñºÔ∏è [Download Models directly from Huggingface ](https://localai.io/models/)
- ü•Ω [Vision API](https://localai.io/features/gpt-vision/)
- üîç [Object Detection](https://localai.io/features/object-detection/)
- üìà [Reranker API](https://localai.io/features/reranker/)
- üÜïüñß [P2P Inferencing](https://localai.io/features/distribute/)
- üÜïüîå [Model Context Protocol (MCP)](https://localai.io/docs/features/mcp/) - Agentic capabilities with external tools and [LocalAGI&#039;s Agentic capabilities](https://github.com/mudler/LocalAGI)
- üîä Voice activity detection (Silero-VAD support)
- üåç Integrated WebUI!

## üß© Supported Backends &amp; Acceleration

LocalAI supports a comprehensive range of AI backends with multiple acceleration options:

### Text Generation &amp; Language Models
| Backend | Description | Acceleration Support |
|---------|-------------|---------------------|
| **llama.cpp** | LLM inference in C/C++ | CUDA 11/12, ROCm, Intel SYCL, Vulkan, Metal, CPU |
| **vLLM** | Fast LLM inference with PagedAttention | CUDA 12, ROCm, Intel |
| **transformers** | HuggingFace transformers framework | CUDA 11/12, ROCm, Intel, CPU |
| **exllama2** | GPTQ inference library | CUDA 12 |
| **MLX** | Apple Silicon LLM inference | Metal (M1/M2/M3+) |
| **MLX-VLM** | Apple Silicon Vision-Language Models | Metal (M1/M2/M3+) |

### Audio &amp; Speech Processing
| Backend | Description | Acceleration Support |
|---------|-------------|---------------------|
| **whisper.cpp** | OpenAI Whisper in C/C++ | CUDA 12, ROCm, Intel SYCL, Vulkan, CPU |
| **faster-whisper** | Fast Whisper with CTranslate2 | CUDA 12, ROCm, Intel, CPU |
| **bark** | Text-to-audio generation | CUDA 12, ROCm, Intel |
| **bark-cpp** | C++ implementation of Bark | CUDA, Metal, CPU |
| **coqui** | Advanced TTS with 1100+ languages | CUDA 12, ROCm, Intel, CPU |
| **kokoro** | Lightweight TTS model | CUDA 12, ROCm, Intel, CPU |
| **chatterbox** | Production-grade TTS | CUDA 11/12, CPU |
| **piper** | Fast neural TTS system | CPU |
| **kitten-tts** | Kitten TTS models | CPU |
| **silero-vad** | Voice Activity Detection | CPU |
| **neutts** | Text-to-speech with voice cloning | CUDA 12, ROCm, CPU |

### Image &amp; Video Generation
| Backend | Description | Acceleration Support |
|---------|-------------|---------------------|
| **stablediffusion.cpp** | Stable Diffusion in C/C++ | CUDA 12, Intel SYCL, Vulkan, CPU |
| **diffusers** | HuggingFace diffusion models | CUDA 11/12, ROCm, Intel, Metal, CPU |

### Specialized AI Tasks
| Backend | Description | Acceleration Support |
|---------|-------------|---------------------|
| **rfdetr** | Real-time object detection | CUDA 12, Intel, CPU |
| **rerankers** | Document reranking API | CUDA 11/12, ROCm, Intel, CPU |
| **local-store** | Vector database | CPU |
| **huggingface** | HuggingFace API integration | API-based |

### Hardware Acceleration Matrix

| Acceleration Type | Supported Backends | Hardware Support |
|-------------------|-------------------|------------------|
| **NVIDIA CUDA 11** | llama.cpp, whisper, stablediffusion, diffusers, rerankers, bark, chatterbox | Nvidia hardware |
| **NVIDIA CUDA 12** | All CUDA-compatible backends | Nvidia hardware |
| **AMD ROCm** | llama.cpp, whisper, vllm, transformers, diffusers, rerankers, coqui, kokoro, bark, neutts | AMD Graphics |
| **Intel oneAPI** | llama.cpp, whisper, stablediffusion, vllm, transformers, diffusers, rfdetr, rerankers, exllama2, coqui, kokoro, bark | Intel Arc, Intel iGPUs |
| **Apple Metal** | llama.cpp, whisper, diffusers, MLX, MLX-VLM, bark-cpp | Apple M1/M2/M3+ |
| **Vulkan** | llama.cpp, whisper, stablediffusion | Cross-platform GPUs |
| **NVIDIA Jetson** | llama.cpp, whisper, stablediffusion, diffusers, rfdetr | ARM64 embedded AI |
| **CPU Optimized** | All backends | AVX/AVX2/AVX512, quantization support |

### üîó Community and integrations

Build and deploy custom containers:
- https://github.com/sozercan/aikit

WebUIs:
- https://github.com/Jirubizu/localai-admin
- https://github.com/go-skynet/LocalAI-frontend
- QA-Pilot(An interactive chat project that leverages LocalAI LLMs for rapid understanding and navigation of GitHub code repository) https://github.com/reid41/QA-Pilot

Agentic Libraries:
- https://github.com/mudler/cogito

MCPs:
- https://github.com/mudler/MCPs

Model galleries
- https://github.com/go-skynet/model-gallery

Voice:
- https://github.com/richiejp/VoxInput

Other:
- Helm chart https://github.com/go-skynet/helm-charts
- VSCode extension https://github.com/badgooooor/localai-vscode-plugin
- Langchain: https://python.langchain.com/docs/integrations/providers/localai/
- Terminal utility https://github.com/djcopley/ShellOracle
- Local Smart assistant https://github.com/mudler/LocalAGI
- Home Assistant https://github.com/sammcj/homeassistant-localai / https://github.com/drndos/hass-openai-custom-conversation / https://github.com/valentinfrlch/ha-gpt4vision
- Discord bot https://github.com/mudler/LocalAGI/tree/main/examples/discord
- Slack bot https://github.com/mudler/LocalAGI/tree/main/examples/slack
- Shell-Pilot(Interact with LLM using LocalAI models via pure shell scripts on your Linux or MacOS system) https://github.com/reid41/shell-pilot
- Telegram bot https://github.com/mudler/LocalAI/tree/master/examples/telegram-bot
- Another Telegram Bot https://github.com/JackBekket/Hellper
- Auto-documentation https://github.com/JackBekket/Reflexia
- Github bot which answer on issue

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[charmbracelet/glow]]></title>
            <link>https://github.com/charmbracelet/glow</link>
            <guid>https://github.com/charmbracelet/glow</guid>
            <pubDate>Wed, 05 Nov 2025 00:51:38 GMT</pubDate>
            <description><![CDATA[Render markdown on the CLI, with pizzazz! üíÖüèª]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/charmbracelet/glow">charmbracelet/glow</a></h1>
            <p>Render markdown on the CLI, with pizzazz! üíÖüèª</p>
            <p>Language: Go</p>
            <p>Stars: 20,748</p>
            <p>Forks: 499</p>
            <p>Stars today: 320 stars today</p>
            <h2>README</h2><pre># Glow

Render markdown on the CLI, with _pizzazz_!

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://stuff.charm.sh/glow/glow-banner-github.gif&quot; alt=&quot;Glow Logo&quot;&gt;
    &lt;a href=&quot;https://github.com/charmbracelet/glow/releases&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/release/charmbracelet/glow.svg&quot; alt=&quot;Latest Release&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://pkg.go.dev/github.com/charmbracelet/glow?tab=doc&quot;&gt;&lt;img src=&quot;https://godoc.org/github.com/golang/gddo?status.svg&quot; alt=&quot;GoDoc&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/charmbracelet/glow/actions&quot;&gt;&lt;img src=&quot;https://github.com/charmbracelet/glow/workflows/build/badge.svg&quot; alt=&quot;Build Status&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://goreportcard.com/report/github.com/charmbracelet/glow&quot;&gt;&lt;img src=&quot;https://goreportcard.com/badge/charmbracelet/glow&quot; alt=&quot;Go ReportCard&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/c2246366-f84b-4847-b431-32a61ca07b74&quot; width=&quot;800&quot; alt=&quot;Glow UI Demo&quot;&gt;
&lt;/p&gt;

## What is it?

Glow is a terminal based markdown reader designed from the ground up to bring
out the beauty‚Äîand power‚Äîof the CLI.

Use it to discover markdown files, read documentation directly on the command
line. Glow will find local markdown files in subdirectories or a local
Git repository.

## Installation

### Package Manager

```bash
# macOS or Linux
brew install glow
```

```bash
# macOS (with MacPorts)
sudo port install glow
```

```bash
# Arch Linux (btw)
pacman -S glow
```

```bash
# Void Linux
xbps-install -S glow
```

```bash
# Nix shell
nix-shell -p glow --command glow
```

```bash
# FreeBSD
pkg install glow
```

```bash
# Solus
eopkg install glow
```

```bash
# Windows (with Chocolatey, Scoop, or Winget)
choco install glow
scoop install glow
winget install charmbracelet.glow
```

```bash
# Android (with termux)
pkg install glow
```

```bash
# Ubuntu (Snapcraft)
sudo snap install glow
```

```bash
# Debian/Ubuntu
sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://repo.charm.sh/apt/gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/charm.gpg
echo &quot;deb [signed-by=/etc/apt/keyrings/charm.gpg] https://repo.charm.sh/apt/ * *&quot; | sudo tee /etc/apt/sources.list.d/charm.list
sudo apt update &amp;&amp; sudo apt install glow
```

```bash
# Fedora/RHEL
echo &#039;[charm]
name=Charm
baseurl=https://repo.charm.sh/yum/
enabled=1
gpgcheck=1
gpgkey=https://repo.charm.sh/yum/gpg.key&#039; | sudo tee /etc/yum.repos.d/charm.repo
sudo yum install glow
```

Or download a binary from the [releases][releases] page. MacOS, Linux, Windows,
FreeBSD and OpenBSD binaries are available, as well as Debian, RPM, and Alpine
packages. ARM builds are also available for macOS, Linux, FreeBSD and OpenBSD.

### Go

Or just install it with `go`:

```bash
go install github.com/charmbracelet/glow/v2@latest
```

### Build (requires Go 1.21+)

```bash
git clone https://github.com/charmbracelet/glow.git
cd glow
go build
```

[releases]: https://github.com/charmbracelet/glow/releases

## The TUI

Simply run `glow` without arguments to start the textual user interface and
browse local. Glow will find local markdown files in the
current directory and below or, if you‚Äôre in a Git repository, Glow will search
the repo.

Markdown files can be read with Glow&#039;s high-performance pager. Most of the
keystrokes you know from `less` are the same, but you can press `?` to list
the hotkeys.

## The CLI

In addition to a TUI, Glow has a CLI for working with Markdown. To format a
document use a markdown source as the primary argument:

```bash
# Read from file
glow README.md

# Read from stdin
echo &quot;[Glow](https://github.com/charmbracelet/glow)&quot; | glow -

# Fetch README from GitHub / GitLab
glow github.com/charmbracelet/glow

# Fetch markdown from HTTP
glow https://host.tld/file.md
```

### Word Wrapping

The `-w` flag lets you set a maximum width at which the output will be wrapped:

```bash
glow -w 60
```

### Paging

CLI output can be displayed in your preferred pager with the `-p` flag. This defaults
to the ANSI-aware `less -r` if `$PAGER` is not explicitly set.

### Styles

You can choose a style with the `-s` flag. When no flag is provided `glow` tries
to detect your terminal&#039;s current background color and automatically picks
either the `dark` or the `light` style for you.

```bash
glow -s [dark|light]
```

Alternatively you can also supply a custom JSON stylesheet:

```bash
glow -s mystyle.json
```

For additional usage details see:

```bash
glow --help
```

Check out the [Glamour Style Section](https://github.com/charmbracelet/glamour/blob/master/styles/gallery/README.md)
to find more styles. Or [make your own](https://github.com/charmbracelet/glamour/tree/master/styles)!

## The Config File

If you find yourself supplying the same flags to `glow` all the time, it&#039;s
probably a good idea to create a config file. Run `glow config`, which will open
it in your favorite $EDITOR. Alternatively you can manually put a file named
`glow.yml` in the default config path of you platform. If you&#039;re not sure where
that is, please refer to `glow --help`.

Here&#039;s an example config:

```yaml
# style name or JSON path (default &quot;auto&quot;)
style: &quot;light&quot;
# mouse wheel support (TUI-mode only)
mouse: true
# use pager to display markdown
pager: true
# at which column should we word wrap?
width: 80
# show all files, including hidden and ignored.
all: false
# show line numbers (TUI-mode only)
showLineNumbers: false
# preserve newlines in the output
preserveNewLines: false
```

## Contributing

See [contributing][contribute].

[contribute]: https://github.com/charmbracelet/glow/contribute

## Feedback

We‚Äôd love to hear your thoughts on this project. Feel free to drop us a note!

- [Twitter](https://twitter.com/charmcli)
- [The Fediverse](https://mastodon.social/@charmcli)
- [Discord](https://charm.sh/chat)

## License

[MIT](https://github.com/charmbracelet/glow/raw/master/LICENSE)

---

Part of [Charm](https://charm.sh).

&lt;a href=&quot;https://charm.sh/&quot;&gt;&lt;img alt=&quot;The Charm logo&quot; src=&quot;https://stuff.charm.sh/charm-badge.jpg&quot; width=&quot;400&quot;&gt;&lt;/a&gt;

CharmÁÉ≠Áà±ÂºÄÊ∫ê ‚Ä¢ Charm loves open source
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[mudler/edgevpn]]></title>
            <link>https://github.com/mudler/edgevpn</link>
            <guid>https://github.com/mudler/edgevpn</guid>
            <pubDate>Wed, 05 Nov 2025 00:51:37 GMT</pubDate>
            <description><![CDATA[‚õµ The immutable, decentralized, statically built p2p VPN without any central server and automatic discovery! Create decentralized introspectable tunnels over p2p with shared tokens]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mudler/edgevpn">mudler/edgevpn</a></h1>
            <p>‚õµ The immutable, decentralized, statically built p2p VPN without any central server and automatic discovery! Create decentralized introspectable tunnels over p2p with shared tokens</p>
            <p>Language: Go</p>
            <p>Stars: 1,488</p>
            <p>Forks: 153</p>
            <p>Stars today: 148 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
  &lt;br&gt;
	&lt;img src=&quot;https://user-images.githubusercontent.com/2420543/144679248-1f6e4c10-a558-424c-b6f5-b3695269c906.png&quot; width=128
         alt=&quot;logo&quot;&gt;&lt;br&gt;
    EdgeVPN

&lt;br&gt;
&lt;/h1&gt;

&lt;h3 align=&quot;center&quot;&gt;Create Decentralized private networks &lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://opensource.org/licenses/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/licence-GPL3-brightgreen&quot;
         alt=&quot;license&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/mudler/edgevpn/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/mudler/edgevpn&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://img.shields.io/badge/made%20with-Go-blue&quot;&gt;
  &lt;img src=&quot;https://goreportcard.com/badge/github.com/mudler/edgevpn&quot; alt=&quot;go report card&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
	 &lt;br&gt;
    Fully Decentralized. Immutable. Portable. Easy to use Statically compiled VPN and a reverse proxy over p2p.&lt;br&gt;
    &lt;b&gt;VPN&lt;/b&gt; -  &lt;b&gt;Reverse Proxy&lt;/b&gt; - &lt;b&gt;Send files securely over p2p&lt;/b&gt; -  &lt;b&gt;Blockchain&lt;/b&gt;
&lt;/p&gt;


EdgeVPN uses libp2p to build private decentralized networks that can be accessed via shared secrets.

It can:

- **Create a VPN** :  Secure VPN between p2p peers
  - Automatically assign IPs to nodes
  - Embedded tiny DNS server to resolve internal/external IPs
  - Create trusted zones to prevent network access if token is leaked

- **Act as a reverse Proxy** : Share a tcp service like you would do with `ngrok`. EdgeVPN let expose TCP services to the p2p network nodes without establishing a VPN connection: creates reverse proxy and tunnels traffic into the p2p network.

- **Send files via p2p** : Send files over p2p between nodes without establishing a VPN connection.

- **Be used as a library**: Plug a distributed p2p ledger easily in your golang code! For example EdgeVPN powers [LocalAI](https://github.com/mudler/LocalAI)&#039;s P2P features (you can learn more about it [here](https://localai.io/features/distribute/)).

See the [documentation](https://mudler.github.io/edgevpn).

# :camera: Screenshots

Dashboard (Dark mode)            |  Dashboard (Light mode)
:-------------------------:|:-------------------------:
![Screenshot 2021-10-31 at 00-12-16 EdgeVPN - Machines index](https://user-images.githubusercontent.com/2420543/163020448-8e9238c1-3b6d-435d-9b25-7729d8779ebd.png) | ![Screenshot 2021-10-31 at 23-03-26 EdgeVPN - Machines index](https://user-images.githubusercontent.com/2420543/163020460-e18c07d7-8426-4992-aab3-0b2fd90279ae.png)

DNS            |  Machine index
:-------------------------:|:-------------------------:
![Screenshot 2021-10-31 at 23-03-44 EdgeVPN - Services index](https://user-images.githubusercontent.com/2420543/163020465-3d481da4-4912-445e-afc0-2614966dcadf.png) | ![Screenshot 2021-10-31 at 23-03-59 EdgeVPN - Files index](https://user-images.githubusercontent.com/2420543/163020462-7821a622-8c13-4971-8abe-9c5b6b491ae8.png)

Services            |  Blockchain index
:-------------------------:|:-------------------------:
![Screenshot 2021-10-31 at 23-04-12 EdgeVPN - Users connected](https://user-images.githubusercontent.com/2420543/163021285-3c5a980d-2562-4c10-b266-7e99f19d8a87.png) | ![Screenshot 2021-10-31 at 23-04-20 EdgeVPN - Blockchain index](https://user-images.githubusercontent.com/2420543/163020457-77ef6e50-40a6-4e3b-83c4-a81db729bd7d.png)


# :new: GUI

A Desktop GUI application (alpha) for Linux is available [here](https://github.com/mudler/edgevpn-gui)

Dashboard            |  Connections index
:-------------------------:|:-------------------------:
![edgevpn-gui-2](https://user-images.githubusercontent.com/2420543/147854909-a223a7c1-5caa-4e90-b0ac-0ae04dc0949d.png) | ![edgevpn-3](https://user-images.githubusercontent.com/2420543/147854904-09d96991-8752-421a-a301-8f0bdd9d5542.png)
![edgevpn-gui](https://user-images.githubusercontent.com/2420543/147854907-1e4a4715-3181-4dc2-8bc0-d052b3bf46d3.png) | 

# Kubernetes 

Check out [c3os](https://github.com/mudler/c3os) for seeing EdgeVPN in action with Kubernetes!

# :running: Installation

Download the precompiled static release in the [releases page](https://github.com/mudler/edgevpn/releases). You can either install it in your system or just run it.

# :computer: Usage

EdgeVPN works by generating tokens (or a configuration file) that can be shared between different machines, hosts or peers to access to a decentralized secured network between them.

Every token is unique and identifies the network,  no central server setup, or specifying hosts ip is required.

To generate a config run:

```bash
# Generate a new config file and use it later as EDGEVPNCONFIG
$ edgevpn -g &gt; config.yaml
```

OR to generate a portable token:

```bash
$ EDGEVPNTOKEN=$(edgevpn -g -b)
```

Note, tokens are config merely encoded in base64, so this is equivalent:

```bash
$ EDGEVPNTOKEN=$(edgevpn -g | tee config.yaml | base64 -w0)
```

All edgevpn commands implies that you either specify a `EDGEVPNTOKEN` (or `--token` as parameter) or a `EDGEVPNCONFIG` as this is the way for `edgevpn` to establish a network between the nodes. 

The configuration file is the network definition and allows you to connect over to your peers securely.

**Warning** Exposing this file or passing-it by is equivalent to give full control to the network.

## :satellite: As a VPN

To start the VPN, simply run `edgevpn` without any argument.

An example of running edgevpn on multiple hosts:

```bash
# on Node A
$ EDGEVPNTOKEN=.. edgevpn --address 10.1.0.11/24
# on Node B
$ EDGEVPNTOKEN=.. edgevpn --address 10.1.0.12/24
# on Node C ...
$ EDGEVPNTOKEN=.. edgevpn --address 10.1.0.13/24
...
```

... and that&#039;s it! the `--address` is a _virtual_ unique IP for each node, and it is actually the ip where the node will be reachable to from the vpn. You can assign IPs freely to the nodes of the network, while you can override the default `edgevpn0` interface with `IFACE` (or `--interface`)

*Note*: It might take up time to build the connection between nodes. Wait at least 5 mins, it depends on the network behind the hosts.


# :question: Is it for me?

EdgeVPN makes VPN decentralization a first strong requirement. 

Its main use is for edge and low-end devices and especially for development.

The decentralized approach has few cons:

- The underlying network is chatty. It uses a Gossip protocol for synchronizing the routing table and p2p. Every blockchain message is broadcasted to all peers, while the traffic is to the host only.
- Might be not suited for low latency workload.

Keep that in mind before using it for your prod networks!

But it has a strong pro: it just works everywhere libp2p works!

# :question: Why? 

First of all it&#039;s my first experiment with libp2p. Second, I always wanted a more &quot;open&quot; `ngrok` alternative, but I always prefer to have &quot;less infra&quot; as possible to maintain. That&#039;s why building something like this on top of `libp2p` makes sense.

# :warning: Warning!

I&#039;m not a security expert, and this software didn&#039;t went through a full security audit, so don&#039;t use and rely on it for sensible traffic and not even for production environment! I did this mostly for fun while I was experimenting with libp2p. 

## Example use case: network-decentralized [k3s](https://github.com/k3s-io/k3s) test cluster

Let&#039;s see a practical example, you are developing something for kubernetes and you want to try a multi-node setup, but you have machines available that are only behind NAT (pity!) and you would really like to leverage HW.

If you are not really interested in network performance (again, that&#039;s for development purposes only!) then you could use `edgevpn` + [k3s](https://github.com/k3s-io/k3s) in this way:

1) Generate edgevpn config: `edgevpn -g &gt; vpn.yaml`
2) Start the vpn:

   on node A: `sudo IFACE=edgevpn0 ADDRESS=10.1.0.3/24 EDGEVPNCONFIG=vpn.yml edgevpn`
   
   on node B: `sudo IFACE=edgevpn0 ADDRESS=10.1.0.4/24 EDGEVPNCONFIG=vpm.yml edgevpn`
3) Start k3s:
 
   on node A: `k3s server --flannel-iface=edgevpn0`
   
   on node B: `K3S_URL=https://10.1.0.3:6443 K3S_TOKEN=xx k3s agent --flannel-iface=edgevpn0 --node-ip 10.1.0.4`

We have used flannel here, but other CNI should work as well.


# :notebook: As a library

EdgeVPN can be used as a library. It is very portable and offers a functional interface.

To join a node in a network from a token, without starting the vpn:

```golang

import (
    node &quot;github.com/mudler/edgevpn/pkg/node&quot;
)

e := node.New(
    node.Logger(l),
    node.LogLevel(log.LevelInfo),
    node.MaxMessageSize(2 &lt;&lt; 20),
    node.FromBase64( mDNSEnabled, DHTEnabled, token ),
    // ....
  )

e.Start(ctx)

```

or to start a VPN:

```golang

import (
    vpn &quot;github.com/mudler/edgevpn/pkg/vpn&quot;
    node &quot;github.com/mudler/edgevpn/pkg/node&quot;
)

opts, err := vpn.Register(vpnOpts...)
if err != nil {
	return err
}

e := edgevpn.New(append(o, opts...)...)

e.Start(ctx)
```

# üßë‚Äçüíª Projects using EdgeVPN

- [Kairos](https://github.com/kairos-io/kairos) - creates Kubernetes clusters with K3s automatically using EdgeVPN networks


# üêú Contribution

You can improve this project by contributing in following ways:

- report bugs
- fix issues
- request features
- asking questions (just open an issue)

and any other way if not mentioned here.

# :notebook: Credits

- The awesome [libp2p](https://github.com/libp2p) library
- [https://github.com/songgao/water](https://github.com/songgao/water) for tun/tap devices in golang
- [Room example](https://github.com/libp2p/go-libp2p/tree/master/examples/chat-with-rendezvous) (shamelessly parts are copied by)
- Logo originally made by [Uniconlabs](https://www.flaticon.com/authors/uniconlabs) from [www.flaticon.com](https://www.flaticon.com/), modified by me

# :notebook: Troubleshooting

If during bootstrap you see messages like:

```
edgevpn[3679]:             * [/ip4/104.131.131.82/tcp/4001] failed to negotiate stream multiplexer: context deadline exceeded     
```

or

```
edgevpn[9971]: 2021/12/16 20:56:34 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 2048 kiB, got: 416 kiB). See https://github.com/lucas-clemente/quic-go/wiki/UDP-Receive-Buffer-Size for details.
```

or generally experiencing poor network performance, it is recommended to increase the maximum buffer size by running:

```
sysctl -w net.core.rmem_max=2500000
```

# :notebook: TODO

- [x] VPN
- [x] Send and receive files via p2p
- [x] Expose remote/local services via p2p tunnelling
- [x] Store arbitrary data on the blockchain
- [x] Allow to persist blockchain on disk

# :notebook: LICENSE

Apache License v2.

```
edgevpn  Copyright (C) 2021 Ettore Di Giacinto
This program comes with ABSOLUTELY NO WARRANTY.
This is free software, and you are welcome to redistribute it
under certain conditions.
```
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[wavetermdev/waveterm]]></title>
            <link>https://github.com/wavetermdev/waveterm</link>
            <guid>https://github.com/wavetermdev/waveterm</guid>
            <pubDate>Wed, 05 Nov 2025 00:51:36 GMT</pubDate>
            <description><![CDATA[An open-source, cross-platform terminal for seamless workflows]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/wavetermdev/waveterm">wavetermdev/waveterm</a></h1>
            <p>An open-source, cross-platform terminal for seamless workflows</p>
            <p>Language: Go</p>
            <p>Stars: 12,801</p>
            <p>Forks: 516</p>
            <p>Stars today: 45 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.waveterm.dev&quot;&gt;
	&lt;picture&gt;
		&lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;./assets/wave-dark.png&quot;&gt;
		&lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;./assets/wave-light.png&quot;&gt;
		&lt;img alt=&quot;Wave Terminal Logo&quot; src=&quot;./assets/wave-light.png&quot; width=&quot;240&quot;&gt;
	&lt;/picture&gt;
  &lt;/a&gt;
  &lt;br/&gt;
&lt;/p&gt;

# Wave Terminal

[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Fwavetermdev%2Fwaveterm.svg?type=shield)](https://app.fossa.com/projects/git%2Bgithub.com%2Fwavetermdev%2Fwaveterm?ref=badge_shield)

Wave is an open-source terminal that combines traditional terminal features with graphical capabilities like file previews, web browsing, and AI assistance. It runs on MacOS, Linux, and Windows.

Modern development involves constantly switching between terminals and browsers - checking documentation, previewing files, monitoring systems, and using AI tools. Wave brings these graphical tools directly into the terminal, letting you control them from the command line. This means you can stay in your terminal workflow while still having access to the visual interfaces you need.

![WaveTerm Screenshot](./assets/wave-screenshot.webp)

## Key Features

- Flexible drag &amp; drop interface to organize terminal blocks, editors, web browsers, and AI assistants
- Built-in editor for seamlessly editing remote files with syntax highlighting and modern editor features
- Rich file preview system for remote files (markdown, images, video, PDFs, CSVs, directories)
- Integrated AI chat with support for multiple models (OpenAI, Claude, Azure, Perplexity, Ollama)
- Command Blocks for isolating and monitoring individual commands with auto-close options
- One-click remote connections with full terminal and file system access
- Rich customization including tab themes, terminal styles, and background images
- Powerful `wsh` command system for managing your workspace from the CLI and sharing data between terminal sessions

## Installation

Wave Terminal works on macOS, Linux, and Windows.

Platform-specific installation instructions can be found [here](https://docs.waveterm.dev/gettingstarted).

You can also install Wave Terminal directly from: [www.waveterm.dev/download](https://www.waveterm.dev/download).

### Minimum requirements

Wave Terminal runs on the following platforms:

- macOS 11 or later (arm64, x64)
- Windows 10 1809 or later (x64)
- Linux based on glibc-2.28 or later (Debian 10, RHEL 8, Ubuntu 20.04, etc.) (arm64, x64)

The WSH helper runs on the following platforms:

- macOS 11 or later (arm64, x64)
- Windows 10 or later (arm64, x64)
- Linux Kernel 2.6.32 or later (x64), Linux Kernel 3.1 or later (arm64)

## Roadmap

Wave is constantly improving! Our roadmap will be continuously updated with our goals for each release. You can find it [here](./ROADMAP.md).

Want to provide input to our future releases? Connect with us on [Discord](https://discord.gg/XfvZ334gwU) or open a [Feature Request](https://github.com/wavetermdev/waveterm/issues/new/choose)!

## Links

- Homepage &amp;mdash; https://www.waveterm.dev
- Download Page &amp;mdash; https://www.waveterm.dev/download
- Documentation &amp;mdash; https://docs.waveterm.dev
- Legacy Documentation &amp;mdash; https://legacydocs.waveterm.dev
- Blog &amp;mdash; https://blog.waveterm.dev
- X &amp;mdash; https://x.com/wavetermdev
- Discord Community &amp;mdash; https://discord.gg/XfvZ334gwU

## Building from Source

See [Building Wave Terminal](BUILD.md).

## Contributing

Wave uses GitHub Issues for issue tracking.

Find more information in our [Contributions Guide](CONTRIBUTING.md), which includes:

- [Ways to contribute](CONTRIBUTING.md#contributing-to-wave-terminal)
- [Contribution guidelines](CONTRIBUTING.md#before-you-start)
- [Storybook](https://docs.waveterm.dev/storybook)

## License

Wave Terminal is licensed under the Apache-2.0 License. For more information on our dependencies, see [here](./ACKNOWLEDGEMENTS.md).
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[open-telemetry/opentelemetry-collector-contrib]]></title>
            <link>https://github.com/open-telemetry/opentelemetry-collector-contrib</link>
            <guid>https://github.com/open-telemetry/opentelemetry-collector-contrib</guid>
            <pubDate>Wed, 05 Nov 2025 00:51:35 GMT</pubDate>
            <description><![CDATA[Contrib repository for the OpenTelemetry Collector]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib">open-telemetry/opentelemetry-collector-contrib</a></h1>
            <p>Contrib repository for the OpenTelemetry Collector</p>
            <p>Language: Go</p>
            <p>Stars: 4,129</p>
            <p>Forks: 3,108</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre>---

&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;
    &lt;a href=&quot;https://opentelemetry.io/docs/collector/getting-started/&quot;&gt;Getting Started&lt;/a&gt;
    &amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://github.com/open-telemetry/opentelemetry-collector/blob/main/CONTRIBUTING.md&quot;&gt;Getting Involved&lt;/a&gt;
    &amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://cloud-native.slack.com/archives/C01N6P7KR6W&quot;&gt;Getting In Touch&lt;/a&gt;
  &lt;/strong&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/open-telemetry/opentelemetry-collector-contrib/actions/workflows/build-and-test.yml?query=branch%3Amain&quot;&gt;
    &lt;img alt=&quot;Build Status&quot; src=&quot;https://img.shields.io/github/actions/workflow/status/open-telemetry/opentelemetry-collector-contrib/build-and-test.yml?branch=main&amp;style=for-the-badge&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://goreportcard.com/report/github.com/open-telemetry/opentelemetry-collector-contrib&quot;&gt;
    &lt;img alt=&quot;Go Report Card&quot; src=&quot;https://goreportcard.com/badge/github.com/open-telemetry/opentelemetry-collector-contrib?style=for-the-badge&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://codecov.io/gh/open-telemetry/opentelemetry-collector-contrib/branch/main/&quot;&gt;
    &lt;img alt=&quot;Codecov Status&quot; src=&quot;https://img.shields.io/codecov/c/github/open-telemetry/opentelemetry-collector-contrib?style=for-the-badge&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/open-telemetry/opentelemetry-collector-contrib/releases&quot;&gt;
    &lt;img alt=&quot;GitHub release (latest by date including pre-releases)&quot; src=&quot;https://img.shields.io/github/v/release/open-telemetry/opentelemetry-collector-contrib?include_prereleases&amp;style=for-the-badge&quot;&gt;
  &lt;/a&gt;
  &lt;img alt=&quot;Beta&quot; src=&quot;https://img.shields.io/badge/status-beta-informational?style=for-the-badge&amp;logo=data:image/png;base64,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&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;
    &lt;a href=&quot;https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/vision.md&quot;&gt;Vision&lt;/a&gt;
    &amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/observability.md&quot;&gt;Observability&lt;/a&gt;
    &amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
    &lt;a href=&quot;https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/security-best-practices.md&quot;&gt;Security&lt;/a&gt;
    &amp;nbsp;&amp;nbsp;&amp;bull;&amp;nbsp;&amp;nbsp;
  &lt;/strong&gt;
&lt;/p&gt;

---

# OpenTelemetry Collector Contrib

This is a repository for OpenTelemetry Collector components that are not suitable for the  [core repository](https://github.com/open-telemetry/opentelemetry-collector) of the collector. 

The official distributions, core and contrib, are available as part of the [opentelemetry-collector-releases](https://github.com/open-telemetry/opentelemetry-collector-releases) repository. Some of the components in this repository are part of the &quot;core&quot; distribution, such as the Jaeger and Prometheus components, but most of the components here are only available as part of the &quot;contrib&quot; distribution. Users of the OpenTelemetry Collector are also encouraged to build their own custom distributions with the [OpenTelemetry Collector Builder](https://github.com/open-telemetry/opentelemetry-collector/tree/main/cmd/builder), using the components they need from the core repository, the contrib repository, and possibly third-party or internal repositories.

Each component has its own support levels, as defined in the following sections. For each signal that a component supports, there&#039;s a stability level, setting the right expectations. It is possible then that a component will be **Stable** for traces but **Alpha** for metrics and **Development** for logs.

## Stability levels

Stability level for components in this repository follow the [definitions](https://github.com/open-telemetry/opentelemetry-collector#stability-levels) from the OpenTelemetry Collector repository.

## Gated features

Some features are hidden behind feature gates before they are part of the main code path for the component. Note that the feature gates themselves might be at different [lifecycle stages](https://github.com/open-telemetry/opentelemetry-collector/tree/main/featuregate#feature-lifecycle).

## Support

Each component is supported either by the community of OpenTelemetry Collector Contrib maintainers, as defined by the GitHub group [@open-telemetry/collector-contrib-maintainer](https://github.com/orgs/open-telemetry/teams/collector-contrib-maintainer), or by specific vendors. See the individual README files for information about the specific components.

The OpenTelemetry Collector Contrib maintainers may at any time downgrade specific components if they are deemed unmaintained or if they pose a risk to the repository and/or binary distribution.

Even though the OpenTelemetry Collector Contrib maintainers are ultimately responsible for the components hosted here, actual support will likely be provided by individual contributors, typically a code owner for the specific component.

## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md).

### Maintainers

- [Alex Boten](https://github.com/codeboten), Honeycomb
- [Andrzej Stencel](https://github.com/andrzej-stencel), Elastic
- [Antoine Toulme](https://github.com/atoulme), Splunk
- [Bogdan Drutu](https://github.com/bogdandrutu), Snowflake
- [Dmitrii Anoshin](https://github.com/dmitryax), Splunk
- [Edmo Vamerlatti Costa](https://github.com/edmocosta), Elastic
- [Evan Bradley](https://github.com/evan-bradley), Dynatrace
- [Pablo Baeyens](https://github.com/mx-psi), DataDog
- [Sean Marciniak](https://github.com/MovieStoreGuy), Splunk
- [Tyler Helmuth](https://github.com/TylerHelmuth), Honeycomb
- [Yang Song](https://github.com/songy23), DataDog

For more information about the maintainer role, see the [community repository](https://github.com/open-telemetry/community/blob/main/guides/contributor/membership.md#maintainer).

### Approvers

- [Andrew Wilkins](https://github.com/axw), Elastic
- [Arthur Silva Sens](https://github.com/ArthurSens), Grafana Labs
- [Braydon Kains](https://github.com/braydonk), Google
- [Christos Markou](https://github.com/ChrsMark), Elastic
- [Curtis Robert](https://github.com/crobert-1), Splunk
- [David Ashpole](https://github.com/dashpole), Google
- [Matt Wear](https://github.com/mwear), Lightstep
- [Paulo Janotti](https://github.com/pjanotti), Splunk
- [Sam DeHaan](https://github.com/dehaansa), Grafana Labs
- [Vihas Makwana](https://github.com/VihasMakwana), Elastic
- [Ziqi Zhao](https://github.com/fatsheep9146), Alibaba

For more information about the approver role, see the [community repository](https://github.com/open-telemetry/community/blob/main/guides/contributor/membership.md#approver).

### Triagers

- [Benedikt Bongartz](https://github.com/frzifus), Red Hat
- [Bogdan Stancu](https://github.com/bogdan-st), Adobe
- [Constan√ßa Manteigas](https://github.com/constanca-m), Elastic
- [Douglas Camata](https://github.com/douglascamata), Coralogix
- [Florian Bacher](https://github.com/bacherfl), Dynatrace
- [Israel Blancas](https://github.com/iblancasa), Coralogix
- [James Moessis](https://github.com/jamesmoessis), Atlassian
- [Jared Tan](https://github.com/JaredTan95), DaoCloud
- [Murphy Chen](https://github.com/Frapschen), DaoCloud
- [Ondrej Dubaj](https://github.com/odubajDT), Dynatrace
- [Roger Coll](https://github.com/rogercoll), Elastic
- Actively seeking contributors to triage issues

For more information about the triager role, see the [community repository](https://github.com/open-telemetry/community/blob/main/guides/contributor/membership.md#triager).

### Emeritus Maintainers

- [Daniel Jaglowski](https://github.com/djaglowski)
- [Juraci Paix√£o Kr√∂hling](https://github.com/jpkrohling)
- [Tigran Najaryan](https://github.com/tigrannajaryan)

For more information about the emeritus role, see the [community repository](https://github.com/open-telemetry/community/blob/main/guides/contributor/membership.md#emeritus-maintainerapprovertriager).

### Emeritus Approvers

- [Anthony Mirabella](https://github.com/Aneurysm9)
- [Bryan Aguilar](https://github.com/bryan-aguilar)
- [Przemek Maciolek](https://github.com/pmm-sumo)
- [Ruslan Kovalov](https://github.com/kovrus)

For more information about the emeritus role, see the [community repository](https://github.com/open-telemetry/community/blob/main/guides/contributor/membership.md#emeritus-maintainerapprovertriager).

### Emeritus Triagers

- [Alolita Sharma](https://github.com/alolita)
- [Gabriel Aszalos](https://github.com/gbbr)
- [Goutham Veeramachaneni](https://github.com/gouthamve)
- [Punya Biswal](https://github.com/punya)
- [Steve Flanders](https://github.com/flands)

For more information about the emeritus role, see the [community repository](https://github.com/open-telemetry/community/blob/main/guides/contributor/membership.md#emeritus-maintainerapprovertriager).

### No Over-Representation

A community member cannot be promoted to be a Collector contrib maintainer if, after their promotion, the resulting maintainers group has more than one-fourth (25%) of the members affiliated with the same employer. Job changes and similar events might result in over-representation, and no new maintainers from the same company can be promoted until representation is balanced again. In the event of confusion or concern, the OpenTelemetry Collector SIG will defer to the CNCF definition of &quot;same employer&quot;.

## PRs and Reviews

When creating a PR please follow the process [described
here](https://github.com/open-telemetry/opentelemetry-collector/blob/main/CONTRIBUTING.md#how-to-structure-prs-to-get-expedient-reviews).

New PRs will be automatically associated with the reviewers based on
[CODEOWNERS](.github/CODEOWNERS). PRs will be also automatically assigned to one of the
maintainers or approvers for facilitation.

The facilitator is responsible for helping the PR author and reviewers to make progress
or if progress cannot be made for closing the PR.

If the reviewers do not have approval rights the facilitator is also responsible
for the official approval that is required for the PR to be merged and if the facilitator
is a maintainer they are responsible for merging the PR as well.

The facilitator is not required to perform a thorough review, but they are encouraged to
enforce Collector best practices and consistency across the codebase and component
behavior. The facilitators will typically rely on codeowner&#039;s detailed review of the code
when making the final approval decision. 
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[qax-os/excelize]]></title>
            <link>https://github.com/qax-os/excelize</link>
            <guid>https://github.com/qax-os/excelize</guid>
            <pubDate>Wed, 05 Nov 2025 00:51:34 GMT</pubDate>
            <description><![CDATA[Go language library for reading and writing Microsoft Excel‚Ñ¢ (XLAM / XLSM / XLSX / XLTM / XLTX) spreadsheets]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/qax-os/excelize">qax-os/excelize</a></h1>
            <p>Go language library for reading and writing Microsoft Excel‚Ñ¢ (XLAM / XLSM / XLSX / XLTM / XLTX) spreadsheets</p>
            <p>Language: Go</p>
            <p>Stars: 19,876</p>
            <p>Forks: 1,847</p>
            <p>Stars today: 4 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;650&quot; src=&quot;./excelize.svg&quot; alt=&quot;Excelize logo&quot;&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://github.com/xuri/excelize/actions/workflows/go.yml&quot;&gt;&lt;img src=&quot;https://github.com/xuri/excelize/actions/workflows/go.yml/badge.svg&quot; alt=&quot;Build Status&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://codecov.io/gh/qax-os/excelize&quot;&gt;&lt;img src=&quot;https://codecov.io/gh/qax-os/excelize/branch/master/graph/badge.svg&quot; alt=&quot;Code Coverage&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://goreportcard.com/report/github.com/xuri/excelize/v2&quot;&gt;&lt;img src=&quot;https://goreportcard.com/badge/github.com/xuri/excelize/v2&quot; alt=&quot;Go Report Card&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://pkg.go.dev/github.com/xuri/excelize/v2&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/go.dev-reference-007d9c?logo=go&amp;logoColor=white&quot; alt=&quot;go.dev&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://opensource.org/licenses/BSD-3-Clause&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/license-bsd-orange.svg&quot; alt=&quot;Licenses&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://www.paypal.com/paypalme/xuri&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Donate-PayPal-green.svg&quot; alt=&quot;Donate&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

# Excelize

## Introduction

Excelize is a library written in pure Go providing a set of functions that allow you to write to and read from XLAM / XLSM / XLSX / XLTM / XLTX files. Supports reading and writing spreadsheet documents generated by Microsoft Excel&amp;trade; 2007 and later. Supports complex components by high compatibility, and provided streaming API for generating or reading data from a worksheet with huge amounts of data. This library needs Go version 1.24.0 or later. The full docs can be seen using go&#039;s built-in documentation tool, or online at [go.dev](https://pkg.go.dev/github.com/xuri/excelize/v2) and [docs reference](https://xuri.me/excelize/).

## Basic Usage

### Installation

```bash
go get github.com/xuri/excelize
```

- If your packages are managed using [Go Modules](https://go.dev/blog/using-go-modules), please install with following command.

```bash
go get github.com/xuri/excelize/v2
```

### Create spreadsheet

Here is a minimal example usage that will create spreadsheet file.

```go
package main

import (
    &quot;fmt&quot;

    &quot;github.com/xuri/excelize/v2&quot;
)

func main() {
    f := excelize.NewFile()
    defer func() {
        if err := f.Close(); err != nil {
            fmt.Println(err)
        }
    }()
    // Create a new sheet.
    index, err := f.NewSheet(&quot;Sheet2&quot;)
    if err != nil {
        fmt.Println(err)
        return
    }
    // Set value of a cell.
    f.SetCellValue(&quot;Sheet2&quot;, &quot;A2&quot;, &quot;Hello world.&quot;)
    f.SetCellValue(&quot;Sheet1&quot;, &quot;B2&quot;, 100)
    // Set active sheet of the workbook.
    f.SetActiveSheet(index)
    // Save spreadsheet by the given path.
    if err := f.SaveAs(&quot;Book1.xlsx&quot;); err != nil {
        fmt.Println(err)
    }
}
```

### Reading spreadsheet

The following constitutes the bare to read a spreadsheet document.

```go
package main

import (
    &quot;fmt&quot;

    &quot;github.com/xuri/excelize/v2&quot;
)

func main() {
    f, err := excelize.OpenFile(&quot;Book1.xlsx&quot;)
    if err != nil {
        fmt.Println(err)
        return
    }
    defer func() {
        // Close the spreadsheet.
        if err := f.Close(); err != nil {
            fmt.Println(err)
        }
    }()
    // Get value from cell by given worksheet name and cell reference.
    cell, err := f.GetCellValue(&quot;Sheet1&quot;, &quot;B2&quot;)
    if err != nil {
        fmt.Println(err)
        return
    }
    fmt.Println(cell)
    // Get all the rows in the Sheet1.
    rows, err := f.GetRows(&quot;Sheet1&quot;)
    if err != nil {
        fmt.Println(err)
        return
    }
    for _, row := range rows {
        for _, colCell := range row {
            fmt.Print(colCell, &quot;\t&quot;)
        }
        fmt.Println()
    }
}
```

### Add chart to spreadsheet file

With Excelize chart generation and management is as easy as a few lines of code. You can build charts based on data in your worksheet or generate charts without any data in your worksheet at all.

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;650&quot; src=&quot;./test/images/chart.png&quot; alt=&quot;Excelize&quot;&gt;&lt;/p&gt;

```go
package main

import (
    &quot;fmt&quot;

    &quot;github.com/xuri/excelize/v2&quot;
)

func main() {
    f := excelize.NewFile()
    defer func() {
        if err := f.Close(); err != nil {
            fmt.Println(err)
        }
    }()
    for idx, row := range [][]interface{}{
        {nil, &quot;Apple&quot;, &quot;Orange&quot;, &quot;Pear&quot;}, {&quot;Small&quot;, 2, 3, 3},
        {&quot;Normal&quot;, 5, 2, 4}, {&quot;Large&quot;, 6, 7, 8},
    } {
        cell, err := excelize.CoordinatesToCellName(1, idx+1)
        if err != nil {
            fmt.Println(err)
            return
        }
        f.SetSheetRow(&quot;Sheet1&quot;, cell, &amp;row)
    }
    if err := f.AddChart(&quot;Sheet1&quot;, &quot;E1&quot;, &amp;excelize.Chart{
        Type: excelize.Col3DClustered,
        Series: []excelize.ChartSeries{
            {
                Name:       &quot;Sheet1!$A$2&quot;,
                Categories: &quot;Sheet1!$B$1:$D$1&quot;,
                Values:     &quot;Sheet1!$B$2:$D$2&quot;,
            },
            {
                Name:       &quot;Sheet1!$A$3&quot;,
                Categories: &quot;Sheet1!$B$1:$D$1&quot;,
                Values:     &quot;Sheet1!$B$3:$D$3&quot;,
            },
            {
                Name:       &quot;Sheet1!$A$4&quot;,
                Categories: &quot;Sheet1!$B$1:$D$1&quot;,
                Values:     &quot;Sheet1!$B$4:$D$4&quot;,
            }},
        Title: []excelize.RichTextRun{
            {
                Text: &quot;Fruit 3D Clustered Column Chart&quot;,
            },
        },
    }); err != nil {
        fmt.Println(err)
        return
    }
    // Save spreadsheet by the given path.
    if err := f.SaveAs(&quot;Book1.xlsx&quot;); err != nil {
        fmt.Println(err)
    }
}
```

### Add picture to spreadsheet file

```go
package main

import (
    &quot;fmt&quot;
    _ &quot;image/gif&quot;
    _ &quot;image/jpeg&quot;
    _ &quot;image/png&quot;

    &quot;github.com/xuri/excelize/v2&quot;
)

func main() {
    f, err := excelize.OpenFile(&quot;Book1.xlsx&quot;)
    if err != nil {
        fmt.Println(err)
        return
    }
    defer func() {
        // Close the spreadsheet.
        if err := f.Close(); err != nil {
            fmt.Println(err)
        }
    }()
    // Insert a picture.
    if err := f.AddPicture(&quot;Sheet1&quot;, &quot;A2&quot;, &quot;image.png&quot;, nil); err != nil {
        fmt.Println(err)
    }
    // Insert a picture to worksheet with scaling.
    if err := f.AddPicture(&quot;Sheet1&quot;, &quot;D2&quot;, &quot;image.jpg&quot;,
        &amp;excelize.GraphicOptions{ScaleX: 0.5, ScaleY: 0.5}); err != nil {
        fmt.Println(err)
    }
    // Insert a picture offset in the cell with printing support.
    enable, disable := true, false
    if err := f.AddPicture(&quot;Sheet1&quot;, &quot;H2&quot;, &quot;image.gif&quot;,
        &amp;excelize.GraphicOptions{
            PrintObject:     &amp;enable,
            LockAspectRatio: false,
            OffsetX:         15,
            OffsetY:         10,
            Locked:          &amp;disable,
        }); err != nil {
        fmt.Println(err)
    }
    // Save the spreadsheet with the origin path.
    if err = f.Save(); err != nil {
        fmt.Println(err)
    }
}
```

## Contributing

Contributions are welcome! Open a pull request to fix a bug, or open an issue to discuss a new feature or change. XML is compliant with [part 1 of the 5th edition of the ECMA-376 Standard for Office Open XML](https://www.ecma-international.org/publications-and-standards/standards/ecma-376/).

## Licenses

This program is under the terms of the BSD 3-Clause License. See [https://opensource.org/licenses/BSD-3-Clause](https://opensource.org/licenses/BSD-3-Clause).

The Excel logo is a trademark of [Microsoft Corporation](https://aka.ms/trademarks-usage). This artwork is an adaptation.

The Go gopher was created by [Renee French](https://go.dev/doc/gopher/README). Licensed under the [Creative Commons 4.0 Attributions license](http://creativecommons.org/licenses/by/4.0/).
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[qualifire-dev/rogue]]></title>
            <link>https://github.com/qualifire-dev/rogue</link>
            <guid>https://github.com/qualifire-dev/rogue</guid>
            <pubDate>Wed, 05 Nov 2025 00:51:33 GMT</pubDate>
            <description><![CDATA[Agents testing framework made easy]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/qualifire-dev/rogue">qualifire-dev/rogue</a></h1>
            <p>Agents testing framework made easy</p>
            <p>Language: Go</p>
            <p>Stars: 414</p>
            <p>Forks: 51</p>
            <p>Stars today: 22 stars today</p>
            <h2>README</h2><pre># Rogue - The AI Agent Evaluator

![](https://pixel.qualifire.ai/api/record/rogue)

&lt;div align=&quot;center&quot;&gt;

![Tests](https://github.com/qualifire-dev/rogue/actions/workflows/test.yml/badge.svg?branch=main)

&lt;img src=&quot;./freddy-rogue.png&quot; width=&quot;200&quot;/&gt;

Join our [Discord community](https://discord.gg/EUfAt7ZDeK)!

&lt;/div&gt;

Rogue is a powerful tool designed to evaluate the performance, compliance, and reliability of AI agents. It pits a dynamic `EvaluatorAgent` against your agent using various protocols, testing it with a range of scenarios to ensure it behaves exactly as intended.


## Architecture

Rogue operates on a **client-server architecture**:

- **Rogue Server**: Contains the core evaluation logic
- **Client Interfaces**: Multiple interfaces that connect to the server:
  - **TUI (Terminal UI)**: Modern terminal interface built with Go and Bubble Tea
  - **Web UI**: Gradio-based web interface
  - **CLI**: Command-line interface for automated evaluation and CI/CD

This architecture allows for flexible deployment and usage patterns, where the server can run independently and multiple clients can connect to it simultaneously.

https://github.com/user-attachments/assets/b5c04772-6916-4aab-825b-6a7476d77787

### Supported Protocols &amp; Transports:
Rogue can communicate with your agent using various protocols and transports:

#### A2A
Rogue supports [Google&#039;s A2A](https://a2a-protocol.org/latest/) protocol over these transports:
1. HTTP

To integrate your agent with Rogue via A2A:
1. Build your agent using any framework of your choice
2. Expose your agent through an A2A-compliant HTTP server
3. Rogue will communicate with your agent using the standardized A2A protocol

The A2A protocol provides a standardized way for agents to communicate, including support for streaming responses, task management, and agent capabilities discovery.

For reference implementations, check out the A2A examples in the [`examples/tshirt_store_agent/`](./examples/tshirt_store_agent/) and [`examples/tshirt_store_langgraph_agent/`](./examples/tshirt_store_langgraph_agent/) directories.

#### MCP
Rogue supports the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/docs/getting-started/intro) over these transports:
1. SSE (Server-Sent Events)
2. STREAMABLE_HTTP

To integrate your agent with Rogue via MCP:
1. Build your agent using any framework of your choice
2. Wrap your agent with an MCP server that exposes a tool named `send_message`
3. Rogue will invoke this tool to communicate with and evaluate your agent

The `send_message` tool should accept a message from Rogue and return your agent&#039;s response. This simple interface allows Rogue to interact with your agent regardless of its internal implementation.

For reference implementations, check out the MCP examples in the [`examples/mcp/`](./examples/mcp/) directory, which demonstrates how to wrap a LangGraph agent with MCP for use with Rogue.

## üî• Quick Start

### Prerequisites

- `uvx` - If not installed, follow [uv installation guide](https://docs.astral.sh/uv/getting-started/installation/)
- Python 3.10+
- An API key for an LLM provider (e.g., OpenAI, Google, Anthropic).

### Installation

#### Option 1: Quick Install (Recommended)

Use our automated install script to get up and running quickly:

```bash
# TUI
uvx rogue-ai

# Web UI
uvx rogue-ai ui

# CLI / CI/CD
uvx rogue-ai cli
```

#### Option 2: Manual Installation

1.  **Clone the repository:**

    ```bash
    git clone https://github.com/qualifire-dev/rogue.git
    cd rogue
    ```

2.  **Install dependencies:**

    If you are using uv:

    ```bash
    uv sync
    ```

    Or, if you are using pip:

    ```bash
    pip install -e .
    ```

3.  **OPTIONALLY: Set up your environment variables:**
    Create a `.env` file in the root directory and add your API keys. Rogue uses `LiteLLM`, so you can set keys for various providers.
    ```env
    OPENAI_API_KEY=&quot;sk-...&quot;
    ANTHROPIC_API_KEY=&quot;sk-...&quot;
    GOOGLE_API_KEY=&quot;...&quot;
    ```

### Running Rogue

Rogue operates on a client-server architecture where the core evaluation logic runs in a backend server, and various clients connect to it for different interfaces.

#### Default Behavior

When you run `uvx rogue-ai` without any mode specified, it:

1. Starts the Rogue server in the background
2. Launches the TUI (Terminal User Interface) client

```bash
uvx rogue-ai
```

#### Available Modes

- **Default (Server + TUI)**: `uvx rogue-ai` - Starts server in background + TUI client
- **Server**: `uvx rogue-ai server` - Runs only the backend server
- **TUI**: `uvx rogue-ai tui` - Runs only the TUI client (requires server running)
- **Web UI**: `uvx rogue-ai ui` - Runs only the Gradio web interface client (requires server running)
- **CLI**: `uvx rogue-ai cli` - Runs non-interactive command-line evaluation (requires server running, ideal for CI/CD)

#### Mode Arguments

##### Server Mode

```bash
uvx rogue-ai server [OPTIONS]
```

**Options:**

- `--host HOST` - Host to run the server on (default: 127.0.0.1 or HOST env var)
- `--port PORT` - Port to run the server on (default: 8000 or PORT env var)
- `--debug` - Enable debug logging

##### TUI Mode

```bash
uvx rogue-ai tui [OPTIONS]
```

##### Web UI Mode

```bash
uvx rogue-ai ui [OPTIONS]
```

**Options:**

- `--rogue-server-url URL` - Rogue server URL (default: http://localhost:8000)
- `--port PORT` - Port to run the UI on
- `--workdir WORKDIR` - Working directory (default: ./.rogue)
- `--debug` - Enable debug logging

##### CLI Mode

```bash
uvx rogue-ai cli [OPTIONS]
```

**Options:**

- `--config-file FILE` - Path to config file
- `--rogue-server-url URL` - Rogue server URL (default: http://localhost:8000)
- `--evaluated-agent-url URL` - URL of the agent to evaluate
- `--evaluated-agent-auth-type TYPE` - Auth method (no_auth, api_key, bearer_token, basic)
- `--evaluated-agent-credentials CREDS` - Credentials for the agent
- `--input-scenarios-file FILE` - Path to scenarios file (default: &lt;workdir&gt;/scenarios.json)
- `--output-report-file FILE` - Path to output report file
- `--judge-llm MODEL` - Model for evaluation and report generation
- `--judge-llm-api-key KEY` - API key for LLM provider
- `--business-context TEXT` - Business context description
- `--business-context-file FILE` - Path to business context file
- `--deep-test-mode` - Enable deep test mode
- `--workdir WORKDIR` - Working directory (default: ./.rogue)
- `--debug` - Enable debug logging

#### Web UI Mode

To launch the Gradio web UI specifically:

```bash
uvx rogue-ai ui
```

Navigate to the URL displayed in your terminal (usually `http://127.0.0.1:7860`) to begin.

---

## Example: Testing the T-Shirt Store Agent

This repository includes a simple example agent that sells T-shirts. You can use it to see Rogue in action.

### Option 1: All-in-One (Recommended)

The easiest way to try Rogue with the example agent is to use the `--example` flag, which starts both Rogue and the example agent automatically:

```bash
uvx rogue-ai --example=tshirt_store
```

This will:

- Start the T-Shirt Store agent on `http://localhost:10001`
- Launch Rogue with the TUI interface
- Automatically clean up when you exit

You can customize the host and port:

```bash
uvx rogue-ai --example=tshirt_store --example-host localhost --example-port 10001
```

### Option 2: Manual Setup

If you prefer to run the example agent separately:

1. **Install example dependencies:**

   If you are using uv:

   ```bash
   uv sync --group examples
   ```

   or, if you are using pip:

   ```bash
   pip install -e .[examples]
   ```

2. **Start the example agent server** in a separate terminal:

   If you are using uv:

   ```bash
   uv run python -m examples.tshirt_store_agent
   ```

   Or using the script command:

   ```bash
   uv run rogue-ai-example-tshirt
   ```

   Or if installed:

   ```bash
   uvx rogue-ai-example-tshirt
   ```

   This will start the agent on `http://localhost:10001`.

3. **Configure Rogue** in the UI to point to the example agent:

   - **Agent URL**: `http://localhost:10001`
   - **Authentication**: `no-auth`

4. **Run the evaluation** and watch Rogue test the T-Shirt agent&#039;s policies!

   You can use either the TUI (`uvx rogue-ai`) or Web UI (`uvx rogue-ai ui`) mode.

---

## üîß CLI Mode

The CLI mode provides a **non-interactive** command-line interface for evaluating AI agents against predefined scenarios. It connects to the Rogue server to perform evaluations and is **ideal for CI/CD pipelines** and automated testing workflows.

### üöÄ Usage

The CLI mode requires the Rogue server to be running. You can either:

1. **Start server separately:**

   ```bash
   # Terminal 1: Start the server
   uvx rogue-ai server

   # Terminal 2: Run CLI evaluation
   uvx rogue-ai cli [OPTIONS]
   ```

2. **Use the default mode (starts server + TUI, then use TUI for evaluation)**

For development or if you prefer to install locally:

```bash
git clone https://github.com/qualifire-dev/rogue.git
cd rogue
uv sync
uv run -m rogue cli [OPTIONS]
```

Or, if you are using pip:

```bash
git clone https://github.com/qualifire-dev/rogue.git
cd rogue
pip install -e .
uv run -m rogue cli [OPTIONS]
```

---

### üìì CLI Arguments

&gt; **Note**: CLI mode is **non-interactive** and designed for automated evaluation workflows, making it perfect for CI/CD pipelines.

| Argument                      | Required                                               | Default Value                   | Description                                                                                                                                             |
| ----------------------------- | ------------------------------------------------------ | ------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |
| --workdir                     | No                                                     | `./.rogue`                      | Directory to store outputs and defaults.                                                                                                                |
| --config-file                 | No                                                     | `&lt;workdir&gt;/user_config.json`    | Path to a config file generated by the UI. Values from this file are used unless overridden via CLI. If the file does not exist, only cli will be used. |
| --rogue-server-url            | No                                                     | `http://localhost:8000`         | URL of the Rogue server to connect to.                                                                                                                  |
| --evaluated-agent-url         | Yes                                                    |                                 | The URL of the agent to evaluate.                                                                                                                       |
| --evaluated-agent-auth-type   | No                                                     | `no_auth`                       | Auth method. Can be one of: `no_auth`, `api_key`, `bearer_token`, `basic`.                                                                              |
| --evaluated-agent-credentials | Yes\*&lt;br/&gt;if `auth_type` is not `no_auth`              |                                 | Credentials for the agent (if required).                                                                                                                |
| --input-scenarios-file        | Yes                                                    | `&lt;workdir&gt;/scenarios.json`      | Path to scenarios file.                                                                                                                                 |
| --output-report-file          | No                                                     | `&lt;workdir&gt;/report.md`           | Where to save the markdown report.                                                                                                                      |
| --judge-llm                   | Yes                                                    |                                 | Model name for LLM evaluation (Litellm format).                                                                                                         |
| --judge-llm-api-key           | No                                                     |                                 | API key for LLM (see environment section).                                                                                                              |
| --business-context            | Yes\*&lt;br/&gt;Unless `--business-context-file` is supplied |                                 | Business context as a string.                                                                                                                           |
| --business-context-file       | Yes\*&lt;br/&gt;Unless `--business-context` is supplied      | `&lt;workdir&gt;/business_context.md` | OR path to file containing the business context.&lt;br/&gt;If both given, `--business-context` has priority                                                   |
| --deep-test-mode              | No                                                     | `False`                         | Enables extended testing behavior.                                                                                                                      |
| --debug                       | No                                                     | `False`                         | Enable verbose logging.                                                                                                                                 |

### üìä Config file

The config file is automatically generated when running the UI. \
We will check for a config file in `&lt;workdir&gt;/user_config.json` and use it if it exists. \
The config file is a JSON object that can contain all or a subset of the fields from the CLI arguments, except for `--config-file`. \
Other keys in the config file are ignored. \
Just remember to use snake_case keys. (e.g. `--evaluated-agent-url` becomes `evaluated_agent_url`).

### Notes

1. ‚ö†Ô∏è Either `--business-context` or `--business-context-file` must be provided.
2. ‚ö†Ô∏è Fields marked as Required are required unless supplied via the config file.

---

### Examples

### With only a config file:

with our business context located at `./.rogue/business_context.md`

#### `./.rogue/user_config.json`

```json
{
  &quot;evaluated_agent_url&quot;: &quot;http://localhost:10001&quot;,
  &quot;judge_llm&quot;: &quot;openai/o4-mini&quot;
}
```

#### Execution

```bash
uvx rogue-ai cli
```

### Same example without a config file:

#### Execution

```bash
uvx rogue-ai cli \
    --evaluated-agent-url http://localhost:10001 \
    --judge-llm openai/o4-mini \
    --business-context-file &#039;./.rogue/business_context.md&#039;
```

---

## Key Features

- **üîÑ Dynamic Scenario Generation**: Automatically creates a comprehensive test suite from your high-level business context.
- **üëÄ Live Evaluation Monitoring**: Watch the interaction between the Evaluator and your agent in a real-time chat interface.
- **üìä Comprehensive Reporting**: Generates a detailed summary of the evaluation, including pass/fail rates, key findings, and recommendations.
- **üîç Multi-Faceted Testing**: Natively supports testing for policy compliance, with a flexible framework to expand to other areas like prompt injection or safety.
- **ü§ñ Broad Model Support**: Compatible with a wide range of models from providers like OpenAI, Google (Gemini), and Anthropic.
- **üéØ User-Friendly Interface**: A simple, step-by-step Gradio UI guides you through configuration, execution, and reporting.

---

## How It Works

Rogue&#039;s workflow is designed to be simple and intuitive, managed entirely through its web interface.

1.  **Configure**: You provide the endpoint and authentication details for the agent you want to test, and select the LLMs you want Rogue to use for its services (scenario generation, judging).
2.  **Generate Scenarios**: You input the &quot;business context&quot; or a high-level description of what your agent is supposed to do. Rogue&#039;s `LLM Service` uses this context to generate a list of relevant test scenarios. You can review and edit these scenarios.
3.  **Run &amp; Evaluate**: You start the evaluation. The `Scenario Evaluation Service` spins up the `EvaluatorAgent`, which begins a conversation with your agent for each scenario. You can watch this conversation happen live.
4.  **View Report**: Once all scenarios are complete, the `LLM Service` analyzes the results and generates a Markdown-formatted report, giving you a clear summary of your agent&#039;s performance.

---

## Contributing

Contributions are welcome! If you&#039;d like to contribute, please follow these steps:

1.  Fork the repository.
2.  Create a new branch (`git checkout -b feature/your-feature-name`).
3.  Make your changes and commit them (`git commit -m &#039;Add some feature&#039;`).
4.  Push to the branch (`git push origin feature/your-feature-name`).
5.  Open a pull request.

Please make sure to update tests as appropriate.

## License

This project is licensed under a License - see the [LICENSE](LICENSE.md) file for details.
This means that you can use this freely and forever but you are not allowed to host and sell this software.

If you have any queries about the license and commercial use for this project please email `admin@qualifire.ai`



</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[trufflesecurity/trufflehog]]></title>
            <link>https://github.com/trufflesecurity/trufflehog</link>
            <guid>https://github.com/trufflesecurity/trufflehog</guid>
            <pubDate>Wed, 05 Nov 2025 00:51:32 GMT</pubDate>
            <description><![CDATA[Find, verify, and analyze leaked credentials]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/trufflesecurity/trufflehog">trufflesecurity/trufflehog</a></h1>
            <p>Find, verify, and analyze leaked credentials</p>
            <p>Language: Go</p>
            <p>Stars: 23,050</p>
            <p>Forks: 2,122</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img alt=&quot;GoReleaser Logo&quot; src=&quot;https://storage.googleapis.com/trufflehog-static-sources/pixel_pig.png&quot; height=&quot;140&quot; /&gt;
  &lt;h2 align=&quot;center&quot;&gt;TruffleHog&lt;/h2&gt;
  &lt;p align=&quot;center&quot;&gt;Find leaked credentials.&lt;/p&gt;
&lt;/p&gt;

---

&lt;div align=&quot;center&quot;&gt;

[![Go Report Card](https://goreportcard.com/badge/github.com/trufflesecurity/trufflehog/v3)](https://goreportcard.com/report/github.com/trufflesecurity/trufflehog/v3)
[![License](https://img.shields.io/badge/license-AGPL--3.0-brightgreen)](/LICENSE)
[![Total Detectors](https://img.shields.io/github/directory-file-count/trufflesecurity/truffleHog/pkg/detectors?label=Total%20Detectors&amp;type=dir)](/pkg/detectors)

&lt;/div&gt;

---

# :mag_right: _Now Scanning_

&lt;div align=&quot;center&quot;&gt;

&lt;img src=&quot;assets/scanning_logos.svg&quot;&gt;

**...and more**

To learn more about TruffleHog and its features and capabilities, visit our [product page](https://trufflesecurity.com/trufflehog?gclid=CjwKCAjwouexBhAuEiwAtW_Zx5IW87JNj97Ci7heFnA5ar6-DuNzT2Y5nIl9DuZ-FOUqx0Qg3vb9nxoClcEQAvD_BwE).

&lt;/div&gt;

# :globe_with_meridians: TruffleHog Enterprise

Are you interested in continuously monitoring **Git, Jira, Slack, Confluence, Microsoft Teams, Sharepoint, and more..** for credentials? We have an enterprise product that can help! Learn more at &lt;https://trufflesecurity.com/trufflehog-enterprise&gt;.

We take the revenue from the enterprise product to fund more awesome open source projects that the whole community can benefit from.

&lt;/div&gt;

# What is TruffleHog üêΩ

TruffleHog is the most powerful secrets **Discovery, Classification, Validation,** and **Analysis** tool. In this context, secret refers to a credential a machine uses to authenticate itself to another machine. This includes API keys, database passwords, private encryption keys, and more...

## Discovery üîç

TruffleHog can look for secrets in many places including Git, chats, wikis, logs, API testing platforms, object stores, filesystems and more

## Classification üìÅ

TruffleHog classifies over 800 secret types, mapping them back to the specific identity they belong to. Is it an AWS secret? Stripe secret? Cloudflare secret? Postgres password? SSL Private key? Sometimes it&#039;s hard to tell looking at it, so TruffleHog classifies everything it finds.

## Validation ‚úÖ

For every secret TruffleHog can classify, it can also log in to confirm if that secret is live or not. This step is critical to know if there‚Äôs an active present danger or not.

## Analysis üî¨

For the 20 some of the most commonly leaked out credential types, instead of sending one request to check if the secret can log in, TruffleHog can send many requests to learn everything there is to know about the secret. Who created it? What resources can it access? What permissions does it have on those resources?

# :loudspeaker: Join Our Community

Have questions? Feedback? Jump into Slack or Discord and hang out with us.

Join our [Slack Community](https://join.slack.com/t/trufflehog-community/shared_invite/zt-pw2qbi43-Aa86hkiimstfdKH9UCpPzQ)

Join the [Secret Scanning Discord](https://discord.gg/8Hzbrnkr7E)

# :tv: Demo

![GitHub scanning demo](https://storage.googleapis.com/truffle-demos/non-interactive.svg)

```bash
docker run --rm -it -v &quot;$PWD:/pwd&quot; trufflesecurity/trufflehog:latest github --org=trufflesecurity
```

# :floppy_disk: Installation

Several options are available for you:

### MacOS users

```bash
brew install trufflehog
```

### Docker:

&lt;sub&gt;&lt;i&gt;_Ensure Docker engine is running before executing the following commands:_&lt;/i&gt;&lt;/sub&gt;

#### &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Unix

```bash
docker run --rm -it -v &quot;$PWD:/pwd&quot; trufflesecurity/trufflehog:latest github --repo https://github.com/trufflesecurity/test_keys
```

#### &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Windows Command Prompt

```bash
docker run --rm -it -v &quot;%cd:/=\%:/pwd&quot; trufflesecurity/trufflehog:latest github --repo https://github.com/trufflesecurity/test_keys
```

#### &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Windows PowerShell

```bash
docker run --rm -it -v &quot;${PWD}:/pwd&quot; trufflesecurity/trufflehog github --repo https://github.com/trufflesecurity/test_keys
```

#### &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;M1 and M2 Mac

```bash
docker run --platform linux/arm64 --rm -it -v &quot;$PWD:/pwd&quot; trufflesecurity/trufflehog:latest github --repo https://github.com/trufflesecurity/test_keys
```

### Binary releases

```bash
Download and unpack from https://github.com/trufflesecurity/trufflehog/releases
```

### Compile from source

```bash
git clone https://github.com/trufflesecurity/trufflehog.git
cd trufflehog; go install
```

### Using installation script

```bash
curl -sSfL https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/scripts/install.sh | sh -s -- -b /usr/local/bin
```

### Using installation script, verify checksum signature (requires cosign to be installed)

```bash
curl -sSfL https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/scripts/install.sh | sh -s -- -v -b /usr/local/bin
```

### Using installation script to install a specific version

```bash
curl -sSfL https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/scripts/install.sh | sh -s -- -b /usr/local/bin &lt;ReleaseTag like v3.56.0&gt;
```

# :closed_lock_with_key: Verifying the artifacts

Checksums are applied to all artifacts, and the resulting checksum file is signed using cosign.

You need the following tool to verify signature:

- [Cosign](https://docs.sigstore.dev/cosign/system_config/installation/)

Verification steps are as follows:

1. Download the artifact files you want, and the following files from the [releases](https://github.com/trufflesecurity/trufflehog/releases) page.

   - trufflehog\_{version}\_checksums.txt
   - trufflehog\_{version}\_checksums.txt.pem
   - trufflehog\_{version}\_checksums.txt.sig

2. Verify the signature:

   ```shell
   cosign verify-blob &lt;path to trufflehog_{version}_checksums.txt&gt; \
   --certificate &lt;path to trufflehog_{version}_checksums.txt.pem&gt; \
   --signature &lt;path to trufflehog_{version}_checksums.txt.sig&gt; \
   --certificate-identity-regexp &#039;https://github\.com/trufflesecurity/trufflehog/\.github/workflows/.+&#039; \
   --certificate-oidc-issuer &quot;https://token.actions.githubusercontent.com&quot;
   ```

3. Once the signature is confirmed as valid, you can proceed to validate that the SHA256 sums align with the downloaded artifact:

   ```shell
   sha256sum --ignore-missing -c trufflehog_{version}_checksums.txt
   ```

Replace `{version}` with the downloaded files version

Alternatively, if you are using the installation script, pass `-v` option to perform signature verification.
This requires Cosign binary to be installed prior to running the installation script.

# :rocket: Quick Start

## 1: Scan a repo for only verified secrets

Command:

```bash
trufflehog git https://github.com/trufflesecurity/test_keys --results=verified
```

Expected output:

```
üê∑üîëüê∑  TruffleHog. Unearth your secrets. üê∑üîëüê∑

Found verified result üê∑üîë
Detector Type: AWS
Decoder Type: PLAIN
Raw result: AKIAYVP4CIPPERUVIFXG
Line: 4
Commit: fbc14303ffbf8fb1c2c1914e8dda7d0121633aca
File: keys
Email: counter &lt;counter@counters-MacBook-Air.local&gt;
Repository: https://github.com/trufflesecurity/test_keys
Timestamp: 2022-06-16 10:17:40 -0700 PDT
...
```

## 2: Scan a GitHub Org for only verified secrets

```bash
trufflehog github --org=trufflesecurity --results=verified
```

## 3: Scan a GitHub Repo for only verified secrets and get JSON output

Command:

```bash
trufflehog git https://github.com/trufflesecurity/test_keys --results=verified --json
```

Expected output:

```
{&quot;SourceMetadata&quot;:{&quot;Data&quot;:{&quot;Git&quot;:{&quot;commit&quot;:&quot;fbc14303ffbf8fb1c2c1914e8dda7d0121633aca&quot;,&quot;file&quot;:&quot;keys&quot;,&quot;email&quot;:&quot;counter \u003ccounter@counters-MacBook-Air.local\u003e&quot;,&quot;repository&quot;:&quot;https://github.com/trufflesecurity/test_keys&quot;,&quot;timestamp&quot;:&quot;2022-06-16 10:17:40 -0700 PDT&quot;,&quot;line&quot;:4}}},&quot;SourceID&quot;:0,&quot;SourceType&quot;:16,&quot;SourceName&quot;:&quot;trufflehog - git&quot;,&quot;DetectorType&quot;:2,&quot;DetectorName&quot;:&quot;AWS&quot;,&quot;DecoderName&quot;:&quot;PLAIN&quot;,&quot;Verified&quot;:true,&quot;Raw&quot;:&quot;AKIAYVP4CIPPERUVIFXG&quot;,&quot;Redacted&quot;:&quot;AKIAYVP4CIPPERUVIFXG&quot;,&quot;ExtraData&quot;:{&quot;account&quot;:&quot;595918472158&quot;,&quot;arn&quot;:&quot;arn:aws:iam::595918472158:user/canarytokens.com@@mirux23ppyky6hx3l6vclmhnj&quot;,&quot;user_id&quot;:&quot;AIDAYVP4CIPPJ5M54LRCY&quot;},&quot;StructuredData&quot;:null}
...
```

## 4: Scan a GitHub Repo + its Issues and Pull Requests

```bash
trufflehog github --repo=https://github.com/trufflesecurity/test_keys --issue-comments --pr-comments
```

## 5: Scan an S3 bucket for high-confidence results (verified + unknown)

```bash
trufflehog s3 --bucket=&lt;bucket name&gt; --results=verified,unknown
```

## 6: Scan S3 buckets using IAM Roles

```bash
trufflehog s3 --role-arn=&lt;iam role arn&gt;
```

## 7: Scan a Github Repo using SSH authentication in Docker

```bash
docker run --rm -v &quot;$HOME/.ssh:/root/.ssh:ro&quot; trufflesecurity/trufflehog:latest git ssh://github.com/trufflesecurity/test_keys
```

## 8: Scan individual files or directories

```bash
trufflehog filesystem path/to/file1.txt path/to/file2.txt path/to/dir
```

## 9: Scan a local git repo

Clone the git repo. For example [test keys](git@github.com:trufflesecurity/test_keys.git) repo.
```bash
$ git clone git@github.com:trufflesecurity/test_keys.git
```

Run trufflehog from the parent directory (outside the git repo).
```bash
$ trufflehog git file://test_keys --results=verified,unknown
```

To guard against malicious git configs in local scanning (see CVE-2025-41390), TruffleHog clones local git repositories to a temporary directory prior to scanning. This follows [Git&#039;s security best practices](https://git-scm.com/docs/git#_security). If you want to specify a custom path to clone the repository to (instead of tmp), you can use the `--clone-path` flag. If you&#039;d like to skip the local cloning process and scan the repository directly (only do this for trusted repos), you can use the `--trust-local-git-config` flag.

## 10: Scan GCS buckets for only verified secrets

```bash
trufflehog gcs --project-id=&lt;project-ID&gt; --cloud-environment --results=verified
```

## 11: Scan a Docker image for only verified secrets

Use the `--image` flag multiple times to scan multiple images.

```bash
# to scan from a remote registry
trufflehog docker --image trufflesecurity/secrets --results=verified

# to scan from the local docker daemon
trufflehog docker --image docker://new_image:tag --results=verified

# to scan from an image saved as a tarball
trufflehog docker --image file://path_to_image.tar --results=verified
```

## 12: Scan in CI

Set the `--since-commit` flag to your default branch that people merge into (ex: &quot;main&quot;). Set the `--branch` flag to your PR&#039;s branch name (ex: &quot;feature-1&quot;). Depending on the CI/CD platform you use, this value can be pulled in dynamically (ex: [CIRCLE_BRANCH in Circle CI](https://circleci.com/docs/variables/) and [TRAVIS_PULL_REQUEST_BRANCH in Travis CI](https://docs.travis-ci.com/user/environment-variables/)). If the repo is cloned and the target branch is already checked out during the CI/CD workflow, then `--branch HEAD` should be sufficient. The `--fail` flag will return an 183 error code if valid credentials are found.

```bash
trufflehog git file://. --since-commit main --branch feature-1 --results=verified,unknown --fail
```

## 13: Scan a Postman workspace

Use the `--workspace-id`, `--collection-id`, `--environment` flags multiple times to scan multiple targets.

```bash
trufflehog postman --token=&lt;postman api token&gt; --workspace-id=&lt;workspace id&gt;
```

## 14: Scan a Jenkins server

```bash
trufflehog jenkins --url https://jenkins.example.com --username admin --password admin
```

## 15: Scan an Elasticsearch server

### Scan a Local Cluster

There are two ways to authenticate to a local cluster with TruffleHog: (1) username and password, (2) service token.

#### Connect to a local cluster with username and password

```bash
trufflehog elasticsearch --nodes 192.168.14.3 192.168.14.4 --username truffle --password hog
```

#### Connect to a local cluster with a service token

```bash
trufflehog elasticsearch --nodes 192.168.14.3 192.168.14.4 --service-token ‚ÄòAAEWVaWM...Rva2VuaSDZ‚Äô
```

### Scan an Elastic Cloud Cluster

To scan a cluster on Elastic Cloud, you‚Äôll need a Cloud ID and API key.

```bash
trufflehog elasticsearch \
  --cloud-id &#039;search-prod:dXMtY2Vx...YjM1ODNlOWFiZGRlNjI0NA==&#039; \
  --api-key &#039;MlVtVjBZ...ZSYlduYnF1djh3NG5FQQ==&#039;
```

## 16. Scan a GitHub Repository for Cross Fork Object References and Deleted Commits

The following command will enumerate deleted and hidden commits on a GitHub repository and then scan them for secrets. This is an alpha release feature.

```bash
trufflehog github-experimental --repo https://github.com/&lt;USER&gt;/&lt;REPO&gt;.git --object-discovery
```

In addition to the normal TruffleHog output, the `--object-discovery` flag creates two files in a new `$HOME/.trufflehog` directory: `valid_hidden.txt` and `invalid.txt`. These are used to track state during commit enumeration, as well as to provide users with a complete list of all hidden and deleted commits (`valid_hidden.txt`). If you&#039;d like to automatically remove these files after scanning, please add the flag `--delete-cached-data`.

**Note**: Enumerating all valid commits on a repository using this method takes between 20 minutes and a few hours, depending on the size of your repository. We added a progress bar to keep you updated on how long the enumeration will take. The actual secret scanning runs extremely fast.

For more information on Cross Fork Object References, please [read our blog post](https://trufflesecurity.com/blog/anyone-can-access-deleted-and-private-repo-data-github).

## 17. Scan Hugging Face

### Scan a Hugging Face Model, Dataset or Space

```bash
trufflehog huggingface --model &lt;model_id&gt; --space &lt;space_id&gt; --dataset &lt;dataset_id&gt;
```

### Scan all Models, Datasets and Spaces belonging to a Hugging Face Organization or User

```bash
trufflehog huggingface --org &lt;orgname&gt; --user &lt;username&gt;
```

(Optionally) When scanning an organization or user, you can skip an entire class of resources with `--skip-models`, `--skip-datasets`, `--skip-spaces` OR a particular resource with `--ignore-models &lt;model_id&gt;`, `--ignore-datasets &lt;dataset_id&gt;`, `--ignore-spaces &lt;space_id&gt;`.

### Scan Discussion and PR Comments

```bash
trufflehog huggingface --model &lt;model_id&gt; --include-discussions --include-prs
```

## 18. Scan stdin Input

```bash
aws s3 cp s3://example/gzipped/data.gz - | gunzip -c | trufflehog stdin
```

# :question: FAQ

- All I see is `üê∑üîëüê∑  TruffleHog. Unearth your secrets. üê∑üîëüê∑` and the program exits, what gives?
  - That means no secrets were detected
- Why is the scan taking a long time when I scan a GitHub org
  - Unauthenticated GitHub scans have rate limits. To improve your rate limits, include the `--token` flag with a personal access token
- It says a private key was verified, what does that mean?
  - A verified result means TruffleHog confirmed the credential is valid by testing it against the service&#039;s API. For private keys, we&#039;ve confirmed the key can be used live for SSH or SSL authentication. Check out our Driftwood blog post to learn more [Blog post](https://trufflesecurity.com/blog/driftwood-know-if-private-keys-are-sensitive/)
- Is there an easy way to ignore specific secrets?
  - If the scanned source [supports line numbers](https://github.com/trufflesecurity/trufflehog/blob/d6375ba92172fd830abb4247cca15e3176448c5d/pkg/engine/engine.go#L358-L365), then you can add a `trufflehog:ignore` comment on the line containing the secret to ignore that secrets.

# :newspaper: What&#039;s new in v3?

TruffleHog v3 is a complete rewrite in Go with many new powerful features.

- We&#039;ve **added over 700 credential detectors that support active verification against their respective APIs**.
- We&#039;ve also added native **support for scanning GitHub, GitLab, Docker, filesystems, S3, GCS, Circle CI and Travis CI**.
- **Instantly verify private keys** against millions of github users and **billions** of TLS certificates using our [Driftwood](https://trufflesecurity.com/blog/driftwood) technology.
- Scan binaries, documents, and other file formats
- Available as a GitHub Action and a pre-commit hook

## What is credential verification?

For every potential credential that is detected, we&#039;ve painstakingly implemented programmatic verification against the API that we think it belongs to. Verification eliminates false positives and provides three result statuses:

- **verified**: Credential confirmed as valid and active by API testing
- **unverified**: Credential detected but not confirmed valid (may be invalid, expired, or verification disabled)  
- **unknown**: Verification attempted but failed due to errors, such as a network or API failure

For example, the [AWS credential detector](pkg/detectors/aws/aws.go) performs a `GetCallerIdentity` API call against the AWS API to verify if an AWS credential is active.

# :memo: Usage

TruffleHog has a sub-command for each source of data that you may want to scan:

- git
- github
- gitlab
- docker
- s3
- filesystem (files and directories)
- syslog
- circleci
- travisci
- gcs (Google Cloud Storage)
- postman
- jenkins
- elasticsearch
- stdin
- multi-scan

Each subcommand can have options that you can see with the `--help` flag provided to the sub command:

```
$ trufflehog git --help
usage: TruffleHog git [&lt;flags&gt;] &lt;uri&gt;

Find credentials in git repositories.

Flags:
  -h, --help                Show context-sensitive help (also try --help-long and --help-man).
      --log-level=0         Logging verbosity on a scale of 0 (info) to 5 (trace). Can be disabled with &quot;-1&quot;.
      --profile             Enables profiling and sets a pprof and fgprof server on :18066.
  -j, --json                Output in JSON format.
      --json-legacy         Use the pre-v3.0 JSON format. Only works with git, gitlab, and github sources.
      --github-actions      Output in GitHub Actions format.
      --concurrency=20           Number of concurrent workers.
      --no-verification     Don&#039;t verify the results.
      --results=RESULTS          Specifies which type(s) of results to output: verified (confirmed valid by API), unknown (verification failed due to error), unverified (detected but not verified), filtered_unverified (unverified but would have been filtered out). Defaults to all types.
      --allow-verification-overlap
                                 Allow verification of similar credentials across detectors
      --filter-unverified   Only output first unverified result per chunk per detector if there are more than one results.
      --filter-entropy=FILTER-ENTROPY
                                 Filter unverified results with Shannon entropy. Start with 3.0.
      --config=CONFIG            Path to configuration file.
      --print-avg-detector-time
                                 Print the average time spent on each detector.
      --no-update           Don&#039;t check for updates.
      --fail                Exit with code 183 if results are found.
      --verifier=VERIFIER ...    Set custom verification endpoints.
      --custom-verifiers-only   Only use custom verification endpoints.
      --archive-max-size=ARCHIVE-MAX-SIZE
                                 Maximum size of archive to scan. (Byte units eg. 512B, 2KB, 4MB)
      --archive-max-depth=ARCHIVE-MAX-DEPTH
                                 Maximum depth of archive to scan.
      --archive-timeout=ARCHIVE-TIMEOUT
                                 Maximum time to spend extracting an archive.
      --include-detectors=&quot;all&quot;  Comma separated list of detector types to include. Protobuf name or IDs may be used, as well as ranges.
      --exclude-detectors=EXCLUDE-DETECTORS
                                 Comma separated list of detector types to exclude. Protobuf name or IDs may be used, as well as ranges. IDs defined here take precedence over the include list.
      --version             Show application version.
  -i, --include-paths=INCLUDE-PATHS
                                 Path to f

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[ekristen/aws-nuke]]></title>
            <link>https://github.com/ekristen/aws-nuke</link>
            <guid>https://github.com/ekristen/aws-nuke</guid>
            <pubDate>Wed, 05 Nov 2025 00:51:31 GMT</pubDate>
            <description><![CDATA[Remove all the resources from an AWS account]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ekristen/aws-nuke">ekristen/aws-nuke</a></h1>
            <p>Remove all the resources from an AWS account</p>
            <p>Language: Go</p>
            <p>Stars: 1,034</p>
            <p>Forks: 97</p>
            <p>Stars today: 15 stars today</p>
            <h2>README</h2><pre># aws-nuke

[![license](https://img.shields.io/github/license/ekristen/aws-nuke.svg)](https://github.com/ekristen/aws-nuke/blob/main/LICENSE)
[![release](https://img.shields.io/github/release/ekristen/aws-nuke.svg)](https://github.com/ekristen/aws-nuke/releases)
[![Go Report Card](https://goreportcard.com/badge/github.com/ekristen/aws-nuke)](https://goreportcard.com/report/github.com/ekristen/aws-nuke)
[![Maintainability](https://api.codeclimate.com/v1/badges/bf05fb12c69f1ea7f257/maintainability)](https://codeclimate.com/github/ekristen/aws-nuke/maintainability)
![GitHub Downloads (all assets, all releases)](https://img.shields.io/github/downloads/ekristen/aws-nuke/total)
![GitHub Downloads (all assets, latest release)](https://img.shields.io/github/downloads/ekristen/aws-nuke/latest/total)



## Overview

Remove all resources from an AWS account.

*aws-nuke* is stable, but it is likely that not all AWS resources are covered by it. Be encouraged to add missing
resources and create a Pull Request or to create an [Issue](https://github.com/ekristen/aws-nuke/issues/new).

## What&#039;s New in Version 3

Version 3 is a rewrite of this tool using [libnuke](https://github.com/ekristen/libnuke) with a focus on improving a number of the outstanding things
that I couldn&#039;t get done with the original project without separating out the core code into a library. See Goals
below for more.

This is not a comprehensive list, but here are some of the highlights:

* New Feature: Signed Darwin Binaries for macOS
* New Feature: Published Homebrew Tap (ekristen/tap/aws-nuke@3)
* New Feature: Global Filters
* New Feature: Run Against All Enabled Regions
* New Feature: Explain Account and Explain Config Commands
* Upcoming Feature: Filter Groups (**in progress**)
* Breaking Change: `root` command no longer triggers the run, must use subcommand `run` (alias: `nuke`)
* Breaking Change: CloudFormation Stacks now support a hold and wait for parent deletion process
* Breaking Change: Nested CloudFormation Stacks are now eligible for deletion and no longer omitted
* Completely rewrote the core of the tool as a dedicated library [libnuke](https://github.com/ekristen/libnuke)
  * This library has over 95% test coverage which makes iteration and new features easier to implement.
* Semantic Releases with notifications on issues / pull requests
* Context is passed throughout the entire library now, including the listing function and the removal function
  * This is in preparation for supporting AWS SDK Go v2
* New Resources
* Broke away from rebuy-de/aws-nuke project as a fork for reasons outlined in the history section

### Goals

- [x] Easier maintainability and bug fixing, see go report and code climate badges above
- [x] Adding additional tests around the core library
- [ ] Adding more tests around specific resource types
- [x] Adding additional resources and tooling to make adding resources easier
- [x] Adding documentation for adding resources and using the tool
- [ ] Consider adding DAG for dependencies between resource types and individual resources
- [ ] Support for AWS SDK Go v2

## Documentation

All documentation is in the [docs/](docs) directory and is built using [Material for Mkdocs](https://squidfunk.github.io/mkdocs-material/). 

It is hosted at [https://ekristen.github.io/aws-nuke/](https://ekristen.github.io/aws-nuke/).

## History of this Fork

**Important:** this is a full fork of the original tool written by the folks over at [rebuy-de](https://github.com/rebuy-de).
This fork became necessary after attempting to make contributions and respond to issues to learn that the current
maintainers only have time to work on the project about once a month and while receptive to bringing in other 
people to help maintain, made it clear it would take time. Considering the feedback cycle was already weeks on 
initial communications, I had to make the hard decision to fork and maintain it.

### libnuke

I also needed a version of this tool for Azure and GCP, and initially I just copied and altered the code I needed for
Azure, but I didn&#039;t want to have to maintain multiple copies of the same code, so I decided to create 
[libnuke](https://github.com/ekristen/libnuke) to abstract all the code that was common between the two tools and write
proper unit tests for it. 

## Attribution, License, and Copyright

The rewrite of this tool to use [libnuke](https://github.com/ekristen/libnuke) would not have been possible without the
hard work that came before me on the original tool by the team and contributors over at [rebuy-de](https://github.com/rebuy-de)
and their original work on [rebuy-de/aws-nuke](https://github.com/rebuy-de/aws-nuke).

This tool is licensed under the MIT license. See the [LICENSE](LICENSE) file for more information. The bulk of this
tool was rewritten to use [libnuke](https://github.com/ekristen/libnuke) which was in part originally sourced from
[rebuy-de/aws-nuke](https://github.com/rebuy-de/aws-nuke).

## Contribute

You can contribute to *aws-nuke* by forking this repository, making your changes and creating a Pull Request against
this repository. If you are unsure how to solve a problem or have other questions about a contributions, please create
a GitHub issue.

</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[gofr-dev/gofr]]></title>
            <link>https://github.com/gofr-dev/gofr</link>
            <guid>https://github.com/gofr-dev/gofr</guid>
            <pubDate>Wed, 05 Nov 2025 00:51:30 GMT</pubDate>
            <description><![CDATA[An opinionated GoLang framework for accelerated microservice development. Built in support for databases and observability.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/gofr-dev/gofr">gofr-dev/gofr</a></h1>
            <p>An opinionated GoLang framework for accelerated microservice development. Built in support for databases and observability.</p>
            <p>Language: Go</p>
            <p>Stars: 14,184</p>
            <p>Forks: 1,716</p>
            <p>Stars today: 52 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;h1 style=&quot;font-size: 100px; font-weight: 500;&quot;&gt;
    &lt;i&gt;Go&lt;/i&gt;Fr
&lt;/h1&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;p&gt;
&lt;img width=&quot;300&quot; alt=&quot;logo&quot; src=&quot;https://github.com/gofr-dev/gofr/assets/44036979/916fe7b1-42fb-4af1-9e0b-4a7a064c243c&quot;&gt;
&lt;h2 style=&quot;font-size: 28px;&quot;&gt;&lt;b&gt;GoFr: An Opinionated Microservice Development Framework&lt;/b&gt;&lt;/h2&gt;
&lt;/p&gt;
&lt;a href=&quot;https://pkg.go.dev/gofr.dev&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GoDoc-Read%20Documentation-blue?style=for-the-badge&quot; alt=&quot;godoc&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://gofr.dev/docs&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GoFr-Docs-orange?style=for-the-badge&quot; alt=&quot;gofr-docs&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://qlty.sh/gh/gofr-dev/projects/gofr&quot;&gt;&lt;img src=&quot;https://qlty.sh/gh/gofr-dev/projects/gofr/maintainability.svg&quot; alt=&quot;Maintainability&quot; height=&quot;27.99&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://qlty.sh/gh/gofr-dev/projects/gofr&quot;&gt;&lt;img src=&quot;https://qlty.sh/gh/gofr-dev/projects/gofr/coverage.svg&quot; alt=&quot;Code Coverage&quot; height=&quot;27.99&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://goreportcard.com/report/gofr.dev&quot;&gt;&lt;img src=&quot;https://goreportcard.com/badge/gofr.dev?style=for-the-badge&quot; alt=&quot;Go Report Card&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://opensource.org/licenses/Apache-2.0&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-Apache_2.0-blue?style=for-the-badge&quot; alt=&quot;Apache 2.0 License&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://discord.gg/wsaSkQTdgq&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/discord-join-us?style=for-the-badge&amp;logo=discord&amp;color=7289DA&quot; alt=&quot;discord&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://gurubase.io/g/gofr&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Gurubase-Ask%20GoFr%20Guru-006BFF?style=for-the-badge&quot; /&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h2&gt;Listed in the &lt;a href=&quot;https://landscape.cncf.io/?selected=go-fr&quot;&gt;CNCF Landscape&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;

## üéØ **Goal**
GoFr is designed to **simplify microservice development**, with key focuses on **Kubernetes deployment** and **out-of-the-box observability**. While capable of building generic applications, **microservices** remain at its core.

---

## üí° **Key Features**

1. **Simple API Syntax**
2. **REST Standards by Default**
3. **Configuration Management**
4. **[Observability](https://gofr.dev/docs/quick-start/observability)** (Logs, Traces, Metrics)
5. **Inbuilt [Auth Middleware](https://gofr.dev/docs/advanced-guide/http-authentication)** &amp; Custom Middleware Support
6. **[gRPC Support](https://gofr.dev/docs/advanced-guide/grpc)**
7. **[HTTP Service](https://gofr.dev/docs/advanced-guide/http-communication)** with Circuit Breaker Support
8. **[Pub/Sub](https://gofr.dev/docs/advanced-guide/using-publisher-subscriber)**
9. **[Health Check](https://gofr.dev/docs/advanced-guide/monitoring-service-health)** for All Datasources
10. **[Database Migration](https://gofr.dev/docs/advanced-guide/handling-data-migrations)**
11. **[Cron Jobs](https://gofr.dev/docs/advanced-guide/using-cron)**
12. **Support for [Changing Log Level](https://gofr.dev/docs/advanced-guide/remote-log-level-change) Without Restarting**
13. **[Swagger Rendering](https://gofr.dev/docs/advanced-guide/swagger-documentation)**
14. **[Abstracted File Systems](https://gofr.dev/docs/advanced-guide/handling-file)**
15. **[Websockets](https://gofr.dev/docs/advanced-guide/websocket)**

---

## üöÄ **Getting Started**

### **Prerequisites**
- GoFr requires **[Go](https://go.dev/)** version **[1.24](https://go.dev/doc/devel/release#go1.24.0)** or above.

### **Installation**
To get started with GoFr, add the following import to your code and use Go‚Äôs module support to automatically fetch dependencies:

```go
import &quot;gofr.dev/pkg/gofr&quot;
```

Alternatively, use the command:

```bash
go get -u gofr.dev/pkg/gofr
```

---

## üèÉ **Running GoFr**

Here&#039;s a simple example to get a GoFr application up and running:

```go
package main

import &quot;gofr.dev/pkg/gofr&quot;

func main() {
	app := gofr.New()

	app.GET(&quot;/greet&quot;, func(ctx *gofr.Context) (any, error) {
		return &quot;Hello World!&quot;, nil
	})

	app.Run() // listens and serves on localhost:8000
}
```

To run this code:

```bash
$ go run main.go
```

Visit [`localhost:8000/greet`](http://localhost:8000/greet) to see the result.

---

## üìÇ **More Examples**

Explore a variety of ready-to-run examples in the [GoFr examples directory](https://github.com/gofr-dev/gofr/tree/development/examples).

---

## üë©‚Äçüíª **Documentation**

- **[GoDoc](https://pkg.go.dev/gofr.dev)**: Official API documentation.
- **[GoFr Documentation](https://gofr.dev/docs)**: Comprehensive guides and resources.

---

## üëç **Contribute**

Join Us in Making GoFr Better

**Share your experience**: If you‚Äôve found GoFr helpful, consider writing a review or tutorial on platforms like **[Medium](https://medium.com/)**, **[Dev.to](https://dev.to/)**, or your personal blog. 
Your insights could help others get started faster!

**Contribute to the project**: Want to get involved? Check out our **[CONTRIBUTING.md](CONTRIBUTING.md)**
guide to learn how you can contribute code, suggest improvements, or report issues.

---

## üîí **Secure Cloning**
To securely clone the GoFr repository, you can use HTTPS or SSH:

### Cloning with HTTPS
```bash
git clone https://github.com/gofr-dev/gofr.git
```
### Cloning with SSH
```bash
git clone git@github.com:gofr-dev/gofr.git
```

### üéÅ **Get a GoFr T-Shirt &amp; Stickers!**

If your PR is merged, or if you contribute by writing articles or promoting GoFr, we invite you to fill out [this form](https://forms.gle/R1Yz7ZzY3U5WWTgy5) to claim your GoFr merchandise as a token of our appreciation! 

### Partners

&lt;img src=&quot;https://resources.jetbrains.com/storage/products/company/brand/logos/jetbrains.png&quot; alt=&quot;JetBrains logo&quot; width=&quot;200&quot;&gt;
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[prometheus/node_exporter]]></title>
            <link>https://github.com/prometheus/node_exporter</link>
            <guid>https://github.com/prometheus/node_exporter</guid>
            <pubDate>Wed, 05 Nov 2025 00:51:29 GMT</pubDate>
            <description><![CDATA[Exporter for machine metrics]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/prometheus/node_exporter">prometheus/node_exporter</a></h1>
            <p>Exporter for machine metrics</p>
            <p>Language: Go</p>
            <p>Stars: 12,735</p>
            <p>Forks: 2,559</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre># Node exporter

[![CircleCI](https://circleci.com/gh/prometheus/node_exporter/tree/master.svg?style=shield)][circleci]
![bsd workflow](https://github.com/prometheus/node_exporter/actions/workflows/bsd.yml/badge.svg)
![golangci-lint workflow](https://github.com/prometheus/node_exporter/actions/workflows/golangci-lint.yml/badge.svg)
[![Docker Repository on Quay](https://quay.io/repository/prometheus/node-exporter/status)][quay]
[![Docker Pulls](https://img.shields.io/docker/pulls/prom/node-exporter.svg?maxAge=604800)][hub]
[![Go Report Card](https://goreportcard.com/badge/github.com/prometheus/node_exporter)][goreportcard]

Prometheus exporter for hardware and OS metrics exposed by \*NIX kernels, written
in Go with pluggable metric collectors.

The [Windows exporter](https://github.com/prometheus-community/windows_exporter) is recommended for Windows users.
To expose NVIDIA GPU metrics, [prometheus-dcgm
](https://github.com/NVIDIA/dcgm-exporter)
can be used.

## Installation and Usage

If you are new to Prometheus and `node_exporter` there is a [simple step-by-step guide](https://prometheus.io/docs/guides/node-exporter/).

The `node_exporter` listens on HTTP port 9100 by default. See the `--help` output for more options.

### Ansible

For automated installs with [Ansible](https://www.ansible.com/), there is the [Prometheus Community role](https://github.com/prometheus-community/ansible).

### Docker

The `node_exporter` is designed to monitor the host system. Deploying in containers requires
extra care in order to avoid monitoring the container itself.

For situations where containerized deployment is needed, some extra flags must be used to allow
the `node_exporter` access to the host namespaces.

Be aware that any non-root mount points you want to monitor will need to be bind-mounted
into the container.

If you start container for host monitoring, specify `path.rootfs` argument.
This argument must match path in bind-mount of host root. The node\_exporter will use
`path.rootfs` as prefix to access host filesystem.

```bash
docker run -d \
  --net=&quot;host&quot; \
  --pid=&quot;host&quot; \
  -v &quot;/:/host:ro,rslave&quot; \
  quay.io/prometheus/node-exporter:latest \
  --path.rootfs=/host
```

For Docker compose, similar flag changes are needed.

```yaml
---
version: &#039;3.8&#039;

services:
  node_exporter:
    image: quay.io/prometheus/node-exporter:latest
    container_name: node_exporter
    command:
      - &#039;--path.rootfs=/host&#039;
    network_mode: host
    pid: host
    restart: unless-stopped
    volumes:
      - &#039;/:/host:ro,rslave&#039;
```

On some systems, the `timex` collector requires an additional Docker flag,
`--cap-add=SYS_TIME`, in order to access the required syscalls.

## Collectors

There is varying support for collectors on each operating system. The tables
below list all existing collectors and the supported systems.

Collectors are enabled by providing a `--collector.&lt;name&gt;` flag.
Collectors that are enabled by default can be disabled by providing a `--no-collector.&lt;name&gt;` flag.
To enable only some specific collector(s), use `--collector.disable-defaults --collector.&lt;name&gt; ...`.

### Include &amp; Exclude flags

A few collectors can be configured to include or exclude certain patterns using dedicated flags. The exclude flags are used to indicate &quot;all except&quot;, while the include flags are used to say &quot;none except&quot;. Note that these flags are mutually exclusive on collectors that support both.

Example:

```txt
--collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
```

List:

Collector | Scope | Include Flag | Exclude Flag
--- | --- | --- | ---
arp | device | --collector.arp.device-include | --collector.arp.device-exclude
cpu | bugs | --collector.cpu.info.bugs-include | N/A
cpu | flags | --collector.cpu.info.flags-include | N/A
diskstats | device | --collector.diskstats.device-include | --collector.diskstats.device-exclude
ethtool | device | --collector.ethtool.device-include | --collector.ethtool.device-exclude
ethtool | metrics | --collector.ethtool.metrics-include | N/A
filesystem | fs-types | --collector.filesystem.fs-types-include | --collector.filesystem.fs-types-exclude
filesystem | mount-points | --collector.filesystem.mount-points-include | --collector.filesystem.mount-points-exclude
hwmon | chip | --collector.hwmon.chip-include | --collector.hwmon.chip-exclude
hwmon | sensor | --collector.hwmon.sensor-include | --collector.hwmon.sensor-exclude
interrupts | name | --collector.interrupts.name-include | --collector.interrupts.name-exclude
netdev | device | --collector.netdev.device-include | --collector.netdev.device-exclude
qdisk | device | --collector.qdisk.device-include | --collector.qdisk.device-exclude
slabinfo | slab-names | --collector.slabinfo.slabs-include | --collector.slabinfo.slabs-exclude
sysctl | all | --collector.sysctl.include | N/A
systemd | unit | --collector.systemd.unit-include | --collector.systemd.unit-exclude

### Enabled by default

Name     | Description | OS
---------|-------------|----
arp | Exposes ARP statistics from `/proc/net/arp`. | Linux
bcache | Exposes bcache statistics from `/sys/fs/bcache/`. | Linux
bonding | Exposes the number of configured and active slaves of Linux bonding interfaces. | Linux
btrfs | Exposes btrfs statistics | Linux
boottime | Exposes system boot time derived from the `kern.boottime` sysctl. | Darwin, Dragonfly, FreeBSD, NetBSD, OpenBSD, Solaris
conntrack | Shows conntrack statistics (does nothing if no `/proc/sys/net/netfilter/` present). | Linux
cpu | Exposes CPU statistics | Darwin, Dragonfly, FreeBSD, Linux, Solaris, OpenBSD
cpufreq | Exposes CPU frequency statistics | Linux, Solaris
diskstats | Exposes disk I/O statistics. | Darwin, Linux, OpenBSD
dmi | Expose Desktop Management Interface (DMI) info from `/sys/class/dmi/id/` | Linux
edac | Exposes error detection and correction statistics. | Linux
entropy | Exposes available entropy. | Linux
exec | Exposes execution statistics. | Dragonfly, FreeBSD
fibrechannel | Exposes fibre channel information and statistics from `/sys/class/fc_host/`. | Linux
filefd | Exposes file descriptor statistics from `/proc/sys/fs/file-nr`. | Linux
filesystem | Exposes filesystem statistics, such as disk space used. | Darwin, Dragonfly, FreeBSD, Linux, OpenBSD
hwmon | Expose hardware monitoring and sensor data from `/sys/class/hwmon/`. | Linux
infiniband | Exposes network statistics specific to InfiniBand and Intel OmniPath configurations. | Linux
ipvs | Exposes IPVS status from `/proc/net/ip_vs` and stats from `/proc/net/ip_vs_stats`. | Linux
loadavg | Exposes load average. | Darwin, Dragonfly, FreeBSD, Linux, NetBSD, OpenBSD, Solaris
mdadm | Exposes statistics about devices in `/proc/mdstat` (does nothing if no `/proc/mdstat` present). | Linux
meminfo | Exposes memory statistics. | Darwin, Dragonfly, FreeBSD, Linux, OpenBSD
netclass | Exposes network interface info from `/sys/class/net/` | Linux
netdev | Exposes network interface statistics such as bytes transferred. | Darwin, Dragonfly, FreeBSD, Linux, OpenBSD
netisr | Exposes netisr statistics | FreeBSD
netstat | Exposes network statistics from `/proc/net/netstat`. This is the same information as `netstat -s`. | Linux
nfs | Exposes NFS client statistics from `/proc/net/rpc/nfs`. This is the same information as `nfsstat -c`. | Linux
nfsd | Exposes NFS kernel server statistics from `/proc/net/rpc/nfsd`. This is the same information as `nfsstat -s`. | Linux
nvme | Exposes NVMe info from `/sys/class/nvme/` | Linux
os | Expose OS release info from `/etc/os-release` or `/usr/lib/os-release` | _any_
powersupplyclass | Exposes Power Supply statistics from `/sys/class/power_supply` | Linux
pressure | Exposes pressure stall statistics from `/proc/pressure/`. | Linux (kernel 4.20+ and/or [CONFIG\_PSI](https://www.kernel.org/doc/html/latest/accounting/psi.html))
rapl | Exposes various statistics from `/sys/class/powercap`. | Linux
schedstat | Exposes task scheduler statistics from `/proc/schedstat`. | Linux
selinux | Exposes SELinux statistics. | Linux
sockstat | Exposes various statistics from `/proc/net/sockstat`. | Linux
softnet | Exposes statistics from `/proc/net/softnet_stat`. | Linux
stat | Exposes various statistics from `/proc/stat`. This includes boot time, forks and interrupts. | Linux
tapestats | Exposes statistics from `/sys/class/scsi_tape`. | Linux
textfile | Exposes statistics read from local disk. The `--collector.textfile.directory` flag must be set. | _any_
thermal | Exposes thermal statistics like `pmset -g therm`. | Darwin
thermal\_zone | Exposes thermal zone &amp; cooling device statistics from `/sys/class/thermal`. | Linux
time | Exposes the current system time. | _any_
timex | Exposes selected adjtimex(2) system call stats. | Linux
udp_queues | Exposes UDP total lengths of the rx_queue and tx_queue from `/proc/net/udp` and `/proc/net/udp6`. | Linux
uname | Exposes system information as provided by the uname system call. | Darwin, FreeBSD, Linux, OpenBSD
vmstat | Exposes statistics from `/proc/vmstat`. | Linux
watchdog | Exposes statistics from `/sys/class/watchdog` | Linux
xfs | Exposes XFS runtime statistics. | Linux (kernel 4.4+)
zfs | Exposes [ZFS](http://open-zfs.org/) performance statistics. | FreeBSD, [Linux](http://zfsonlinux.org/), Solaris

### Disabled by default

`node_exporter` also implements a number of collectors that are disabled by default.  Reasons for this vary by
collector, and may include:
* High cardinality
* Prolonged runtime that exceeds the Prometheus `scrape_interval` or `scrape_timeout`
* Significant resource demands on the host

You can enable additional collectors as desired by adding them to your
init system&#039;s or service supervisor&#039;s startup configuration for
`node_exporter` but caution is advised.  Enable at most one at a time,
testing first on a non-production system, then by hand on a single
production node.  When enabling additional collectors, you should
carefully monitor the change by observing the `
scrape_duration_seconds` metric to ensure that collection completes
and does not time out.  In addition, monitor the
`scrape_samples_post_metric_relabeling` metric to see the changes in
cardinality.

Name     | Description | OS
---------|-------------|----
buddyinfo | Exposes statistics of memory fragments as reported by /proc/buddyinfo. | Linux
cgroups | A summary of the number of active and enabled cgroups | Linux
cpu\_vulnerabilities | Exposes CPU vulnerability information from sysfs. | Linux
devstat | Exposes device statistics | Dragonfly, FreeBSD
drm | Expose GPU metrics using sysfs / DRM, `amdgpu` is the only driver which exposes this information through DRM | Linux
drbd | Exposes Distributed Replicated Block Device statistics (to version 8.4) | Linux
ethtool | Exposes network interface information and network driver statistics equivalent to `ethtool`, `ethtool -S`, and `ethtool -i`. | Linux
interrupts | Exposes detailed interrupts statistics. | Linux, OpenBSD
ksmd | Exposes kernel and system statistics from `/sys/kernel/mm/ksm`. | Linux
lnstat | Exposes stats from `/proc/net/stat/`. | Linux
logind | Exposes session counts from [logind](http://www.freedesktop.org/wiki/Software/systemd/logind/). | Linux
meminfo\_numa | Exposes memory statistics from `/sys/devices/system/node/node[0-9]*/meminfo`, `/sys/devices/system/node/node[0-9]*/numastat`. | Linux
mountstats | Exposes filesystem statistics from `/proc/self/mountstats`. Exposes detailed NFS client statistics. | Linux
network_route | Exposes the routing table as metrics | Linux
pcidevice | Exposes pci devices&#039; information including their link status and parent devices. | Linux
perf | Exposes perf based metrics (Warning: Metrics are dependent on kernel configuration and settings). | Linux
processes | Exposes aggregate process statistics from `/proc`. | Linux
qdisc | Exposes [queuing discipline](https://en.wikipedia.org/wiki/Network_scheduler#Linux_kernel) statistics | Linux
slabinfo | Exposes slab statistics from `/proc/slabinfo`. Note that permission of `/proc/slabinfo` is usually 0400, so set it appropriately. | Linux
softirqs | Exposes detailed softirq statistics from `/proc/softirqs`. | Linux
sysctl | Expose sysctl values from `/proc/sys`. Use `--collector.sysctl.include(-info)` to configure. | Linux
swap | Expose swap information from `/proc/swaps`. | Linux
systemd | Exposes service and system status from [systemd](http://www.freedesktop.org/wiki/Software/systemd/). | Linux
tcpstat | Exposes TCP connection status information from `/proc/net/tcp` and `/proc/net/tcp6`. (Warning: the current version has potential performance issues in high load situations.) | Linux
wifi | Exposes WiFi device and station statistics. | Linux
xfrm | Exposes statistics from `/proc/net/xfrm_stat` | Linux
zoneinfo | Exposes NUMA memory zone metrics. | Linux

### Deprecated

These collectors are deprecated and will be removed in the next major release.

Name     | Description | OS
---------|-------------|----
ntp | Exposes local NTP daemon health to check [time](./docs/TIME.md) | _any_
runit | Exposes service status from [runit](http://smarden.org/runit/). | _any_
supervisord | Exposes service status from [supervisord](http://supervisord.org/). | _any_

### Perf Collector

The `perf` collector may not work out of the box on some Linux systems due to kernel
configuration and security settings. To allow access, set the following `sysctl`
parameter:

```
sysctl -w kernel.perf_event_paranoid=X
```

- 2 allow only user-space measurements (default since Linux 4.6).
- 1 allow both kernel and user measurements (default before Linux 4.6).
- 0 allow access to CPU-specific data but not raw tracepoint samples.
- -1 no restrictions.

Depending on the configured value different metrics will be available, for most
cases `0` will provide the most complete set. For more information see [`man 2
perf_event_open`](http://man7.org/linux/man-pages/man2/perf_event_open.2.html).

By default, the `perf` collector will only collect metrics of the CPUs that
`node_exporter` is running on (ie
[`runtime.NumCPU`](https://golang.org/pkg/runtime/#NumCPU). If this is
insufficient (e.g. if you run `node_exporter` with its CPU affinity set to
specific CPUs), you can specify a list of alternate CPUs by using the
`--collector.perf.cpus` flag. For example, to collect metrics on CPUs 2-6, you
would specify: `--collector.perf --collector.perf.cpus=2-6`. The CPU
configuration is zero indexed and can also take a stride value; e.g.
`--collector.perf --collector.perf.cpus=1-10:5` would collect on CPUs
1, 5, and 10.

The `perf` collector is also able to collect
[tracepoint](https://www.kernel.org/doc/html/latest/core-api/tracepoint.html)
counts when using the `--collector.perf.tracepoint` flag. Tracepoints can be
found using [`perf list`](http://man7.org/linux/man-pages/man1/perf.1.html) or
from debugfs. And example usage of this would be
`--collector.perf.tracepoint=&quot;sched:sched_process_exec&quot;`.

### Sysctl Collector

The `sysctl` collector can be enabled with `--collector.sysctl`. It supports exposing numeric sysctl values
as metrics using the `--collector.sysctl.include` flag and string values as info metrics by using the
`--collector.sysctl.include-info` flag. The flags can be repeated. For sysctl with multiple numeric values,
an optional mapping can be given to expose each value as its own metric. Otherwise an `index` label is used
to identify the different fields.

#### Examples
##### Numeric values
###### Single values
Using `--collector.sysctl.include=vm.user_reserve_kbytes`:
`vm.user_reserve_kbytes = 131072` -&gt; `node_sysctl_vm_user_reserve_kbytes 131072`

###### Multiple values
A sysctl can contain multiple values, for example:
```
net.ipv4.tcp_rmem = 4096	131072	6291456
```
Using `--collector.sysctl.include=net.ipv4.tcp_rmem` the collector will expose:
```
node_sysctl_net_ipv4_tcp_rmem{index=&quot;0&quot;} 4096
node_sysctl_net_ipv4_tcp_rmem{index=&quot;1&quot;} 131072
node_sysctl_net_ipv4_tcp_rmem{index=&quot;2&quot;} 6291456
```
If the indexes have defined meaning like in this case, the values can be mapped to multiple metrics by appending the mapping to the --collector.sysctl.include flag:
Using `--collector.sysctl.include=net.ipv4.tcp_rmem:min,default,max` the collector will expose:
```
node_sysctl_net_ipv4_tcp_rmem_min 4096
node_sysctl_net_ipv4_tcp_rmem_default 131072
node_sysctl_net_ipv4_tcp_rmem_max 6291456
```

##### String values
String values need to be exposed as info metric. The user selects them by using the `--collector.sysctl.include-info` flag.

###### Single values
`kernel.core_pattern = core` -&gt; `node_sysctl_info{key=&quot;kernel.core_pattern_info&quot;, value=&quot;core&quot;} 1`

###### Multiple values
Given the following sysctl:
```
kernel.seccomp.actions_avail = kill_process kill_thread trap errno trace log allow
```
Setting `--collector.sysctl.include-info=kernel.seccomp.actions_avail` will yield:
```
node_sysctl_info{key=&quot;kernel.seccomp.actions_avail&quot;, index=&quot;0&quot;, value=&quot;kill_process&quot;} 1
node_sysctl_info{key=&quot;kernel.seccomp.actions_avail&quot;, index=&quot;1&quot;, value=&quot;kill_thread&quot;} 1
...
```

### Textfile Collector

The `textfile` collector is similar to the [Pushgateway](https://github.com/prometheus/pushgateway),
in that it allows exporting of statistics from batch jobs. It can also be used
to export static metrics, such as what role a machine has. The Pushgateway
should be used for service-level metrics. The `textfile` module is for metrics
that are tied to a machine.

To use it, set the `--collector.textfile.directory` flag on the `node_exporter` commandline. The
collector will parse all files in that directory matching the glob `*.prom`
using the [text
format](http://prometheus.io/docs/instrumenting/exposition_formats/). **Note:** Timestamps are not supported.

To atomically push completion time for a cron job:
```
echo my_batch_job_completion_time $(date +%s) &gt; /path/to/directory/my_batch_job.prom.$$
mv /path/to/directory/my_batch_job.prom.$$ /path/to/directory/my_batch_job.prom
```

To statically set roles for a machine using labels:
```
echo &#039;role{role=&quot;application_server&quot;} 1&#039; &gt; /path/to/directory/role.prom.$$
mv /path/to/directory/role.prom.$$ /path/to/directory/role.prom
```

### Filtering enabled collectors

The `node_exporter` will expose all metrics from enabled collectors by default.  This is the recommended way to collect metrics to avoid errors when comparing metrics of different families.

For advanced use the `node_exporter` can be passed an optional list of collectors to filter metrics. The parameters `collect[]` and `exclude[]` can be used multiple times (but cannot be combined).  In Prometheus configuration you can use this syntax under the [scrape config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#&lt;scrape_config&gt;).

Collect only `cpu` and `meminfo` collector metrics:
```
  params:
    collect[]:
      - cpu
      - meminfo
```

Collect all enabled collector metrics but exclude `netdev`:
```
  params:
    exclude[]:
      - netdev
```

This can be useful for having different Prometheus servers collect specific metrics from nodes.

## Development building and running

Prerequisites:

* [Go compiler](https://golang.org/dl/)
* RHEL/CentOS: `glibc-static` package.

Building:

    git clone https://github.com/prometheus/node_exporter.git
    cd node_exporter
    make build
    ./node_exporter &lt;flags&gt;

To see all available configuration flags:

    ./node_exporter -h

## Running tests

    make test

## TLS endpoint

**EXPERIMENTAL**

The exporter supports TLS via a new web configuration file.

```console
./node_exporter --web.config.file=web-config.yml
```

See the [exporter-toolkit web-configuration](https://github.com/prometheus/exporter-toolkit/blob/master/docs/web-configuration.md) for more details.

[travis]: https://travis-ci.org/prometheus/node_exporter
[hub]: https://hub.docker.com/r/prom/node-exporter/
[circleci]: https://circleci.com/gh/prometheus/node_exporter
[quay]: https://quay.io/rep

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[mayswind/ezbookkeeping]]></title>
            <link>https://github.com/mayswind/ezbookkeeping</link>
            <guid>https://github.com/mayswind/ezbookkeeping</guid>
            <pubDate>Wed, 05 Nov 2025 00:51:28 GMT</pubDate>
            <description><![CDATA[A lightweight, self-hosted personal finance app with a user-friendly interface and powerful bookkeeping features.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mayswind/ezbookkeeping">mayswind/ezbookkeeping</a></h1>
            <p>A lightweight, self-hosted personal finance app with a user-friendly interface and powerful bookkeeping features.</p>
            <p>Language: Go</p>
            <p>Stars: 2,785</p>
            <p>Forks: 280</p>
            <p>Stars today: 35 stars today</p>
            <h2>README</h2><pre># ezBookkeeping
[![License](https://img.shields.io/badge/license-MIT-green.svg)](https://github.com/mayswind/ezbookkeeping/blob/master/LICENSE)
[![Go Report](https://goreportcard.com/badge/github.com/mayswind/ezbookkeeping)](https://goreportcard.com/report/github.com/mayswind/ezbookkeeping)
[![Latest Release](https://img.shields.io/github/release/mayswind/ezbookkeeping.svg?style=flat)](https://github.com/mayswind/ezbookkeeping/releases)
[![Latest Build](https://img.shields.io/github/actions/workflow/status/mayswind/ezbookkeeping/build-snapshot.yml?branch=main)](https://github.com/mayswind/ezbookkeeping/actions)
[![Latest Docker Image Size](https://img.shields.io/docker/image-size/mayswind/ezbookkeeping.svg?style=flat)](https://hub.docker.com/r/mayswind/ezbookkeeping)
[![Docker Pulls](https://img.shields.io/docker/pulls/mayswind/ezbookkeeping)](https://hub.docker.com/r/mayswind/ezbookkeeping)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/mayswind/ezbookkeeping)

[![Recommend By HelloGitHub](https://api.hellogithub.com/v1/widgets/recommend.svg?rid=ded5af09da574ec1811ddb154f1b2093&amp;claim_uid=LT7EZxeBukCnh0K)](https://hellogithub.com/en/repository/mayswind/ezbookkeeping)
[![Trending](https://trendshift.io/api/badge/repositories/12917)](https://trendshift.io/repositories/12917)

## Introduction
ezBookkeeping is a lightweight, self-hosted personal finance app with a user-friendly interface and powerful bookkeeping features. It&#039;s easy to deploy, and you can start it with just one single Docker command. Designed to be resource-efficient and highly scalable, it can run smoothly on devices as small as a Raspberry Pi, or scale up to NAS, MicroServers, and even large cluster environments.

ezBookkeeping offers tailored interfaces for both mobile and desktop devices. With support for PWA (Progressive Web Apps), you can even [add it to your mobile home screen](https://raw.githubusercontent.com/wiki/mayswind/ezbookkeeping/img/mobile/add_to_home_screen.gif) and use it like a native app.

Live Demo: [https://ezbookkeeping-demo.mayswind.net](https://ezbookkeeping-demo.mayswind.net)

## Features
- **Open Source &amp; Self-Hosted**
    - Built for privacy and control
- **Lightweight &amp; Fast**
    - Optimized for performance, runs smoothly even on low-resource environments
- **Easy Installation**
    - Docker-ready
    - Supports SQLite, MySQL, PostgreSQL
    - Cross-platform (Windows, macOS, Linux)
    - Works on x86, amd64, ARM architectures
- **User-Friendly Interface**
    - UI optimized for both mobile and desktop
    - PWA support for native-like mobile experience
    - Dark mode
- **AI-Powered Features**
    - Receipt image recognition
    - Supports MCP (Model Context Protocol) for AI integration
- **Powerful Bookkeeping**
    - Two-level accounts and categories
    - Attach images to transactions
    - Location tracking with maps
    - Recurring transactions
    - Advanced filtering, search, visualization, and analysis
- **Localization &amp; Globalization**
    - Multi-language and multi-currency support
    - Automatic exchange rates
    - Multi-timezone awareness
    - Custom formats for dates, numbers, and currencies
- **Security**
    - Two-factor authentication (2FA)
    - Login rate limiting
    - Application lock (PIN code / WebAuthn)
- **Data Import/Export**
    - Supports CSV, OFX, QFX, QIF, IIF, Camt.053, MT940, GnuCash, Firefly III, Beancount, and more

## Screenshots
### Desktop Version
[![ezBookkeeping](https://raw.githubusercontent.com/wiki/mayswind/ezbookkeeping/img/desktop/en.png)](https://raw.githubusercontent.com/wiki/mayswind/ezbookkeeping/img/desktop/en.png)

### Mobile Version
[![ezBookkeeping](https://raw.githubusercontent.com/wiki/mayswind/ezbookkeeping/img/mobile/en.png)](https://raw.githubusercontent.com/wiki/mayswind/ezbookkeeping/img/mobile/en.png)

## Installation
### Run with Docker
Visit [Docker Hub](https://hub.docker.com/r/mayswind/ezbookkeeping) to see all images and tags.

**Latest Release:**

    $ docker run -p8080:8080 mayswind/ezbookkeeping

**Latest Daily Build:**

    $ docker run -p8080:8080 mayswind/ezbookkeeping:latest-snapshot

### Install from Binary
Download the latest release: [https://github.com/mayswind/ezbookkeeping/releases](https://github.com/mayswind/ezbookkeeping/releases)

**Linux / macOS**

    $ ./ezbookkeeping server run

**Windows**

    &gt; .\ezbookkeeping.exe server run

By default, ezBookkeeping listens on port 8080. You can then visit `http://{YOUR_HOST_ADDRESS}:8080/` .

### Build from Source
Make sure you have [Golang](https://golang.org/), [GCC](https://gcc.gnu.org/), [Node.js](https://nodejs.org/) and [NPM](https://www.npmjs.com/) installed. Then download the source code, and follow these steps:

**Linux / macOS**

    $ ./build.sh package -o ezbookkeeping.tar.gz

All the files will be packaged in `ezbookkeeping.tar.gz`.

**Windows**

    &gt; .\build.bat package -o ezbookkeeping.zip

or

    PS &gt; .\build.ps1 package -Output ezbookkeeping.zip

All the files will be packaged in `ezbookkeeping.zip`.

You can also build a Docker image. Make sure you have [Docker](https://www.docker.com/) installed, then follow these steps:

**Linux**

    $ ./build.sh docker

## Contributing
We welcome contributions of all kinds.

Found a bug? [Submit an issue](https://github.com/mayswind/ezbookkeeping/issues)

Want to contribute code? Feel free to fork and send a pull request.

Contributions of all kinds ‚Äî bug reports, feature suggestions, documentation improvements, or code ‚Äî are highly appreciated.

Check out our [Contributor Graph](https://github.com/mayswind/ezbookkeeping/graphs/contributors) to see the amazing people who&#039;ve already helped.

## Translating
Help make ezBookkeeping accessible to users around the world. If you want to contribute a translation, please refer to our [translation guide](https://ezbookkeeping.mayswind.net/translating).

Currently available translations:

| Tag | Language | Contributors |
| --- | --- | --- |
| de | Deutsch | [@chrgm](https://github.com/chrgm) |
| en | English | / |
| es | Espa√±ol | [@Miguelonlonlon](https://github.com/Miguelonlonlon) |
| fr | Fran√ßais | [@brieucdlf](https://github.com/brieucdlf) |
| it | Italiano | [@waron97](https://github.com/waron97) |
| ja | Êó•Êú¨Ë™û | [@tkymmm](https://github.com/tkymmm) |
| ko | ÌïúÍµ≠Ïñ¥ | [@overworks](https://github.com/overworks) |
| nl | Nederlands | [@automagic](https://github.com/automagics) |
| pt-BR | Portugu√™s (Brasil) | [@thecodergus](https://github.com/thecodergus) |
| ru | –†—É—Å—Å–∫–∏–π | [@artegoser](https://github.com/artegoser) |
| th | ‡πÑ‡∏ó‡∏¢ | [@natthavat28](https://github.com/natthavat28) |
| uk | –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ | [@nktlitvinenko](https://github.com/nktlitvinenko) |
| vi | Ti·∫øng Vi·ªát | [@f97](https://github.com/f97) |
| zh-Hans | ‰∏≠Êñá (ÁÆÄ‰Ωì) | / |
| zh-Hant | ‰∏≠Êñá (ÁπÅÈ´î) | / |

Don&#039;t see your language? Help us add it.

## Documentation
1. [English](https://ezbookkeeping.mayswind.net)
1. [‰∏≠Êñá (ÁÆÄ‰Ωì)](https://ezbookkeeping.mayswind.net/zh_Hans)

## License
[MIT](https://github.com/mayswind/ezbookkeeping/blob/master/LICENSE)
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[OpenCSGs/csghub-server]]></title>
            <link>https://github.com/OpenCSGs/csghub-server</link>
            <guid>https://github.com/OpenCSGs/csghub-server</guid>
            <pubDate>Wed, 05 Nov 2025 00:51:27 GMT</pubDate>
            <description><![CDATA[csghub-server is the backend server for CSGHub which helps user to manage datasets, modes, and also run Model Inference, Finetune and Application Spaces.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenCSGs/csghub-server">OpenCSGs/csghub-server</a></h1>
            <p>csghub-server is the backend server for CSGHub which helps user to manage datasets, modes, and also run Model Inference, Finetune and Application Spaces.</p>
            <p>Language: Go</p>
            <p>Stars: 1,167</p>
            <p>Forks: 242</p>
            <p>Stars today: 32 stars today</p>
            <h2>README</h2><pre>*[English](README_en.md) ‚àô [ÁÆÄ‰Ωì‰∏≠Êñá](README_cn.md) ‚àô [Êó•Êú¨Ë™û](README_ja.md)*

`CSGHub Server` is a part of the open source and reliable large model assets management platform - [CSGHub](https://github.com/OpenCSGs/CSGHub/). It focuses on management of models„ÄÅdatasets and other LLM assets through REST API„ÄÇ

## Key FeaturesÔºö
- Creation and Management of users and orgnizations
- Auto-tagging of model and dataset labels
- Search for users, organizations, models, and data
- Online preview of dataset files, like `.parquet` file
- Content moderation for both text and image 
- Download of individual files, including LFS files
- Tracking of model and dataset activity data, such as downloads and likes volume

## Demo
In order to help users to quickly understand the features and usage of CSGHub, we have recorded a demo video. You can watch this video to get a quick understanding of the main features and operation procedures of this program.
- CSGHub Demo video is as blewÔºåyou can also check it at [YouTube](https://www.youtube.com/watch?v=SFDISpqowXs) or [Bilibili](https://www.bilibili.com/video/BV12T4y187bv/)
&lt;video width=&quot;658&quot; height=&quot;432&quot; src=&quot;https://github-production-user-asset-6210df.s3.amazonaws.com/3232817/296556812-205d07f2-de9d-4a7f-b3f5-83514a71453e.mp4&quot;&gt;&lt;/video&gt;

Please visit the [OpenCSG website](https://portal.opencsg.com/models) to experience the powerful management features. 

## Quick Start
&gt; System resource requirements: 4c CPU/8GB memory

Please install Docker yourself. This project has been tested in Ubuntu22 environment.

You can quickly deploy the localized `CSGHub Server` service through docker-compose:
```shell
# The API token should be at least 128 characters long, and HTTP requests to csghub-server require the API token to be sent as a Bearer token for authentication.
export STARHUB_SERVER_API_TOKEN=&lt;API token&gt;
mkdir -m 777 gitea minio_data
curl -L https://raw.githubusercontent.com/OpenCSGs/csghub-server/main/docker-compose.yml -o docker-compose.yml
docker-compose -f docker-compose.yml up -d
```

## Start CSGHub Server Services Locally

CSGHub supports TOML format for config files. When starting any service from the command line, you can specify the config file with the `--config` option:

```
go run cmd/csghub-server/main.go start server --config local.toml
go run cmd/csghub-server/main.go deploy runner --config local.toml
...
```

We provide an [example config file](common/config/config.toml.example), you can rename it, modify as needed and use. All available configurations are defined in [this Go file](common/config/config.go). The TOML configuration uses snake_case naming convention, and names automatically map to corresponding struct field names.

## Technical Architecture
&lt;div align=center&gt;
  &lt;img src=&quot;docs/csghub_server-arch.png&quot; alt=&quot;csghub-server architecture&quot; width=&quot;800px&quot;&gt;
&lt;/div&gt;

### Extensible and customizable

- Supports different git servers, such as Gitea, GitLab, etc.
- Supports flexible configuration of the LFS storage system, and you can choose to use local or any third-party cloud storage service that is compatible with the S3 protocol.
- Enable content moderation on demand, and choose any third-party content moderation service.

## Roadmap
- [x] Support more Git Servers: Currently supports Gitea, and plans to support mainstream Git repositories in the future.
- [x] Git LFS: Git LFS supports large files, and supports Git command operations and online download through the Web UI. 
- [x] DataSet online viewer: Data set preview, supports the Top20/TopN loading preview of LFS format data sets. 
- [x] Model/Dataset AutoTag: Supports custom metadata and automatic extraction of model/dataset tags. 
- [x] S3 Protocol Support: Supports S3 (MinIO) storage protocol, providing higher reliability and storage cost-effectiveness.
- [ ] Model format convert: Conversion of mainstream model formats.
- [x] Model oneclick deploy: Supports integration with OpenCSG llm-inference, one-click to start model inference.

## License
We use the Apache 2.0 license, the content of which is detailed in the `LICENSE` file.

## Contributing
If you wish to contribute, please follow the [Contribution Guidelines](docs/en/contributing.md). We are very excited about your contributions!

Before you begin development, we highly recommend checking out our [Backend Developer Guides](contribute/), which provide helpful information to ensure a smooth development process.

## Acknowledgments
This project is based on open source projects such as Gin, DuckDB, minio, and Gitea. We would like to express our sincere gratitude to them for their open source contributions!

### CONTACT WITH US
If you meet any problem during usage, you can contact with us by any following way:
1. initiate an issue in github
2. join our WeChat group by scaning wechat helper qrcode
3. join our offical discord channel: [OpenCSG Discord Channel](https://discord.gg/bXnu4C9BkR)
4. join our slack workspace:[OpenCSG Slack Channel](https://join.slack.com/t/opencsghq/shared_invite/zt-2fmtem7hs-s_RmMeoOIoF1qzslql2q~A)
&lt;div style=&quot;display:inline-block&quot;&gt;
&lt;img src=&quot;https://github.com/OpenCSGs/csghub/blob/main/docs/images/wechat-assistant-new.png&quot; width=&#039;200&#039;&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;img src=&quot;https://github.com/OpenCSGs/csghub/blob/main/docs/images/discord-qrcode.png&quot; width=&#039;200&#039;&gt;
  &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
&lt;img src=&quot;https://github.com/OpenCSGs/csghub/blob/main/docs/images/slack-qrcode.png&quot; width=&#039;200&#039;&gt;
&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[kubernetes/autoscaler]]></title>
            <link>https://github.com/kubernetes/autoscaler</link>
            <guid>https://github.com/kubernetes/autoscaler</guid>
            <pubDate>Wed, 05 Nov 2025 00:51:26 GMT</pubDate>
            <description><![CDATA[Autoscaling components for Kubernetes]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/kubernetes/autoscaler">kubernetes/autoscaler</a></h1>
            <p>Autoscaling components for Kubernetes</p>
            <p>Language: Go</p>
            <p>Stars: 8,662</p>
            <p>Forks: 4,239</p>
            <p>Stars today: 1 star today</p>
            <h2>README</h2><pre># Kubernetes Autoscaler

[![Release Charts](https://github.com/kubernetes/autoscaler/actions/workflows/release.yaml/badge.svg)](https://github.com/kubernetes/autoscaler/actions/workflows/release.yaml) [![Tests](https://github.com/kubernetes/autoscaler/actions/workflows/ci.yaml/badge.svg)](https://github.com/kubernetes/autoscaler/actions/workflows/ci.yaml) [![GoDoc Widget]][GoDoc]

This repository contains autoscaling-related components for Kubernetes.

## What&#039;s inside

[Cluster Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler) - a component that automatically adjusts the size of a Kubernetes
Cluster so that all pods have a place to run and there are no unneeded nodes. Supports several public cloud providers. Version 1.0 (GA) was released with kubernetes 1.8.

[Cluster Autoscaler Helm Chart](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/charts) - Supported Helm chart for Cluster Autoscaler.

[Vertical Pod Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler) - a set of components that automatically adjust the
amount of CPU and memory requested by pods running in the Kubernetes Cluster. Current state - beta.

[Vertical Pod Autoscaler Helm Chart](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler/charts) - Supported Helm chart for Vertical Pod Autoscaler.

[Addon Resizer](https://github.com/kubernetes/autoscaler/tree/master/addon-resizer) - a simplified version of vertical pod autoscaler that modifies
resource requests of a deployment based on the number of nodes in the Kubernetes Cluster. Current state - beta.

## Contact Info

Interested in autoscaling? Want to talk? Have questions, concerns or great ideas?

Please join us on #sig-autoscaling at https://kubernetes.slack.com/, or join one
of our weekly meetings.  See [the Kubernetes Community Repo](https://github.com/kubernetes/community/blob/master/sig-autoscaling/README.md) for more information.

## Getting the Code

Fork the repository in the cloud:
1. Visit https://github.com/kubernetes/autoscaler
1. Click Fork button (top right) to establish a cloud-based fork.

The code must be checked out as a subdirectory of `k8s.io`, and not `github.com`.

```shell
mkdir -p $GOPATH/src/k8s.io
cd $GOPATH/src/k8s.io
# Replace &quot;$YOUR_GITHUB_USERNAME&quot; below with your github username
git clone https://github.com/$YOUR_GITHUB_USERNAME/autoscaler.git
cd autoscaler
```

Please refer to Kubernetes [Github workflow guide] for more details.

[GoDoc]: https://godoc.org/k8s.io/autoscaler
[GoDoc Widget]: https://godoc.org/k8s.io/autoscaler?status.svg
[Github workflow guide]: https://github.com/kubernetes/community/blob/master/contributors/guide/github-workflow.md
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[gin-gonic/gin]]></title>
            <link>https://github.com/gin-gonic/gin</link>
            <guid>https://github.com/gin-gonic/gin</guid>
            <pubDate>Wed, 05 Nov 2025 00:51:25 GMT</pubDate>
            <description><![CDATA[Gin is a high-performance HTTP web framework written in Go. It provides a Martini-like API but with significantly better performance‚Äîup to 40 times faster‚Äîthanks to httprouter. Gin is designed for building REST APIs, web applications, and microservices.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/gin-gonic/gin">gin-gonic/gin</a></h1>
            <p>Gin is a high-performance HTTP web framework written in Go. It provides a Martini-like API but with significantly better performance‚Äîup to 40 times faster‚Äîthanks to httprouter. Gin is designed for building REST APIs, web applications, and microservices.</p>
            <p>Language: Go</p>
            <p>Stars: 86,816</p>
            <p>Forks: 8,475</p>
            <p>Stars today: 21 stars today</p>
            <h2>README</h2><pre># Gin Web Framework

&lt;img align=&quot;right&quot; width=&quot;159px&quot; src=&quot;https://raw.githubusercontent.com/gin-gonic/logo/master/color.png&quot;&gt;

[![Build Status](https://github.com/gin-gonic/gin/actions/workflows/gin.yml/badge.svg?branch=master)](https://github.com/gin-gonic/gin/actions/workflows/gin.yml)
[![codecov](https://codecov.io/gh/gin-gonic/gin/branch/master/graph/badge.svg)](https://codecov.io/gh/gin-gonic/gin)
[![Go Report Card](https://goreportcard.com/badge/github.com/gin-gonic/gin)](https://goreportcard.com/report/github.com/gin-gonic/gin)
[![Go Reference](https://pkg.go.dev/badge/github.com/gin-gonic/gin?status.svg)](https://pkg.go.dev/github.com/gin-gonic/gin?tab=doc)
[![Sourcegraph](https://sourcegraph.com/github.com/gin-gonic/gin/-/badge.svg)](https://sourcegraph.com/github.com/gin-gonic/gin?badge)
[![Open Source Helpers](https://www.codetriage.com/gin-gonic/gin/badges/users.svg)](https://www.codetriage.com/gin-gonic/gin)
[![Release](https://img.shields.io/github/release/gin-gonic/gin.svg?style=flat-square)](https://github.com/gin-gonic/gin/releases)

## üì∞ [Announcing Gin 1.11.0!](https://gin-gonic.com/en/blog/news/gin-1-11-0-release-announcement/)

Read about the latest features and improvements in Gin 1.11.0 on our official blog.

---

Gin is a high-performance HTTP web framework written in [Go](https://go.dev/). It provides a Martini-like API but with significantly better performance‚Äîup to 40 times faster‚Äîthanks to [httprouter](https://github.com/julienschmidt/httprouter). Gin is designed for building REST APIs, web applications, and microservices where speed and developer productivity are essential.

**Why choose Gin?**

Gin combines the simplicity of Express.js-style routing with Go&#039;s performance characteristics, making it ideal for:

- Building high-throughput REST APIs
- Developing microservices that need to handle many concurrent requests
- Creating web applications that require fast response times
- Prototyping web services quickly with minimal boilerplate

**Gin&#039;s key features:**

- **Zero allocation router** - Extremely memory-efficient routing with no heap allocations
- **High performance** - Benchmarks show superior speed compared to other Go web frameworks
- **Middleware support** - Extensible middleware system for authentication, logging, CORS, etc.
- **Crash-free** - Built-in recovery middleware prevents panics from crashing your server
- **JSON validation** - Automatic request/response JSON binding and validation
- **Route grouping** - Organize related routes and apply common middleware
- **Error management** - Centralized error handling and logging
- **Built-in rendering** - Support for JSON, XML, HTML templates, and more
- **Extensible** - Large ecosystem of community middleware and plugins

## Getting Started

### Prerequisites

- **Go version**: Gin requires [Go](https://go.dev/) version [1.24](https://go.dev/doc/devel/release#go1.24.0) or above
- **Basic Go knowledge**: Familiarity with Go syntax and package management is helpful

### Installation

With [Go&#039;s module support](https://go.dev/wiki/Modules#how-to-use-modules), simply import Gin in your code and Go will automatically fetch it during build:

```go
import &quot;github.com/gin-gonic/gin&quot;
```

### Your First Gin Application

Here&#039;s a complete example that demonstrates Gin&#039;s simplicity:

```go
package main

import (
  &quot;net/http&quot;

  &quot;github.com/gin-gonic/gin&quot;
)

func main() {
  // Create a Gin router with default middleware (logger and recovery)
  r := gin.Default()
  
  // Define a simple GET endpoint
  r.GET(&quot;/ping&quot;, func(c *gin.Context) {
    // Return JSON response
    c.JSON(http.StatusOK, gin.H{
      &quot;message&quot;: &quot;pong&quot;,
    })
  })
  
  // Start server on port 8080 (default)
  // Server will listen on 0.0.0.0:8080 (localhost:8080 on Windows)
  r.Run()
}
```

**Running the application:**

1. Save the code above as `main.go`
2. Run the application:

   ```sh
   go run main.go
   ```

3. Open your browser and visit [`http://localhost:8080/ping`](http://localhost:8080/ping)
4. You should see: `{&quot;message&quot;:&quot;pong&quot;}`

**What this example demonstrates:**

- Creating a Gin router with default middleware
- Defining HTTP endpoints with simple handler functions
- Returning JSON responses
- Starting an HTTP server

### Next Steps

After running your first Gin application, explore these resources to learn more:

#### üìö Learning Resources

- **[Gin Quick Start Guide](docs/doc.md)** - Comprehensive tutorial with API examples and build configurations
- **[Example Repository](https://github.com/gin-gonic/examples)** - Ready-to-run examples demonstrating various Gin use cases:
  - REST API development
  - Authentication &amp; middleware
  - File uploads and downloads
  - WebSocket connections
  - Template rendering

## üìñ Documentation

### API Reference

- **[Go.dev API Documentation](https://pkg.go.dev/github.com/gin-gonic/gin)** - Complete API reference with examples

### User Guides

The comprehensive documentation is available on [gin-gonic.com](https://gin-gonic.com) in multiple languages:

- [English](https://gin-gonic.com/en/docs/) | [ÁÆÄ‰Ωì‰∏≠Êñá](https://gin-gonic.com/zh-cn/docs/) | [ÁπÅÈ´î‰∏≠Êñá](https://gin-gonic.com/zh-tw/docs/)
- [Êó•Êú¨Ë™û](https://gin-gonic.com/ja/docs/) | [ÌïúÍµ≠Ïñ¥](https://gin-gonic.com/ko-kr/docs/) | [Espa√±ol](https://gin-gonic.com/es/docs/)
- [Turkish](https://gin-gonic.com/tr/docs/) | [Persian](https://gin-gonic.com/fa/docs/) | [Portugu√™s](https://gin-gonic.com/pt/docs/)
- [Russian](https://gin-gonic.com/ru/docs/) | [Indonesian](https://gin-gonic.com/id/docs/)

### Official Tutorials

- [Go.dev Tutorial: Developing a RESTful API with Go and Gin](https://go.dev/doc/tutorial/web-service-gin)

## ‚ö° Performance Benchmarks

Gin demonstrates exceptional performance compared to other Go web frameworks. It uses a custom version of [HttpRouter](https://github.com/julienschmidt/httprouter) for maximum efficiency. [View detailed benchmarks ‚Üí](/BENCHMARKS.md)

**Gin vs. Other Go Frameworks** (GitHub API routing benchmark):

| Benchmark name                 |       (1) |             (2) |          (3) |             (4) |
| ------------------------------ | --------: | --------------: | -----------: | --------------: |
| BenchmarkGin_GithubAll         | **43550** | **27364 ns/op** |   **0 B/op** | **0 allocs/op** |
| BenchmarkAce_GithubAll         |     40543 |     29670 ns/op |       0 B/op |     0 allocs/op |
| BenchmarkAero_GithubAll        |     57632 |     20648 ns/op |       0 B/op |     0 allocs/op |
| BenchmarkBear_GithubAll        |      9234 |    216179 ns/op |   86448 B/op |   943 allocs/op |
| BenchmarkBeego_GithubAll       |      7407 |    243496 ns/op |   71456 B/op |   609 allocs/op |
| BenchmarkBone_GithubAll        |       420 |   2922835 ns/op |  720160 B/op |  8620 allocs/op |
| BenchmarkChi_GithubAll         |      7620 |    238331 ns/op |   87696 B/op |   609 allocs/op |
| BenchmarkDenco_GithubAll       |     18355 |     64494 ns/op |   20224 B/op |   167 allocs/op |
| BenchmarkEcho_GithubAll        |     31251 |     38479 ns/op |       0 B/op |     0 allocs/op |
| BenchmarkGocraftWeb_GithubAll  |      4117 |    300062 ns/op |  131656 B/op |  1686 allocs/op |
| BenchmarkGoji_GithubAll        |      3274 |    416158 ns/op |   56112 B/op |   334 allocs/op |
| BenchmarkGojiv2_GithubAll      |      1402 |    870518 ns/op |  352720 B/op |  4321 allocs/op |
| BenchmarkGoJsonRest_GithubAll  |      2976 |    401507 ns/op |  134371 B/op |  2737 allocs/op |
| BenchmarkGoRestful_GithubAll   |       410 |   2913158 ns/op |  910144 B/op |  2938 allocs/op |
| BenchmarkGorillaMux_GithubAll  |       346 |   3384987 ns/op |  251650 B/op |  1994 allocs/op |
| BenchmarkGowwwRouter_GithubAll |     10000 |    143025 ns/op |   72144 B/op |   501 allocs/op |
| BenchmarkHttpRouter_GithubAll  |     55938 |     21360 ns/op |       0 B/op |     0 allocs/op |
| BenchmarkHttpTreeMux_GithubAll |     10000 |    153944 ns/op |   65856 B/op |   671 allocs/op |
| BenchmarkKocha_GithubAll       |     10000 |    106315 ns/op |   23304 B/op |   843 allocs/op |
| BenchmarkLARS_GithubAll        |     47779 |     25084 ns/op |       0 B/op |     0 allocs/op |
| BenchmarkMacaron_GithubAll     |      3266 |    371907 ns/op |  149409 B/op |  1624 allocs/op |
| BenchmarkMartini_GithubAll     |       331 |   3444706 ns/op |  226551 B/op |  2325 allocs/op |
| BenchmarkPat_GithubAll         |       273 |   4381818 ns/op | 1483152 B/op | 26963 allocs/op |
| BenchmarkPossum_GithubAll      |     10000 |    164367 ns/op |   84448 B/op |   609 allocs/op |
| BenchmarkR2router_GithubAll    |     10000 |    160220 ns/op |   77328 B/op |   979 allocs/op |
| BenchmarkRivet_GithubAll       |     14625 |     82453 ns/op |   16272 B/op |   167 allocs/op |
| BenchmarkTango_GithubAll       |      6255 |    279611 ns/op |   63826 B/op |  1618 allocs/op |
| BenchmarkTigerTonic_GithubAll  |      2008 |    687874 ns/op |  193856 B/op |  4474 allocs/op |
| BenchmarkTraffic_GithubAll     |       355 |   3478508 ns/op |  820744 B/op | 14114 allocs/op |
| BenchmarkVulcan_GithubAll      |      6885 |    193333 ns/op |   19894 B/op |   609 allocs/op |

- (1): Total Repetitions achieved in constant time, higher means more confident result
- (2): Single Repetition Duration (ns/op), lower is better
- (3): Heap Memory (B/op), lower is better
- (4): Average Allocations per Repetition (allocs/op), lower is better

## üîå Middleware Ecosystem

Gin has a rich ecosystem of middleware for common web development needs. Explore community-contributed middleware:

- **[gin-contrib](https://github.com/gin-contrib)** - Official middleware collection including:
  - Authentication (JWT, Basic Auth, Sessions)
  - CORS, Rate limiting, Compression
  - Logging, Metrics, Tracing
  - Static file serving, Template engines
  
- **[gin-gonic/contrib](https://github.com/gin-gonic/contrib)** - Additional community middleware

## üè¢ Production Usage

Gin powers many high-traffic applications and services in production:

- **[gorush](https://github.com/appleboy/gorush)** - High-performance push notification server
- **[fnproject](https://github.com/fnproject/fn)** - Container-native, serverless platform
- **[photoprism](https://github.com/photoprism/photoprism)** - AI-powered personal photo management
- **[lura](https://github.com/luraproject/lura)** - Ultra-performant API Gateway framework
- **[picfit](https://github.com/thoas/picfit)** - Real-time image processing server
- **[dkron](https://github.com/distribworks/dkron)** - Distributed job scheduling system

## ü§ù Contributing

Gin is the work of hundreds of contributors from around the world. We welcome and appreciate your contributions!

### How to Contribute

- üêõ **Report bugs** - Help us identify and fix issues
- üí° **Suggest features** - Share your ideas for improvements
- üìù **Improve documentation** - Help make our docs clearer
- üîß **Submit code** - Fix bugs or implement new features
- üß™ **Write tests** - Improve our test coverage

### Getting Started with Contributing

1. Check out our [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines
2. Join our community discussions and ask questions

**All contributions are valued and help make Gin better for everyone!**
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[aaPanel/BillionMail]]></title>
            <link>https://github.com/aaPanel/BillionMail</link>
            <guid>https://github.com/aaPanel/BillionMail</guid>
            <pubDate>Wed, 05 Nov 2025 00:51:24 GMT</pubDate>
            <description><![CDATA[BillionMail gives you open-source MailServer, NewsLetter, Email Marketing ‚Äî fully self-hosted, dev-friendly, and free from monthly fees. Join the discord: https://discord.gg/asfXzBUhZr]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/aaPanel/BillionMail">aaPanel/BillionMail</a></h1>
            <p>BillionMail gives you open-source MailServer, NewsLetter, Email Marketing ‚Äî fully self-hosted, dev-friendly, and free from monthly fees. Join the discord: https://discord.gg/asfXzBUhZr</p>
            <p>Language: Go</p>
            <p>Stars: 12,032</p>
            <p>Forks: 1,118</p>
            <p>Stars today: 22 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;
  &lt;h1&gt;&lt;a href=&quot;https://www.billionmail.com/&quot; target=&quot;_blank&quot;&gt;BillionMail üìß&lt;/a&gt;&lt;/h1&gt;


## An Open-Source MailServer, NewsLetter, Email Marketing Solution for Smarter Campaigns

[![][license-shield]][license-link] [![][docs-shield]][docs-link] [![][github-release-shield]][github-release-link] [![][github-stars-shield]][github-stars-link]

English | [ÁÆÄ‰Ωì‰∏≠Êñá](README-zh_CN.md) | [Êó•Êú¨Ë™û](README-ja.md) | [T√ºrk√ße](README-ja.md)
&lt;/div&gt;
&lt;br/&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/13842&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13842&quot; alt=&quot;aaPanel%2FBillionMail | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

## What is BillionMail?

BillionMail is a **future open-source Mail server, Email marketing platform** designed to help businesses and individuals manage their email campaigns with ease. Whether you&#039;re sending newsletters, promotional emails, or transactional messages, this tool will provide **full control** over your email marketing efforts. With features like **advanced analytics**, and **customer management**, you&#039;ll be able to create, send, and track emails like a pro.

![BillionMail Banner](https://www.billionmail.com/home.png?v1)

# Just 3 steps to send a billion emails!
**Billion emails. Any business. Guaranteed.**

### Step 1Ô∏è‚É£ Install BillionMail: 
‚úÖ It takes **only 8Ô∏è‚É£ minutes** from installation to **‚úÖ successful email sending**
```shell
cd /opt &amp;&amp; git clone https://github.com/aaPanel/BillionMail &amp;&amp; cd BillionMail &amp;&amp; bash install.sh
```


### Step 2Ô∏è‚É£: Connect Your Domain
- Add the sending domain
- Verify DNS records
- Auto-enable free SSL


### Step 3Ô∏è‚É£: Build Your Campaign

- Write or paste your email
- Choose list &amp; tags
- Set send time or send now


&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/embed/UHgxZa_9jGs?si=0-f1B5hDtcWImvQv&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.youtube.com/vi/UHgxZa_9jGs/maxresdefault.jpg&quot; alt=&quot;&quot; width=&quot;80%&quot;&gt;
    &lt;br /&gt;
    &lt;img src=&quot;https://www.iconfinder.com/icons/317714/download/png/16&quot; alt=&quot;YouTube&quot; width=&quot;16&quot;/&gt;
    &lt;b&gt;Watch on Youtube&lt;/b&gt;
  &lt;/a&gt;
&lt;/div&gt;


## Other installation methods

### One-click installation on aaPanel
üëâ https://www.aapanel.com/new/download.html  (Log in to ‚úÖaaPanel --&gt; üê≥Docker --&gt; 1Ô∏è‚É£OneClick install)




**Docker**
```shell
cd /opt &amp;&amp; git clone https://github.com/aaPanel/BillionMail &amp;&amp; cd BillionMail &amp;&amp; cp env_init .env &amp;&amp; docker compose up -d || docker-compose up -d
```

## Management script
- Management help

  `bm help`

- View Login default info

  `bm default`

- Show domain DNS record

  `bm show-record`

- Update BillionMail

  `bm update`

## Live Demo
BillionMail Demo: [https://demo.billionmail.com/billionmail](https://demo.billionmail.com/billionmail)

Username: `billionmail` 

Password: `billionmail` 


## WebMail

BillionMail has integrated **RoundCube**, you can access WebMail via `/roundcube/`.

## Why BillionMail?

Most email marketing platforms are either **expensive**, **closed-source**, or **lack essential features**. BillionMail aims to be different:

‚úÖ **Fully Open-Source** ‚Äì No hidden costs, no vendor lock-in.  
üìä **Advanced Analytics** ‚Äì Track email delivery, open rates, click-through rates, and more.  
üìß **Unlimited Sending** ‚Äì No restrictions on the number of emails you can send.  
üé® **Customizable Templates** ‚Äì Custom professional marketing templates for reuse.
üîí **Privacy-First** ‚Äì Your data stays with you, no third-party tracking.  
üöÄ **Self-Hosted** ‚Äì Run it on your own server for complete control.  

## How You Can Help üåü

BillionMail is a **community-driven project**, and we need your support to get started! Here&#039;s how you can help:

1. **Star This Repository**: Show your interest by starring this repo.  
2. **Spread the Word**: Share BillionMail with your network‚Äîdevelopers, marketers, and open-source enthusiasts.  
3. **Share Feedback**: Let us know what features you&#039;d like to see in BillionMail by opening an issue or joining the discussion.  
4. **Contribute**: Once development begins, we&#039;ll welcome contributions from the community. Stay tuned for updates!

---

üìß **BillionMail ‚Äì The Future of Open-Source Email Marketing.**

## Issues

If you encounter any issues or have feature requests, please [open an issue](https://github.com/aaPanel/BillionMail/issues). Be sure to include:

- A clear description of the problem or request.
- Steps to reproduce the issue (if applicable).
- Screenshots or error logs (if applicable).

## Install Now:
‚úÖIt takes **only 8 minutes** from installation to **successful email sending**
```shell
cd /opt &amp;&amp; git clone https://github.com/aaPanel/BillionMail &amp;&amp; cd BillionMail &amp;&amp; bash install.sh
```


**Install with Docker:** (Please install Docker and docker-compose-plugin manually, and modify .env file)
```shell
cd /opt &amp;&amp; git clone https://github.com/aaPanel/BillionMail &amp;&amp; cd BillionMail &amp;&amp; cp env_init .env &amp;&amp; docker compose up -d || docker-compose up -d
```

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=aapanel/billionmail&amp;type=Date)](https://www.star-history.com/#aapanel/billionmail&amp;Date)

## License

BillionMail is licensed under the **AGPLv3 License**. This means you can:

‚úÖ Use the software for free.  
‚úÖ Modify and distribute the code.  
‚úÖ Use it privately without restrictions.

See the [LICENSE](LICENSE) file for more details.

---

&lt;!-- BillionMail official link --&gt;
[docs-link]: https://www.billionmail.com/

&lt;!-- BillionMail Other link--&gt;
[license-link]: https://www.gnu.org/licenses/agpl-3.0.html
[github-release-link]: https://github.com/aaPanel/BillionMail/releases/latest
[github-stars-link]: https://github.com/aaPanel/BillionMail
[github-issues-link]: https://github.com/aaPanel/BillionMail/issues

&lt;!-- Shield link--&gt;
[docs-shield]: https://img.shields.io/badge/documentation-148F76
[github-release-shield]: https://img.shields.io/github/v/release/aaPanel/BillionMail
[github-stars-shield]: https://img.shields.io/github/stars/aaPanel/BillionMail?color=%231890FF&amp;style=flat-square¬†¬†¬†
[license-shield]: https://img.shields.io/github/license/aaPanel/BillionMail
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[grafana/mimir]]></title>
            <link>https://github.com/grafana/mimir</link>
            <guid>https://github.com/grafana/mimir</guid>
            <pubDate>Wed, 05 Nov 2025 00:51:23 GMT</pubDate>
            <description><![CDATA[Grafana Mimir provides horizontally scalable, highly available, multi-tenant, long-term storage for Prometheus.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/grafana/mimir">grafana/mimir</a></h1>
            <p>Grafana Mimir provides horizontally scalable, highly available, multi-tenant, long-term storage for Prometheus.</p>
            <p>Language: Go</p>
            <p>Stars: 4,757</p>
            <p>Forks: 661</p>
            <p>Stars today: 2 stars today</p>
            <h2>README</h2><pre># Grafana Mimir

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;images/logo.png&quot; alt=&quot;Grafana Mimir logo&quot; width=&quot;400&quot;&gt;&lt;/p&gt;

Grafana Mimir is an open source software project that provides a scalable long-term storage for [Prometheus](https://prometheus.io). Some of the core strengths of Grafana Mimir include:

- **Easy to install and maintain:** Grafana Mimir‚Äôs extensive documentation, tutorials, and deployment tooling make it quick to get started. Using its monolithic mode, you can get Grafana Mimir up and running with just one binary and no additional dependencies. Once deployed, the best-practice dashboards, alerts, and runbooks packaged with Grafana Mimir make it easy to monitor the health of the system.
- **Massive scalability:** You can run Grafana Mimir&#039;s horizontally-scalable architecture across multiple machines, resulting in the ability to process orders of magnitude more time series than a single Prometheus instance. Internal testing shows that Grafana Mimir handles up to 1 billion active time series.
- **Global view of metrics:** Grafana Mimir enables you to run queries that aggregate series from multiple Prometheus instances, giving you a global view of your systems. Its query engine extensively parallelizes query execution, so that even the highest-cardinality queries complete with blazing speed.
- **Cheap, durable metric storage:** Grafana Mimir uses object storage for long-term data storage, allowing it to take advantage of this ubiquitous, cost-effective, high-durability technology. It is compatible with multiple object store implementations, including AWS S3, Google Cloud Storage, Azure Blob Storage, OpenStack Swift, as well as any S3-compatible object storage.
- **High availability:** Grafana Mimir replicates incoming metrics, ensuring that no data is lost in the event of machine failure. Its horizontally scalable architecture also means that it can be restarted, upgraded, or downgraded with zero downtime, which means no interruptions to metrics ingestion or querying.
- **Natively multi-tenant:** Grafana Mimir‚Äôs multi-tenant architecture enables you to isolate data and queries from independent teams or business units, making it possible for these groups to share the same cluster. Advanced limits and quality-of-service controls ensure that capacity is shared fairly among tenants.

## Migrating to Grafana Mimir

If you&#039;re migrating to Grafana Mimir, refer to the following documents:

- [Migrating from Thanos or Prometheus to Grafana Mimir](https://grafana.com/docs/mimir/latest/set-up/migrate/migrate-from-thanos-or-prometheus/).
- [Migrating from Cortex to Grafana Mimir](https://grafana.com/docs/mimir/latest/set-up/migrate/migrate-from-cortex/)

## Deploying Grafana Mimir

For information about how to deploy Grafana Mimir, refer to [Deploy Grafana Mimir](https://grafana.com/docs/mimir/latest/operators-guide/deploy-grafana-mimir/).

## Getting started

If you‚Äôre new to Grafana Mimir, read the [Get started guide](https://grafana.com/docs/mimir/latest/get-started/).

Before deploying Grafana Mimir in a production environment, read:

1. [An overview of Grafana Mimir‚Äôs architecture](https://grafana.com/docs/mimir/latest/operators-guide/architecture/)
1. [Configure Grafana Mimir](https://grafana.com/docs/mimir/latest/operators-guide/configure/)
1. [Run Grafana Mimir in production](https://grafana.com/docs/mimir/latest/operators-guide/run-production-environment/)

## Documentation

Refer to the following links to access Grafana Mimir documentation:

- [Latest release](https://grafana.com/docs/mimir/latest/)
- [Upcoming release](https://grafana.com/docs/mimir/next/), at the tip of the `main` branch

## Contributing

To contribute to Grafana Mimir, refer to [Contributing to Grafana Mimir](https://github.com/grafana/mimir/tree/main/docs/internal/contributing).

## Join the Grafana Mimir discussion

If you have any questions or feedback regarding Grafana Mimir, join the [Grafana Mimir Discussion](https://github.com/grafana/mimir/discussions). Alternatively, consider joining the monthly [Grafana Mimir Community Call](https://docs.google.com/document/d/1E4jJcGicvLTyMEY6cUFFZUg_I8ytrBuW8r5yt1LyMv4).

Your feedback is always welcome, and you can also share it via the [`#mimir` Slack channel](https://slack.grafana.com/).

## License

Grafana Mimir is distributed under [AGPL-3.0-only](LICENSE).
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[weaviate/weaviate]]></title>
            <link>https://github.com/weaviate/weaviate</link>
            <guid>https://github.com/weaviate/weaviate</guid>
            <pubDate>Wed, 05 Nov 2025 00:51:22 GMT</pubDate>
            <description><![CDATA[Weaviate is an open-source vector database that stores both objects and vectors, allowing for the combination of vector search with structured filtering with the fault tolerance and scalability of a cloud-native database‚Äã.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/weaviate/weaviate">weaviate/weaviate</a></h1>
            <p>Weaviate is an open-source vector database that stores both objects and vectors, allowing for the combination of vector search with structured filtering with the fault tolerance and scalability of a cloud-native database‚Äã.</p>
            <p>Language: Go</p>
            <p>Stars: 14,937</p>
            <p>Forks: 1,124</p>
            <p>Stars today: 17 stars today</p>
            <h2>README</h2><pre># Weaviate &lt;img alt=&#039;Weaviate logo&#039; src=&#039;https://weaviate.io/img/site/weaviate-logo-light.png&#039; width=&#039;148&#039; align=&#039;right&#039; /&gt;

[![GitHub Repo stars](https://img.shields.io/github/stars/weaviate/weaviate?style=social)](https://github.com/weaviate/weaviate)
[![Go Reference](https://pkg.go.dev/badge/github.com/weaviate/weaviate.svg)](https://pkg.go.dev/github.com/weaviate/weaviate)
[![Build Status](https://github.com/weaviate/weaviate/actions/workflows/.github/workflows/pull_requests.yaml/badge.svg?branch=main)](https://github.com/weaviate/weaviate/actions/workflows/.github/workflows/pull_requests.yaml)
[![Go Report Card](https://goreportcard.com/badge/github.com/weaviate/weaviate)](https://goreportcard.com/report/github.com/weaviate/weaviate)
[![Coverage Status](https://codecov.io/gh/weaviate/weaviate/branch/main/graph/badge.svg)](https://codecov.io/gh/weaviate/weaviate)
[![Slack](https://img.shields.io/badge/slack--channel-blue?logo=slack)](https://weaviate.io/slack)

**Weaviate** is an open-source, cloud-native vector database that stores both objects and vectors, enabling semantic search at scale. It combines vector similarity search with keyword filtering, retrieval-augmented generation (RAG), and reranking in a single query interface. Common use cases include RAG systems, semantic and image search, recommendation engines, chatbots, and content classification.

Weaviate supports two approaches to store vectors: automatic vectorization at import using [integrated models](https://docs.weaviate.io/weaviate/model-providers) (OpenAI, Cohere, HuggingFace, and others) or direct import of [pre-computed vector embeddings](https://docs.weaviate.io/weaviate/starter-guides/custom-vectors). Production deployments benefit from built-in multi-tenancy, replication, RBAC authorization, and [many other features](#weaviate-features).

To get started quickly, have a look at one of these tutorials:

- [Quickstart - Weaviate Cloud](https://docs.weaviate.io/weaviate/quickstart)
- [Quickstart - local Docker instance](https://docs.weaviate.io/weaviate/quickstart/local)

## Installation

Weaviate offers multiple installation and deployment options:

- [Docker](https://docs.weaviate.io/deploy/installation-guides/docker-installation)
- [Kubernetes](https://docs.weaviate.io/deploy/installation-guides/k8s-installation)
- [Weaviate Cloud](https://console.weaviate.cloud)

See the [installation docs](https://docs.weaviate.io/deploy) for more deployment options, such as [AWS](https://docs.weaviate.io/deploy/installation-guides/aws-marketplace) and [GCP](https://docs.weaviate.io/deploy/installation-guides/gcp-marketplace).

## Getting started

You can easily start Weaviate and a local vector embedding model with [Docker](https://docs.docker.com/desktop/).
Create a `docker-compose.yml` file:

```yml
services:
  weaviate:
    image: cr.weaviate.io/semitechnologies/weaviate:1.32.2
    ports:
      - &quot;8080:8080&quot;
      - &quot;50051:50051&quot;
    environment:
      ENABLE_MODULES: text2vec-model2vec
      MODEL2VEC_INFERENCE_API: http://text2vec-model2vec:8080

  # A lightweight embedding model that will generate vectors from objects during import
  text2vec-model2vec:
    image: cr.weaviate.io/semitechnologies/model2vec-inference:minishlab-potion-base-32M
```

Start Weaviate and the embedding service with:

```bash
docker compose up -d
```

Install the Python client (or use another [client library](#client-libraries-and-apis)):

```bash
pip install -U weaviate-client
```

The following Python example shows how easy it is to populate a Weaviate database with data, create vector embeddings and perform semantic search:

```python
import weaviate
from weaviate.classes.config import Configure, DataType, Property

# Connect to Weaviate
client = weaviate.connect_to_local()

# Create a collection
client.collections.create(
    name=&quot;Article&quot;,
    properties=[Property(name=&quot;content&quot;, data_type=DataType.TEXT)],
    vector_config=Configure.Vectors.text2vec_model2vec(),  # Use a vectorizer to generate embeddings during import
    # vector_config=Configure.Vectors.self_provided()  # If you want to import your own pre-generated embeddings
)

# Insert objects and generate embeddings
articles = client.collections.get(&quot;Article&quot;)
articles.data.insert_many(
    [
        {&quot;content&quot;: &quot;Vector databases enable semantic search&quot;},
        {&quot;content&quot;: &quot;Machine learning models generate embeddings&quot;},
        {&quot;content&quot;: &quot;Weaviate supports hybrid search capabilities&quot;},
    ]
)

# Perform semantic search
results = articles.query.near_text(query=&quot;Search objects by meaning&quot;, limit=1)
print(results.objects[0])

client.close()
```

This example uses the `Model2Vec` vectorizer, but you can choose any other [embedding model provider](https://docs.weaviate.io/weaviate/model-providers) or [bring your own pre-generated vectors](https://docs.weaviate.io/weaviate/starter-guides/custom-vectors).

## Client libraries and APIs

Weaviate provides client libraries for several programming languages:

- [Python](https://docs.weaviate.io/weaviate/client-libraries/python)
- [JavaScript/TypeScript](https://docs.weaviate.io/weaviate/client-libraries/typescript)
- [Java](https://docs.weaviate.io/weaviate/client-libraries/java)
- [Go](https://docs.weaviate.io/weaviate/client-libraries/go)
- C# (üöß Coming soon üöß)

There are also additional [community-maintained libraries](https://docs.weaviate.io/weaviate/client-libraries/community).

Weaviate exposes [REST API](https://docs.weaviate.io/weaviate/api/rest), [gRPC API](https://docs.weaviate.io/weaviate/api/grpc), and [GraphQL API](https://docs.weaviate.io/weaviate/api/graphql) to communicate with the database server.

## Weaviate features

These features enable you to build AI-powered applications:

- **‚ö° Fast Search Performance**: Perform complex semantic [searches](https://docs.weaviate.io/weaviate/search/similarity) over billions of vectors in milliseconds. Weaviate&#039;s architecture is built in Go for speed and reliability, ensuring your AI applications are highly responsive even under heavy load. See our [ANN benchmarks](https://docs.weaviate.io/weaviate/benchmarks/ann) for more info.

- **üîå Flexible Vectorization**: Seamlessly vectorize data at import time with [integrated vectorizers](https://docs.weaviate.io/weaviate/model-providers) from OpenAI, Cohere, HuggingFace, Google, and more. Or you can import [your own vector embeddings](https://docs.weaviate.io/weaviate/starter-guides/custom-vectors).

- **üîç Advanced Hybrid &amp; Image Search**: Combine the power of semantic search with traditional [keyword (BM25) search](https://docs.weaviate.io/weaviate/search/bm25), [image search](https://docs.weaviate.io/weaviate/search/image) and [advanced filtering](https://docs.weaviate.io/weaviate/search/filters) to get the best results with a single API call.

- **ü§ñ Integrated RAG &amp; Reranking**: Go beyond simple retrieval with built-in [generative search (RAG)](https://docs.weaviate.io/weaviate/search/generative) and [reranking](https://docs.weaviate.io/weaviate/search/rerank) capabilities. Power sophisticated Q&amp;A systems, chatbots, and summarizers directly from your database without additional tooling.

- **üìà Production-Ready &amp; Scalable**: Weaviate is built for mission-critical applications. Go from rapid prototyping to production at scale with native support for [horizontal scaling](https://docs.weaviate.io/deploy/configuration/horizontal-scaling), [multi-tenancy](https://docs.weaviate.io/weaviate/manage-collections/multi-tenancy), [replication](https://docs.weaviate.io/deploy/configuration/replication), and fine-grained [role-based access control (RBAC)](https://docs.weaviate.io/weaviate/configuration/rbac).

- **üí∞ Cost-Efficient Operations**: Radically lower resource consumption and operational costs with built-in [vector compression](https://docs.weaviate.io/weaviate/configuration/compression). Vector quantization and multi-vector encoding reduce memory usage with minimal impact on search performance.

For a complete list of all functionalities, visit the [official Weaviate documentation](https://docs.weaviate.io).

## Useful resources

### Demo projects &amp; recipes

These demos are working applications that highlight some of Weaviate&#039;s capabilities. Their source code is available on GitHub.

- [Elysia](https://elysia.weaviate.io) ([GitHub](https://github.com/weaviate/elysia)): Elysia is a decision tree based agentic system which intelligently decides what tools to use, what results have been obtained, whether it should continue the process or whether its goal has been completed.
- [Verba](https://verba.weaviate.io) ([GitHub](https://github.com/weaviate/verba)): A community-driven open-source application designed to offer an end-to-end, streamlined, and user-friendly interface for Retrieval-Augmented Generation (RAG) out of the box.
- [Healthsearch](https://healthsearch.weaviate.io) ([GitHub](https://github.com/weaviate/healthsearch-demo)): An open-source project aimed at showcasing the potential of leveraging user-written reviews and queries to retrieve supplement products based on specific health effects.
- [Awesome-Moviate](https://awesome-moviate.weaviate.io/) ([GitHub](https://github.com/weaviate-tutorials/awesome-moviate)): A movie search and recommendation engine that allows keyword-based (BM25), semantic, and hybrid searches.

We also maintain extensive repositories of **Jupyter Notebooks** and **TypeScript code snippets** that cover how to use Weaviate features and integrations:

- [Weaviate Python Recipes](https://github.com/weaviate/recipes/)
- [Weaviate TypeScript Recipes](https://github.com/weaviate/recipes-ts/)

### Blog posts

- [What is a Vector Database](https://weaviate.io/blog/what-is-a-vector-database)
- [What is Vector Search](https://weaviate.io/blog/vector-search-explained)
- [What is Hybrid Search](https://weaviate.io/blog/hybrid-search-explained)
- [How to Choose an Embedding Model](https://weaviate.io/blog/how-to-choose-an-embedding-model)
- [What is RAG](https://weaviate.io/blog/introduction-to-rag)
- [RAG Evaluation](https://weaviate.io/blog/rag-evaluation)
- [Advanced RAG Techniques](https://weaviate.io/blog/advanced-rag)
- [What is Multimodal RAG](https://weaviate.io/blog/multimodal-rag)
- [What is Agentic RAG](https://weaviate.io/blog/what-is-agentic-rag)
- [What is Graph RAG](https://weaviate.io/blog/graph-rag)
- [Overview of Late Interaction Models](https://weaviate.io/blog/late-interaction-overview)

### Integrations

Weaviate integrates with many external services:

| Category                                                                                   | Description                                                | Integrations                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| ------------------------------------------------------------------------------------------ | ---------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **[Cloud Hyperscalers](https://docs.weaviate.io/integrations/cloud-hyperscalers)**         | Large-scale computing and storage                          | [AWS](https://docs.weaviate.io/integrations/cloud-hyperscalers/aws), [Google](https://docs.weaviate.io/integrations/cloud-hyperscalers/google)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| **[Compute Infrastructure](https://docs.weaviate.io/integrations/compute-infrastructure)** | Run and scale containerized applications                   | [Modal](https://docs.weaviate.io/integrations/compute-infrastructure/modal), [Replicate](https://docs.weaviate.io/integrations/compute-infrastructure/replicate), [Replicated](https://docs.weaviate.io/integrations/compute-infrastructure/replicated)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| **[Data Platforms](https://docs.weaviate.io/integrations/data-platforms)**                 | Data ingestion and web scraping                            | [Airbyte](https://docs.weaviate.io/integrations/data-platforms/airbyte), [Aryn](https://docs.weaviate.io/integrations/data-platforms/aryn), [Boomi](https://docs.weaviate.io/integrations/data-platforms/boomi), [Box](https://docs.weaviate.io/integrations/data-platforms/box), [Confluent](https://docs.weaviate.io/integrations/data-platforms/confluent), [Astronomer](https://docs.weaviate.io/integrations/data-platforms/astronomer), [Context Data](https://docs.weaviate.io/integrations/data-platforms/context-data), [Databricks](https://docs.weaviate.io/integrations/data-platforms/databricks), [Firecrawl](https://docs.weaviate.io/integrations/data-platforms/firecrawl), [IBM](https://docs.weaviate.io/integrations/data-platforms/ibm), [Unstructured](https://docs.weaviate.io/integrations/data-platforms/unstructured)                |
| **[LLM and Agent Frameworks](https://docs.weaviate.io/integrations/llm-agent-frameworks)** | Build agents and generative AI applications                | [Agno](https://docs.weaviate.io/integrations/llm-agent-frameworks/agno), [Composio](https://docs.weaviate.io/integrations/llm-agent-frameworks/composio), [CrewAI](https://docs.weaviate.io/integrations/llm-agent-frameworks/crewai), [DSPy](https://docs.weaviate.io/integrations/llm-agent-frameworks/dspy), [Dynamiq](https://docs.weaviate.io/integrations/llm-agent-frameworks/dynamiq), [Haystack](https://docs.weaviate.io/integrations/llm-agent-frameworks/haystack), [LangChain](https://docs.weaviate.io/integrations/llm-agent-frameworks/langchain), [LlamaIndex](https://docs.weaviate.io/integrations/llm-agent-frameworks/llamaindex), [N8n](https://docs.weaviate.io/integrations/llm-agent-frameworks/n8n), [Semantic Kernel](https://docs.weaviate.io/integrations/llm-agent-frameworks/semantic-kernel)                                   |
| **[Operations](https://docs.weaviate.io/integrations/operations)**                         | Tools for monitoring and analyzing generative AI workflows | [AIMon](https://docs.weaviate.io/integrations/operations/aimon), [Arize](https://docs.weaviate.io/integrations/operations/arize), [Cleanlab](https://docs.weaviate.io/integrations/operations/cleanlab), [Comet](https://docs.weaviate.io/integrations/operations/comet), [DeepEval](https://docs.weaviate.io/integrations/operations/deepeval), [Langtrace](https://docs.weaviate.io/integrations/operations/langtrace), [LangWatch](https://docs.weaviate.io/integrations/operations/langwatch), [Nomic](https://docs.weaviate.io/integrations/operations/nomic), [Patronus AI](https://docs.weaviate.io/integrations/operations/patronus), [Ragas](https://docs.weaviate.io/integrations/operations/ragas), [TruLens](https://docs.weaviate.io/integrations/operations/trulens), [Weights &amp; Biases](https://docs.weaviate.io/integrations/operations/wandb) |

## Contributing

We welcome and appreciate contributions! Please see our [Contributor guide](https://docs.weaviate.io/contributor-guide) for the development setup, code style guidelines, testing requirements and the pull request process.

Join our [Slack community](https://weaviate.io/slack) or [Community forum](https://forum.weaviate.io/) to discuss ideas and get help.

## License

BSD 3-Clause License. See [LICENSE](./LICENSE) for details.
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[docker/mcp-registry]]></title>
            <link>https://github.com/docker/mcp-registry</link>
            <guid>https://github.com/docker/mcp-registry</guid>
            <pubDate>Wed, 05 Nov 2025 00:51:21 GMT</pubDate>
            <description><![CDATA[Official Docker MCP registry]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/docker/mcp-registry">docker/mcp-registry</a></h1>
            <p>Official Docker MCP registry</p>
            <p>Language: Go</p>
            <p>Stars: 311</p>
            <p>Forks: 389</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre># üê≥ Official Docker MCP Registry

Welcome to the Official Docker MCP (Model Context Protocol) Registry! This repository serves as a curated catalog of MCP servers that can be easily discovered, deployed, and integrated with any MCP Client and compatible with Docker tooling. 

Entries in this catalog will be available at: 
- [MCP catalog](https://hub.docker.com/mcp) 
- [Docker Desktop&#039;s MCP Toolkit](https://www.docker.com/products/docker-desktop/) 
- [Docker Hub `mcp` namespace](https://hub.docker.com/u/mcp) (for MCP servers built by Docker)

## ü§ñ What is MCP?
The Model Context Protocol (MCP) is an open standard that enables AI assistants to securely connect with external data sources and tools. Read more at [MCP Official Documentation](https://modelcontextprotocol.io/introduction).

## ‚ú® Why Use the Docker MCP Registry?
- **Enterprise Security**: MCP servers built by Docker include cryptographic signatures, provenance tracking, and Software Bills of Materials (SBOMs) for maximum trust and compliance
- **Container Isolation**: All MCP servers run in isolated containers, protecting your host system from potential security vulnerabilities
- **Curated Quality**: All MCP servers undergo review to ensure they meet quality and security standards
- **Easy Discovery**: Browse and find MCP servers for your specific use cases or share yours to millions of developers using Docker tools
- **Docker Integration**: Seamless deployment with Docker containers

## ü§ù Contributing to the Docker MCP Registry
We welcome contributions to the Official Docker MCP Registry! If you&#039;d like to contribute, you can submit a PR with the metadata information and it will be added to the [MCP catalog](https://hub.docker.com/mcp), to [Docker Desktop&#039;s MCP Toolkit](https://www.docker.com/products/docker-desktop/), and (for MCP servers images built by Docker) in `mcp` namespace in [Docker Hub](https://hub.docker.com/u/mcp).

To add your MCP server to the registry, please review the [CONTRIBUTING](CONTRIBUTING.md) guide for detailed instructions. We support two types of submissions:

### üèóÔ∏è Option A: Docker-Built Image (Recommended)
Have Docker build and maintain your server image with enhanced security features. You&#039;ll submit the required information via pull request and upon approval Docker will build, sign, and publish your image to mcp/your-server-name on Docker Hub and the catalog entry will be available in the catalog in 24 hours.

_**Benefits: Your image will include cryptographic signatures, provenance tracking, SBOMs, and automatic security updates**_

### üì¶ Option B: Self-Provided Pre-Built Image
In this option, you&#039;ll provide an already built image which will be used directly in the catalog. 

_**Note: Self-built images still benefit from container isolation but won&#039;t include the enhanced security features of Docker-built images.**_

## ‚úèÔ∏è Modifying or Removing Servers
To request modifications or removal of an existing MCP Server please open an issue explaining the reason for the edit/removal.

## ‚úÖ Compliance and Quality Standards
All MCP servers in this registry must:
- Follow security best practices
- Include comprehensive documentation
- Provide working Docker deployment
- Maintain compatibility with MCP standards
- Include proper error handling and logging

_**Non-compliant servers will be reviewed and may be removed from the registry.**_

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[grafana/mcp-grafana]]></title>
            <link>https://github.com/grafana/mcp-grafana</link>
            <guid>https://github.com/grafana/mcp-grafana</guid>
            <pubDate>Wed, 05 Nov 2025 00:51:20 GMT</pubDate>
            <description><![CDATA[MCP server for Grafana]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/grafana/mcp-grafana">grafana/mcp-grafana</a></h1>
            <p>MCP server for Grafana</p>
            <p>Language: Go</p>
            <p>Stars: 1,791</p>
            <p>Forks: 179</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre># Grafana MCP server

[![Unit Tests](https://github.com/grafana/mcp-grafana/actions/workflows/unit.yml/badge.svg)](https://github.com/grafana/mcp-grafana/actions/workflows/unit.yml)
[![Integration Tests](https://github.com/grafana/mcp-grafana/actions/workflows/integration.yml/badge.svg)](https://github.com/grafana/mcp-grafana/actions/workflows/integration.yml)
[![E2E Tests](https://github.com/grafana/mcp-grafana/actions/workflows/e2e.yml/badge.svg)](https://github.com/grafana/mcp-grafana/actions/workflows/e2e.yml)
[![Go Reference](https://pkg.go.dev/badge/github.com/grafana/mcp-grafana.svg)](https://pkg.go.dev/github.com/grafana/mcp-grafana)
[![MCP Catalog](https://archestra.ai/mcp-catalog/api/badge/quality/grafana/mcp-grafana)](https://archestra.ai/mcp-catalog/grafana__mcp-grafana)

A [Model Context Protocol][mcp] (MCP) server for Grafana.

This provides access to your Grafana instance and the surrounding ecosystem.

## Requirements

- **Grafana version 9.0 or later** is required for full functionality. Some features, particularly datasource-related operations, may not work correctly with earlier versions due to missing API endpoints.

## Features

_The following features are currently available in MCP server. This list is for informational purposes only and does not represent a roadmap or commitment to future features._

### Dashboards

- **Search for dashboards:** Find dashboards by title or other metadata
- **Get dashboard by UID:** Retrieve full dashboard details using its unique identifier. _Warning: Large dashboards can consume significant context window space._
- **Get dashboard summary:** Get a compact overview of a dashboard including title, panel count, panel types, variables, and metadata without the full JSON to minimize context window usage
- **Get dashboard property:** Extract specific parts of a dashboard using JSONPath expressions (e.g., `$.title`, `$.panels[*].title`) to fetch only needed data and reduce context window consumption
- **Update or create a dashboard:** Modify existing dashboards or create new ones. _Warning: Requires full dashboard JSON which can consume large amounts of context window space._
- **Patch dashboard:** Apply specific changes to a dashboard without requiring the full JSON, significantly reducing context window usage for targeted modifications
- **Get panel queries and datasource info:** Get the title, query string, and datasource information (including UID and type, if available) from every panel in a dashboard

#### Context Window Management

The dashboard tools now include several strategies to manage context window usage effectively ([issue #101](https://github.com/grafana/mcp-grafana/issues/101)):

- **Use `get_dashboard_summary`** for dashboard overview and planning modifications
- **Use `get_dashboard_property`** with JSONPath when you only need specific dashboard parts
- **Avoid `get_dashboard_by_uid`** unless you specifically need the complete dashboard JSON

### Datasources

- **List and fetch datasource information:** View all configured datasources and retrieve detailed information about each.
  - _Supported datasource types: Prometheus, Loki._

### Prometheus Querying

- **Query Prometheus:** Execute PromQL queries (supports both instant and range metric queries) against Prometheus datasources.
- **Query Prometheus metadata:** Retrieve metric metadata, metric names, label names, and label values from Prometheus datasources.

### Loki Querying

- **Query Loki logs and metrics:** Run both log queries and metric queries using LogQL against Loki datasources.
- **Query Loki metadata:** Retrieve label names, label values, and stream statistics from Loki datasources.

### Incidents

- **Search, create, and update incidents:** Manage incidents in Grafana Incident, including searching, creating, and adding activities to incidents.

### Sift Investigations

- **List Sift investigations:** Retrieve a list of Sift investigations, with support for a limit parameter.
- **Get Sift investigation:** Retrieve details of a specific Sift investigation by its UUID.
- **Get Sift analyses:** Retrieve a specific analysis from a Sift investigation.
- **Find error patterns in logs:** Detect elevated error patterns in Loki logs using Sift.
- **Find slow requests:** Detect slow requests using Sift (Tempo).

### Alerting

- **List and fetch alert rule information:** View alert rules and their statuses (firing/normal/error/etc.) in Grafana.
- **List contact points:** View configured notification contact points in Grafana.

### Grafana OnCall

- **List and manage schedules:** View and manage on-call schedules in Grafana OnCall.
- **Get shift details:** Retrieve detailed information about specific on-call shifts.
- **Get current on-call users:** See which users are currently on call for a schedule.
- **List teams and users:** View all OnCall teams and users.
- **List alert groups:** View and filter alert groups from Grafana OnCall by various criteria including state, integration, labels, and time range.
- **Get alert group details:** Retrieve detailed information about a specific alert group by its ID.

### Admin

- **List teams:** View all configured teams in Grafana.
- **List Users:** View all users in an organization in Grafana.

### Navigation

- **Generate deeplinks:** Create accurate deeplink URLs for Grafana resources instead of relying on LLM URL guessing.
  - **Dashboard links:** Generate direct links to dashboards using their UID (e.g., `http://localhost:3000/d/dashboard-uid`)
  - **Panel links:** Create links to specific panels within dashboards with viewPanel parameter (e.g., `http://localhost:3000/d/dashboard-uid?viewPanel=5`)
  - **Explore links:** Generate links to Grafana Explore with pre-configured datasources (e.g., `http://localhost:3000/explore?left={&quot;datasource&quot;:&quot;prometheus-uid&quot;}`)
  - **Time range support:** Add time range parameters to links (`from=now-1h&amp;to=now`)
  - **Custom parameters:** Include additional query parameters like dashboard variables or refresh intervals

The list of tools is configurable, so you can choose which tools you want to make available to the MCP client.
This is useful if you don&#039;t use certain functionality or if you don&#039;t want to take up too much of the context window.
To disable a category of tools, use the `--disable-&lt;category&gt;` flag when starting the server. For example, to disable
the OnCall tools, use `--disable-oncall`, or to disable navigation deeplink generation, use `--disable-navigation`.

#### RBAC Permissions

Each tool requires specific RBAC permissions to function properly. When creating a service account for the MCP server, ensure it has the necessary permissions based on which tools you plan to use. The permissions listed are the minimum required actions - you may also need appropriate scopes (e.g., `datasources:*`, `dashboards:*`, `folders:*`) depending on your use case.

Tip: If you&#039;re not familiar with Grafana RBAC or you want a quicker, simpler setup instead of configuring many granular scopes, you can assign a built-in role such as `Editor` to the service account. The `Editor` role grants broad read/write access that will allow most MCP server operations; it is less granular (and therefore less restrictive) than manually-applied scopes, so use it only when convenience is more important than strict least-privilege access.

**Note:** Grafana Incident and Sift tools use basic Grafana roles instead of fine-grained RBAC permissions:
- **Viewer role:** Required for read-only operations (list incidents, get investigations)
- **Editor role:** Required for write operations (create incidents, modify investigations)

For more information about Grafana RBAC, see the [official documentation](https://grafana.com/docs/grafana/latest/administration/roles-and-permissions/access-control/).

#### RBAC Scopes

Scopes define the specific resources that permissions apply to. Each action requires both the appropriate permission and scope combination.

**Common Scope Patterns:**

- **Broad access:** Use `*` wildcards for organization-wide access

  - `datasources:*` - Access to all datasources
  - `dashboards:*` - Access to all dashboards
  - `folders:*` - Access to all folders
  - `teams:*` - Access to all teams

- **Limited access:** Use specific UIDs or IDs to restrict access to individual resources
  - `datasources:uid:prometheus-uid` - Access only to a specific Prometheus datasource
  - `dashboards:uid:abc123` - Access only to dashboard with UID `abc123`
  - `folders:uid:xyz789` - Access only to folder with UID `xyz789`
  - `teams:id:5` - Access only to team with ID `5`
  - `global.users:id:123` - Access only to user with ID `123`

**Examples:**

- **Full MCP server access:** Grant broad permissions for all tools

  ```
  datasources:* (datasources:read, datasources:query)
  dashboards:* (dashboards:read, dashboards:create, dashboards:write)
  folders:* (for dashboard creation and alert rules)
  teams:* (teams:read)
  global.users:* (users:read)
  ```

- **Limited datasource access:** Only query specific Prometheus and Loki instances

  ```
  datasources:uid:prometheus-prod (datasources:query)
  datasources:uid:loki-prod (datasources:query)
  ```

- **Dashboard-specific access:** Read only specific dashboards
  ```
  dashboards:uid:monitoring-dashboard (dashboards:read)
  dashboards:uid:alerts-dashboard (dashboards:read)
  ```

### Tools

| Tool                              | Category    | Description                                                        | Required RBAC Permissions               | Required Scopes                                     |
| --------------------------------- | ----------- | ------------------------------------------------------------------ | --------------------------------------- | --------------------------------------------------- |
| `list_teams`                      | Admin       | List all teams                                                     | `teams:read`                            | `teams:*` or `teams:id:1`                           |
| `list_users_by_org`               | Admin       | List all users in an organization                                  | `users:read`                            | `global.users:*` or `global.users:id:123`           |
| `search_dashboards`               | Search      | Search for dashboards                                              | `dashboards:read`                       | `dashboards:*` or `dashboards:uid:abc123`           |
| `get_dashboard_by_uid`            | Dashboard   | Get a dashboard by uid                                             | `dashboards:read`                       | `dashboards:uid:abc123`                             |
| `update_dashboard`                | Dashboard   | Update or create a new dashboard                                   | `dashboards:create`, `dashboards:write` | `dashboards:*`, `folders:*` or `folders:uid:xyz789` |
| `get_dashboard_panel_queries`     | Dashboard   | Get panel title, queries, datasource UID and type from a dashboard | `dashboards:read`                       | `dashboards:uid:abc123`                             |
| `get_dashboard_property`          | Dashboard   | Extract specific parts of a dashboard using JSONPath expressions   | `dashboards:read`                       | `dashboards:uid:abc123`                             |
| `get_dashboard_summary`           | Dashboard   | Get a compact summary of a dashboard without full JSON             | `dashboards:read`                       | `dashboards:uid:abc123`                             |
| `list_datasources`                | Datasources | List datasources                                                   | `datasources:read`                      | `datasources:*`                                     |
| `get_datasource_by_uid`           | Datasources | Get a datasource by uid                                            | `datasources:read`                      | `datasources:uid:prometheus-uid`                    |
| `get_datasource_by_name`          | Datasources | Get a datasource by name                                           | `datasources:read`                      | `datasources:*` or `datasources:uid:loki-uid`       |
| `query_prometheus`                | Prometheus  | Execute a query against a Prometheus datasource                    | `datasources:query`                     | `datasources:uid:prometheus-uid`                    |
| `list_prometheus_metric_metadata` | Prometheus  | List metric metadata                                               | `datasources:query`                     | `datasources:uid:prometheus-uid`                    |
| `list_prometheus_metric_names`    | Prometheus  | List available metric names                                        | `datasources:query`                     | `datasources:uid:prometheus-uid`                    |
| `list_prometheus_label_names`     | Prometheus  | List label names matching a selector                               | `datasources:query`                     | `datasources:uid:prometheus-uid`                    |
| `list_prometheus_label_values`    | Prometheus  | List values for a specific label                                   | `datasources:query`                     | `datasources:uid:prometheus-uid`                    |
| `list_incidents`                  | Incident    | List incidents in Grafana Incident                                 | Viewer role                             | N/A                                                 |
| `create_incident`                 | Incident    | Create an incident in Grafana Incident                             | Editor role                             | N/A                                                 |
| `add_activity_to_incident`        | Incident    | Add an activity item to an incident in Grafana Incident            | Editor role                             | N/A                                                 |
| `get_incident`                    | Incident    | Get a single incident by ID                                        | Viewer role                             | N/A                                                 |
| `query_loki_logs`                 | Loki        | Query and retrieve logs using LogQL (either log or metric queries) | `datasources:query`                     | `datasources:uid:loki-uid`                          |
| `list_loki_label_names`           | Loki        | List all available label names in logs                             | `datasources:query`                     | `datasources:uid:loki-uid`                          |
| `list_loki_label_values`          | Loki        | List values for a specific log label                               | `datasources:query`                     | `datasources:uid:loki-uid`                          |
| `query_loki_stats`                | Loki        | Get statistics about log streams                                   | `datasources:query`                     | `datasources:uid:loki-uid`                          |
| `list_alert_rules`                | Alerting    | List alert rules                                                   | `alert.rules:read`                      | `folders:*` or `folders:uid:alerts-folder`          |
| `get_alert_rule_by_uid`           | Alerting    | Get alert rule by UID                                              | `alert.rules:read`                      | `folders:uid:alerts-folder`                         |
| `list_contact_points`             | Alerting    | List notification contact points                                   | `alert.notifications:read`              | Global scope                                        |
| `list_oncall_schedules`           | OnCall      | List schedules from Grafana OnCall                                 | `grafana-oncall-app.schedules:read`     | Plugin-specific scopes                              |
| `get_oncall_shift`                | OnCall      | Get details for a specific OnCall shift                            | `grafana-oncall-app.schedules:read`     | Plugin-specific scopes                              |
| `get_current_oncall_users`        | OnCall      | Get users currently on-call for a specific schedule                | `grafana-oncall-app.schedules:read`     | Plugin-specific scopes                              |
| `list_oncall_teams`               | OnCall      | List teams from Grafana OnCall                                     | `grafana-oncall-app.user-settings:read` | Plugin-specific scopes                              |
| `list_oncall_users`               | OnCall      | List users from Grafana OnCall                                     | `grafana-oncall-app.user-settings:read` | Plugin-specific scopes                              |
| `list_alert_groups`               | OnCall      | List alert groups from Grafana OnCall with filtering options       | `grafana-oncall-app.alert-groups:read`  | Plugin-specific scopes                              |
| `get_alert_group`                 | OnCall      | Get a specific alert group from Grafana OnCall by its ID           | `grafana-oncall-app.alert-groups:read`  | Plugin-specific scopes                              |
| `get_sift_investigation`          | Sift        | Retrieve an existing Sift investigation by its UUID                | Viewer role                             | N/A                                                 |
| `get_sift_analysis`               | Sift        | Retrieve a specific analysis from a Sift investigation             | Viewer role                             | N/A                                                 |
| `list_sift_investigations`        | Sift        | Retrieve a list of Sift investigations with an optional limit      | Viewer role                             | N/A                                                 |
| `find_error_pattern_logs`         | Sift        | Finds elevated error patterns in Loki logs.                        | Editor role                             | N/A                                                 |
| `find_slow_requests`              | Sift        | Finds slow requests from the relevant tempo datasources.           | Editor role                             | N/A                                                 |
| `list_pyroscope_label_names`      | Pyroscope   | List label names matching a selector                               | `datasources:query`                     | `datasources:uid:pyroscope-uid`                     |
| `list_pyroscope_label_values`     | Pyroscope   | List label values matching a selector for a label name             | `datasources:query`                     | `datasources:uid:pyroscope-uid`                     |
| `list_pyroscope_profile_types`    | Pyroscope   | List available profile types                                       | `datasources:query`                     | `datasources:uid:pyroscope-uid`                     |
| `fetch_pyroscope_profile`         | Pyroscope   | Fetches a profile in DOT format for analysis                       | `datasources:query`                     | `datasources:uid:pyroscope-uid`                     |
| `get_assertions`                  | Asserts     | Get assertion summary for a given entity                           | Plugin-specific permissions             | Plugin-specific scopes                              |
| `generate_deeplink`               | Navigation  | Generate accurate deeplink URLs for Grafana resources              | None (read-only URL generation)         | N/A                                                 |

## CLI Flags Reference

The `mcp-grafana` binary supports various command-line flags for configuration:

**Transport Options:**
- `-t, --transport`: Transport type (`stdio`, `sse`, or `streamable-http`) - default: `stdio`
- `--address`: The host and port for SSE/streamable-http server - default: `localhost:8000`
- `--base-path`: Base path for the SSE/streamable-http server
- `--endpoint-path`: Endpoint

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[containerd/containerd]]></title>
            <link>https://github.com/containerd/containerd</link>
            <guid>https://github.com/containerd/containerd</guid>
            <pubDate>Wed, 05 Nov 2025 00:51:19 GMT</pubDate>
            <description><![CDATA[An open and reliable container runtime]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/containerd/containerd">containerd/containerd</a></h1>
            <p>An open and reliable container runtime</p>
            <p>Language: Go</p>
            <p>Stars: 19,517</p>
            <p>Forks: 3,677</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre>![containerd banner light mode](https://raw.githubusercontent.com/cncf/artwork/master/projects/containerd/horizontal/color/containerd-horizontal-color.png#gh-light-mode-only)
![containerd banner dark mode](https://raw.githubusercontent.com/cncf/artwork/master/projects/containerd/horizontal/white/containerd-horizontal-white.png#gh-dark-mode-only)

[![PkgGoDev](https://pkg.go.dev/badge/github.com/containerd/containerd/v2)](https://pkg.go.dev/github.com/containerd/containerd/v2)
[![Build Status](https://github.com/containerd/containerd/actions/workflows/ci.yml/badge.svg?event=merge_group)](https://github.com/containerd/containerd/actions?query=workflow%3ACI+event%3Amerge_group)
[![Nightlies](https://github.com/containerd/containerd/workflows/Nightly/badge.svg)](https://github.com/containerd/containerd/actions?query=workflow%3ANightly)
[![Go Report Card](https://goreportcard.com/badge/github.com/containerd/containerd/v2)](https://goreportcard.com/report/github.com/containerd/containerd/v2)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1271/badge)](https://bestpractices.coreinfrastructure.org/projects/1271)
[![OpenSSF Scorecard](https://api.scorecard.dev/projects/github.com/containerd/containerd/badge)](https://scorecard.dev/viewer/?uri=github.com/containerd/containerd)
[![Check Links](https://github.com/containerd/containerd/actions/workflows/links.yml/badge.svg)](https://github.com/containerd/containerd/actions/workflows/links.yml)

containerd is an industry-standard container runtime with an emphasis on simplicity, robustness, and portability. It is available as a daemon for Linux and Windows, which can manage the complete container lifecycle of its host system: image transfer and storage, container execution and supervision, low-level storage and network attachments, etc.

containerd is a member of CNCF with [&#039;graduated&#039;](https://landscape.cncf.io/?selected=containerd) status.

containerd is designed to be embedded into a larger system, rather than being used directly by developers or end-users.

![architecture](docs/historical/design/architecture.png)

## Announcements

### containerd v2.0 is now released!
See [`docs/containerd-2.0.md`](docs/containerd-2.0.md).

### Now Recruiting

We are a large inclusive OSS project that is welcoming help of any kind shape or form:
* Documentation help is needed to make the product easier to consume and extend.
* We need OSS community outreach/organizing help to get the word out; manage
and create messaging and educational content; and help with social media, community forums/groups, and google groups.
* We are actively inviting new [security advisors](https://github.com/containerd/project/blob/main/GOVERNANCE.md#security-advisors) to join the team.
* New subprojects are being created, core and non-core that could use additional development help.
* Each of the [containerd projects](https://github.com/containerd) has a list of issues currently being worked on or that need help resolving.
  - If the issue has not already been assigned to someone or has not made recent progress, and you are interested, please inquire.
  - If you are interested in starting with a smaller/beginner-level issue, look for issues with an `exp/beginner` tag, for example [containerd/containerd beginner issues.](https://github.com/containerd/containerd/issues?q=is%3Aissue+is%3Aopen+label%3Aexp%2Fbeginner)

## Getting Started

See our documentation on [containerd.io](https://containerd.io):
* [for ops and admins](docs/ops.md)
* [namespaces](docs/namespaces.md)
* [client options](docs/client-opts.md)

To get started contributing to containerd, see [CONTRIBUTING](CONTRIBUTING.md).

If you are interested in trying out containerd see our example at [Getting Started](docs/getting-started.md).

## Nightly builds

There are nightly builds available for download [here](https://github.com/containerd/containerd/actions?query=workflow%3ANightly).
Binaries are generated from `main` branch every night for `Linux` and `Windows`.

Please be aware: nightly builds might have critical bugs, it&#039;s not recommended for use in production and no support provided.

## Kubernetes (k8s) CI Dashboard Group

The [k8s CI dashboard group for containerd](https://testgrid.k8s.io/containerd) contains test results regarding
the health of kubernetes when run against main and a number of containerd release branches.

- [containerd-periodics](https://testgrid.k8s.io/containerd-periodic)

## Runtime Requirements

Runtime requirements for containerd are very minimal. Most interactions with
the Linux and Windows container feature sets are handled via [runc](https://github.com/opencontainers/runc) and/or
OS-specific libraries (e.g. [hcsshim](https://github.com/Microsoft/hcsshim) for Microsoft).
The current required version of `runc` is described in [RUNC.md](docs/RUNC.md).

There are specific features
used by containerd core code and snapshotters that will require a minimum kernel
version on Linux. With the understood caveat of distro kernel versioning, a
reasonable starting point for Linux is a minimum 4.x kernel version.

The overlay filesystem snapshotter, used by default, uses features that were
finalized in the 4.x kernel series. If you choose to use btrfs, there may
be more flexibility in kernel version (minimum recommended is 3.18), but will
require the btrfs kernel module and btrfs tools to be installed on your Linux
distribution.

To use Linux checkpoint and restore features, you will need `criu` installed on
your system. See more details in [Checkpoint and Restore](#checkpoint-and-restore).

Build requirements for developers are listed in [BUILDING](BUILDING.md).


## Supported Registries

Any registry which is compliant with the [OCI Distribution Specification](https://github.com/opencontainers/distribution-spec)
is supported by containerd.

For configuring registries, see [registry host configuration documentation](docs/hosts.md)

## Features

For a detailed overview of containerd&#039;s core concepts and the features it supports,
please refer to the [FEATURES.MD](./docs/features.md) document.

### Releases and API Stability

Please see [RELEASES.md](RELEASES.md) for details on versioning and stability
of containerd components.

Downloadable 64-bit Intel/AMD binaries of all official releases are available on
our [releases page](https://github.com/containerd/containerd/releases).

For other architectures and distribution support, you will find that many
Linux distributions package their own containerd and provide it across several
architectures, such as [Canonical&#039;s Ubuntu packaging](https://launchpad.net/ubuntu/bionic/+package/containerd).

#### Enabling command auto-completion

Starting with containerd 1.4, the urfave client feature for auto-creation of bash and zsh
autocompletion data is enabled. To use the autocomplete feature in a bash shell for example, source
the autocomplete/ctr file in your `.bashrc`, or manually like:

```
$ source ./contrib/autocomplete/ctr
```

#### Distribution of `ctr` autocomplete for bash and zsh

For bash, copy the `contrib/autocomplete/ctr` script into
`/etc/bash_completion.d/` and rename it to `ctr`. The `zsh_autocomplete`
file is also available and can be used similarly for zsh users.

Provide documentation to users to `source` this file into their shell if
you don&#039;t place the autocomplete file in a location where it is automatically
loaded for the user&#039;s shell environment.

### CRI

`cri` is a [containerd](https://containerd.io/) plugin implementation of the Kubernetes [container runtime interface (CRI)](https://github.com/kubernetes/cri-api/blob/master/pkg/apis/runtime/v1/api.proto). With it, you are able to use containerd as the container runtime for a Kubernetes cluster.

![cri](./docs/cri/cri.png)

#### CRI Status

`cri` is a native plugin of containerd. Since containerd 1.1, the cri plugin is built into the release binaries and enabled by default.

The `cri` plugin has reached GA status, representing that it is:
* Feature complete
* Works with Kubernetes 1.10 and above
* Passes all [CRI validation tests](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/cri-validation.md).
* Passes all [node e2e tests](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/e2e-node-tests.md).
* Passes all [e2e tests](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-testing/e2e-tests.md).

See results on the containerd k8s [test dashboard](https://testgrid.k8s.io/containerd)

#### Validating Your `cri` Setup
A Kubernetes incubator project, [cri-tools](https://github.com/kubernetes-sigs/cri-tools), includes programs for exercising CRI implementations. More importantly, cri-tools includes the program `critest` which is used for running [CRI Validation Testing](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/cri-validation.md).

#### CRI Guides
* [Installing with Ansible and Kubeadm](contrib/ansible/README.md)
* [For Non-Ansible Users, Preforming a Custom Installation Using the Release Tarball and Kubeadm](docs/getting-started.md)
* [CRI Plugin Testing Guide](./docs/cri/testing.md)
* [Debugging Pods, Containers, and Images with `crictl`](./docs/cri/crictl.md)
* [Configuring `cri` Plugins](./docs/cri/config.md)
* [Configuring containerd](https://github.com/containerd/containerd/blob/main/docs/man/containerd-config.8.md)

### Communication

For async communication and long-running discussions please use issues and pull requests on the GitHub repo.
This will be the best place to discuss design and implementation.

For sync communication catch us in the `#containerd` and `#containerd-dev` Slack channels on Cloud Native Computing Foundation&#039;s (CNCF) Slack - `cloud-native.slack.com`. Everyone is welcome to join and chat. [Get Invite to CNCF Slack.](https://slack.cncf.io)

Join our next community meeting hosted on Zoom. The schedule is posted on the [CNCF Calendar](https://www.cncf.io/calendar/) (search &#039;containerd&#039; to filter).

### Security audit

Security audits for the containerd project are hosted on our website. Please see the [security page at containerd.io](https://containerd.io/security/) for more information.

### Reporting security issues

Please follow the instructions at [containerd/project](https://github.com/containerd/project/blob/main/SECURITY.md#reporting-a-vulnerability)

## Licenses

The containerd codebase is released under the [Apache 2.0 license](LICENSE).
The README.md file and files in the &quot;docs&quot; folder are licensed under the
Creative Commons Attribution 4.0 International License. You may obtain a
copy of the license, titled CC-BY-4.0, at http://creativecommons.org/licenses/by/4.0/.

## Project details

**containerd** is the primary open source project within the broader containerd GitHub organization.
However, all projects within the repo have common maintainership, governance, and contributing
guidelines which are stored in a `project` repository commonly for all containerd projects.

Please find all these core project documents, including the:
 * [Project governance](https://github.com/containerd/project/blob/main/GOVERNANCE.md),
 * [Maintainers](https://github.com/containerd/project/blob/main/MAINTAINERS),
 * and [Contributing guidelines](https://github.com/containerd/project/blob/main/CONTRIBUTING.md)

information in our [`containerd/project`](https://github.com/containerd/project) repository.

## Adoption

Interested to see who is using containerd? Are you using containerd in a project?
Please add yourself via pull request to our [ADOPTERS.md](./ADOPTERS.md) file.
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
    </channel>
</rss>