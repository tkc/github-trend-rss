<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for go - Go Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for go.</description>
        <lastBuildDate>Wed, 14 Jan 2026 00:05:37 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2026, GitHub</copyright>
        <item>
            <title><![CDATA[Tencent/WeKnora]]></title>
            <link>https://github.com/Tencent/WeKnora</link>
            <guid>https://github.com/Tencent/WeKnora</guid>
            <pubDate>Wed, 14 Jan 2026 00:05:37 GMT</pubDate>
            <description><![CDATA[LLM-powered framework for deep document understanding, semantic retrieval, and context-aware answers using RAG paradigm.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Tencent/WeKnora">Tencent/WeKnora</a></h1>
            <p>LLM-powered framework for deep document understanding, semantic retrieval, and context-aware answers using RAG paradigm.</p>
            <p>Language: Go</p>
            <p>Stars: 11,490</p>
            <p>Forks: 1,262</p>
            <p>Stars today: 109 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;img src=&quot;./docs/images/logo.png&quot; alt=&quot;WeKnora Logo&quot; height=&quot;120&quot;/&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;a href=&quot;https://trendshift.io/repositories/15289&quot; target=&quot;_blank&quot;&gt;
      &lt;img src=&quot;https://trendshift.io/api/badge/repositories/15289&quot; alt=&quot;Tencent%2FWeKnora | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
    &lt;/a&gt;
  &lt;/picture&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://weknora.weixin.qq.com&quot; target=&quot;_blank&quot;&gt;
        &lt;img alt=&quot;ÂÆòÊñπÁΩëÁ´ô&quot; src=&quot;https://img.shields.io/badge/ÂÆòÊñπÁΩëÁ´ô-WeKnora-4e6b99&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://chatbot.weixin.qq.com&quot; target=&quot;_blank&quot;&gt;
        &lt;img alt=&quot;ÂæÆ‰ø°ÂØπËØùÂºÄÊîæÂπ≥Âè∞&quot; src=&quot;https://img.shields.io/badge/ÂæÆ‰ø°ÂØπËØùÂºÄÊîæÂπ≥Âè∞-5ac725&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/Tencent/WeKnora/blob/main/LICENSE&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/License-MIT-ffffff?labelColor=d4eaf7&amp;color=2e6cc4&quot; alt=&quot;License&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;./CHANGELOG.md&quot;&gt;
        &lt;img alt=&quot;Version&quot; src=&quot;https://img.shields.io/badge/version-0.2.6-2e6cc4?labelColor=d4eaf7&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
| &lt;b&gt;English&lt;/b&gt; | &lt;a href=&quot;./README_CN.md&quot;&gt;&lt;b&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;./README_JA.md&quot;&gt;&lt;b&gt;Êó•Êú¨Ë™û&lt;/b&gt;&lt;/a&gt; |
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;h4 align=&quot;center&quot;&gt;

  [Overview](#-overview) ‚Ä¢ [Architecture](#-architecture) ‚Ä¢ [Key Features](#-key-features) ‚Ä¢ [Getting Started](#-getting-started) ‚Ä¢ [API Reference](#-api-reference) ‚Ä¢ [Developer Guide](#-developer-guide)
  
  &lt;/h4&gt;
&lt;/p&gt;

# üí° WeKnora - LLM-Powered Document Understanding &amp; Retrieval Framework

## üìå Overview

[**WeKnora**](https://weknora.weixin.qq.com) is an LLM-powered framework designed for deep document understanding and semantic retrieval, especially for handling complex, heterogeneous documents. 

It adopts a modular architecture that combines multimodal preprocessing, semantic vector indexing, intelligent retrieval, and large language model inference. At its core, WeKnora follows the **RAG (Retrieval-Augmented Generation)** paradigm, enabling high-quality, context-aware answers by combining relevant document chunks with model reasoning.

**Website:** https://weknora.weixin.qq.com

## ‚ú® Latest Updates

**v0.2.0 Highlights:**

- ü§ñ **Agent Mode**: New ReACT Agent mode that can call built-in tools, MCP tools, and web search, providing comprehensive summary reports through multiple iterations and reflection
- üìö **Multi-Type Knowledge Bases**: Support for FAQ and document knowledge base types, with new features including folder import, URL import, tag management, and online entry
- ‚öôÔ∏è **Conversation Strategy**: Support for configuring Agent models, normal mode models, retrieval thresholds, and Prompts, with precise control over multi-turn conversation behavior
- üåê **Web Search**: Support for extensible web search engines with built-in DuckDuckGo search engine
- üîå **MCP Tool Integration**: Support for extending Agent capabilities through MCP, with built-in uvx and npx launchers, supporting multiple transport methods
- üé® **New UI**: Optimized conversation interface with Agent mode/normal mode switching, tool call process display, and comprehensive knowledge base management interface upgrade
- ‚ö° **Infrastructure Upgrade**: Introduced MQ async task management, support for automatic database migration, and fast development mode

## üîí Security Notice

**Important:** Starting from v0.1.3, WeKnora includes login authentication functionality to enhance system security. For production deployments, we strongly recommend:

- Deploy WeKnora services in internal/private network environments rather than public internet
- Avoid exposing the service directly to public networks to prevent potential information leakage
- Configure proper firewall rules and access controls for your deployment environment
- Regularly update to the latest version for security patches and improvements

## üèóÔ∏è Architecture

![weknora-architecture.png](./docs/images/architecture.png)

WeKnora employs a modern modular design to build a complete document understanding and retrieval pipeline. The system primarily includes document parsing, vector processing, retrieval engine, and large model inference as core modules, with each component being flexibly configurable and extendable.

## üéØ Key Features

- **ü§ñ Agent Mode**: Support for ReACT Agent mode that can use built-in tools to retrieve knowledge bases, MCP tools, and web search tools to access external services, providing comprehensive summary reports through multiple iterations and reflection
- **üîç Precise Understanding**: Structured content extraction from PDFs, Word documents, images and more into unified semantic views
- **üß† Intelligent Reasoning**: Leverages LLMs to understand document context and user intent for accurate Q&amp;A and multi-turn conversations
- **üìö Multi-Type Knowledge Bases**: Support for FAQ and document knowledge base types, with folder import, URL import, tag management, and online entry capabilities
- **üîß Flexible Extension**: All components from parsing and embedding to retrieval and generation are decoupled for easy customization
- **‚ö° Efficient Retrieval**: Hybrid retrieval strategies combining keywords, vectors, and knowledge graphs, with cross-knowledge base retrieval support
- **üåê Web Search**: Support for extensible web search engines with built-in DuckDuckGo search engine
- **üîå MCP Tool Integration**: Support for extending Agent capabilities through MCP, with built-in uvx and npx launchers, supporting multiple transport methods
- **‚öôÔ∏è Conversation Strategy**: Support for configuring Agent models, normal mode models, retrieval thresholds, and Prompts, with precise control over multi-turn conversation behavior
- **üéØ User-Friendly**: Intuitive web interface and standardized APIs for zero technical barriers
- **üîí Secure &amp; Controlled**: Support for local deployment and private cloud, ensuring complete data sovereignty

## üìä Application Scenarios

| Scenario | Applications | Core Value |
|---------|----------|----------|
| **Enterprise Knowledge Management** | Internal document retrieval, policy Q&amp;A, operation manual search | Improve knowledge discovery efficiency, reduce training costs |
| **Academic Research Analysis** | Paper retrieval, research report analysis, scholarly material organization | Accelerate literature review, assist research decisions |
| **Product Technical Support** | Product manual Q&amp;A, technical documentation search, troubleshooting | Enhance customer service quality, reduce support burden |
| **Legal &amp; Compliance Review** | Contract clause retrieval, regulatory policy search, case analysis | Improve compliance efficiency, reduce legal risks |
| **Medical Knowledge Assistance** | Medical literature retrieval, treatment guideline search, case analysis | Support clinical decisions, improve diagnosis quality |

## üß© Feature Matrix

| Module | Support                                                                        | Description                                                                                                                                                        |
|---------|--------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Agent Mode | ‚úÖ ReACT Agent Mode                                                             | Support for using built-in tools to retrieve knowledge bases, MCP tools, and web search, with cross-knowledge base retrieval and multiple iterations               |
| Knowledge Base Types | ‚úÖ FAQ / Document                                                               | Support for creating FAQ and document knowledge base types, with folder import, URL import, tag management, and online entry                                       |
| Document Formats | ‚úÖ PDF / Word / Txt / Markdown / Images (with OCR / Caption)                    | Support for structured and unstructured documents with text extraction from images                                                                                 |
| Model Management | ‚úÖ Centralized configuration, built-in model sharing                            | Centralized model configuration with model selection in knowledge base settings, support for multi-tenant shared built-in models                                   |
| Embedding Models | ‚úÖ Local models, BGE / GTE APIs, etc.                                           | Customizable embedding models, compatible with local deployment and cloud vector generation APIs                                                                   |
| Vector DB Integration | ‚úÖ PostgreSQL (pgvector), Elasticsearch                                         | Support for mainstream vector index backends, flexible switching for different retrieval scenarios                                                                 |
| Retrieval Strategies | ‚úÖ BM25 / Dense Retrieval / GraphRAG                                            | Support for sparse/dense recall and knowledge graph-enhanced retrieval with customizable retrieve-rerank-generate pipelines                                        |
| LLM Integration | ‚úÖ Support for Qwen, DeepSeek, etc., with thinking/non-thinking mode switching  | Compatible with local models (e.g., via Ollama) or external API services with flexible inference configuration                                                     |
| Conversation Strategy | ‚úÖ Agent models, normal mode models, retrieval thresholds, Prompt configuration | Support for configuring Agent models, normal mode models, retrieval thresholds, online Prompt configuration, precise control over multi-turn conversation behavior |
| Web Search | ‚úÖ Extensible search engines, DuckDuckGo / Google                               | Support for extensible web search engines with built-in DuckDuckGo search engine                                                                                   |
| MCP Tools | ‚úÖ uvx, npx launchers, Stdio/HTTP Streamable/SSE                                | Support for extending Agent capabilities through MCP, with built-in uvx and npx launchers, supporting three transport methods                                      |
| QA Capabilities | ‚úÖ Context-aware, multi-turn dialogue, prompt templates                         | Support for complex semantic modeling, instruction control and chain-of-thought Q&amp;A with configurable prompts and context windows                                  |
| E2E Testing | ‚úÖ Retrieval+generation process visualization and metric evaluation             | End-to-end testing tools for evaluating recall hit rates, answer coverage, BLEU/ROUGE and other metrics                                                            |
| Deployment Modes | ‚úÖ Support for local deployment / Docker images                                 | Meets private, offline deployment and flexible operation requirements, with fast development mode support                                                          |
| User Interfaces | ‚úÖ Web UI + RESTful API                                                         | Interactive interface and standard API endpoints, with Agent mode/normal mode switching and tool call process display                                              |
| Task Management | ‚úÖ MQ async tasks, automatic database migration                                 | MQ-based async task state maintenance, support for automatic database schema and data migration during version upgrades                                            |

## üöÄ Getting Started

### üõ† Prerequisites

Make sure the following tools are installed on your system:

* [Docker](https://www.docker.com/)
* [Docker Compose](https://docs.docker.com/compose/)
* [Git](https://git-scm.com/)

### üì¶ Installation

#### ‚ë† Clone the repository

```bash
# Clone the main repository
git clone https://github.com/Tencent/WeKnora.git
cd WeKnora
```

#### ‚ë° Configure environment variables

```bash
# Copy example env file
cp .env.example .env

# Edit .env and set required values
# All variables are documented in the .env.example comments
```

#### ‚ë¢ Start the services (include Ollama)

Check the images that need to be started in the .env file.

```bash
./scripts/start_all.sh
```

or

```bash
make start-all
```

#### ‚ë¢.0 Start ollama services (Optional)

```bash
ollama serve &gt; /dev/null 2&gt;&amp;1 &amp;
```

#### ‚ë¢.1 Activate different combinations of features

- Minimum core services
```bash
docker compose up -d
```

- All features enabled
```bash
docker-compose --profile full up -d
```

- Tracing logs required
```bash
docker-compose --profile jaeger up -d
```

- Neo4j knowledge graph required
```bash
docker-compose --profile neo4j up -d
```

- Minio file storage service required
```bash
docker-compose --profile minio up -d
```

- Multiple options combination
```bash
docker-compose --profile neo4j --profile minio up -d
```

#### ‚ë£ Stop the services

```bash
./scripts/start_all.sh --stop
# Or
make stop-all
```

### üåê Access Services

Once started, services will be available at:

* Web UI: `http://localhost`
* Backend API: `http://localhost:8080`
* Jaeger Tracing: `http://localhost:16686`

### üîå Using WeChat Dialog Open Platform

WeKnora serves as the core technology framework for the [WeChat Dialog Open Platform](https://chatbot.weixin.qq.com), providing a more convenient usage approach:

- **Zero-code Deployment**: Simply upload knowledge to quickly deploy intelligent Q&amp;A services within the WeChat ecosystem, achieving an &quot;ask and answer&quot; experience
- **Efficient Question Management**: Support for categorized management of high-frequency questions, with rich data tools to ensure accurate, reliable, and easily maintainable answers
- **WeChat Ecosystem Integration**: Through the WeChat Dialog Open Platform, WeKnora&#039;s intelligent Q&amp;A capabilities can be seamlessly integrated into WeChat Official Accounts, Mini Programs, and other WeChat scenarios, enhancing user interaction experiences

### üîó Access WeKnora via MCP Server

#### 1Ô∏è‚É£ Clone the repository
```
git clone https://github.com/Tencent/WeKnora
```

#### 2Ô∏è‚É£ Configure MCP Server
&gt; It is recommended to directly refer to the [MCP Configuration Guide](./mcp-server/MCP_CONFIG.md) for configuration.

Configure the MCP client to connect to the server:
```json
{
  &quot;mcpServers&quot;: {
    &quot;weknora&quot;: {
      &quot;args&quot;: [
        &quot;path/to/WeKnora/mcp-server/run_server.py&quot;
      ],
      &quot;command&quot;: &quot;python&quot;,
      &quot;env&quot;:{
        &quot;WEKNORA_API_KEY&quot;:&quot;Enter your WeKnora instance, open developer tools, check the request header x-api-key starting with sk&quot;,
        &quot;WEKNORA_BASE_URL&quot;:&quot;http(s)://your-weknora-address/api/v1&quot;
      }
    }
  }
}
```

Run directly using stdio command:
```
pip install weknora-mcp-server
python -m weknora-mcp-server
```

## üîß Initialization Configuration Guide

To help users quickly configure various models and reduce trial-and-error costs, we&#039;ve improved the original configuration file initialization method by adding a Web UI interface for model configuration. Before using, please ensure the code is updated to the latest version. The specific steps are as follows:
If this is your first time using this project, you can skip steps ‚ë†‚ë° and go directly to steps ‚ë¢‚ë£.

### ‚ë† Stop the services

```bash
./scripts/start_all.sh --stop
```

### ‚ë° Clear existing data tables (recommended when no important data exists)

```bash
make clean-db
```

### ‚ë¢ Compile and start services

```bash
./scripts/start_all.sh
```

### ‚ë£ Access Web UI

http://localhost

On your first visit, you will be automatically redirected to the registration/login page. After completing registration, please create a new knowledge base and finish the relevant settings on its configuration page.

## üì± Interface Showcase

### Web UI Interface

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;b&gt;Knowledge Base Management&lt;/b&gt;&lt;br/&gt;&lt;img src=&quot;./docs/images/knowledgebases.png&quot; alt=&quot;Knowledge Base Management&quot;&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;Conversation Settings&lt;/b&gt;&lt;br/&gt;&lt;img src=&quot;./docs/images/settings.png&quot; alt=&quot;Conversation Settings&quot;&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td colspan=&quot;2&quot;&gt;&lt;b&gt;Agent Mode Tool Call Process&lt;/b&gt;&lt;br/&gt;&lt;img src=&quot;./docs/images/agent-qa.png&quot; alt=&quot;Agent Mode Tool Call Process&quot;&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

**Knowledge Base Management:** Support for creating FAQ and document knowledge base types, with multiple import methods including drag-and-drop, folder import, and URL import. Automatically identifies document structures and extracts core knowledge to establish indexes. Supports tag management and online entry. The system clearly displays processing progress and document status, achieving efficient knowledge base management.

**Agent Mode:** Support for ReACT Agent mode that can use built-in tools to retrieve knowledge bases, call user-configured MCP tools and web search tools to access external services, providing comprehensive summary reports through multiple iterations and reflection. Supports cross-knowledge base retrieval, allowing selection of multiple knowledge bases for simultaneous retrieval.

**Conversation Strategy:** Support for configuring Agent models, normal mode models, retrieval thresholds, and online Prompt configuration, with precise control over multi-turn conversation behavior and retrieval execution methods. The conversation input box supports Agent mode/normal mode switching, enabling/disabling web search, and selecting conversation models.

### Document Knowledge Graph

WeKnora supports transforming documents into knowledge graphs, displaying the relationships between different sections of the documents. Once the knowledge graph feature is enabled, the system analyzes and constructs an internal semantic association network that not only helps users understand document content but also provides structured support for indexing and retrieval, enhancing the relevance and breadth of search results.

For detailed configuration, please refer to the [Knowledge Graph Configuration Guide](./docs/KnowledgeGraph.md).

### MCP Server

Please refer to the [MCP Configuration Guide](./mcp-server/MCP_CONFIG.md) for the necessary setup.

## üìò API Reference

Troubleshooting FAQ: [Troubleshooting FAQ](./docs/QA.md)

Detailed API documentation is available at: [API Docs](./docs/api/README.md)

## üß≠ Developer Guide

### ‚ö° Fast Development Mode (Recommended)

If you need to frequently modify code, **you don&#039;t need to rebuild Docker images every time**! Use fast development mode:

```bash
# Method 1: Using Make commands (Recommended)
make dev-start      # Start infrastructure
make dev-app        # Start backend (new terminal)
make dev-frontend   # Start frontend (new terminal)

# Method 2: One-click start
./scripts/quick-dev.sh

# Method 3: Using scripts
./scripts/dev.sh start     # Start infrastructure
./scripts/dev.sh app       # Start backend (new terminal)
./scripts/dev.sh frontend  # Start frontend (new terminal)
```

**Development Advantages:**
- ‚úÖ Frontend modifications auto hot-reload (no restart needed)
- ‚úÖ Backend modifications quick restart (5-10 seconds, supports Air hot-reload)
- ‚úÖ No need to rebuild Docker images
- ‚úÖ Support IDE breakpoint debugging

**Detailed Documentation:** [Development Environment Quick Start](./docs/ÂºÄÂèëÊåáÂçó.md)

### üìÅ Directory Structure

```
WeKnora/
‚îú‚îÄ‚îÄ client/      # go client
‚îú‚îÄ‚îÄ cmd/         # Main entry point
‚îú‚îÄ‚îÄ config/      # Configuration files
‚îú‚îÄ‚îÄ docker/      # docker images files
‚îú‚îÄ‚îÄ docreader/   # Document parsing app
‚îú‚îÄ‚îÄ docs/        # Project documentation
‚îú‚îÄ‚îÄ frontend/    # Frontend app
‚îú‚îÄ‚îÄ internal/    # Core business logic
‚îú‚îÄ‚îÄ mcp-server/  # MCP server
‚îú‚îÄ‚îÄ migrations/  # DB migration scripts
‚îî‚îÄ‚îÄ scripts/     # Shell scripts
```

## ü§ù Contributing

We welcome community contributions! For suggestions, bugs, or feature requests, please submit an [Issue](https://github.com/Tencent/WeKnora/issues) or directly create a Pull Request.

### üéØ How to Contribute

- üêõ **Bug Fixes

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[microsoft/typescript-go]]></title>
            <link>https://github.com/microsoft/typescript-go</link>
            <guid>https://github.com/microsoft/typescript-go</guid>
            <pubDate>Wed, 14 Jan 2026 00:05:36 GMT</pubDate>
            <description><![CDATA[Staging repo for development of native port of TypeScript]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/typescript-go">microsoft/typescript-go</a></h1>
            <p>Staging repo for development of native port of TypeScript</p>
            <p>Language: Go</p>
            <p>Stars: 23,724</p>
            <p>Forks: 790</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre># TypeScript 7

[Not sure what this is? Read the announcement post!](https://devblogs.microsoft.com/typescript/typescript-native-port/)

## Preview

A preview build is available on npm as [`@typescript/native-preview`](https://www.npmjs.com/package/@typescript/native-preview).

```sh
npm install @typescript/native-preview
npx tsgo # Use this as you would tsc.
```

A preview VS Code extension is [available on the VS Code marketplace](https://marketplace.visualstudio.com/items?itemName=TypeScriptTeam.native-preview).

To use this, set this in your VS Code settings:

```json
{
    &quot;typescript.experimental.useTsgo&quot;: true
}
```

## What Works So Far?

This is still a work in progress and is not yet at full feature parity with TypeScript. Bugs may exist. Please check this list carefully before logging a new issue or assuming an intentional change.

| Feature | Status | Notes |
|---------|--------|-------|
| Program creation | done | Same files and module resolution as TS 5.9. Not all resolution modes supported yet. |
| Parsing/scanning | done | Exact same syntax errors as TS 5.9 |
| Commandline and `tsconfig.json` parsing | done | Done, though `tsconfig` errors may not be as helpful. |
| Type resolution | done | Same types as TS 5.9. |
| Type checking | done | Same errors, locations, and messages as TS 5.9. Types printback in errors may display differently. |
| JavaScript-specific inference and JSDoc | in progress | Mostly complete, but intentionally lacking some features. Declaration emit not complete. |
| JSX | done | - |
| Declaration emit | in progress | Most common features are in place, but some edge cases and feature flags are still unhandled. |
| Emit (JS output) | in progress | `target: esnext` well-supported, other targets may have gaps. |
| Watch mode | prototype | Watches files and rebuilds, but no incremental rechecking. Not optimized. |
| Build mode / project references | done | - |
| Incremental build | done | - |
| Language service (LSP) | in progress | Most functionality. More features coming soon. |
| API | not ready | - |

Definitions:

 * **done** aka &quot;believed done&quot;: We&#039;re not currently aware of any deficits or major left work to do. OK to log bugs
 * **in progress**: currently being worked on; some features may work and some might not. OK to log panics, but nothing else please
 * **prototype**: proof-of-concept only; do not log bugs
 * **not ready**: either haven&#039;t even started yet, or far enough from ready that you shouldn&#039;t bother messing with it yet

## Other Notes

Long-term, we expect that this repo and its contents will be merged into `microsoft/TypeScript`.
As a result, the repo and issue tracker for typescript-go will eventually be closed, so treat discussions/issues accordingly.

For a list of intentional changes with respect to TypeScript 5.9, see CHANGES.md.

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit [Contributor License Agreements](https://cla.opensource.microsoft.com).

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
[Microsoft&#039;s Trademark &amp; Brand Guidelines](https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party&#039;s policies.
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[docker/compose]]></title>
            <link>https://github.com/docker/compose</link>
            <guid>https://github.com/docker/compose</guid>
            <pubDate>Wed, 14 Jan 2026 00:05:35 GMT</pubDate>
            <description><![CDATA[Define and run multi-container applications with Docker]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/docker/compose">docker/compose</a></h1>
            <p>Define and run multi-container applications with Docker</p>
            <p>Language: Go</p>
            <p>Stars: 36,795</p>
            <p>Forks: 5,691</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre># Table of Contents
- [Docker Compose](#docker-compose)
- [Where to get Docker Compose](#where-to-get-docker-compose)
    + [Windows and macOS](#windows-and-macos)
    + [Linux](#linux)
- [Quick Start](#quick-start)
- [Contributing](#contributing)
- [Legacy](#legacy)

# Docker Compose

[![GitHub release](https://img.shields.io/github/v/release/docker/compose.svg?style=flat-square)](https://github.com/docker/compose/releases/latest)
[![PkgGoDev](https://img.shields.io/badge/go.dev-docs-007d9c?style=flat-square&amp;logo=go&amp;logoColor=white)](https://pkg.go.dev/github.com/docker/compose/v5)
[![Build Status](https://img.shields.io/github/actions/workflow/status/docker/compose/ci.yml?label=ci&amp;logo=github&amp;style=flat-square)](https://github.com/docker/compose/actions?query=workflow%3Aci)
[![Go Report Card](https://goreportcard.com/badge/github.com/docker/compose/v5?style=flat-square)](https://goreportcard.com/report/github.com/docker/compose/v5)
[![Codecov](https://codecov.io/gh/docker/compose/branch/main/graph/badge.svg?token=HP3K4Y4ctu)](https://codecov.io/gh/docker/compose)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/docker/compose/badge)](https://api.securityscorecards.dev/projects/github.com/docker/compose)
![Docker Compose](logo.png?raw=true &quot;Docker Compose Logo&quot;)

Docker Compose is a tool for running multi-container applications on Docker
defined using the [Compose file format](https://compose-spec.io).
A Compose file is used to define how one or more containers that make up
your application are configured.
Once you have a Compose file, you can create and start your application with a
single command: `docker compose up`.

&gt; **Note**: About Docker Swarm
&gt; Docker Swarm used to rely on the legacy compose file format but did not adopt the compose specification
&gt; so is missing some of the recent enhancements in the compose syntax. After 
&gt; [acquisition by Mirantis](https://www.mirantis.com/software/swarm/) swarm isn&#039;t maintained by Docker Inc, and
&gt; as such some Docker Compose features aren&#039;t accessible to swarm users.

# Where to get Docker Compose

### Windows and macOS

Docker Compose is included in
[Docker Desktop](https://www.docker.com/products/docker-desktop/)
for Windows and macOS.

### Linux

You can download Docker Compose binaries from the
[release page](https://github.com/docker/compose/releases) on this repository.

Rename the relevant binary for your OS to `docker-compose` and copy it to `$HOME/.docker/cli-plugins`

Or copy it into one of these folders to install it system-wide:

* `/usr/local/lib/docker/cli-plugins` OR `/usr/local/libexec/docker/cli-plugins`
* `/usr/lib/docker/cli-plugins` OR `/usr/libexec/docker/cli-plugins`

(might require making the downloaded file executable with `chmod +x`)


Quick Start
-----------

Using Docker Compose is a three-step process:
1. Define your app&#039;s environment with a `Dockerfile` so it can be
   reproduced anywhere.
2. Define the services that make up your app in `compose.yaml` so
   they can be run together in an isolated environment.
3. Lastly, run `docker compose up` and Compose will start and run your entire
   app.

A Compose file looks like this:

```yaml
services:
  web:
    build: .
    ports:
      - &quot;5000:5000&quot;
    volumes:
      - .:/code
  redis:
    image: redis
```

Contributing
------------

Want to help develop Docker Compose? Check out our
[contributing documentation](CONTRIBUTING.md).

If you find an issue, please report it on the
[issue tracker](https://github.com/docker/compose/issues/new/choose).

Legacy
-------------

The Python version of Compose is available under the `v1` [branch](https://github.com/docker/compose/tree/v1).
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[projectdiscovery/nuclei]]></title>
            <link>https://github.com/projectdiscovery/nuclei</link>
            <guid>https://github.com/projectdiscovery/nuclei</guid>
            <pubDate>Wed, 14 Jan 2026 00:05:34 GMT</pubDate>
            <description><![CDATA[Nuclei is a fast, customizable vulnerability scanner powered by the global security community and built on a simple YAML-based DSL, enabling collaboration to tackle trending vulnerabilities on the internet. It helps you find vulnerabilities in your applications, APIs, networks, DNS, and cloud configurations.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/projectdiscovery/nuclei">projectdiscovery/nuclei</a></h1>
            <p>Nuclei is a fast, customizable vulnerability scanner powered by the global security community and built on a simple YAML-based DSL, enabling collaboration to tackle trending vulnerabilities on the internet. It helps you find vulnerabilities in your applications, APIs, networks, DNS, and cloud configurations.</p>
            <p>Language: Go</p>
            <p>Stars: 26,508</p>
            <p>Forks: 3,047</p>
            <p>Stars today: 19 stars today</p>
            <h2>README</h2><pre>![nuclei](/static/nuclei-cover-image.png)

&lt;div align=&quot;center&quot;&gt;
  
  &lt;a href=&quot;https://github.com/projectdiscovery/nuclei/blob/main/README.md&quot;&gt;`English`&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://github.com/projectdiscovery/nuclei/blob/main/README_CN.md&quot;&gt;`‰∏≠Êñá`&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://github.com/projectdiscovery/nuclei/blob/main/README_KR.md&quot;&gt;`Korean`&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://github.com/projectdiscovery/nuclei/blob/main/README_ID.md&quot;&gt;`Indonesia`&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://github.com/projectdiscovery/nuclei/blob/main/README_ES.md&quot;&gt;`Spanish`&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://github.com/projectdiscovery/nuclei/blob/main/README_JP.md&quot;&gt;`Êó•Êú¨Ë™û`&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://github.com/projectdiscovery/nuclei/blob/main/README_PT-BR.md&quot;&gt;`Portuguese`&lt;/a&gt;
  
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;

&lt;a href=&quot;https://docs.projectdiscovery.io/tools/nuclei/overview?utm_source=github&amp;utm_medium=web&amp;utm_campaign=nuclei_readme&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Documentation-%23000000.svg?style=for-the-badge&amp;logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiNmZmZmZmYiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIiBjbGFzcz0ibHVjaWRlIGx1Y2lkZS1ib29rLW9wZW4iPjxwYXRoIGQ9Ik0xMiA3djE0Ii8+PHBhdGggZD0iTTMgMThhMSAxIDAgMCAxLTEtMVY0YTEgMSAwIDAgMSAxLTFoNWE0IDQgMCAwIDEgNCA0IDQgNCAwIDAgMSA0LTRoNWExIDEgMCAwIDEgMSAxdjEzYTEgMSAwIDAgMS0xIDFoLTZhMyAzIDAgMCAwLTMgMyAzIDMgMCAwIDAtMy0zeiIvPjwvc3ZnPg==&amp;logoColor=white&quot;&gt;&lt;/a&gt;
&amp;nbsp;&amp;nbsp;
&lt;a href=&quot;https://github.com/projectdiscovery/nuclei-templates&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Templates Library-%23000000.svg?style=for-the-badge&amp;logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgaGVpZ2h0PSIxNiIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiNmZmZmZmYiIHN0cm9rZS13aWR0aD0iMS41IiBzdHJva2UtbGluZWNhcD0icm91bmQiIHN0cm9rZS1saW5lam9pbj0icm91bmQiIGNsYXNzPSJsdWNpZGUgbHVjaWRlLXNoaWVsZCI+PHBhdGggZD0iTTIwIDEzYzAgNS0zLjUgNy41LTcuNjYgOC45NWExIDEgMCAwIDEtLjY3LS4wMUM3LjUgMjAuNSA0IDE4IDQgMTNWNmExIDEgMCAwIDEgMS0xYzIgMCA0LjUtMS4yIDYuMjQtMi43MmExLjE3IDEuMTcgMCAwIDEgMS41MiAwQzE0LjUxIDMuODEgMTcgNSAxOSA1YTEgMSAwIDAgMSAxIDF6Ii8+PC9zdmc+&amp;logoColor=white&quot;&gt;&lt;/a&gt;
&amp;nbsp;&amp;nbsp;
&lt;a href=&quot;https://discord.gg/projectdiscovery?utm_source=github&amp;utm_medium=web&amp;utm_campaign=nuclei_readme&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-%235865F2.svg?style=for-the-badge&amp;logo=discord&amp;logoColor=white&quot;&gt;&lt;/a&gt;

&lt;hr&gt;

&lt;/p&gt;

&lt;br&gt;

**Nuclei is a modern, high-performance vulnerability scanner that leverages simple YAML-based templates. It empowers you to design custom vulnerability detection scenarios that mimic real-world conditions, leading to zero false positives.**

- Simple YAML format for creating and customizing vulnerability templates.
- Contributed by thousands of security professionals to tackle trending vulnerabilities.
- Reduce false positives by simulating real-world steps to verify a vulnerability.
- Ultra-fast parallel scan processing and request clustering.
- Integrate into CI/CD pipelines for vulnerability detection and regression testing.
- Supports multiple protocols like TCP, DNS, HTTP, SSL, WHOIS, JavaScript, Code and more.
- Integrate with Jira, Splunk, GitHub, Elastic, GitLab.

&lt;br&gt;
&lt;br&gt;

## Table of Contents

- [**`Get Started`**](#get-started)
  - [_`1. Nuclei CLI`_](#1-nuclei-cli)
  - [_`2. Pro and Enterprise Editions`_](#2-pro-and-enterprise-editions)
- [**`Documentation`**](#documentation)
  - [_`Command Line Flags`_](#command-line-flags)
  - [_`Single target scan`_](#single-target-scan)
  - [_`Scanning multiple targets`_](#scanning-multiple-targets)
  - [_`Network scan`_](#network-scan)
  - [_`Scanning with your custom template`_](#scanning-with-your-custom-template)
  - [_`Connect Nuclei to ProjectDiscovery_`_](#connect-nuclei-to-projectdiscovery)
- [**`Nuclei Templates, Community and Rewards`**](#nuclei-templates-community-and-rewards-) üíé
- [**`Our Mission`**](#our-mission)
- [**`Contributors`**](#contributors-heart) ‚ù§
- [**`License`**](#license)

&lt;br&gt;
&lt;br&gt;

## Get Started

### **1. Nuclei CLI**

_Install Nuclei on your machine. Get started by following the installation guide [**`here`**](https://docs.projectdiscovery.io/tools/nuclei/install?utm_source=github&amp;utm_medium=web&amp;utm_campaign=nuclei_readme). Additionally, We provide [**`a free cloud tier`**](https://cloud.projectdiscovery.io/sign-up) and comes with a generous monthly free limits:_

- Store and visualize your vulnerability findings
- Write and manage your nuclei templates
- Access latest nuclei templates
- Discover and store your targets

&gt; [!Important]
&gt; |**This project is in active development**. Expect breaking changes with releases. Review the release changelog before updating.|
&gt; |:--------------------------------|
&gt; | This project is primarily built to be used as a standalone CLI tool. **Running nuclei as a service may pose security risks.** It&#039;s recommended to use with caution and additional security measures. |

&lt;br&gt;

### **2. Pro and Enterprise Editions**

_For security teams and enterprises, we provide a cloud-hosted service built on top of Nuclei OSS, fine-tuned to help you continuously run vulnerability scans at scale with your team and existing workflows:_

- 50x faster scans
- Large scale scanning with high accuracy
- Integrations with cloud services (AWS, GCP, Azure, Cloudflare, Fastly, Terraform, Kubernetes)
- Jira, Slack, Linear, APIs and Webhooks
- Executive and compliance reporting
- Plus: Real-time scanning, SAML SSO, SOC 2 compliant platform (with EU and US hosting options), shared team workspaces, and more
- We&#039;re constantly [**`adding new features`**](https://feedback.projectdiscovery.io/changelog)!
- **Ideal for:** Pentesters, security teams, and enterprises

[**`Sign up to Pro`**](https://projectdiscovery.io/pricing?utm_source=github&amp;utm_medium=web&amp;utm_campaign=nuclei_readme) or [**`Talk to our team`**](https://projectdiscovery.io/request-demo?utm_source=github&amp;utm_medium=web&amp;utm_campaign=nuclei_readme) if you have large organization and complex requirements.

&lt;br&gt;
&lt;br&gt;

## Documentation

Browse the full Nuclei [**`documentation here`**](https://docs.projectdiscovery.io/tools/nuclei/running). If you‚Äôre new to Nuclei, check out our [**`foundational YouTube series`**](https://www.youtube.com/playlist?list=PLZRbR9aMzTTpItEdeNSulo8bYsvil80Rl).

&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://www.youtube.com/watch?v=b5qMyQvL1ZA&amp;list=PLZRbR9aMzTTpItEdeNSulo8bYsvil80Rl&amp;utm_source=github&amp;utm_medium=web&amp;utm_campaign=nuclei_readme&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;/static/nuclei-getting-started.png&quot; width=&quot;350px&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=nFXygQdtjyw&amp;utm_source=github&amp;utm_medium=web&amp;utm_campaign=nuclei_readme&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;/static/nuclei-write-your-first-template.png&quot; width=&quot;350px&quot;&gt;&lt;/a&gt;

&lt;/div&gt;

&lt;br&gt;

### Installation

`nuclei` requires **go &gt;= 1.24.2** to install successfully. Run the following command to get the repo:

```sh
go install -v github.com/projectdiscovery/nuclei/v3/cmd/nuclei@latest
```

To learn more about installing nuclei, see `https://docs.projectdiscovery.io/tools/nuclei/install`.

### Command Line Flags

To display all the flags for the tool:

```sh
nuclei -h
```

&lt;details&gt;
  &lt;summary&gt;Expand full help flags&lt;/summary&gt;

```yaml
Nuclei is a fast, template based vulnerability scanner focusing
on extensive configurability, massive extensibility and ease of use.

Usage:
  ./nuclei [flags]

Flags:
TARGET:
   -u, -target string[]          target URLs/hosts to scan
   -l, -list string              path to file containing a list of target URLs/hosts to scan (one per line)
   -eh, -exclude-hosts string[]  hosts to exclude to scan from the input list (ip, cidr, hostname)
   -resume string                resume scan from and save to specified file (clustering will be disabled)
   -sa, -scan-all-ips            scan all the IP&#039;s associated with dns record
   -iv, -ip-version string[]     IP version to scan of hostname (4,6) - (default 4)

TARGET-FORMAT:
   -im, -input-mode string        mode of input file (list, burp, jsonl, yaml, openapi, swagger) (default &quot;list&quot;)
   -ro, -required-only            use only required fields in input format when generating requests
   -sfv, -skip-format-validation  skip format validation (like missing vars) when parsing input file

TEMPLATES:
   -nt, -new-templates                    run only new templates added in latest nuclei-templates release
   -ntv, -new-templates-version string[]  run new templates added in specific version
   -as, -automatic-scan                   automatic web scan using wappalyzer technology detection to tags mapping
   -t, -templates string[]                list of template or template directory to run (comma-separated, file)
   -turl, -template-url string[]          template url or list containing template urls to run (comma-separated, file)
   -ai, -prompt string                    generate and run template using ai prompt
   -w, -workflows string[]                list of workflow or workflow directory to run (comma-separated, file)
   -wurl, -workflow-url string[]          workflow url or list containing workflow urls to run (comma-separated, file)
   -validate                              validate the passed templates to nuclei
   -nss, -no-strict-syntax                disable strict syntax check on templates
   -td, -template-display                 displays the templates content
   -tl                                    list all templates matching current filters
   -tgl                                   list all available tags
   -sign                                  signs the templates with the private key defined in NUCLEI_SIGNATURE_PRIVATE_KEY env variable
   -code                                  enable loading code protocol-based templates
   -dut, -disable-unsigned-templates      disable running unsigned templates or templates with mismatched signature
   -esc, -enable-self-contained           enable loading self-contained templates
   -egm, -enable-global-matchers          enable loading global matchers templates
   -file                                  enable loading file templates

FILTERING:
   -a, -author string[]               templates to run based on authors (comma-separated, file)
   -tags string[]                     templates to run based on tags (comma-separated, file)
   -etags, -exclude-tags string[]     templates to exclude based on tags (comma-separated, file)
   -itags, -include-tags string[]     tags to be executed even if they are excluded either by default or configuration
   -id, -template-id string[]         templates to run based on template ids (comma-separated, file, allow-wildcard)
   -eid, -exclude-id string[]         templates to exclude based on template ids (comma-separated, file)
   -it, -include-templates string[]   path to template file or directory to be executed even if they are excluded either by default or configuration
   -et, -exclude-templates string[]   path to template file or directory to exclude (comma-separated, file)
   -em, -exclude-matchers string[]    template matchers to exclude in result
   -s, -severity value[]              templates to run based on severity. Possible values: info, low, medium, high, critical, unknown
   -es, -exclude-severity value[]     templates to exclude based on severity. Possible values: info, low, medium, high, critical, unknown
   -pt, -type value[]                 templates to run based on protocol type. Possible values: dns, file, http, headless, tcp, workflow, ssl, websocket, whois, code, javascript
   -ept, -exclude-type value[]        templates to exclude based on protocol type. Possible values: dns, file, http, headless, tcp, workflow, ssl, websocket, whois, code, javascript
   -tc, -template-condition string[]  templates to run based on expression condition

OUTPUT:
   -o, -output string            output file to write found issues/vulnerabilities
   -sresp, -store-resp           store all request/response passed through nuclei to output directory
   -srd, -store-resp-dir string  store all request/response passed through nuclei to custom directory (default &quot;output&quot;)
   -silent                       display findings only
   -nc, -no-color                disable output content coloring (ANSI escape codes)
   -j, -jsonl                    write output in JSONL(ines) format
   -irr, -include-rr -omit-raw   include request/response pairs in the JSON, JSONL, and Markdown outputs (for findings only) [DEPRECATED use -omit-raw] (default true)
   -or, -omit-raw                omit request/response pairs in the JSON, JSONL, and Markdown outputs (for findings only)
   -ot, -omit-template           omit encoded template in the JSON, JSONL output
   -nm, -no-meta                 disable printing result metadata in cli output
   -ts, -timestamp               enables printing timestamp in cli output
   -rdb, -report-db string       nuclei reporting database (always use this to persist report data)
   -ms, -matcher-status          display match failure status
   -me, -markdown-export string  directory to export results in markdown format
   -se, -sarif-export string     file to export results in SARIF format
   -je, -json-export string      file to export results in JSON format
   -jle, -jsonl-export string    file to export results in JSONL(ine) format
   -rd, -redact string[]         redact given list of keys from query parameter, request header and body

CONFIGURATIONS:
   -config string                        path to the nuclei configuration file
   -tp, -profile string                  template profile config file to run
   -tpl, -profile-list                   list community template profiles
   -fr, -follow-redirects                enable following redirects for http templates
   -fhr, -follow-host-redirects          follow redirects on the same host
   -mr, -max-redirects int               max number of redirects to follow for http templates (default 10)
   -dr, -disable-redirects               disable redirects for http templates
   -rc, -report-config string            nuclei reporting module configuration file
   -H, -header string[]                  custom header/cookie to include in all http request in header:value format (cli, file)
   -V, -var value                        custom vars in key=value format
   -r, -resolvers string                 file containing resolver list for nuclei
   -sr, -system-resolvers                use system DNS resolving as error fallback
   -dc, -disable-clustering              disable clustering of requests
   -passive                              enable passive HTTP response processing mode
   -fh2, -force-http2                    force http2 connection on requests
   -ev, -env-vars                        enable environment variables to be used in template
   -cc, -client-cert string              client certificate file (PEM-encoded) used for authenticating against scanned hosts
   -ck, -client-key string               client key file (PEM-encoded) used for authenticating against scanned hosts
   -ca, -client-ca string                client certificate authority file (PEM-encoded) used for authenticating against scanned hosts
   -sml, -show-match-line                show match lines for file templates, works with extractors only
   -ztls                                 use ztls library with autofallback to standard one for tls13 [Deprecated] autofallback to ztls is enabled by default
   -sni string                           tls sni hostname to use (default: input domain name)
   -dka, -dialer-keep-alive value        keep-alive duration for network requests.
   -lfa, -allow-local-file-access        allows file (payload) access anywhere on the system
   -lna, -restrict-local-network-access  blocks connections to the local / private network
   -i, -interface string                 network interface to use for network scan
   -at, -attack-type string              type of payload combinations to perform (batteringram,pitchfork,clusterbomb)
   -sip, -source-ip string               source ip address to use for network scan
   -rsr, -response-size-read int         max response size to read in bytes
   -rss, -response-size-save int         max response size to read in bytes (default 1048576)
   -reset                                reset removes all nuclei configuration and data files (including nuclei-templates)
   -tlsi, -tls-impersonate               enable experimental client hello (ja3) tls randomization
   -hae, -http-api-endpoint string       experimental http api endpoint

INTERACTSH:
   -iserver, -interactsh-server string  interactsh server url for self-hosted instance (default: oast.pro,oast.live,oast.site,oast.online,oast.fun,oast.me)
   -itoken, -interactsh-token string    authentication token for self-hosted interactsh server
   -interactions-cache-size int         number of requests to keep in the interactions cache (default 5000)
   -interactions-eviction int           number of seconds to wait before evicting requests from cache (default 60)
   -interactions-poll-duration int      number of seconds to wait before each interaction poll request (default 5)
   -interactions-cooldown-period int    extra time for interaction polling before exiting (default 5)
   -ni, -no-interactsh                  disable interactsh server for OAST testing, exclude OAST based templates

FUZZING:
   -ft, -fuzzing-type string           overrides fuzzing type set in template (replace, prefix, postfix, infix)
   -fm, -fuzzing-mode string           overrides fuzzing mode set in template (multiple, single)
   -fuzz                               enable loading fuzzing templates (Deprecated: use -dast instead)
   -dast                               enable / run dast (fuzz) nuclei templates
   -dts, -dast-server                  enable dast server mode (live fuzzing)
   -dtr, -dast-report                  write dast scan report to file
   -dtst, -dast-server-token string    dast server token (optional)
   -dtsa, -dast-server-address string  dast server address (default &quot;localhost:9055&quot;)
   -dfp, -display-fuzz-points          display fuzz points in the output for debugging
   -fuzz-param-frequency int           frequency of uninteresting parameters for fuzzing before skipping (default 10)
   -fa, -fuzz-aggression string        fuzzing aggression level controls payload count for fuzz (low, medium, high) (default &quot;low&quot;)
   -cs, -fuzz-scope string[]           in scope url regex to be followed by fuzzer
   -cos, -fuzz-out-scope string[]      out of scope url regex to be excluded by fuzzer

UNCOVER:
   -uc, -uncover                  enable uncover engine
   -uq, -uncover-query string[]   uncover search query
   -ue, -uncover-engine string[]  uncover search engine (shodan,censys,fofa,shodan-idb,quake,hunter,zoomeye,netlas,criminalip,publicwww,hunterhow,google) (default shodan)
   -uf, -uncover-field string     uncover fields to return (ip,port,host) (default &quot;ip:port&quot;)
   -ul, -uncover-limit int        uncover results to return (default 100)
   -ur, -uncover-ratelimit int    override ratelimit of engines with unknown ratelimit (default 60 req/min) (default 60)

RATE-LIMIT:
   -rl, -rate-limit int               maximum number of requests to send per second (default 150)
   -rld, -rate-limit-duration value   maximum number of requests to send per second (default 1s)
   -rlm, -rate-limit-minute int       maximum number of requests to send per minute (DEPRECATED)
   -bs, -bulk-size int                maximum number of hosts to be analyzed in parallel per template (default 25)
   -c, -concurrency int               maximum number of templates to be executed in parallel (default 25)
   -hbs, -headless-bulk-size int      maximum number of headless hosts to be analyzed in parallel per template (default 10)
   -headc, -headless-concurrency int  maximum number of headless templates to be executed in parallel 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[derailed/k9s]]></title>
            <link>https://github.com/derailed/k9s</link>
            <guid>https://github.com/derailed/k9s</guid>
            <pubDate>Wed, 14 Jan 2026 00:05:33 GMT</pubDate>
            <description><![CDATA[üê∂ Kubernetes CLI To Manage Your Clusters In Style!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/derailed/k9s">derailed/k9s</a></h1>
            <p>üê∂ Kubernetes CLI To Manage Your Clusters In Style!</p>
            <p>Language: Go</p>
            <p>Stars: 32,447</p>
            <p>Forks: 2,048</p>
            <p>Stars today: 15 stars today</p>
            <h2>README</h2><pre>&lt;img src=&quot;assets/k9s.png&quot; alt=&quot;k9s&quot;&gt;

## K9s - Kubernetes CLI To Manage Your Clusters In Style!

K9s provides a terminal UI to interact with your Kubernetes clusters.
The aim of this project is to make it easier to navigate, observe and manage
your applications in the wild. K9s continually watches Kubernetes
for changes and offers subsequent commands to interact with your observed resources.

---

## Note...

K9s is not pimped out by a big corporation with deep pockets.
It is a complex OSS project that demands a lot of my time to maintain and support.
K9s will always remain OSS and therefore free! That said, if you feel k9s makes your day to day Kubernetes journey a tad brighter, saves you time and makes you more productive, please consider [sponsoring us!](https://github.com/sponsors/derailed)
Your donations will go a long way in keeping our servers lights on and beers in our fridge!

**Thank you!**

---

[![Go Report Card](https://goreportcard.com/badge/github.com/derailed/k9s?)](https://goreportcard.com/report/github.com/derailed/k9s)
[![golangci badge](https://github.com/golangci/golangci-web/blob/master/src/assets/images/badge_a_plus_flat.svg)](https://golangci.com/r/github.com/derailed/k9s)
[![Docker Pulls](https://img.shields.io/docker/pulls/derailed/k9s.svg?maxAge=604800)](https://hub.docker.com/r/derailed/k9s/)
[![release](https://img.shields.io/github/release-pre/derailed/k9s.svg)](https://github.com/derailed/k9s/releases)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/mum4k/termdash/blob/master/LICENSE)
[![Releases](https://img.shields.io/github/downloads/derailed/k9s/total.svg)](https://github.com/derailed/k9s/releases)

---

## Screenshots

1. Pods
      &lt;img src=&quot;assets/screen_po.png&quot;/&gt;
2. Logs
      &lt;img src=&quot;assets/screen_logs.png&quot;/&gt;
3. Deployments
      &lt;img src=&quot;assets/screen_dp.png&quot;/&gt;

---

## Demo Videos/Recordings

* [K9s v0.40.0 -Column Blow- Sneak peek](https://youtu.be/iy6RDozAM4A)
* [K9s v0.31.0 Configs+Sneak peek](https://youtu.be/X3444KfjguE)
* [K9s v0.30.0 Sneak peek](https://youtu.be/mVBc1XneRJ4)
* [Vulnerability Scans](https://youtu.be/ULkl0MsaidU)
* [K9s v0.29.0](https://youtu.be/oiU3wmoAkBo)
* [K9s v0.21.3](https://youtu.be/wG8KCwDAhnw)
* [K9s v0.19.X](https://youtu.be/kj-WverKZ24)
* [K9s v0.18.0](https://www.youtube.com/watch?v=zMnD5e53yRw)
* [K9s v0.17.0](https://www.youtube.com/watch?v=7S33CNLAofk&amp;feature=youtu.be)
* [K9s Pulses](https://asciinema.org/a/UbXKPal6IWpTaVAjBBFmizcGN)
* [K9s v0.15.1](https://youtu.be/7Fx4XQ2ftpM)
* [K9s v0.13.0](https://www.youtube.com/watch?v=qaeR2iK7U0o&amp;t=15s)
* [K9s v0.9.0](https://www.youtube.com/watch?v=bxKfqumjW4I)
* [K9s v0.7.0 Features](https://youtu.be/83jYehwlql8)
* [K9s v0 Demo](https://youtu.be/k7zseUhaXeU)

---

## Documentation

Please refer to our [K9s documentation](https://k9scli.io) site for installation, usage, customization and tips.

---

## Slack Channel

Wanna discuss K9s features with your fellow `K9sers` or simply show your support for this tool?

* Channel: [K9sersSlack](https://k9sers.slack.com/)
* Invite: [K9slackers Invite](https://join.slack.com/t/k9sers/shared_invite/zt-3360a389v-ElLHrb0Dp1kAXqYUItSAFA)

---

## Installation

K9s is available on Linux, macOS and Windows platforms.
Binaries for Linux, Windows and Mac are available as tarballs in the [release page](https://github.com/derailed/k9s/releases).

* Via [Homebrew](https://brew.sh/) for macOS or Linux

   ```shell
   brew install derailed/k9s/k9s
   ```

* Via [MacPorts](https://www.macports.org)

   ```shell
   sudo port install k9s
   ```

* Via [snap](https://snapcraft.io/k9s) for Linux

  ```shell
  snap install k9s --devmode
  ```

* On Arch Linux

  ```shell
  pacman -S k9s
  ```

* On OpenSUSE Linux distribution

  ```shell
  zypper install k9s
  ```

* On FreeBSD

  ```shell
  pkg install k9s
  ```

* On Ubuntu

  ```shell
  wget https://github.com/derailed/k9s/releases/latest/download/k9s_linux_amd64.deb &amp;&amp; apt install ./k9s_linux_amd64.deb &amp;&amp; rm k9s_linux_amd64.deb
  ```

* On Fedora (42+)

  ```shell
  dnf install k9s
  ```

* Via [Winget](https://github.com/microsoft/winget-cli) for Windows

  ```shell
  winget install k9s
  ```

* Via [Scoop](https://scoop.sh) for Windows

  ```shell
  scoop install k9s
  ```

* Via [Chocolatey](https://chocolatey.org/packages/k9s) for Windows

  ```shell
  choco install k9s
  ```

* Via a GO install

  ```shell
  # NOTE: The dev version will be in effect!
  go install github.com/derailed/k9s@latest
  ```

* Via [Webi](https://webinstall.dev) for Linux and macOS

  ```shell
  curl -sS https://webinstall.dev/k9s | bash
  ```

* Via [pkgx](https://pkgx.dev/pkgs/k9scli.io/) for Linux and macOS

  ```shell
  pkgx k9s
  ```

* Via [gah](https://github.com/marverix/gah) for Linux and macOS

  ```shell
  gah install k9s
  ```

* Via [Webi](https://webinstall.dev) for Windows

  ```shell
  curl.exe -A MS https://webinstall.dev/k9s | powershell
  ```

* As a [Docker Desktop Extension](https://docs.docker.com/desktop/extensions/) (for the Docker Desktop built in Kubernetes Server)

  ```shell
  docker extension install spurin/k9s-dd-extension:latest
  ```

---

## Building From Source

 K9s is currently using GO v1.23.X or above.
 In order to build K9s from source you must:

 1. Clone the repo
 2. Build and run the executable

      ```shell
      make build &amp;&amp; ./execs/k9s
      ```

---

## Running with Docker

### Running the official Docker image

  You can run k9s as a Docker container by mounting your `KUBECONFIG`:

  ```shell
  docker run --rm -it -v $KUBECONFIG:/root/.kube/config derailed/k9s
  ```

  For default path it would be:

  ```shell
  docker run --rm -it -v ~/.kube/config:/root/.kube/config derailed/k9s
  ```

### Building your own Docker image

  You can build your own Docker image of k9s from the [Dockerfile](Dockerfile) with the following:

  ```shell
  docker build -t k9s-docker:v0.0.1 .
  ```

  You can get the latest stable `kubectl` version and pass it to the `docker build` command with the `--build-arg` option.
  You can use the `--build-arg` option to pass any valid `kubectl` version (like `v1.18.0` or `v1.19.1`).

  ```shell
  KUBECTL_VERSION=$(make kubectl-stable-version 2&gt;/dev/null)
  docker build --build-arg KUBECTL_VERSION=${KUBECTL_VERSION} -t k9s-docker:0.1 .
  ```

  Run your container:

  ```shell
  docker run --rm -it -v ~/.kube/config:/root/.kube/config k9s-docker:0.1
  ```

---

## PreFlight Checks

* K9s uses 256 colors terminal mode. On `Nix system make sure TERM is set accordingly.

    ```shell
    export TERM=xterm-256color
    ```

* In order to issue resource edit commands make sure your EDITOR and KUBE_EDITOR env vars are set.

    ```shell
    # Kubectl edit command will use this env var.
    export KUBE_EDITOR=my_fav_editor
    ```

* K9s prefers recent kubernetes versions ie 1.28+

---

## K8S Compatibility Matrix

|         k9s        | k8s client |
| ------------------ | ---------- |
|     &gt;= v0.27.0     |   1.26.1   |
| v0.26.7 - v0.26.6  |   1.25.3   |
| v0.26.5 - v0.26.4  |   1.25.1   |
| v0.26.3 - v0.26.1  |   1.24.3   |
| v0.26.0 - v0.25.19 |   1.24.2   |
| v0.25.18 - v0.25.3 |   1.22.3   |
| v0.25.2 - v0.25.0  |   1.22.0   |
|      &lt;= v0.24      |   1.21.3   |

---

## The Command Line

```shell
# List current version
k9s version

# To get info about K9s runtime (logs, configs, etc..)
k9s info

# List all available CLI options
k9s help

# To run K9s in a given namespace
k9s -n mycoolns

# Start K9s in an existing KubeConfig context
k9s --context coolCtx

# Start K9s in readonly mode - with all cluster modification commands disabled
k9s --readonly
```

## Logs And Debug Logs

Given the nature of the ui k9s does produce logs to a specific location.
To view the logs and turn on debug mode, use the following commands:

```shell
# Find out where the logs are stored
k9s info
```

```text
 ____  __.________
|    |/ _/   __   \______
|      &lt; \____    /  ___/
|    |  \   /    /\___ \
|____|__ \ /____//____  &gt;
        \/            \/

Version:           vX.Y.Z
Config:            /Users/fernand/.config/k9s/config.yaml
Logs:              /Users/fernand/.local/state/k9s/k9s.log
Dumps dir:         /Users/fernand/.local/state/k9s/screen-dumps
Benchmarks dir:    /Users/fernand/.local/state/k9s/benchmarks
Skins dir:         /Users/fernand/.local/share/k9s/skins
Contexts dir:      /Users/fernand/.local/share/k9s/clusters
Custom views file: /Users/fernand/.local/share/k9s/views.yaml
Plugins file:      /Users/fernand/.local/share/k9s/plugins.yaml
Hotkeys file:      /Users/fernand/.local/share/k9s/hotkeys.yaml
Alias file:        /Users/fernand/.local/share/k9s/aliases.yaml
```

### View K9s logs

```shell
tail -f /Users/fernand/.local/data/k9s/k9s.log
```

### Start K9s in debug mode

```shell
k9s -l debug
```

### Customize logs destination

You can override the default log file destination either with the `--logFile` argument:

```shell
k9s --logFile /tmp/k9s.log
less /tmp/k9s.log
```

Or through the `K9S_LOGS_DIR` environment variable:

```shell
K9S_LOGS_DIR=/var/log k9s
less /var/log/k9s.log
```

## Key Bindings

K9s uses aliases to navigate most K8s resources.

| Action                                                                          | Command                       | Comment                                                                |
|---------------------------------------------------------------------------------|-------------------------------|------------------------------------------------------------------------|
| Show active keyboard mnemonics and help                                         | `?`                           |                                                                        |
| Show all available resource alias                                               | `ctrl-a`                      |                                                                        |
| To bail out of K9s                                                              | `:quit`, `:q`, `ctrl-c`       |                                                                        |
| To go up/back to the previous view                                              | `esc`                         | If you have crumbs on, this will go to the previous one                |
| View a Kubernetes resource using singular/plural or short-name                  | `:`pod‚èé                       | accepts singular, plural, short-name or alias ie pod or pods           |
| View a Kubernetes resource in a given namespace                                 | `:`pod ns-x‚èé                  |                                                                        |
| View filtered pods (New v0.30.0!)                                               | `:`pod /fred‚èé                 | View all pods filtered by fred                                         |
| View labeled pods (New v0.30.0!)                                                | `:`pod app=fred,env=dev‚èé      | View all pods with labels matching app=fred and env=dev                |
| View pods in a given context (New v0.30.0!)                                     | `:`pod @ctx1‚èé                 | View all pods in context ctx1. Switches out your current k9s context!  |
| Filter out a resource view given a filter                                       | `/`filter‚èé                    | Regex2 supported ie `fred|blee` to filter resources named fred or blee |
| Inverse regex filter                                                            | `/`! filter‚èé                  | Keep everything that *doesn&#039;t* match.                                  |
| Filter resource view by labels                                                  | `/`-l label-selector‚èé         |                                                                        |
| Fuzzy find a resource given a filter                                            | `/`-f filter‚èé                 |                                                                        |
| Bails out of view/command/filter mode                                           | `&lt;esc&gt;`                       |                                                                        |
| Key mapping to describe, view, edit, view logs,...                              | `d`,`v`, `e`, `l`,...         |                                                                        |
| To view and switch to another Kubernetes context (Pod view)                     | `:`ctx‚èé                       |                                                                        |
| To view and switch directly to another Kubernetes context (Last used view)      | `:`ctx context-name‚èé          |                                                                        |
| To view and switch to another Kubernetes namespace                              | `:`ns‚èé                        |                                                                        |
| To switch back to the last active command (like how &quot;cd -&quot; works)               | `-`                           | Navigation that adds breadcrumbs to the bottom are not commands        |
| To go back and forward through the command history                              | back: `[`, forward: `]`       | Same as above                                                          |
| To view all saved resources                                                     | `:`screendump or sd‚èé          |                                                                        |
| To delete a resource (TAB and ENTER to confirm)                                 | `ctrl-d`                      |                                                                        |
| To kill a resource (no confirmation dialog, equivalent to kubectl delete --now) | `ctrl-k`                      |                                                                        |
| Launch pulses view                                                              | `:`pulses or pu‚èé              |                                                                        |
| Launch XRay view                                                                | `:`xray RESOURCE [NAMESPACE]‚èé | RESOURCE can be one of po, svc, dp, rs, sts, ds, NAMESPACE is optional |
| Launch Popeye view                                                              | `:`popeye or pop‚èé             | See [popeye](#popeye)                                                  |

---

## K9s Configuration

  K9s keeps its configurations as YAML files inside of a `k9s` directory and the location depends on your operating system. K9s leverages [XDG](https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html) to load its various configurations files. For information on the default locations for your OS please see [this link](https://github.com/adrg/xdg/blob/master/README.md). If you are still confused a quick `k9s info` will reveal where k9s is loading its configurations from. Alternatively, you can set `K9S_CONFIG_DIR` to tell K9s the directory location to pull its configurations from.

  | Unix            | macOS                              | Windows               |
  |-----------------|------------------------------------|-----------------------|
  | `~/.config/k9s` | `~/Library/Application Support/k9s` | `%LOCALAPPDATA%\k9s`  |

  &gt; NOTE: This is still in flux and will change while in pre-release stage!

You can now override the context portForward default address configuration by setting an env variable that can override all clusters portForward local address using `K9S_DEFAULT_PF_ADDRESS=a.b.c.d`

  ```yaml
  # $XDG_CONFIG_HOME/k9s/config.yaml
  k9s:
    # Enable periodic refresh of resource browser windows. Default false
    liveViewAutoRefresh: false
    # !!New!! v0.50.8...
    # Extends the list of supported GPU vendors. The key is the vendor name, the value must correspond to k8s resource driver designation.
    # Default known GPU vendors:
    # nvidia: nvidia.com/gpu
	  # nvidia-shared: nvidia.com/gpu.shared
	  # amd: amd.com/gpu
	  # intel: gpu.intel.com/i915
    gpuVendors:
      bozo: bozo/gpu  # extends the gpu vendor and add &quot;bozo&quot;
    # The path to screen dump. Default: &#039;%temp_dir%/k9s-screens-%username%&#039; (k9s info)
    screenDumpDir: /tmp/dumps
    # Represents ui poll intervals in seconds. Default 2.0 secs. Minimum value is 2.0 - values below will be capped to the minimum.
    refreshRate: 2
    # Overrides the default k8s api server requests timeout. Defaults 120s
    apiServerTimeout: 15s
    # Number of retries once the connection to the api-server is lost. Default 15.
    maxConnRetry: 5
    # Indicates whether modification commands like delete/kill/edit are disabled. Default is false
    readOnly: false
    # This setting allows users to specify the default view, but it is not set by default.
    defaultView: &quot;&quot;
    # Toggles whether k9s should exit when CTRL-C is pressed. When set to true, you will need to exit k9s via the :quit command. Default is false.
    noExitOnCtrlC: false
    #UI settings
    ui:
      # Enable mouse support. Default false
      enableMouse: false
      # Set to true to hide K9s header. Default false
      headless: false
      # Set to true to hide the K9S logo Default false
      logoless: false
      # Set to true to hide K9s crumbs. Default false
      crumbsless: false
      # Set to true to suppress the K9s splash screen on start. Default false. Note that for larger clusters or higher latency connections, there may be no resources visible initially until local caches have finished populating.
      splashless: false
      # Toggles icons display as not all terminal support these chars. Default: true
      noIcons: false
      # Toggles reactive UI. This option provide for watching on disk artifacts changes and update the UI live Defaults to false.
      reactive: false
      # By default all contexts will use the dracula skin unless explicitly overridden in the context config file.
      skin: dracula # =&gt; assumes the file skins/dracula.yaml is present in the  $XDG_DATA_HOME/k9s/skins directory. Can be overriden with K9S_SKIN.
      # Convert dark skins to light, or vice versa, preserving hue. Default: false
      invert: false
      # Allows to set certain views default fullscreen mode. (yaml, helm history, describe, value_extender, details, logs) Default false
      defaultsToFullScreen: false
      # Show full resource GVR (Group/Version/Resource) vs just R. Default: false.
      useFullGVRTitle: false
    # Toggles icons display as not all terminal support these chars.
    noIcons: false
    # Toggles whether k9s should check for the latest revision from the GitHub repository releases. Default is false.
    skipLatestRevCheck: false
    # When altering kubeconfig or using multiple kube configs, k9s will clean up clusters configurations that are no longer in use. Setting this flag to true will keep k9s from cleaning up inactive cluster configs. Defaults to false.
    keepMissingClusters: false
    # Logs configuration
    logger:
      # Defines the number of lines to return. Default 100
      tail: 200
      # Defines the total number of log lines to allow in the view. Default 1000
      buffer: 500
      # Represents how far to go back in the log timeline in seconds. Setting to -1 will tail logs. Default is -1.
      sinceSeconds: 300 # =&gt; tail the last 5 mins.
      # Toggles log line wrap. Default false
      textWrap: false
      # Autoscroll in logs will be disabled. Default is false.
      disableAutoscroll: false
      # Enable column locking when autoscroll is enabled. Default is false.
      columnLock: false
      # Toggles log line timestamp info. Default false
      showTime: false
    # Provide shell pod customization when nodeShell feature gate is enabled!
    shellPod:
      # The shell pod image to use.
      image: killerAdmin
      # The namespace to launch to shell pod into

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[charmbracelet/lipgloss]]></title>
            <link>https://github.com/charmbracelet/lipgloss</link>
            <guid>https://github.com/charmbracelet/lipgloss</guid>
            <pubDate>Wed, 14 Jan 2026 00:05:32 GMT</pubDate>
            <description><![CDATA[Style definitions for nice terminal layouts üëÑ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/charmbracelet/lipgloss">charmbracelet/lipgloss</a></h1>
            <p>Style definitions for nice terminal layouts üëÑ</p>
            <p>Language: Go</p>
            <p>Stars: 10,319</p>
            <p>Forks: 298</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre># Lip Gloss

&lt;p&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://stuff.charm.sh/lipgloss/lip-gloss-light-2025-06.png&quot; width=&quot;340&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://stuff.charm.sh/lipgloss/lip-gloss-dark-2025-06.png&quot; width=&quot;340&quot;&gt;
      &lt;img src=&quot;https://stuff.charm.sh/lipgloss/lip-gloss-light-2025-06.png&quot; width=&quot;340&quot; /&gt;
    &lt;/picture&gt;
    &lt;br&gt;
    &lt;a href=&quot;https://github.com/charmbracelet/lipgloss/releases&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/release/charmbracelet/lipgloss.svg&quot; alt=&quot;Latest Release&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://pkg.go.dev/github.com/charmbracelet/lipgloss?tab=doc&quot;&gt;&lt;img src=&quot;https://godoc.org/github.com/golang/gddo?status.svg&quot; alt=&quot;GoDoc&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/charmbracelet/lipgloss/actions&quot;&gt;&lt;img src=&quot;https://github.com/charmbracelet/lipgloss/workflows/build/badge.svg&quot; alt=&quot;Build Status&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

Style definitions for nice terminal layouts. Built with TUIs in mind.

![Lip Gloss example](https://github.com/user-attachments/assets/92560e60-d70e-4ce0-b39e-a60bb933356b)

Lip Gloss takes an expressive, declarative approach to terminal rendering.
Users familiar with CSS will feel at home with Lip Gloss.

```go

import &quot;github.com/charmbracelet/lipgloss&quot;

var style = lipgloss.NewStyle().
    Bold(true).
    Foreground(lipgloss.Color(&quot;#FAFAFA&quot;)).
    Background(lipgloss.Color(&quot;#7D56F4&quot;)).
    PaddingTop(2).
    PaddingLeft(4).
    Width(22)

fmt.Println(style.Render(&quot;Hello, kitty&quot;))
```

## Colors

Lip Gloss supports the following color profiles:

### ANSI 16 colors (4-bit)

```go
lipgloss.Color(&quot;5&quot;)  // magenta
lipgloss.Color(&quot;9&quot;)  // red
lipgloss.Color(&quot;12&quot;) // light blue
```

### ANSI 256 Colors (8-bit)

```go
lipgloss.Color(&quot;86&quot;)  // aqua
lipgloss.Color(&quot;201&quot;) // hot pink
lipgloss.Color(&quot;202&quot;) // orange
```

### True Color (16,777,216 colors; 24-bit)

```go
lipgloss.Color(&quot;#0000FF&quot;) // good ol&#039; 100% blue
lipgloss.Color(&quot;#04B575&quot;) // a green
lipgloss.Color(&quot;#3C3C3C&quot;) // a dark gray
```

...as well as a 1-bit ASCII profile, which is black and white only.

The terminal&#039;s color profile will be automatically detected, and colors outside
the gamut of the current palette will be automatically coerced to their closest
available value.

### Adaptive Colors

You can also specify color options for light and dark backgrounds:

```go
lipgloss.AdaptiveColor{Light: &quot;236&quot;, Dark: &quot;248&quot;}
```

The terminal&#039;s background color will automatically be detected and the
appropriate color will be chosen at runtime.

### Complete Colors

CompleteColor specifies exact values for True Color, ANSI256, and ANSI color
profiles.

```go
lipgloss.CompleteColor{TrueColor: &quot;#0000FF&quot;, ANSI256: &quot;86&quot;, ANSI: &quot;5&quot;}
```

Automatic color degradation will not be performed in this case and it will be
based on the color specified.

### Complete Adaptive Colors

You can use `CompleteColor` with `AdaptiveColor` to specify the exact values for
light and dark backgrounds without automatic color degradation.

```go
lipgloss.CompleteAdaptiveColor{
    Light: CompleteColor{TrueColor: &quot;#d7ffae&quot;, ANSI256: &quot;193&quot;, ANSI: &quot;11&quot;},
    Dark:  CompleteColor{TrueColor: &quot;#d75fee&quot;, ANSI256: &quot;163&quot;, ANSI: &quot;5&quot;},
}
```

## Inline Formatting

Lip Gloss supports the usual ANSI text formatting options:

```go
var style = lipgloss.NewStyle().
    Bold(true).
    Italic(true).
    Faint(true).
    Blink(true).
    Strikethrough(true).
    Underline(true).
    Reverse(true)
```

## Block-Level Formatting

Lip Gloss also supports rules for block-level formatting:

```go
// Padding
var style = lipgloss.NewStyle().
    PaddingTop(2).
    PaddingRight(4).
    PaddingBottom(2).
    PaddingLeft(4)

// Margins
var style = lipgloss.NewStyle().
    MarginTop(2).
    MarginRight(4).
    MarginBottom(2).
    MarginLeft(4)
```

There is also shorthand syntax for margins and padding, which follows the same
format as CSS:

```go
// 2 cells on all sides
lipgloss.NewStyle().Padding(2)

// 2 cells on the top and bottom, 4 cells on the left and right
lipgloss.NewStyle().Margin(2, 4)

// 1 cell on the top, 4 cells on the sides, 2 cells on the bottom
lipgloss.NewStyle().Padding(1, 4, 2)

// Clockwise, starting from the top: 2 cells on the top, 4 on the right, 3 on
// the bottom, and 1 on the left
lipgloss.NewStyle().Margin(2, 4, 3, 1)
```

## Aligning Text

You can align paragraphs of text to the left, right, or center.

```go
var style = lipgloss.NewStyle().
    Width(24).
    Align(lipgloss.Left).  // align it left
    Align(lipgloss.Right). // no wait, align it right
    Align(lipgloss.Center) // just kidding, align it in the center
```

## Width and Height

Setting a minimum width and height is simple and straightforward.

```go
var style = lipgloss.NewStyle().
    SetString(&quot;What‚Äôs for lunch?&quot;).
    Width(24).
    Height(32).
    Foreground(lipgloss.Color(&quot;63&quot;))
```

## Borders

Adding borders is easy:

```go
// Add a purple, rectangular border
var style = lipgloss.NewStyle().
    BorderStyle(lipgloss.NormalBorder()).
    BorderForeground(lipgloss.Color(&quot;63&quot;))

// Set a rounded, yellow-on-purple border to the top and left
var anotherStyle = lipgloss.NewStyle().
    BorderStyle(lipgloss.RoundedBorder()).
    BorderForeground(lipgloss.Color(&quot;228&quot;)).
    BorderBackground(lipgloss.Color(&quot;63&quot;)).
    BorderTop(true).
    BorderLeft(true)

// Make your own border
var myCuteBorder = lipgloss.Border{
    Top:         &quot;._.:*:&quot;,
    Bottom:      &quot;._.:*:&quot;,
    Left:        &quot;|*&quot;,
    Right:       &quot;|*&quot;,
    TopLeft:     &quot;*&quot;,
    TopRight:    &quot;*&quot;,
    BottomLeft:  &quot;*&quot;,
    BottomRight: &quot;*&quot;,
}
```

There are also shorthand functions for defining borders, which follow a similar
pattern to the margin and padding shorthand functions.

```go
// Add a thick border to the top and bottom
lipgloss.NewStyle().
    Border(lipgloss.ThickBorder(), true, false)

// Add a double border to the top and left sides. Rules are set clockwise
// from top.
lipgloss.NewStyle().
    Border(lipgloss.DoubleBorder(), true, false, false, true)
```

For more on borders see [the docs][docs].

## Copying Styles

Just use assignment:

```go
style := lipgloss.NewStyle().Foreground(lipgloss.Color(&quot;219&quot;))

copiedStyle := style // this is a true copy

wildStyle := style.Blink(true) // this is also true copy, with blink added

```

Since `Style` data structures contains only primitive types, assigning a style
to another effectively creates a new copy of the style without mutating the
original.

## Inheritance

Styles can inherit rules from other styles. When inheriting, only unset rules
on the receiver are inherited.

```go
var styleA = lipgloss.NewStyle().
    Foreground(lipgloss.Color(&quot;229&quot;)).
    Background(lipgloss.Color(&quot;63&quot;))

// Only the background color will be inherited here, because the foreground
// color will have been already set:
var styleB = lipgloss.NewStyle().
    Foreground(lipgloss.Color(&quot;201&quot;)).
    Inherit(styleA)
```

## Unsetting Rules

All rules can be unset:

```go
var style = lipgloss.NewStyle().
    Bold(true).                        // make it bold
    UnsetBold().                       // jk don&#039;t make it bold
    Background(lipgloss.Color(&quot;227&quot;)). // yellow background
    UnsetBackground()                  // never mind
```

When a rule is unset, it won&#039;t be inherited or copied.

## Enforcing Rules

Sometimes, such as when developing a component, you want to make sure style
definitions respect their intended purpose in the UI. This is where `Inline`
and `MaxWidth`, and `MaxHeight` come in:

```go
// Force rendering onto a single line, ignoring margins, padding, and borders.
someStyle.Inline(true).Render(&quot;yadda yadda&quot;)

// Also limit rendering to five cells
someStyle.Inline(true).MaxWidth(5).Render(&quot;yadda yadda&quot;)

// Limit rendering to a 5x5 cell block
someStyle.MaxWidth(5).MaxHeight(5).Render(&quot;yadda yadda&quot;)
```

## Tabs

The tab character (`\t`) is rendered differently in different terminals (often
as 8 spaces, sometimes 4). Because of this inconsistency, Lip Gloss converts
tabs to 4 spaces at render time. This behavior can be changed on a per-style
basis, however:

```go
style := lipgloss.NewStyle() // tabs will render as 4 spaces, the default
style = style.TabWidth(2)    // render tabs as 2 spaces
style = style.TabWidth(0)    // remove tabs entirely
style = style.TabWidth(lipgloss.NoTabConversion) // leave tabs intact
```

## Rendering

Generally, you just call the `Render(string...)` method on a `lipgloss.Style`:

```go
style := lipgloss.NewStyle().Bold(true).SetString(&quot;Hello,&quot;)
fmt.Println(style.Render(&quot;kitty.&quot;)) // Hello, kitty.
fmt.Println(style.Render(&quot;puppy.&quot;)) // Hello, puppy.
```

But you could also use the Stringer interface:

```go
var style = lipgloss.NewStyle().SetString(&quot;‰Ω†Â•ΩÔºåÁå´Âí™„ÄÇ&quot;).Bold(true)
fmt.Println(style) // ‰Ω†Â•ΩÔºåÁå´Âí™„ÄÇ
```

### Custom Renderers

Custom renderers allow you to render to a specific outputs. This is
particularly important when you want to render to different outputs and
correctly detect the color profile and dark background status for each, such as
in a server-client situation.

```go
func myLittleHandler(sess ssh.Session) {
    // Create a renderer for the client.
    renderer := lipgloss.NewRenderer(sess)

    // Create a new style on the renderer.
    style := renderer.NewStyle().Background(lipgloss.AdaptiveColor{Light: &quot;63&quot;, Dark: &quot;228&quot;})

    // Render. The color profile and dark background state will be correctly detected.
    io.WriteString(sess, style.Render(&quot;Heyyyyyyy&quot;))
}
```

For an example on using a custom renderer over SSH with [Wish][wish] see the
[SSH example][ssh-example].

## Utilities

In addition to pure styling, Lip Gloss also ships with some utilities to help
assemble your layouts.

### Joining Paragraphs

Horizontally and vertically joining paragraphs is a cinch.

```go
// Horizontally join three paragraphs along their bottom edges
lipgloss.JoinHorizontal(lipgloss.Bottom, paragraphA, paragraphB, paragraphC)

// Vertically join two paragraphs along their center axes
lipgloss.JoinVertical(lipgloss.Center, paragraphA, paragraphB)

// Horizontally join three paragraphs, with the shorter ones aligning 20%
// from the top of the tallest
lipgloss.JoinHorizontal(0.2, paragraphA, paragraphB, paragraphC)
```

### Measuring Width and Height

Sometimes you‚Äôll want to know the width and height of text blocks when building
your layouts.

```go
// Render a block of text.
var style = lipgloss.NewStyle().
    Width(40).
    Padding(2)
var block string = style.Render(someLongString)

// Get the actual, physical dimensions of the text block.
width := lipgloss.Width(block)
height := lipgloss.Height(block)

// Here&#039;s a shorthand function.
w, h := lipgloss.Size(block)
```

### Placing Text in Whitespace

Sometimes you‚Äôll simply want to place a block of text in whitespace.

```go
// Center a paragraph horizontally in a space 80 cells wide. The height of
// the block returned will be as tall as the input paragraph.
block := lipgloss.PlaceHorizontal(80, lipgloss.Center, fancyStyledParagraph)

// Place a paragraph at the bottom of a space 30 cells tall. The width of
// the text block returned will be as wide as the input paragraph.
block := lipgloss.PlaceVertical(30, lipgloss.Bottom, fancyStyledParagraph)

// Place a paragraph in the bottom right corner of a 30x80 cell space.
block := lipgloss.Place(30, 80, lipgloss.Right, lipgloss.Bottom, fancyStyledParagraph)
```

You can also style the whitespace. For details, see [the docs][docs].

## Rendering Tables

Lip Gloss ships with a table rendering sub-package.

```go
import &quot;github.com/charmbracelet/lipgloss/table&quot;
```

Define some rows of data.

```go
rows := [][]string{
    {&quot;Chinese&quot;, &quot;ÊÇ®Â•Ω&quot;, &quot;‰Ω†Â•Ω&quot;},
    {&quot;Japanese&quot;, &quot;„Åì„Çì„Å´„Å°„ÅØ&quot;, &quot;„ÇÑ„ÅÇ&quot;},
    {&quot;Arabic&quot;, &quot;ÿ£ŸáŸÑŸäŸÜ&quot;, &quot;ÿ£ŸáŸÑÿß&quot;},
    {&quot;Russian&quot;, &quot;–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ&quot;, &quot;–ü—Ä–∏–≤–µ—Ç&quot;},
    {&quot;Spanish&quot;, &quot;Hola&quot;, &quot;¬øQu√© tal?&quot;},
}
```

Use the table package to style and render the table.

```go
var (
    purple    = lipgloss.Color(&quot;99&quot;)
    gray      = lipgloss.Color(&quot;245&quot;)
    lightGray = lipgloss.Color(&quot;241&quot;)

    headerStyle  = lipgloss.NewStyle().Foreground(purple).Bold(true).Align(lipgloss.Center)
    cellStyle    = lipgloss.NewStyle().Padding(0, 1).Width(14)
    oddRowStyle  = cellStyle.Foreground(gray)
    evenRowStyle = cellStyle.Foreground(lightGray)
)

t := table.New().
    Border(lipgloss.NormalBorder()).
    BorderStyle(lipgloss.NewStyle().Foreground(purple)).
    StyleFunc(func(row, col int) lipgloss.Style {
        switch {
        case row == table.HeaderRow:
            return headerStyle
        case row%2 == 0:
            return evenRowStyle
        default:
            return oddRowStyle
        }
    }).
    Headers(&quot;LANGUAGE&quot;, &quot;FORMAL&quot;, &quot;INFORMAL&quot;).
    Rows(rows...)

// You can also add tables row-by-row
t.Row(&quot;English&quot;, &quot;You look absolutely fabulous.&quot;, &quot;How&#039;s it going?&quot;)
```

Print the table.

```go
fmt.Println(t)
```

![Table Example](https://github.com/charmbracelet/lipgloss/assets/42545625/6e4b70c4-f494-45da-a467-bdd27df30d5d)

&gt; [!WARNING]
&gt; Table `Rows` need to be declared before `Offset` otherwise it does nothing.

### Table Borders

There are helpers to generate tables in markdown or ASCII style:

#### Markdown Table

```go
table.New().Border(lipgloss.MarkdownBorder()).BorderTop(false).BorderBottom(false)
```

```
| LANGUAGE |    FORMAL    | INFORMAL  |
|----------|--------------|-----------|
| Chinese  | N«ên h«éo      | N«ê h«éo    |
| French   | Bonjour      | Salut     |
| Russian  | Zdravstvuyte | Privet    |
| Spanish  | Hola         | ¬øQu√© tal? |
```

#### ASCII Table

```go
table.New().Border(lipgloss.ASCIIBorder())
```

```
+----------+--------------+-----------+
| LANGUAGE |    FORMAL    | INFORMAL  |
+----------+--------------+-----------+
| Chinese  | N«ên h«éo      | N«ê h«éo    |
| French   | Bonjour      | Salut     |
| Russian  | Zdravstvuyte | Privet    |
| Spanish  | Hola         | ¬øQu√© tal? |
+----------+--------------+-----------+
```

For more on tables see [the docs](https://pkg.go.dev/github.com/charmbracelet/lipgloss?tab=doc) and [examples](https://github.com/charmbracelet/lipgloss/tree/master/examples/table).

## Rendering Lists

Lip Gloss ships with a list rendering sub-package.

```go
import &quot;github.com/charmbracelet/lipgloss/list&quot;
```

Define a new list.

```go
l := list.New(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)
```

Print the list.

```go
fmt.Println(l)

// ‚Ä¢ A
// ‚Ä¢ B
// ‚Ä¢ C
```

Lists have the ability to nest.

```go
l := list.New(
    &quot;A&quot;, list.New(&quot;Artichoke&quot;),
    &quot;B&quot;, list.New(&quot;Baking Flour&quot;, &quot;Bananas&quot;, &quot;Barley&quot;, &quot;Bean Sprouts&quot;),
    &quot;C&quot;, list.New(&quot;Cashew Apple&quot;, &quot;Cashews&quot;, &quot;Coconut Milk&quot;, &quot;Curry Paste&quot;, &quot;Currywurst&quot;),
    &quot;D&quot;, list.New(&quot;Dill&quot;, &quot;Dragonfruit&quot;, &quot;Dried Shrimp&quot;),
    &quot;E&quot;, list.New(&quot;Eggs&quot;),
    &quot;F&quot;, list.New(&quot;Fish Cake&quot;, &quot;Furikake&quot;),
    &quot;J&quot;, list.New(&quot;Jicama&quot;),
    &quot;K&quot;, list.New(&quot;Kohlrabi&quot;),
    &quot;L&quot;, list.New(&quot;Leeks&quot;, &quot;Lentils&quot;, &quot;Licorice Root&quot;),
)
```

Print the list.

```go
fmt.Println(l)
```

&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;600&quot; alt=&quot;image&quot; src=&quot;https://github.com/charmbracelet/lipgloss/assets/42545625/0dc9f440-0748-4151-a3b0-7dcf29dfcdb0&quot;&gt;
&lt;/p&gt;

Lists can be customized via their enumeration function as well as using
`lipgloss.Style`s.

```go
enumeratorStyle := lipgloss.NewStyle().Foreground(lipgloss.Color(&quot;99&quot;)).MarginRight(1)
itemStyle := lipgloss.NewStyle().Foreground(lipgloss.Color(&quot;212&quot;)).MarginRight(1)

l := list.New(
    &quot;Glossier&quot;,
    &quot;Claire‚Äôs Boutique&quot;,
    &quot;Nyx&quot;,
    &quot;Mac&quot;,
    &quot;Milk&quot;,
    ).
    Enumerator(list.Roman).
    EnumeratorStyle(enumeratorStyle).
    ItemStyle(itemStyle)
```

Print the list.

&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;600&quot; alt=&quot;List example&quot; src=&quot;https://github.com/charmbracelet/lipgloss/assets/42545625/360494f1-57fb-4e13-bc19-0006efe01561&quot;&gt;
&lt;/p&gt;

In addition to the predefined enumerators (`Arabic`, `Alphabet`, `Roman`, `Bullet`, `Tree`),
you may also define your own custom enumerator:

```go
l := list.New(&quot;Duck&quot;, &quot;Duck&quot;, &quot;Duck&quot;, &quot;Duck&quot;, &quot;Goose&quot;, &quot;Duck&quot;, &quot;Duck&quot;)

func DuckDuckGooseEnumerator(l list.Items, i int) string {
    if l.At(i).Value() == &quot;Goose&quot; {
        return &quot;Honk ‚Üí&quot;
    }
    return &quot;&quot;
}

l = l.Enumerator(DuckDuckGooseEnumerator)
```

Print the list:

&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;600&quot; alt=&quot;image&quot; src=&quot;https://github.com/charmbracelet/lipgloss/assets/42545625/157aaf30-140d-4948-9bb4-dfba46e5b87e&quot;&gt;
&lt;/p&gt;

If you need, you can also build lists incrementally:

```go
l := list.New()

for i := 0; i &lt; repeat; i++ {
    l.Item(&quot;Lip Gloss&quot;)
}
```

## Rendering Trees

Lip Gloss ships with a tree rendering sub-package.

```go
import &quot;github.com/charmbracelet/lipgloss/tree&quot;
```

Define a new tree.

```go
t := tree.Root(&quot;.&quot;).
    Child(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)
```

Print the tree.

```go
fmt.Println(t)

// .
// ‚îú‚îÄ‚îÄ A
// ‚îú‚îÄ‚îÄ B
// ‚îî‚îÄ‚îÄ C
```

Trees have the ability to nest.

```go
t := tree.Root(&quot;.&quot;).
    Child(&quot;macOS&quot;).
    Child(
        tree.New().
            Root(&quot;Linux&quot;).
            Child(&quot;NixOS&quot;).
            Child(&quot;Arch Linux (btw)&quot;).
            Child(&quot;Void Linux&quot;),
        ).
    Child(
        tree.New().
            Root(&quot;BSD&quot;).
            Child(&quot;FreeBSD&quot;).
            Child(&quot;OpenBSD&quot;),
    )
```

Print the tree.

```go
fmt.Println(t)
```

&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;663&quot; alt=&quot;Tree Example (simple)&quot; src=&quot;https://github.com/user-attachments/assets/5ef14eb8-a5d4-4f94-8834-e15d1e714f89&quot;&gt;
&lt;/p&gt;

Trees can be customized via their enumeration function as well as using
`lipgloss.Style`s.

```go
enumeratorStyle := lipgloss.NewStyle().Foreground(lipgloss.Color(&quot;63&quot;)).MarginRight(1)
rootStyle := lipgloss.NewStyle().Foreground(lipgloss.Color(&quot;35&quot;))
itemStyle := lipgloss.NewStyle().Foreground(lipgloss.Color(&quot;212&quot;))

t := tree.
    Root(&quot;‚Åú Makeup&quot;).
    Child(
        &quot;Glossier&quot;,
        &quot;Fenty Beauty&quot;,
        tree.New().Child(
            &quot;Gloss Bomb Universal Lip Luminizer&quot;,
            &quot;Hot Cheeks Velour Blushlighter&quot;,
        ),
        &quot;Nyx&quot;,
        &quot;Mac&quot;,
        &quot;Milk&quot;,
    ).
    Enumerator(tree.RoundedEnumerator).
    EnumeratorStyle(enumeratorStyle).
    RootStyle(rootStyle).
    ItemStyle(itemStyle)
```

Print the tree.

&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;663&quot; alt=&quot;Tree Example (makeup)&quot; src=&quot;https://github.com/user-attachments/assets/06d12d87-744a-4c89-bd98-45de9094a97e&quot;&gt;
&lt;/p&gt;

The predefined enumerators for trees are `DefaultEnumerator` and `RoundedEnumerator`.

If you need, you can also build trees incrementally:

```go
t := tree.New()

for i := 0; i &lt; repeat; i++ {
    t.Child(&quot;Lip Gloss&quot;)
}
```

---

## FAQ

&lt;details&gt;
&lt;summary&gt;
Why are things misaligning? Why are borders at the wrong widths?
&lt;/summary&gt;
&lt;p&gt;This is most likely due to your locale and encoding, particularly with
regard to Chinese, Japanese, and Korean (for example, &lt;code&gt;zh_CN.UTF-8&lt;/code&gt;
or &lt;code&gt;ja_JP.UTF-8&lt;/code&gt;). The most direct way to fix this is to set
&lt;code&gt;RUNEWIDTH_EASTASIAN=0&lt;/code&gt; in your environment.&lt;/p&gt;

&lt;p&gt;For details see &lt;a href=&quot;https://github.com/charmbracelet/lipgloss/issues/40&quot;&gt;https://github.com/charmbracelet/lipgloss/issues/40.&lt;/a&gt;&lt;/p&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
Why isn&#039;t Lip Gloss displaying colors?
&lt;/summary&gt;
&lt;p&gt;Lip Gloss automatically degrades colors to the best available option in the
given terminal, and if output&#039;s not a TTY it will remove color output entirely.
This is common when running tests, CI, or when piping output elsewhere.&lt;/p&gt;

&lt;p&gt;If necessary, you can force a color profile in your tests with
&lt;a href=&quot;https://pkg.go.dev/github.com/charmbracelet/lipgloss#SetColorProfile&quot;&gt;&lt;code&gt;SetColorProfile&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

```go
import (
    &quot;github.com/charmbracelet/lipgloss&quot;
    &quot;github.com/muesli/termenv&quot;
)

lipgloss.SetColorProfile(termenv.TrueColor)
```

_Note:_ this option limits the flexibility of your application and can cause
ANSI escape codes to be output in cases where that might not be desired. Take
careful note of your use case and environment before choosing to force a color
profile.

&lt;/details&gt;

## What about [Bubble Tea][tea]?

Lip Gloss doesn‚Äôt replace Bubble Tea. Rather, it is an excellent Bubble Tea
companion. It was designed to make assembling terminal u

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[kubernetes-sigs/kueue]]></title>
            <link>https://github.com/kubernetes-sigs/kueue</link>
            <guid>https://github.com/kubernetes-sigs/kueue</guid>
            <pubDate>Wed, 14 Jan 2026 00:05:31 GMT</pubDate>
            <description><![CDATA[Kubernetes-native Job Queueing]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/kubernetes-sigs/kueue">kubernetes-sigs/kueue</a></h1>
            <p>Kubernetes-native Job Queueing</p>
            <p>Language: Go</p>
            <p>Stars: 2,253</p>
            <p>Forks: 504</p>
            <p>Stars today: 1 star today</p>
            <h2>README</h2><pre># Kueue

[![GoReport Widget]][GoReport Status]
[![Latest Release](https://img.shields.io/github/v/release/kubernetes-sigs/kueue?include_prereleases)](https://github.com/kubernetes-sigs/kueue/releases/latest)

[GoReport Widget]: https://goreportcard.com/badge/sigs.k8s.io/kueue
[GoReport Status]: https://goreportcard.com/report/sigs.k8s.io/kueue

&lt;img src=&quot;https://github.com/kubernetes-sigs/kueue/blob/main/site/static/images/logo.svg&quot; width=&quot;100&quot; alt=&quot;kueue logo&quot;&gt;

Kueue is a set of APIs and controller for [job](https://kueue.sigs.k8s.io/docs/concepts/workload)
[queueing](https://kueue.sigs.k8s.io/docs/concepts#queueing). It is a job-level manager that decides when
a job should be [admitted](https://kueue.sigs.k8s.io/docs/concepts#admission) to start (as in pods can be
created) and when it should stop (as in active pods should be deleted).

Read the [overview](https://kueue.sigs.k8s.io/docs/overview/) and watch the Kueue-related [talks &amp; presentations](https://kueue.sigs.k8s.io/docs/talks_and_presentations/) to learn more.

## Features overview

- **Job management:** Support job queueing based on [priorities](https://kueue.sigs.k8s.io/docs/concepts/workload/#priority) with different [strategies](https://kueue.sigs.k8s.io/docs/concepts/cluster_queue/#queueing-strategy): `StrictFIFO` and `BestEffortFIFO`.
- **Advanced Resource management:** Comprising: [resource flavor fungibility](https://kueue.sigs.k8s.io/docs/concepts/cluster_queue/#flavorfungibility), [Fair Sharing](https://kueue.sigs.k8s.io/docs/concepts/preemption/#fair-sharing), [cohorts](https://kueue.sigs.k8s.io/docs/concepts/cluster_queue/#cohort) and [preemption](https://kueue.sigs.k8s.io/docs/concepts/cluster_queue/#preemption) with a variety of policies between different tenants.
- **Integrations:** Built-in support for popular jobs, e.g. [BatchJob](https://kueue.sigs.k8s.io/docs/tasks/run/jobs/), [Kubeflow training jobs](https://kueue.sigs.k8s.io/docs/tasks/run/kubeflow/), [RayJob](https://kueue.sigs.k8s.io/docs/tasks/run/rayjobs/), [RayCluster](https://kueue.sigs.k8s.io/docs/tasks/run/rayclusters/), [JobSet](https://kueue.sigs.k8s.io/docs/tasks/run/jobsets/),  [plain Pod and Pod Groups](https://kueue.sigs.k8s.io/docs/tasks/run/plain_pods/).
- **System insight:** Build-in [prometheus metrics](https://kueue.sigs.k8s.io/docs/reference/metrics/) to help monitor the state of the system, and on-demand visibility endpoint for [monitoring of pending workloads](https://kueue.sigs.k8s.io/docs/tasks/manage/monitor_pending_workloads/pending_workloads_on_demand/).
- **AdmissionChecks:** A mechanism for internal or external components to influence whether a workload can be [admitted](https://kueue.sigs.k8s.io/docs/concepts/admission_check/).
- **Advanced autoscaling support:** Integration with cluster-autoscaler&#039;s [provisioningRequest](https://kueue.sigs.k8s.io/docs/concepts/admission_check/provisioning_request/#job-using-a-provisioningrequest) via admissionChecks.
- **All-or-nothing with ready Pods:** A timeout-based implementation of [All-or-nothing scheduling](https://kueue.sigs.k8s.io/docs/tasks/manage/setup_wait_for_pods_ready/).
- **Partial admission and dynamic reclaim:** mechanisms to run a job with [reduced parallelism](https://kueue.sigs.k8s.io/docs/tasks/run/jobs/#partial-admission), based on available quota, and to [release](https://kueue.sigs.k8s.io/docs/concepts/workload/#dynamic-reclaim) the quota the pods complete..
- **Mixing training and inference**: Simultaneous management of batch workloads along with serving workloads (such as [Deployments](https://kueue.sigs.k8s.io/docs/tasks/run/deployment/) or [StatefulSets](https://kueue.sigs.k8s.io/docs/tasks/run/statefulset/))
- **Multi-cluster job dispatching:** called [MultiKueue](https://kueue.sigs.k8s.io/docs/concepts/multikueue/), allows to search for capacity and off-load the main cluster.
- **Topology-Aware Scheduling**: Allows to optimize the Pod-to-Pod communication throughput by [scheduling aware of the data-center topology](https://kueue.sigs.k8s.io/docs/concepts/topology_aware_scheduling/).

## Production Readiness status

- ‚úîÔ∏è API version: v1beta2, respecting [Kubernetes Deprecation Policy](https://kubernetes.io/docs/reference/using-api/deprecation-policy/)
- ‚úîÔ∏è Up-to-date [documentation](https://kueue.sigs.k8s.io/docs).
- ‚úîÔ∏è Test Coverage:
  - ‚úîÔ∏è Unit Test [testgrid](https://testgrid.k8s.io/sig-scheduling#periodic-kueue-test-unit-main).
  - ‚úîÔ∏è Integration Test [testgrid](https://testgrid.k8s.io/sig-scheduling#periodic-kueue-test-integration-main)
  - ‚úîÔ∏è Integration MultiKueue Tests [testgrid](https://testgrid.k8s.io/sig-scheduling#periodic-kueue-test-integration-multikueue-main)
  - ‚úîÔ∏è E2E Tests for Kubernetes
    [1.32](https://testgrid.k8s.io/sig-scheduling#periodic-kueue-test-e2e-main-1-32),
    [1.33](https://testgrid.k8s.io/sig-scheduling#periodic-kueue-test-e2e-main-1-33),
    [1.34](https://testgrid.k8s.io/sig-scheduling#periodic-kueue-test-e2e-main-1-34),
    [1.35](https://testgrid.k8s.io/sig-scheduling#periodic-kueue-test-e2e-main-1-35),
    on Kind.
  - ‚úîÔ∏è E2E TAS Test [testgrid](https://testgrid.k8s.io/sig-scheduling#periodic-kueue-test-e2e-tas-main)
  - ‚úîÔ∏è E2E Custom Configs Test [testgrid](https://testgrid.k8s.io/sig-scheduling#periodic-kueue-test-e2e-customconfigs-main)
  - ‚úîÔ∏è E2E Cert Manager Test [testgrid](https://testgrid.k8s.io/sig-scheduling#periodic-kueue-test-e2e-certmanager-main)
  - ‚úîÔ∏è Performance Test [testgrid](https://testgrid.k8s.io/sig-scheduling#periodic-kueue-test-scheduling-perf-main)
- ‚úîÔ∏è Scalability verification via [performance tests](https://github.com/kubernetes-sigs/kueue/tree/main/test/performance).
- ‚úîÔ∏è Monitoring via [metrics](https://kueue.sigs.k8s.io/docs/reference/metrics).
- ‚úîÔ∏è Security: RBAC based accessibility.
- ‚úîÔ∏è Stable [release](RELEASE.md) cycle (2-3 months).
- ‚úîÔ∏è [Adopters](https://kueue.sigs.k8s.io/docs/adopters/) running on production.

  _Based on community feedback, we continue to simplify and evolve the API to
  address new use cases_.

## Installation

**Requires Kubernetes 1.29 or newer**.

To install the latest release of Kueue in your cluster, run the following command:

```shell
kubectl apply --server-side -f https://github.com/kubernetes-sigs/kueue/releases/download/v0.15.2/manifests.yaml
```

The controller runs in the `kueue-system` namespace.

Read the [installation guide](https://kueue.sigs.k8s.io/docs/installation/) to learn more.

## Usage

A minimal configuration can be set by running the [examples](site/static/examples):

```shell
kubectl apply -f examples/admin/single-clusterqueue-setup.yaml
```

Then you can run a job with:

```shell
kubectl create -f examples/jobs/sample-job.yaml
```

Learn more about:

- Kueue [concepts](https://kueue.sigs.k8s.io/docs/concepts).
- Common and advanced [tasks](https://kueue.sigs.k8s.io/docs/tasks).

## Roadmap

High-level overview of the main priorities for 2025:
- Improve user experience for [MultiKueue](https://kueue.sigs.k8s.io/docs/concepts/multikueue/) - multi-cluster Job dispatching, in particular:
  * sequential attempts to try worker clusters [#3757](https://github.com/kubernetes-sigs/kueue/issues/3757)
  * log retrieval from worker clusters [3526](https://github.com/kubernetes-sigs/kueue/issues/3526)
- Improve user experience for [Topology Aware Scheduling](https://kueue.sigs.k8s.io/docs/concepts/topology_aware_scheduling/), in particular:
  * make Topology Aware Scheduling compatible with cohorts and preemption [#3761](https://github.com/kubernetes-sigs/kueue/issues/3761)
  * optimize the algorithm to minimize fragmentation [#3756](https://github.com/kubernetes-sigs/kueue/issues/3756)
  * better accuracy of scheduling by tighter integration with kube-scheduler [#3755](https://github.com/kubernetes-sigs/kueue/issues/3755)
  * reduce friction by defaulting the PodSet annotations [#3754](https://github.com/kubernetes-sigs/kueue/issues/3754)
- Productization of the Kueue dashboard [#940](https://github.com/kubernetes-sigs/kueue/issues/940)
- Support Hierarchical Cohorts with FairSharing [#3759](https://github.com/kubernetes-sigs/kueue/issues/3759)
- Improved support for AI inference, including:
  * partial preemption of serving workloads [#3762](https://github.com/kubernetes-sigs/kueue/issues/3762)
  * LeaderWorkerSet support [#3232](https://github.com/kubernetes-sigs/kueue/issues/3232)
- Progress towards the stable API (v1beta2) [#768](https://github.com/kubernetes-sigs/kueue/issues/768)

Long-term aspirational goals:
- Integration with workflow frameworks [#74](https://github.com/kubernetes-sigs/kueue/issues/74)
- Support dynamically-sized Jobs [#77](https://github.com/kubernetes-sigs/kueue/issues/77)
- Budget support [#28](https://github.com/kubernetes-sigs/kueue/issues/28)
- Flavor assignment strategies, e.g. _minimizing cost_ vs _minimizing borrowing_ [#312](https://github.com/kubernetes-sigs/kueue/issues/312)
- Cooperative preemption support for workloads that implement checkpointing [#477](https://github.com/kubernetes-sigs/kueue/issues/477)
- Delayed preemption for two-stage admission [#3758](https://github.com/kubernetes-sigs/kueue/issues/3758)
- Support Structured Parameters (DRA) in Kueue [#2941](https://github.com/kubernetes-sigs/kueue/issues/2941)
- Graduate the API to v1 [#3476](https://github.com/kubernetes-sigs/kueue/issues/3476)

## Community, discussion, contribution, and support

Learn how to engage with the Kubernetes community on the [community page](http://kubernetes.io/community/)
and the [contributor&#039;s guide](CONTRIBUTING.md).

You can reach the maintainers of this project at:

- [Slack](https://kubernetes.slack.com/messages/wg-batch)
- [Mailing List](https://groups.google.com/a/kubernetes.io/g/wg-batch)

### Graphic assets

- [Kueue](https://github.com/cncf/artwork/tree/main/projects/kubernetes/sub-projects/kueue)
- [KueueViz](https://github.com/cncf/artwork/tree/main/projects/kubernetes/sub-projects/kueueviz)

### Code of conduct

Participation in the Kubernetes community is governed by the [Kubernetes Code of Conduct](code-of-conduct.md).
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[VictoriaMetrics/VictoriaLogs]]></title>
            <link>https://github.com/VictoriaMetrics/VictoriaLogs</link>
            <guid>https://github.com/VictoriaMetrics/VictoriaLogs</guid>
            <pubDate>Wed, 14 Jan 2026 00:05:30 GMT</pubDate>
            <description><![CDATA[Fast and easy to use database for logs, which can efficiently handle terabytes of logs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/VictoriaMetrics/VictoriaLogs">VictoriaMetrics/VictoriaLogs</a></h1>
            <p>Fast and easy to use database for logs, which can efficiently handle terabytes of logs</p>
            <p>Language: Go</p>
            <p>Stars: 1,165</p>
            <p>Forks: 79</p>
            <p>Stars today: 47 stars today</p>
            <h2>README</h2><pre># VictoriaLogs

[![Latest Release](https://img.shields.io/github/v/release/VictoriaMetrics/VictoriaLogs?sort=semver&amp;label=&amp;logo=github&amp;labelColor=gray&amp;color=gray&amp;link=https%3A%2F%2Fgithub.com%2FVictoriaMetrics%2FVictoriaLogs%2Freleases%2Flatest)](https://github.com/VictoriaMetrics/VictoriaLogs/releases)
![Docker Pulls](https://img.shields.io/docker/pulls/victoriametrics/victoria-logs?label=&amp;logo=docker&amp;logoColor=white&amp;labelColor=2496ED&amp;color=2496ED&amp;link=https%3A%2F%2Fhub.docker.com%2Fr%2Fvictoriametrics%2Fvictoria-logs)
[![Go Report](https://goreportcard.com/badge/github.com/VictoriaMetrics/VictoriaLogs?link=https%3A%2F%2Fgoreportcard.com%2Freport%2Fgithub.com%2FVictoriaMetrics%2FVictoriaLogs)](https://goreportcard.com/report/github.com/VictoriaMetrics/VictoriaLogs)
[![Build Status](https://github.com/VictoriaMetrics/VictoriaLogs/actions/workflows/main.yml/badge.svg?branch=master&amp;link=https%3A%2F%2Fgithub.com%2FVictoriaMetrics%2FVictoriaLogs%2Factions)](https://github.com/VictoriaMetrics/VictoriaLogs/actions/workflows/main.yml)
[![codecov](https://codecov.io/gh/VictoriaMetrics/VictoriaLogs/branch/master/graph/badge.svg?link=https%3A%2F%2Fcodecov.io%2Fgh%2FVictoriaMetrics%2FVictoriaLogs)](https://app.codecov.io/gh/VictoriaMetrics/VictoriaLogs)
[![License](https://img.shields.io/github/license/VictoriaMetrics/VictoriaLogs?labelColor=green&amp;label=&amp;link=https%3A%2F%2Fgithub.com%2FVictoriaMetrics%2FVictoriaLogs%2Fblob%2Fmaster%2FLICENSE)](https://github.com/VictoriaMetrics/VictoriaLogs/blob/master/LICENSE)
![Slack](https://img.shields.io/badge/Join-4A154B?logo=slack&amp;link=https%3A%2F%2Fslack.victoriametrics.com)
[![X](https://img.shields.io/twitter/follow/VictoriaMetrics?style=flat&amp;label=Follow&amp;color=black&amp;logo=x&amp;labelColor=black&amp;link=https%3A%2F%2Fx.com%2FVictoriaMetrics)](https://x.com/VictoriaMetrics/)
[![Reddit](https://img.shields.io/reddit/subreddit-subscribers/VictoriaMetrics?style=flat&amp;label=Join&amp;labelColor=red&amp;logoColor=white&amp;logo=reddit&amp;link=https%3A%2F%2Fwww.reddit.com%2Fr%2FVictoriaMetrics)](https://www.reddit.com/r/VictoriaMetrics/)

VictoriaLogs is a fast easy to use database for logs.

Here are some resources and information about VictoriaLogs:

- Playgrounds: [playground for the built-in web UI](https://play-vmlogs.victoriametrics.com/), [playground for Grafana plugin for VictoriaLogs](https://play-grafana.victoriametrics.com/d/be5zidev72m80f/k8s-logs-via-victorialogs)
- [Documentation](https://docs.victoriametrics.com/victorialogs/)
- Available: [Binary releases](https://github.com/VictoriaMetrics/VictoriaLogs/releases/latest), docker images [Docker Hub](https://hub.docker.com/r/victoriametrics/victoria-logs/) and [Quay](https://quay.io/repository/victoriametrics/victoria-logs), [Source code](https://github.com/VictoriaMetrics/VictoriaLogs)
- Deployment types: [Single-node version](https://docs.victoriametrics.com/victorialogs/), [Cluster version](https://docs.victoriametrics.com/victorialogs/cluster/)
- Changelog: [CHANGELOG](https://docs.victoriametrics.com/victorialogs/changelog/), and [How to upgrade](https://docs.victoriametrics.com/victorialogs/#upgrading)
- Community: [Slack](https://slack.victoriametrics.com/), [X (Twitter)](https://x.com/VictoriaMetrics), [LinkedIn](https://www.linkedin.com/company/victoriametrics/), [YouTube](https://www.youtube.com/@VictoriaMetrics)

Both the single-node and the cluster versions of VictoriaLogs are open source and free to use.

## Community and contributions

Feel free asking any questions regarding VictoriaLogs:

* [Slack Inviter](https://slack.victoriametrics.com/) and [Slack channel](https://victoriametrics.slack.com/)
* [X (Twitter)](https://x.com/VictoriaMetrics/)
* [Linkedin](https://www.linkedin.com/company/victoriametrics/)
* [Reddit](https://www.reddit.com/r/VictoriaMetrics/)
* [Telegram-en](https://t.me/VictoriaMetrics_en)
* [Telegram-ru](https://t.me/VictoriaLogs_ru)
* [Mastodon](https://mastodon.social/@victoriametrics/)

If you like VictoriaLogs and want to contribute, then please [read these docs](https://docs.victoriametrics.com/victoriametrics/contributing/).

Thank you for your cooperation!
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[gravitational/teleport]]></title>
            <link>https://github.com/gravitational/teleport</link>
            <guid>https://github.com/gravitational/teleport</guid>
            <pubDate>Wed, 14 Jan 2026 00:05:29 GMT</pubDate>
            <description><![CDATA[The easiest, and most secure way to access and protect all of your infrastructure.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/gravitational/teleport">gravitational/teleport</a></h1>
            <p>The easiest, and most secure way to access and protect all of your infrastructure.</p>
            <p>Language: Go</p>
            <p>Stars: 19,658</p>
            <p>Forks: 1,974</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre>Teleport provides connectivity, authentication, access controls and audit for
infrastructure.

You might use Teleport to:

* Set up single sign-on (SSO) for all of your cloud and on-prem
  infrastructure.
* Protect access to servers, Kubernetes clusters, databases, Windows
  desktops, web applications, and cloud APIs without long-lived keys or
  passwords.
* Establish secure tunnels to reach resources behind NATs and firewalls
  without VPNs or bastion hosts.
* Record and audit activity across SSH, Kubernetes, database, RDP, and web
  sessions.
* Apply consistent Role-Based and Attribute-Based Access Control (RBAC/ABAC)
  across users, machines, workloads, and resource types.
* Enforce least privilege and Just-in-Time (JIT) access requests for
  elevated roles or sensitive systems.
* Maintain a single identity and access layer for both human users and
  workloads.

Teleport works with SSH, Kubernetes, databases, RDP, cloud consoles,
internal web services, Git repositories, and Model Context Protocol (MCP)
servers.

&lt;div align=&quot;center&quot;&gt;
   &lt;a href=&quot;https://goteleport.com/download&quot;&gt;
   &lt;img src=&quot;./assets/img/hero-teleport-platform.png&quot; width=750/&gt;
   &lt;/a&gt;
   &lt;div align=&quot;center&quot; style=&quot;padding: 25px&quot;&gt;
      &lt;a href=&quot;https://goteleport.com/download&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/github/v/release/gravitational/teleport?sort=semver&amp;label=Release&amp;color=651FFF&quot; /&gt;
      &lt;/a&gt;
      &lt;a href=&quot;https://golang.org/&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/github/go-mod/go-version/gravitational/teleport?color=7fd5ea&quot; /&gt;
      &lt;/a&gt;
      &lt;a href=&quot;https://github.com/gravitational/teleport/blob/master/CODE_OF_CONDUCT.md&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/badge/Contribute-üôå-green.svg&quot; /&gt;
      &lt;/a&gt;
      &lt;a href=&quot;https://www.gnu.org/licenses/agpl-3.0.en.html&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/badge/AGPL-3.0-red.svg&quot; /&gt;
      &lt;/a&gt;
   &lt;/div&gt;
&lt;/div&gt;
&lt;/br&gt;

## More Information
[Teleport Getting Started](https://goteleport.com/docs/get-started/)  
[Teleport Architecture](https://goteleport.com/docs/reference/architecture/)  
[Reference Guides](https://goteleport.com/docs/reference/)  
[FAQ](https://goteleport.com/docs/faq)


## Table of Contents

1. [Introduction](#introduction)
1. [Why We Built Teleport](#why-we-built-teleport)
1. [Supporting &amp; Contributing](#supporting--contributing)
1. [Installing &amp; Running](#installing--running)
1. [Docker](#docker)
1. [Building Teleport](#building-teleport)
1. [License](#license)
1. [FAQ](#faq)

## Introduction

Teleport includes an identity-aware access proxy, a CA that issues short-lived
certificates, a unified access control system, and a tunneling system to access
resources behind the firewall.

Teleport is a single Go binary that integrates with multiple protocols and
cloud services, including

* [SSH nodes](https://goteleport.com/docs/enroll-resources/server-access/introduction/)
* [Kubernetes clusters](https://goteleport.com/docs/enroll-resources/kubernetes-access/introduction/)
* [PostgreSQL, MongoDB, CockroachDB and MySQL
  databases](https://goteleport.com/docs/enroll-resources/database-access/)
* [Model Context Protocol](https://goteleport.com/docs/connect-your-client/model-context-protocol/)
* [Internal Web apps](https://goteleport.com/docs/enroll-resources/application-access/introduction/)
* [Windows Hosts](https://goteleport.com/docs/enroll-resources/desktop-access/introduction/)
* [Networked servers](https://goteleport.com/docs/enroll-resources/server-access/introduction/)

You can set up Teleport as a [Linux
daemon](https://goteleport.com/docs/admin-guides/deploy-a-cluster/linux-demo)
or a [Kubernetes
deployment](https://goteleport.com/docs/admin-guides/deploy-a-cluster/helm-deployments/).

Teleport focuses on best practices for infrastructure security, including:

- No shared secrets such as SSH keys or Kubernetes tokens; Teleport uses
  certificate-based auth with automatic expiration for all protocols.
- Multi-factor authentication (MFA) for everything.
- Single sign-on (SSO) for everything via GitHub Auth, OpenID Connect, or
  SAML with endpoints like Okta or Microsoft Entra ID.
- Session sharing for collaborative troubleshooting for issues.
- Infrastructure introspection to view the status of every SSH node, database
  instance, Kubernetes cluster, or internal web app through the Teleport CLI
  or Web UI.

Teleport uses [Go crypto](https://godoc.org/golang.org/x/crypto). It is
_fully compatible with OpenSSH_, `sshd` servers, and `ssh` clients,
Kubernetes clusters and more.

| Project Links                                                  | Description                                                                                                                 |
|----------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------|
| [Teleport Website](https://goteleport.com/)                    | The official website of the project.                                                                                        |
| [Documentation](https://goteleport.com/docs/)                  | Admin guide, user manual and more.                                                                                          |
| [Features](https://goteleport.com/docs/feature-matrix/)        | Explore the complete list of Teleport capabilities.                                                                         |
| [Blog](https://goteleport.com/blog/)                           | Our blog where we publish Teleport news and helpful articles.                                                               |
| [Forum](https://github.com/gravitational/teleport/discussions) | Ask us a setup question or post tutorials, feedback, or ideas.                                                              |
| [Developer Tools](https://goteleport.com/resources/tools/)     | Dozens of free browser-based tools for code processing, cryptography, data transformation, and more.                        |
| [Teleport Academy](https://goteleport.com/learn/)              | How-to guides, best practices, and deep dives into topics like SSH, Kubernetes, MCP, and more.                              |
| [Slack](https://goteleport.com/slack)                          | Need help with your setup? Ping us in our Slack channel.                                                                    |
| [Cloud  &amp; Self-Hosted](https://goteleport.com/pricing/)        | Teleport Enterprise is a cloud-hosted option for teams that require easy and secure access to their computing environments. |

## Why We Built Teleport

While working together at Rackspace, the creators of Teleport noticed that
most cloud users struggle with setting up and configuring infrastructure
security. Many popular tools designed for this are complex to understand and
expensive to maintain across modern, distributed computing infrastructure.

We decided to build a solution that&#039;s easy to use, understand, and scale. A
real-time representation of all your servers in the same room as you, as if
they were magically **teleported**. And thus, Teleport was born! 

Today, Teleport is trusted by everyone from hobbyists to hyperscalers to
simplify security across cloud CLIs and consoles, Kubernetes clusters, SSH
servers, databases, internal web apps, and Model Context Protocol (MCP) used
by AI agents.

[Learn more about Teleport and our history](https://goteleport.com/about/)

## Supporting &amp; Contributing

We aim to make Teleport easy to adopt and contribute to, starting with clear and comprehensive [documentation](https://goteleport.com/docs/). 

If you have questions, are exploring ideas, or want to sanity-check something, please start with a GitHub Discussion. Discussions help us answer questions, explore use cases, and decide together whether something should become a bug report or feature request.

- Start a conversation in [Teleport Discussions](https://github.com/gravitational/teleport/discussions)  
  This is the best place to ask questions, share ideas, and get help. Our engineers actively participate there, and discussions can be promoted to issues when there is a clear, actionable next step.

- Issues are for confirmed bugs and well-defined feature requests  
  If something has already been validated as a bug or an enhancement, feel free to open an issue. When in doubt, start a discussion and we will help guide it.

- Enterprise and POC support  
  If you are evaluating Teleport Enterprise or need more responsive support during a POC, we can set up a dedicated Slack channel. You can [reach out to us through our website](https://goteleport.com/contact-sales/) to get started.

## Installing and Running

To set up a single-instance Teleport cluster, follow our [getting started
guide](https://goteleport.com/docs/admin-guides/deploy-a-cluster/linux-demo/).
You can then register your servers, Kubernetes clusters, and other
infrastructure with your Teleport cluster.

You can also get started with Teleport Enterprise Cloud, a managed Teleport
deployment that makes it easier to enable secure access to your
infrastructure.

[Sign up for a free trial](https://goteleport.com/signup/) of Teleport
Enterprise Cloud, and follow this guide to [register your first
server](https://goteleport.com/docs/get-started/).

## Docker

### Deploy Teleport

If you wish to deploy Teleport inside a Docker container see the
[installation guide](https://goteleport.com/docs/installation/#running-teleport-on-docker).

### For Local Testing and Development

To run a full test suite locally, see [the test dependencies
list](BUILD_macos.md#local-tests-dependencies)

## Building Teleport

The `teleport` repository contains the Teleport daemon binary (written in Go)
and a web UI written in TypeScript.

If your intention is to build and deploy for use in a production infrastructure
a released tag should be used.  The default branch, `master`, is the current
development branch for an upcoming major version.  Get the latest release tags
listed at https://goteleport.com/download/ and then use that tag in the `git
clone`. For example `git clone
https://github.com/gravitational/teleport.git -b v18.5.0` gets release
v18.5.0.

### Dockerized Build

It is often easiest to build with Docker, which ensures that all required
tooling is available for the build. To execute a dockerized build, ensure
that docker is installed and running, and execute:

```
make -C build.assets build-binaries
```

This command will build Linux binaries matching the host architecture.
It is not possible to cross-compile to a different target architecture.

### Local Build

#### Dependencies

The following dependencies are required to build Teleport from source. For
maximum compatibility, install the versions of these dependencies using the
versions listed in [`build.assets/versions.mk`](/build.assets/versions.mk):

1. [`Go`](https://golang.org/dl/)
1. [`Rust`](https://www.rust-lang.org/tools/install)
1. [`Node.js`](https://nodejs.org/en/download/)
1. [`libfido2`](https://github.com/Yubico/libfido2)
1. [`pkg-config`](https://www.freedesktop.org/wiki/Software/pkg-config/)

For an example of dev environment setup on macOS, see [these
instructions](/BUILD_macos.md).

#### Perform a build

&gt;**Important**
&gt;
&gt;* The Go compiler is somewhat sensitive to the amount of memory: you will
   need **at least** 1GB of virtual memory to compile Teleport. A 512MB
   instance without swap will **not** work.
&gt;* This will build the latest version of Teleport. 

Get the source

```shell
git clone https://github.com/gravitational/teleport.git
cd teleport
```

To perform a build

```shell
make full
```

`tsh` dynamically links against libfido2 by default, to support development
environments, as long as the library itself can be found:

```shell
$ brew install libfido2 pkg-config  # Replace with your package manager of choice

$ make build/tsh
&gt; libfido2 found, setting FIDO2=dynamic
&gt; (...)
```

Release binaries are linked statically against libfido2. You may switch the
linking mode using the FIDO2 variable:

```shell
make build/tsh FIDO2=dynamic # dynamic linking
make build/tsh FIDO2=static  # static linking, for an easy setup use `make enter`
                             # or `build.assets/macos/build-fido2-macos.sh`.
make build/tsh FIDO2=off     # doesn&#039;t link libfido2 in any way
```

`tsh` builds with Touch ID support require access to an Apple Developer
account. If you are a Teleport maintainer, ask the team for access.

#### Build output and run locally

If the build succeeds, the installer will place the binaries in the `build`
directory.

Before starting, create default data directories:

```shell
sudo mkdir -p -m0700 /var/lib/teleport
sudo chown $USER /var/lib/teleport
```

#### Running Teleport in a hot reload mode

To speed up your development process, you can run Teleport using
[`CompileDaemon`](https://github.com/githubnemo/CompileDaemon). This will
build and run the Teleport binary, and then rebuild and restart it whenever
any Go source files change.

1. Install CompileDaemon:

    ```shell
    go install github.com/githubnemo/CompileDaemon@latest
    ```

    Note that we use `go install` instead of the suggested `go get`, because
    we don&#039;t want CompileDaemon to become a dependency of the project.

1. Build and run the Teleport binary:

    ```shell
    make teleport-hot-reload
    ```

    By default, this runs a `teleport start` command. If you want to
    customize the command, for example by providing a custom config file
    location, you can use the `TELEPORT_ARGS` parameter:

    ```shell
    make teleport-hot-reload TELEPORT_ARGS=&#039;start --config=/path/to/config.yaml&#039;
    ```

Note that you still need to run [`make grpc`](api/proto/README.md) if you
modify any Protocol Buffers files to regenerate the generated Go sources;
regenerating these sources should in turn cause the CompileDaemon to rebuild
and restart Teleport.

### Web UI

The Teleport Web UI resides in the [web](web) directory.

#### Rebuilding Web UI for development

To rebuild the Teleport UI package, run the following command:

```bash
make docker-ui
```

Then you can replace Teleport Web UI files with the files from the
newly-generated `/dist` folder.

To enable speedy iterations on the Web UI, you can run a [local web-dev
server](web#web-ui).

You can also tell Teleport to load the Web UI assets from the source
directory. To enable this behavior, set the environment variable `DEBUG=1`
and rebuild with the default target:

```bash
# Run Teleport as a single-node cluster in development mode:
DEBUG=1 ./build/teleport start -d
```

Keep the server running in this mode, and make your UI changes in `/dist`
directory. For instructions about how to update the Web UI, read [the `web`
README](web#readme).

### Managing dependencies

All dependencies are managed using [Go
modules](https://blog.golang.org/using-go-modules). Here are the
instructions for some common tasks:

#### Add a new dependency

Latest version:

```bash
go get github.com/new/dependency
```

and update the source to use this dependency.


To get a specific version, use `go get
github.com/new/dependency@version` instead.

#### Set dependency to a specific version

```bash
go get github.com/new/dependency@version
```

#### Update dependency to the latest version

```bash
go get -u github.com/new/dependency
```

#### Update all dependencies

```bash
go get -u all
```

#### Debugging dependencies

Why is a specific package imported?

`go mod why $pkgname`

Why is a specific module imported?

`go mod why -m $modname`

Why is a specific version of a module imported?

`go mod graph | grep $modname`

## License

Teleport is distributed in multiple forms with different licensing
implications.

The Teleport API module (all code in this repository under `/api`) is
available under the [Apache 2.0 license](./api/LICENSE).

The remainder of the source code in this repository is available under the
[GNU Affero General Public License](./LICENSE). Users compiling Teleport
from source must comply with the terms of this license.

Teleport Community Edition builds distributed on
http://goteleport.com/download are available under a [modified Apache 2.0
license](./build.assets/LICENSE-community).

## FAQ

### Is Teleport production-ready?

Yes, Teleport is production-ready and used to protect and facilitate
access to the most precious and mission-critical applications at many of
today&#039;s leading companies. You can learn more about the companies using
Teleport in production [on our website](https://goteleport.com/case-study/).

### Is Teleport secure?

Yes, Teleport has completed several security audits from nationally and
internationally recognized technology security companies. We publicize
audit results, our security philosophy, and related information on our
[trust page](https://trust.goteleport.com/).

### What resources does Teleport support?

Teleport secures access to a [broad set of infrastructure
resources](https://goteleport.com/docs/enroll-resources), including Linux
servers, Windows desktops, Kubernetes clusters, databases, internal web
applications, cloud provider APIs and consoles (such as AWS, Azure, and
GCP), and Model Context Protocol (MCP) servers used by AI agents.

### How is Teleport deployed?

Teleport can be [deployed to fit most
environments](https://goteleport.com/docs/feature-matrix/#platform-integrations-management-licensing-and-deployment),
either as a self-hosted cluster on Linux or Kubernetes or using Teleport
Enterprise Cloud. In all cases, Teleport agents run close to your
resources and connect through an Auth Service and Proxy Service that
enforces identity, access control, and audit.

### Is Teleport an identity provider (IdP)?

Teleport uses existing IdPs (Okta, Google Workspace, Microsoft Entra ID,
or GitHub) to issue short-lived certificates and apply access policies.
Teleport can also be [configured to act as a SAML
IdP](https://goteleport.com/docs/identity-governance/idps/) to authenticate
users into applications when needed.

### Does Teleport require credential handling or secrets management?

Teleport eliminates long-lived passwords, SSH keys, database credentials,
credential rotations, and vault processes by issuing [short-lived,
auto-expiring mTLS and SSH
certificates](https://goteleport.com/docs/reference/architecture/authentication/#short-lived-certificates)
bound to human or non-human identity.

### Is Teleport a Privileged Access Management (PAM) solution?

Teleport provides modern PAM software capabilities like strong
authentication, session recording, policy-based access, and JIT elevation
without secrets, credential rotation, or vault dependencies. This enables
controlled, audited access to servers, Kubernetes, databases, cloud
consoles, and other privileged environments using short-lived certificates
and role-based policies.

### Is Teleport a Just-in-Time (JIT) access solution?

Teleport enables [JIT access through time-bound Access
Requests](https://goteleport.com/docs/identity-governance/access-requests/).
Users request the roles or resources they temporarily need, policies decide
whether approval is required, and privileges automatically expire. This
approach maintains least privilege while keeping access workflows
efficient and predictable.

### Does Teleport secure access to Kubernetes?

Teleport can [proxy and secure Kubernetes
access](https://goteleport.com/docs/enroll-resources/kubernetes-access/introduction/)
with identity-based authentication, role-based access controls, and
detailed auditing of kubectl activity.

### Does Teleport support SPIFFE?

Teleport supports [SPIFFE-compatible identities for
workloads](https://goteleport.com/docs/machine-workload-identity/workload-identity/spiffe/),
allowing it to participate in SPIFFE ecosystems and federation.
Teleport issues short-lived SVIDs and can integrate with external PKI
hierarchi

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[openbao/openbao]]></title>
            <link>https://github.com/openbao/openbao</link>
            <guid>https://github.com/openbao/openbao</guid>
            <pubDate>Wed, 14 Jan 2026 00:05:28 GMT</pubDate>
            <description><![CDATA[OpenBao exists to provide a software solution to manage, store, and distribute sensitive data including secrets, certificates, and keys.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/openbao/openbao">openbao/openbao</a></h1>
            <p>OpenBao exists to provide a software solution to manage, store, and distribute sensitive data including secrets, certificates, and keys.</p>
            <p>Language: Go</p>
            <p>Stars: 5,212</p>
            <p>Forks: 308</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre># OpenBao


----

**Please note**: We take OpenBao&#039;s security and our users&#039; trust very seriously. If you believe you have found a security issue in OpenBao, _please responsibly disclose_ by contacting us at [openbao-security@lists.openssf.org](mailto:openbao-security@lists.openssf.org).

----

[![OpenSSF Scorecard](https://api.scorecard.dev/projects/github.com/openbao/openbao/badge)](https://scorecard.dev/viewer/?uri=github.com/openbao/openbao) [![OpenSSF Best Practices](https://www.bestpractices.dev/projects/9126/badge)](https://www.bestpractices.dev/projects/9126)

- [Website](https://www.openbao.org)
- [Mailing List](https://lists.openssf.org/g/openbao)
- [GitHub Discussions](https://github.com/openbao/openbao/discussions)
- [Chat Server](https://linuxfoundation.zulipchat.com/)
  - [`#openssf-openbao-discussion`](https://linuxfoundation.zulipchat.com/#narrow/channel/529890-openssf-openbao-discussion)
  - [`#openssf-openbao-support`](https://linuxfoundation.zulipchat.com/#narrow/channel/530381-openssf-openbao-support)
  - [`#openssf-openbao-tsc`](https://linuxfoundation.zulipchat.com/#narrow/channel/530382-openssf-openbao-tsc)
  - Working Groups:
    - [`#openssf-openbao-wg-namespaces`](https://linuxfoundation.zulipchat.com/#narrow/channel/532995-openssf-openbao-wg-namespaces)
    - [`#openssf-openbao-wg-pkcs11`](https://linuxfoundation.zulipchat.com/#narrow/channel/532994-openssf-openbao-wg-pkcs11)
    - [`#openssf-openbao-wg-scalability`](https://linuxfoundation.zulipchat.com/#narrow/channel/532998-openssf-openbao-wg-scalability)
    - [`#openssf-openbao-wg-supply`](https://linuxfoundation.zulipchat.com/#narrow/channel/532999-openssf-openbao-wg-supply)
    - [`#openssf-openbao-wg-ui`](https://linuxfoundation.zulipchat.com/#narrow/channel/532997-openssf-openbao-wg-ui)

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;300&quot; alt=&quot;OpenBao Mascot&quot; src=&quot;https://raw.githubusercontent.com/openbao/artwork/main/color/openbao-color.svg&quot;&gt;
&lt;/p&gt;

**OpenBao exists to provide a software solution to manage, store, and distribute sensitive data including secrets, certificates, and keys. The OpenBao community intends to provide this software under an OSI-approved open-source license, led by a community run under open governance principles.**

A modern system requires access to a multitude of secrets: database credentials, API keys for external services, credentials for service-oriented architecture communication, etc. Understanding who is accessing what secrets is already very difficult and platform-specific. Adding on key rolling, secure storage, and detailed audit logs is almost impossible without a custom solution. This is where OpenBao steps in.

The key features of OpenBao are:

* **Secure Secret Storage**: Arbitrary key/value secrets can be stored
  in OpenBao. OpenBao encrypts these secrets prior to writing them to persistent
  storage, so gaining access to the raw storage isn&#039;t enough to access
  your secrets. OpenBao can write to disk, [PostgreSQL](https://www.postgresql.org/),
  and more.

* **Dynamic Secrets**: OpenBao can generate secrets on-demand for some
  systems, such as AWS or SQL databases. For example, when an application
  needs to access an S3 bucket, it asks OpenBao for credentials, and OpenBao
  will generate an AWS keypair with valid permissions on demand. After
  creating these dynamic secrets, OpenBao will also automatically revoke them
  after the lease is up.

* **Data Encryption**: OpenBao can encrypt and decrypt data without storing
  it. This allows security teams to define encryption parameters and
  developers to store encrypted data in a location such as a SQL database without
  having to design their own encryption methods.

* **Leasing and Renewal**: All secrets in OpenBao have a _lease_ associated
  with them. At the end of the lease, OpenBao will automatically revoke that
  secret. Clients are able to renew leases via built-in renew APIs.

* **Revocation**: OpenBao has built-in support for secret revocation. OpenBao
  can revoke not only single secrets, but a tree of secrets, for example,
  all secrets read by a specific user, or all secrets of a particular type.
  Revocation assists in key rolling as well as locking down systems in the
  case of an intrusion.

Documentation, Getting Started, and Certification Exams
-------------------------------

Documentation is available on the [OpenBao website](https://www.openbao.org/docs/).

Developing OpenBao
--------------------

If you wish to work on OpenBao itself or any of its built-in systems, you&#039;ll
first need [Go](https://www.golang.org) installed on your machine.

For local dev first make sure Go is properly installed, including setting up a
[GOPATH](https://golang.org/doc/code.html#GOPATH). Ensure that `$GOPATH/bin` is in
your path as some distributions bundle the old version of build tools. Next, clone this
repository. OpenBao uses [Go Modules](https://github.com/golang/go/wiki/Modules),
so it is recommended that you clone the repository ***outside*** of the GOPATH.
You can then download any required build tools by bootstrapping your environment:

```sh
$ make bootstrap
...
```

To compile a development version of OpenBao, run `make` or `make dev`. This will
put the OpenBao binary in the `bin` and `$GOPATH/bin` folders:

```sh
$ make dev
...
$ bin/bao
...
```

To compile a development version of OpenBao with the UI, run `make static-dist dev-ui`. This will
put the OpenBao binary in the `bin` and `$GOPATH/bin` folders:

```sh
$ make static-dist dev-ui
...
$ bin/bao
...
```

To run tests, type `make test`. Note: this requires Docker to be installed. If
this exits with exit status 0, then everything is working!

```sh
$ make test
...
```

If you&#039;re developing a specific package, you can run tests for just that
package by specifying the `TEST` variable. For example below, only
`vault` package tests will be run.

```sh
$ make test TEST=./vault
...
```

### Importing OpenBao

This repository publishes two libraries that may be imported by other projects:
`github.com/openbao/openbao/api/v2` and `github.com/openbao/openbao/sdk/v2`.

Note that this repository also contains OpenBao (the product), and as with most Go
projects, OpenBao uses Go modules to manage its dependencies. The mechanism to do
that is the [go.mod](./go.mod) file. As it happens, the presence of that file
also makes it theoretically possible to import OpenBao as a dependency into other
projects. Some other projects have made a practice of doing so in order to take
advantage of testing tooling that was developed for testing OpenBao itself. This
is not, and has never been, a supported way to use the OpenBao project. We aren&#039;t
likely to fix bugs relating to failure to import `github.com/openbao/openbao`
into your project.

See also the section &quot;Docker-based tests&quot; below.

### Acceptance Tests

OpenBao has comprehensive [acceptance tests](https://en.wikipedia.org/wiki/Acceptance_testing)
covering most of the features of the secret and auth methods.

If you&#039;re working on a feature of a secret or auth method and want to
verify it is functioning (and also hasn&#039;t broken anything else), we recommend
running the acceptance tests.

**Warning:** The acceptance tests create/destroy/modify *real resources*, which
may incur real costs in some cases. In the presence of a bug, it is technically
possible that broken backends could leave dangling data behind. Therefore,
please run the acceptance tests at your own risk. At the very least,
we recommend running them in their own private account for whatever backend
you&#039;re testing.

To run the acceptance tests, invoke `make testacc`:

```sh
$ make testacc TEST=./builtin/logical/pki
...
```

The `TEST` variable is required, and you should specify the folder where the
backend is. The `TESTARGS` variable is recommended to filter down to a specific
resource to test, since testing all of them at once can sometimes take a very
long time.

Acceptance tests typically require other environment variables to be set for
things such as access keys. The test itself should error early and tell
you what to set, so it is not documented here.

### Docker-based Tests

We have created an experimental new testing mechanism inspired by NewTestCluster.
An example of how to use it:

```go
import (
  &quot;testing&quot;
  &quot;github.com/openbao/openbao/sdk/v2/helper/testcluster/docker&quot;
)

func Test_Something_With_Docker(t *testing.T) {
  opts := &amp;docker.DockerClusterOptions{
    ImageRepo: &quot;openbao/openbao&quot;,
    ImageTag:    &quot;latest&quot;,
  }
  cluster := docker.NewTestDockerCluster(t, opts)
  defer cluster.Cleanup()

  client := cluster.Nodes()[0].APIClient()
  _, err := client.Logical().Read(&quot;sys/storage/raft/configuration&quot;)
  if err != nil {
    t.Fatal(err)
  }
}
```

Here is a more realistic example of how we use it in practice.  `DefaultOptions` uses
`hashicorp/vault:latest` as the repo and tag, but it also looks at the environment
variable `BAO_BINARY`. If populated, it will copy the local file referenced by
`BAO_BINARY` into the container. This is useful when testing local changes.

Optionally you can set `COMMIT_SHA`, which will be appended to the image name we
build as a debugging convenience.

```go
func Test_Custom_Build_With_Docker(t *testing.T) {
  opts := docker.DefaultOptions(t)
  cluster := docker.NewTestDockerCluster(t, opts)
  defer cluster.Cleanup()
}
```

Finally, here&#039;s an example of running an existing OSS docker test with a custom binary:

```bash
$ GOOS=linux make dev
$ VAULT_BINARY=$(pwd)/bin/bao go test -run &#039;TestRaft_Configuration_Docker&#039; ./vault/external_tests/raft/raft_binary
ok      github.com/openbao/openbao/vault/external_tests/raft/raft_binary        20.960s
```
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[edoardottt/cariddi]]></title>
            <link>https://github.com/edoardottt/cariddi</link>
            <guid>https://github.com/edoardottt/cariddi</guid>
            <pubDate>Wed, 14 Jan 2026 00:05:27 GMT</pubDate>
            <description><![CDATA[Take a list of domains, crawl urls and scan for endpoints, secrets, api keys, file extensions, tokens and more]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/edoardottt/cariddi">edoardottt/cariddi</a></h1>
            <p>Take a list of domains, crawl urls and scan for endpoints, secrets, api keys, file extensions, tokens and more</p>
            <p>Language: Go</p>
            <p>Stars: 3,202</p>
            <p>Forks: 282</p>
            <p>Stars today: 103 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/edoardottt/images/blob/main/cariddi/logo.png&quot;&gt;&lt;br&gt;
  &lt;b&gt;Take a list of domains, crawl urls and scan for endpoints, secrets, api keys, file extensions, tokens and more&lt;/b&gt;&lt;br&gt;
  &lt;br&gt;
  &lt;!-- go-report-card --&gt;
  &lt;a href=&quot;https://goreportcard.com/report/github.com/edoardottt/cariddi&quot;&gt;
    &lt;img src=&quot;https://goreportcard.com/badge/github.com/edoardottt/cariddi&quot; alt=&quot;go-report-card&quot; /&gt;
  &lt;/a&gt;
  &lt;!-- workflows --&gt;
  &lt;a href=&quot;https://github.com/edoardottt/cariddi/actions&quot;&gt;
    &lt;img src=&quot;https://github.com/edoardottt/cariddi/actions/workflows/go.yml/badge.svg?branch=main&quot; alt=&quot;workflows&quot; /&gt;
  &lt;/a&gt;
  &lt;br&gt;
  &lt;sub&gt;
    Coded with üíô by edoardottt
  &lt;/sub&gt;
  &lt;br&gt;
  &lt;!--Tweet button--&gt;
  &lt;a href=&quot;https://twitter.com/intent/tweet?url=https://github.com/edoardottt/cariddi&amp;text=Take%20a%20list%20of%20domains,%20crawl%20urls%20and%20scan%20for%20endpoints,%20secrets,%20api%20keys,%20file%20extensions,%20tokens%20and%20more...%20%23network%20%23security%20%23infosec%20%23oss%20%23github%20%23bugbounty%20%23linux&quot; target=&quot;_blank&quot;&gt;Share on Twitter!
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;#installation-&quot;&gt;Install&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;#usage-&quot;&gt;Usage&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;#get-started-&quot;&gt;Get Started&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;#changelog-&quot;&gt;Changelog&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;#contributing-&quot;&gt;Contributing&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;#license-&quot;&gt;License&lt;/a&gt;
&lt;/p&gt;

&lt;!--[![asciicast](https://asciinema.org/a/415989.svg)](https://asciinema.org/a/415989)--&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/edoardottt/images/blob/main/cariddi/cariddi.gif&quot;&gt;
&lt;/p&gt;

Installation üì°
----------

#### Homebrew

```console
brew install cariddi
```

#### Snap

```console
sudo snap install cariddi
```

#### Golang

```console
go install -v github.com/edoardottt/cariddi/cmd/cariddi@latest
```

#### Pacman

```console
pacman -Syu cariddi
```

#### NixOS

```console
nix-shell -p cariddi
```

#### Building from source

You need [Go](https://go.dev/) (&gt;=1.24.0)

&lt;details&gt;
  &lt;summary&gt;Building from source for Linux and Windows&lt;/summary&gt;

#### Linux

```console
git clone https://github.com/edoardottt/cariddi.git
cd cariddi
go get ./...
make linux # (to install)
make unlinux # (to uninstall)
```

One-liner: `git clone https://github.com/edoardottt/cariddi.git &amp;&amp; cd cariddi &amp;&amp; go get ./... &amp;&amp; make linux`

#### Windows 

Note that the executable works only in cariddi folder.

```console
git clone https://github.com/edoardottt/cariddi.git
cd cariddi
go get ./...
.\make.bat windows # (to install)
.\make.bat unwindows # (to uninstall)
```

&lt;/details&gt;

Usage üí°
----------

If you want to scan only a single target you can use

```console
echo https://edoardottt.com/ | cariddi
```

With multiple targets you can use a file instead, e.g. urls.txt containing:

```console
https://edoardottt.com/
http://testphp.vulnweb.com/
```

For Windows:

- use `powershell.exe -Command &quot;cat urls.txt | .\cariddi.exe&quot;` inside the Command prompt
- or just `cat urls.txt | cariddi.exe` using PowerShell

### Basics

- `cariddi -version` (Print the version)
- `cariddi -h` (Print the help)
- `cariddi -examples` (Print the examples)

### Scan options

- `cat urls.txt | cariddi -intensive` (Crawl searching also subdomains, same as `*.target.com`)
- `cat urls.txt | cariddi -s` (Hunt for secrets)
- `cat urls.txt | cariddi -err` (Hunt for errors in websites)
- `cat urls.txt | cariddi -e` (Hunt for juicy endpoints)
- `cat urls.txt | cariddi -info` (Hunt for useful informations in websites)
- `cat urls.txt | cariddi -ext 2` (Hunt for juicy (level 2 out of 7) files)
- `cat urls.txt | cariddi -e -ef endpoints_file` (Hunt for custom endpoints)
- `cat urls.txt | cariddi -s -sf secrets_file` (Hunt for custom secrets)
- `cat urls.txt | cariddi -ie pdf,png,jpg` (Ignore these extensions while scanning)

Default: png, svg, jpg, jpeg, bmp, jfif, gif, webp, woff, woff2, ttf, tiff, tif are ignored while scanning for secrets, info and errors.

### Configuration

- `cat urls.txt | cariddi -proxy http://127.0.0.1:8080` (Set a Proxy, http and socks5 supported)
- `cat urls.txt | cariddi -d 2` (2 seconds between a page crawled and another)
- `cat urls.txt | cariddi -c 200` (Set the concurrency level to 200)
- `cat urls.txt | cariddi -i forum,blog,community,open` (Ignore urls containing these words)
- `cat urls.txt | cariddi -it ignore_file` (Ignore urls containing at least one line in the input file)
- `cat urls.txt | cariddi -cache` (Use the .cariddi_cache folder as cache)
- `cat urls.txt | cariddi -t 5` (Set the timeout for the requests)
- `cat urls.txt | cariddi -headers &quot;Cookie: auth=admin;type=2;; X-Custom: customHeader&quot;`
- `cat urls.txt | cariddi -headersfile headers.txt` (Read from an external file custom headers)
- `cat urls.txt | cariddi -ua &quot;Custom User Agent&quot;` (Use a custom User Agent)
- `cat urls.txt | cariddi -rua` (Use a random browser user agent on every request)

### Output

- `cat urls.txt | cariddi -plain` (Print only results)
- `cat urls.txt | cariddi -ot target_name` (Results in txt file)
- `cat urls.txt | cariddi -oh target_name` (Results in html file)
- `cat urls.txt | cariddi -json` (Print the output as JSON in stdout)
- `cat urls.txt | cariddi -sr` (Store HTTP responses)
- `cat urls.txt | cariddi -debug` (Print debug information while crawling)
- `cat urls.txt | cariddi -md 3` (Max 3 depth levels)

Get Started üéâ
----------

`cariddi -h` prints the help.

```console
Usage of cariddi:
  -c int
     Concurrency level. (default 20)
  -cache
     Use the .cariddi_cache folder as cache.
  -d int
     Delay between a page crawled and another.
  -debug
     Print debug information while crawling.
  -e Hunt for juicy endpoints.
  -ef string
     Use an external file (txt, one per line) to use custom parameters for endpoints hunting.
  -err
     Hunt for errors in websites.
  -examples
     Print the examples.
  -ext int
     Hunt for juicy file extensions. Integer from 1(juicy) to 7(not juicy).
  -h Print the help.
  -headers string
     Use custom headers for each request E.g. -headers &quot;Cookie: auth=yes;;Client: type=2&quot;.
  -headersfile string
     Read from an external file custom headers (same format of headers flag).
  -json
     Print the output as JSON in stdout.
  -md
     Maximum depth level the crawler will follow from the initial target URL.
  -i string
     Ignore the URL containing at least one of the elements of this array.
  -ie value
     Comma-separated list of extensions to ignore while scanning.
  -info
     Hunt for useful informations in websites.
  -intensive
     Crawl searching for resources matching 2nd level domain.
  -it string
     Ignore the URL containing at least one of the lines of this file.
  -oh string
     Write the output into an HTML file.
  -ot string
     Write the output into a TXT file.
  -plain
     Print only the results.
  -proxy string
     Set a Proxy to be used (http and socks5 supported).
  -rua
     Use a random browser user agent on every request.
  -s Hunt for secrets.
  -sf string
     Use an external file (txt, one per line) to use custom regexes for secrets hunting.
  -sr
     Store HTTP responses.
  -t int
     Set timeout for the requests. (default 10)
  -ua string
     Use a custom User Agent.
  -version
     Print the version.
```

&lt;details&gt;
  &lt;summary&gt;Click to understand &lt;strong&gt;How to integrate cariddi with Burpsuite&lt;/strong&gt;&lt;/summary&gt;

   Normally you use Burpsuite within your browser, so you just have to trust the burpsuite&#039;s certificate in the browser and you&#039;re done.  
   In order to use cariddi with the BurpSuite proxy you should do some steps further.  

   If you try to use cariddi with the option `-proxy http://127.0.0.1:8080` you will find this error in the burpsuite error log section:  

   ```bash
   Received fatal alert: bad_certificate (or something similar related to the certificate).
   ```

   To make cariddi working fine with Burpsuite you have also to trust the certificate within your entire pc, not just only the browser. These are the steps you have to follow:

   Go to Proxy tab in Bupsuite, then Options. Click on the CA Certificate button and export Certificate in DER format  

   ```bash
   openssl x509 -in burp.der -inform DER -out burp.pem -outform PEM
   sudo chown root:root burp.pem
   sudo chmod 644 burp.pem
   sudo cp burp.pem /usr/local/share/ca-certificates/
   sudo c_rehash
   cd /etc/ssl/certs/
   sudo ln -s /usr/local/share/ca-certificates/burp.pem
   sudo c_rehash .
   ```

   Source: Trust Burp Proxy certificate in Debian/Ubuntu  

   After these steps, in order to use cariddi with Burpsuite you have to:  

   1. Open Burpsuite, making sure that the proxy is listening.  
   2. Use cariddi with the flag `-proxy http://127.0.0.1:8080`.  
   3. You will see that requests and responses will be logged in Burpsuite.

&lt;/details&gt;

Changelog üìå
-------

Detailed changes for each release are documented in the [release notes](https://github.com/edoardottt/cariddi/releases).

Contributing üõ†
-------

Just open an [issue](https://github.com/edoardottt/cariddi/issues)/[pull request](https://github.com/edoardottt/cariddi/pulls).

Before opening a pull request, download [golangci-lint](https://golangci-lint.run/usage/install/) and run

```console
golangci-lint run
```

If there aren&#039;t errors, go ahead :)

Test using [https://edoardottt.github.io/cariddi-test/](https://edoardottt.github.io/cariddi-test/)

```console
echo &quot;https://edoardottt.github.io/cariddi-test/&quot; | cariddi
```

**Help me build this!**

Special thanks to: [go-colly](http://go-colly.org/), [ocervell](https://github.com/ocervell), [zricethezav](https://github.com/gitleaks/gitleaks/blob/master/config/gitleaks.toml), [projectdiscovery](https://github.com/projectdiscovery/nuclei-templates/tree/master/file/keys), [tomnomnom](https://github.com/tomnomnom/gf/tree/master/examples), [RegexPassive](https://github.com/hahwul/RegexPassive) and [all the contributors](https://github.com/edoardottt/cariddi/graphs/contributors).

License üìù
-------

This repository is under [GNU General Public License v3.0](https://github.com/edoardottt/cariddi/blob/main/LICENSE).  
[edoardottt.com](https://edoardottt.com/) to contact me.
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[safing/portmaster]]></title>
            <link>https://github.com/safing/portmaster</link>
            <guid>https://github.com/safing/portmaster</guid>
            <pubDate>Wed, 14 Jan 2026 00:05:26 GMT</pubDate>
            <description><![CDATA[üèî Love Freedom - ‚ùå Block Mass Surveillance]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/safing/portmaster">safing/portmaster</a></h1>
            <p>üèî Love Freedom - ‚ùå Block Mass Surveillance</p>
            <p>Language: Go</p>
            <p>Stars: 11,738</p>
            <p>Forks: 433</p>
            <p>Stars today: 27 stars today</p>
            <h2>README</h2><pre># Get Peace of Mind &lt;br&gt; with [Easy Privacy](https://safing.io/)

Portmaster is a free and open-source application firewall that does the heavy lifting for you.
Restore privacy and take back control over all your computer&#039;s network activity.

With great defaults your privacy improves without any effort. And if you want to configure and control everything down to the last detail - Portmaster has you covered too. Developed in the EU üá™üá∫, Austria.

__[Download for Free](https://safing.io/download/)__

__[About Us](https://safing.io/about/)__

![Portmaster User Interface](https://safing.io/assets/img/page-specific/landing/portmaster-thumbnail.png?)

_seen on:_  

[&lt;img src=&quot;https://safing.io/assets/img/external/heise_online.svg&quot; height=&quot;35&quot;&gt;](https://www.heise.de/tests/Datenschutz-Firewall-Portmaster-im-Test-9611687.html)
&amp;nbsp;&amp;nbsp;&amp;nbsp;
[![ghacks.net](https://safing.io/assets/img/external/ghacks.png)](https://www.ghacks.net/2022/11/08/portmaster-1-0-released-open-source-application-firewall/)
&amp;nbsp;&amp;nbsp;&amp;nbsp;
[![Techlore](https://safing.io/assets/img/external/techlore.png)](https://www.youtube.com/watch?v=E8cTRhGtmcM)
&amp;nbsp;&amp;nbsp;&amp;nbsp;
[![Lifehacker](https://safing.io/assets/img/external/logos/lifehacker.webp)](https://lifehacker.com/the-lesser-known-apps-everyone-should-install-on-a-new-1850223434)

## [Features](https://safing.io/features/)

1. Monitor All Network Activity
2. Full Control: Block Anything
3. Automatically Block Trackers &amp; Malware
4. Set Global &amp; Per‚ÄëApp Settings
5. Secure DNS (Doh/DoT)
6. Record and Search Network Activity ([$](https://safing.io/pricing/))
7. Per-App Bandwidth Usage ([$](https://safing.io/pricing/))
8. [SPN, our Next-Gen Privacy Network](https://safing.io/spn/) ([$$](https://safing.io/pricing/))

# Technical Introduction

Portmaster is a privacy suite for your Windows and Linux desktop.

### Base Technology

- Portmaster integrates into network stack using nfqueue on Linux and a kernel driver (WFP) on Windows.
- Packets are intercepted at the raw packet level - every packet is seen and can be stopped.
- Ownership of connections is found using eBPF and `/proc` on Linux and a kernel driver and the IP Helper API (`iphlpapi.dll`) on Windows.
- Most settings can be defined per app, which can be matched in different ways.
- Support for special processes with weird or concealed paths/actors:
  - Snap, AppImage and Script support on Linux
  - Windows Store apps and svchost.exe system services support on Windows
- Everything is 100% local on your device. (except the SPN, naturally)
  - Updates are fully signed and downloaded automatically.
  - Intelligence data (block lists, geoip) is downloaded and applied automatically.
- The Portmaster Core Service runs as a system service, the UI elements (App, Notifier) run in user context.
- The main UI still uses electron as a wrapper :/ - but this will change in the future. You can also open the UI in the browser

### Feature: Secure DNS

- Portmaster intercepts &quot;astray&quot; DNS queries and reroutes them to itself for seamless integration.
- DNS queries are resolved by the default or configured DoT/DoH resolvers.
- Full support for split horizon and horizon validation to defend against rebinding attacks.

### Feature: Privacy Filter

- Define allowed network scopes: Localhost, LAN, Internet, P2P, Inbound.
- Easy rules based on Internet entities: Domain, IP, Country and more.
- Filter Lists block common malware, ad, tracker domains etc.

### Feature: Network History ($)

- Record connections and their details in a local database and search all of it later
- Auto-delete old history or delete on demand

### Feature: Bandwidth Visibility ($)

- Monitor bandwidth usage per connection and app

### Feature: SPN - Safing Privacy Network ($$)

- A Privacy Network aimed at use cases &quot;between&quot; VPN and Tor.
- Uses onion encryption over multiple hops just like Tor.
- Routes are chosen to cover most distance within the network to increase privacy.
- Exits are chosen near the destination server. This automatically geo-unblocks in many cases.
- Exclude apps and domains/entities from using SPN.
- Change routing algorithm and focus per app.
- Nodes are hosted by Safing (company behind Portmaster) and the community.
- Speeds are pretty decent (&gt;100MBit/s).
- Further Reading: [SPN Whitepaper](https://safing.io/files/whitepaper/Gate17.pdf)

## Documentation

All details and guides in the dedicated [wiki](https://wiki.safing.io/)

- [Getting Started](https://wiki.safing.io/en/Portmaster/App)
- Install
  - [on Windows](https://wiki.safing.io/en/Portmaster/Install/Windows)
  - [on Linux](https://wiki.safing.io/en/Portmaster/Install/Linux)
- [Contribute](https://wiki.safing.io/en/Contribute)
- [VPN Compatibility](https://wiki.safing.io/en/Portmaster/App/Compatibility#vpn-compatibly)
- [Software Compatibility](https://wiki.safing.io/en/Portmaster/App/Compatibility)
- [Architecture](https://wiki.safing.io/en/Portmaster/Architecture)
- [Settings Handbook](https://docs.safing.io/portmaster/settings)
- [Portmaster Developer API](https://docs.safing.io/portmaster/api)

# Build Portmaster Yourself (WIP)

1. [Install Earthly CLI](https://earthly.dev/get-earthly)
2. [Install Docker Engine](https://docs.docker.com/engine/install/)
3. Run `earthly +release`
4. Find artifacts in `./dist`
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[projectcalico/calico]]></title>
            <link>https://github.com/projectcalico/calico</link>
            <guid>https://github.com/projectcalico/calico</guid>
            <pubDate>Wed, 14 Jan 2026 00:05:25 GMT</pubDate>
            <description><![CDATA[Cloud native networking and network security]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/projectcalico/calico">projectcalico/calico</a></h1>
            <p>Cloud native networking and network security</p>
            <p>Language: Go</p>
            <p>Stars: 6,995</p>
            <p>Forks: 1,526</p>
            <p>Stars today: 2 stars today</p>
            <h2>README</h2><pre>[![Go Report Card](https://goreportcard.com/badge/github.com/projectcalico/calico)](https://goreportcard.com/report/github.com/projectcalico/calico)
[![ArtifactHub](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/tigera-operator)](https://artifacthub.io/packages/helm/projectcalico/tigera-operator)
[![License](https://img.shields.io/badge/license-Apache-blue.svg)](calico/LICENSE)
[![GoPkg](https://pkg.go.dev/badge/k8s.io/kubernetes.svg)](https://pkg.go.dev/github.com/projectcalico/api)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/6064/badge)](https://bestpractices.coreinfrastructure.org/projects/6064)

&lt;div align=center&gt;
&lt;h1&gt;Calico&lt;/h1&gt;
&lt;h2&gt;
&lt;a href=&quot;https://projectcalico.docs.tigera.io/getting-started/kubernetes/quickstart&quot;&gt;Quickstart&lt;/a&gt; |
&lt;a href=&quot;https://projectcalico.docs.tigera.io&quot;&gt;Docs&lt;/a&gt; |
&lt;a href=&quot;CONTRIBUTING.md&quot;&gt;Contribute&lt;/a&gt; |
&lt;a href=&quot;https://slack.projectcalico.org&quot;&gt;Slack&lt;/a&gt; |
&lt;a href=&quot;https://github.com/projectcalico/calico/releases&quot;&gt;Releases&lt;/a&gt;
&lt;/h2&gt;
&lt;/div&gt;

## üêæ Welcome to Project Calico!

Project Calico, created and maintained by [Tigera][tigera], is an open-source project with an active development and user community. Calico Open Source has grown to be the most widely adopted solution for container networking and security, powering 8M+ nodes daily across 166 countries.

## üåü Why use Calico?

- **Data Plane Choice**: eBPF, standard Linux, Windows, and VPP ‚Äî versatility in network solutions.
- **Interoperability**: Works across multiple distros, multiple clouds, bare metal, and VMs.
- **Optimized Performance**: Engineered for high speed and low CPU usage, maximizing your cluster investments.
- **Scalable Architecture**: Grows seamlessly with your Kubernetes clusters without sacrificing performance.
- **Advanced Security**: Get granular access controls and WireGuard encryption.
- **Kubernetes Networking Policy Support**: Continually defining excellence in Kubernetes network policy standards and support.
- **Vibrant Contributor Community**: Over 200 contributors from a wide array of global companies.
- **Flexible networking**: An array of networking tools at your disposal, including BGP, VXLAN, service advertisement, and more.

&lt;div align=center&gt;
&lt;img src=&quot;https://www.tigera.io/app/uploads/2024/02/Ecosystem_shrunken_2023.svg&quot;&gt;
&lt;/div&gt;

## ü§ù Join the Calico Community

- [Calico Big Cats][big-cats]: Become an ambassador and share your journey
- [Community Meetings][community-meetings]: Engage and contribute
- [Contribute on GitHub][first-issues]: Start with &#039;good first issues&#039;
- [Connect on Slack][slack]: Join the conversation with fellow contributors and our developers

## üí° Contributing to Project Calico

- [Get Started with Project Calico][get-started]
- [Repositories][repos]
- [Contribute to our docs][docs-contrib]
- Documentation: [Dive into our training and resources][resources]
- [Make Calico better][issues]

## üõ†Ô∏è Projects We Maintain

- [Calico Golang API][api]
- [Calico operator][operator]
- [VPP dataplane][vpp]
- [Calico BIRD][bird]

## üì¢ Stay Connected

- Subscribe: [Join our newsletter][news]
- [YouTube channel for updates &amp; tutorials][youtube]
- [Technical Blog][blog]
- [Careers][join]: Passionate about open source? Join our team.

[tigera]: https://www.tigera.io/
[big-cats]: https://www.tigera.io/project-calico/calico-big-cats-ambassador-program/#meet-calico-big-cats
[community-meetings]: https://calendar.google.com/calendar/u/0/embed?src=tigera.io_uunmavdev5ndovf0hc4frtl0i0@group.calendar.google.com
[first-issues]: https://github.com/projectcalico/calico/labels/good%20first%20issue
[slack]: https://slack.projectcalico.org/
[get-started]: https://docs.tigera.io/calico/latest/about
[repos]: https://github.com/orgs/projectcalico/repositories
[docs-contrib]: https://github.com/projectcalico/calico/blob/master/CONTRIBUTING_DOCS.md
[resources]: https://docs.tigera.io/calico/latest/about/training-resources
[issues]: https://github.com/projectcalico/calico/issues
[api]: https://github.com/projectcalico/api
[operator]: https://github.com/tigera/operator
[vpp]: https://github.com/projectcalico/vpp-dataplane
[news]: https://www.tigera.io/project-calico/#:~:text=Join%20Calico%20Open%20Source%20community%20newsletter
[youtube]: https://www.youtube.com/channel/UCFpTnXDNcBoXI4gqCDmegFA
[blog]: https://www.tigera.io/blog/?_sft_category=technical-blog
[join]: https://www.tigera.io/careers/
[bird]: https://github.com/projectcalico/bird
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[kubernetes/dashboard]]></title>
            <link>https://github.com/kubernetes/dashboard</link>
            <guid>https://github.com/kubernetes/dashboard</guid>
            <pubDate>Wed, 14 Jan 2026 00:05:24 GMT</pubDate>
            <description><![CDATA[General-purpose web UI for Kubernetes clusters]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/kubernetes/dashboard">kubernetes/dashboard</a></h1>
            <p>General-purpose web UI for Kubernetes clusters</p>
            <p>Language: Go</p>
            <p>Stars: 15,419</p>
            <p>Forks: 4,290</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre># Kubernetes Dashboard

[![Go Report Card](https://goreportcard.com/badge/github.com/kubernetes/dashboard)](https://goreportcard.com/report/github.com/kubernetes/dashboard)
[![Coverage Status](https://codecov.io/github/kubernetes/dashboard/coverage.svg?branch=master)](https://codecov.io/github/kubernetes/dashboard?branch=master)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/kubernetes/dashboard/blob/master/LICENSE)

## IMPORTANT

**This project is now archived and no longer maintained due to lack of active maintainers and contributors.**

Thank you to everyone who used, starred, or contributed to this project! Feel free to fork this repository if you want to continue development yourself.

Please consider using **[Headlamp](https://github.com/kubernetes-sigs/headlamp)** instead. It was recently moved under the sig-ui. From the creators:
&gt; Headlamp is an easy-to-use and extensible Kubernetes web UI.

## Introduction

Kubernetes Dashboard is a general purpose, web-based UI for Kubernetes clusters. It allows users to manage applications running in the cluster and troubleshoot them, as well as manage the cluster itself.

As of version 7.0.0, we have dropped support for Manifest-based installation. Only Helm-based installation is supported now. Due to multi-container setup and hard dependency on Kong gateway API proxy
it would not be feasible to easily support Manifest-based installation.

Additionally, we have changed the versioning scheme and dropped `appVersion` from Helm chart. It is because, with a multi-container setup, every module is now versioned separately. Helm chart version
can be considered an app version now.

![Dashboard UI workloads page](docs/images/overview.png)

## Installation

Kubernetes Dashboard supports only Helm-based installation currently as it is faster and gives us better control
over all dependencies required by Dashboard to run. We now use a single-container, DBless [Kong](https://hub.docker.com/r/kong/kong-gateway) installation
as a gateway that connects all our containers and exposes the UI. Users can then use any ingress controller or proxy
in front of kong gateway. To find out more about ways to customize your installation check out [helm chart values](charts/kubernetes-dashboard/values.yaml).

In order to install Kubernetes Dashboard simply run:
```console
# Add kubernetes-dashboard repository
helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
# Deploy a Helm Release named &quot;kubernetes-dashboard&quot; using the kubernetes-dashboard chart
helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard
```

For more information about our Helm chart visit [ArtifactHub](https://artifacthub.io/packages/helm/k8s-dashboard/kubernetes-dashboard).

## Documentation

Dashboard documentation can be found in the [docs](docs/README.md) directory which contains:

* [Common](docs/common/README.md): Entry-level overview.
* [User Guide](docs/user/README.md): Helpful information for users.
* [How to access Dashboard](docs/user/accessing-dashboard/README.md) - Everything you need to know to get access to you Kubernetes Dashboard instance after installation.
* [Access Control](docs/user/access-control/README.md): Find out how to control access to your Kubernetes Dashboard and [create sample user](docs/user/access-control/creating-sample-user.md) that can be used to log in.
* [Developer Guide](DEVELOPMENT.md): Important information for contributors that would like to test, run and work on Dashboard locally.

## Community, discussion, contribution, and support

Learn how to engage with the Kubernetes community on the [community page](http://kubernetes.io/community/).

You can reach the maintainers of this project at:

* [**#sig-ui on Kubernetes Slack**](https://kubernetes.slack.com)
* [**kubernetes-sig-ui mailing list** ](https://groups.google.com/forum/#!forum/kubernetes-sig-ui)
* [**Issue tracker**](https://github.com/kubernetes/dashboard/issues)
* [**SIG info**](https://github.com/kubernetes/community/tree/master/sig-ui)
* [**Roles**](ROLES.md)

### Contribution

Learn how to start contributing to the [Contributing Guideline](CONTRIBUTING.md).

### Code of conduct

Participation in the Kubernetes community is governed by the [Kubernetes Code of Conduct](code-of-conduct.md).

## License

[Apache License 2.0](https://github.com/kubernetes/dashboard/blob/master/LICENSE)

----
_Copyright 2019 [The Kubernetes Dashboard Authors](https://github.com/kubernetes/dashboard/graphs/contributors)_
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[jackc/pgx]]></title>
            <link>https://github.com/jackc/pgx</link>
            <guid>https://github.com/jackc/pgx</guid>
            <pubDate>Wed, 14 Jan 2026 00:05:23 GMT</pubDate>
            <description><![CDATA[PostgreSQL driver and toolkit for Go]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jackc/pgx">jackc/pgx</a></h1>
            <p>PostgreSQL driver and toolkit for Go</p>
            <p>Language: Go</p>
            <p>Stars: 13,159</p>
            <p>Forks: 982</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre>[![Go Reference](https://pkg.go.dev/badge/github.com/jackc/pgx/v5.svg)](https://pkg.go.dev/github.com/jackc/pgx/v5)
[![Build Status](https://github.com/jackc/pgx/actions/workflows/ci.yml/badge.svg)](https://github.com/jackc/pgx/actions/workflows/ci.yml)

# pgx - PostgreSQL Driver and Toolkit

pgx is a pure Go driver and toolkit for PostgreSQL.

The pgx driver is a low-level, high performance interface that exposes PostgreSQL-specific features such as `LISTEN` /
`NOTIFY` and `COPY`. It also includes an adapter for the standard `database/sql` interface.

The toolkit component is a related set of packages that implement PostgreSQL functionality such as parsing the wire protocol
and type mapping between PostgreSQL and Go. These underlying packages can be used to implement alternative drivers,
proxies, load balancers, logical replication clients, etc.

## Example Usage

```go
package main

import (
	&quot;context&quot;
	&quot;fmt&quot;
	&quot;os&quot;

	&quot;github.com/jackc/pgx/v5&quot;
)

func main() {
	// urlExample := &quot;postgres://username:password@localhost:5432/database_name&quot;
	conn, err := pgx.Connect(context.Background(), os.Getenv(&quot;DATABASE_URL&quot;))
	if err != nil {
		fmt.Fprintf(os.Stderr, &quot;Unable to connect to database: %v\n&quot;, err)
		os.Exit(1)
	}
	defer conn.Close(context.Background())

	var name string
	var weight int64
	err = conn.QueryRow(context.Background(), &quot;select name, weight from widgets where id=$1&quot;, 42).Scan(&amp;name, &amp;weight)
	if err != nil {
		fmt.Fprintf(os.Stderr, &quot;QueryRow failed: %v\n&quot;, err)
		os.Exit(1)
	}

	fmt.Println(name, weight)
}
```

See the [getting started guide](https://github.com/jackc/pgx/wiki/Getting-started-with-pgx) for more information.

## Features

* Support for approximately 70 different PostgreSQL types
* Automatic statement preparation and caching
* Batch queries
* Single-round trip query mode
* Full TLS connection control
* Binary format support for custom types (allows for much quicker encoding/decoding)
* `COPY` protocol support for faster bulk data loads
* Tracing and logging support
* Connection pool with after-connect hook for arbitrary connection setup
* `LISTEN` / `NOTIFY`
* Conversion of PostgreSQL arrays to Go slice mappings for integers, floats, and strings
* `hstore` support
* `json` and `jsonb` support
* Maps `inet` and `cidr` PostgreSQL types to `netip.Addr` and `netip.Prefix`
* Large object support
* NULL mapping to pointer to pointer
* Supports `database/sql.Scanner` and `database/sql/driver.Valuer` interfaces for custom types
* Notice response handling
* Simulated nested transactions with savepoints

## Choosing Between the pgx and database/sql Interfaces

The pgx interface is faster. Many PostgreSQL specific features such as `LISTEN` / `NOTIFY` and `COPY` are not available
through the `database/sql` interface.

The pgx interface is recommended when:

1. The application only targets PostgreSQL.
2. No other libraries that require `database/sql` are in use.

It is also possible to use the `database/sql` interface and convert a connection to the lower-level pgx interface as needed.

## Testing

See [CONTRIBUTING.md](./CONTRIBUTING.md) for setup instructions.

## Architecture

See the presentation at Golang Estonia, [PGX Top to Bottom](https://www.youtube.com/watch?v=sXMSWhcHCf8) for a description of pgx architecture.

## Supported Go and PostgreSQL Versions

pgx supports the same versions of Go and PostgreSQL that are supported by their respective teams. For [Go](https://golang.org/doc/devel/release.html#policy) that is the two most recent major releases and for [PostgreSQL](https://www.postgresql.org/support/versioning/) the major releases in the last 5 years. This means pgx supports Go 1.24 and higher and PostgreSQL 13 and higher. pgx also is tested against the latest version of [CockroachDB](https://www.cockroachlabs.com/product/).

## Version Policy

pgx follows semantic versioning for the documented public API on stable releases. `v5` is the latest stable major version.

## PGX Family Libraries

### [github.com/jackc/pglogrepl](https://github.com/jackc/pglogrepl)

pglogrepl provides functionality to act as a client for PostgreSQL logical replication.

### [github.com/jackc/pgmock](https://github.com/jackc/pgmock)

pgmock offers the ability to create a server that mocks the PostgreSQL wire protocol. This is used internally to test pgx by purposely inducing unusual errors. pgproto3 and pgmock together provide most of the foundational tooling required to implement a PostgreSQL proxy or MitM (such as for a custom connection pooler).

### [github.com/jackc/tern](https://github.com/jackc/tern)

tern is a stand-alone SQL migration system.

### [github.com/jackc/pgerrcode](https://github.com/jackc/pgerrcode)

pgerrcode contains constants for the PostgreSQL error codes.

## Adapters for 3rd Party Types

* [github.com/jackc/pgx-gofrs-uuid](https://github.com/jackc/pgx-gofrs-uuid)
* [github.com/jackc/pgx-shopspring-decimal](https://github.com/jackc/pgx-shopspring-decimal)
* [github.com/twpayne/pgx-geos](https://github.com/twpayne/pgx-geos) ([PostGIS](https://postgis.net/) and [GEOS](https://libgeos.org/) via [go-geos](https://github.com/twpayne/go-geos))
* [github.com/vgarvardt/pgx-google-uuid](https://github.com/vgarvardt/pgx-google-uuid)


## Adapters for 3rd Party Tracers

* [github.com/jackhopner/pgx-xray-tracer](https://github.com/jackhopner/pgx-xray-tracer)
* [github.com/exaring/otelpgx](https://github.com/exaring/otelpgx)

## Adapters for 3rd Party Loggers

These adapters can be used with the tracelog package.

* [github.com/jackc/pgx-go-kit-log](https://github.com/jackc/pgx-go-kit-log)
* [github.com/jackc/pgx-log15](https://github.com/jackc/pgx-log15)
* [github.com/jackc/pgx-logrus](https://github.com/jackc/pgx-logrus)
* [github.com/jackc/pgx-zap](https://github.com/jackc/pgx-zap)
* [github.com/jackc/pgx-zerolog](https://github.com/jackc/pgx-zerolog)
* [github.com/mcosta74/pgx-slog](https://github.com/mcosta74/pgx-slog)
* [github.com/kataras/pgx-golog](https://github.com/kataras/pgx-golog)

## 3rd Party Libraries with PGX Support

### [github.com/pashagolub/pgxmock](https://github.com/pashagolub/pgxmock)

pgxmock is a mock library implementing pgx interfaces.
pgxmock has one and only purpose - to simulate pgx behavior in tests, without needing a real database connection.

### [github.com/georgysavva/scany](https://github.com/georgysavva/scany)

Library for scanning data from a database into Go structs and more.

### [github.com/vingarcia/ksql](https://github.com/vingarcia/ksql)

A carefully designed SQL client for making using SQL easier,
more productive, and less error-prone on Golang.

### [github.com/otan/gopgkrb5](https://github.com/otan/gopgkrb5)

Adds GSSAPI / Kerberos authentication support.

### [github.com/wcamarao/pmx](https://github.com/wcamarao/pmx)

Explicit data mapping and scanning library for Go structs and slices.

### [github.com/stephenafamo/scan](https://github.com/stephenafamo/scan)

Type safe and flexible package for scanning database data into Go types.
Supports, structs, maps, slices and custom mapping functions.

### [github.com/z0ne-dev/mgx](https://github.com/z0ne-dev/mgx)

Code first migration library for native pgx (no database/sql abstraction).

### [github.com/amirsalarsafaei/sqlc-pgx-monitoring](https://github.com/amirsalarsafaei/sqlc-pgx-monitoring)

A database monitoring/metrics library for pgx and sqlc. Trace, log and monitor your sqlc query performance using OpenTelemetry.

### [https://github.com/nikolayk812/pgx-outbox](https://github.com/nikolayk812/pgx-outbox)

Simple Golang implementation for transactional outbox pattern for PostgreSQL using jackc/pgx driver.

### [https://github.com/Arlandaren/pgxWrappy](https://github.com/Arlandaren/pgxWrappy)

Simplifies working with the pgx library, providing convenient scanning of nested structures.

### [https://github.com/KoNekoD/pgx-colon-query-rewriter](https://github.com/KoNekoD/pgx-colon-query-rewriter)

Implementation of the pgx query rewriter to use &#039;:&#039; instead of &#039;@&#039; in named query parameters.
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[usememos/memos]]></title>
            <link>https://github.com/usememos/memos</link>
            <guid>https://github.com/usememos/memos</guid>
            <pubDate>Wed, 14 Jan 2026 00:05:22 GMT</pubDate>
            <description><![CDATA[An open-source, self-hosted note-taking service. Your thoughts, your data, your control ‚Äî no tracking, no ads, no subscription fees.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/usememos/memos">usememos/memos</a></h1>
            <p>An open-source, self-hosted note-taking service. Your thoughts, your data, your control ‚Äî no tracking, no ads, no subscription fees.</p>
            <p>Language: Go</p>
            <p>Stars: 53,880</p>
            <p>Forks: 3,866</p>
            <p>Stars today: 411 stars today</p>
            <h2>README</h2><pre># Memos

&lt;img align=&quot;right&quot; height=&quot;96px&quot; src=&quot;https://raw.githubusercontent.com/usememos/.github/refs/heads/main/assets/logo-rounded.png&quot; alt=&quot;Memos&quot; /&gt;

An open-source, self-hosted note-taking service. Your thoughts, your data, your control ‚Äî no tracking, no ads, no subscription fees.

[![Home](https://img.shields.io/badge/üè†-usememos.com-blue?style=flat-square)](https://usememos.com)
[![Live Demo](https://img.shields.io/badge/‚ú®-Try%20Demo-orange?style=flat-square)](https://demo.usememos.com/)
[![Docs](https://img.shields.io/badge/üìö-Documentation-green?style=flat-square)](https://usememos.com/docs)
[![Discord](https://img.shields.io/badge/üí¨-Discord-5865f2?style=flat-square&amp;logo=discord&amp;logoColor=white)](https://discord.gg/tfPJa4UmAv)
[![Docker Pulls](https://img.shields.io/docker/pulls/neosmemo/memos?style=flat-square&amp;logo=docker)](https://hub.docker.com/r/neosmemo/memos)

&lt;img src=&quot;https://raw.githubusercontent.com/usememos/.github/refs/heads/main/assets/demo.png&quot; alt=&quot;Memos Demo Screenshot&quot; height=&quot;512&quot; /&gt;

### üíé Featured Sponsors

[**Warp** ‚Äî The AI-powered terminal built for speed and collaboration](https://go.warp.dev/memos)

&lt;a href=&quot;https://go.warp.dev/memos&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/warpdotdev/brand-assets/main/Github/Sponsor/Warp-Github-LG-02.png&quot; alt=&quot;Warp - The AI-powered terminal built for speed and collaboration&quot; width=&quot;512&quot; /&gt;
&lt;/a&gt;

---

[**LambdaTest** - Cross-browser testing cloud](https://www.lambdatest.com/?utm_source=memos&amp;utm_medium=sponsor)
  
&lt;a href=&quot;https://www.lambdatest.com/?utm_source=memos&amp;utm_medium=sponsor&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
  &lt;img src=&quot;https://www.lambdatest.com/blue-logo.png&quot; alt=&quot;LambdaTest - Cross-browser testing cloud&quot; height=&quot;50&quot; /&gt;
&lt;/a&gt;

## Overview

Memos is a privacy-first, self-hosted knowledge base that works seamlessly for personal notes, team wikis, and knowledge management. Built with Go and React, it offers lightning-fast performance without compromising on features or usability.

**Why choose Memos over cloud services?**

| Feature           | Memos                          | Cloud Services                |
| ----------------- | ------------------------------ | ----------------------------- |
| **Privacy**       | ‚úÖ Self-hosted, zero telemetry | ‚ùå Your data on their servers |
| **Cost**          | ‚úÖ Free forever, MIT license   | ‚ùå Subscription fees          |
| **Performance**   | ‚úÖ Instant load, no latency    | ‚ö†Ô∏è Depends on internet        |
| **Ownership**     | ‚úÖ Full control &amp; export       | ‚ùå Vendor lock-in             |
| **API Access**    | ‚úÖ Full REST + gRPC APIs       | ‚ö†Ô∏è Limited or paid            |
| **Customization** | ‚úÖ Open source, forkable       | ‚ùå Closed ecosystem           |

## Features

- **üîí Privacy-First Architecture**

  - Self-hosted on your infrastructure with zero telemetry
  - Complete data ownership and export capabilities
  - No tracking, no ads, no vendor lock-in

- **üìù Markdown Native**

  - Full markdown support
  - Plain text storage ‚Äî take your data anywhere

- **‚ö° Blazing Fast**

  - Built with Go backend and React frontend
  - Optimized for performance at any scale

- **üê≥ Simple Deployment**

  - One-line Docker installation
  - Supports SQLite, MySQL, and PostgreSQL

- **üîó Developer-Friendly**

  - Full REST and gRPC APIs
  - Easy integration with existing workflows

- **üé® Beautiful Interface**
  - Clean, minimal design and dark mode support
  - Mobile-responsive layout

## Quick Start

### Docker (Recommended)

```bash
docker run -d \
  --name memos \
  -p 5230:5230 \
  -v ~/.memos:/var/opt/memos \
  neosmemo/memos:stable
```

Open `http://localhost:5230` and start writing!

### Try the Live Demo

Don&#039;t want to install yet? Try our [live demo](https://demo.usememos.com/) first!

### Other Installation Methods

- **Docker Compose** - Recommended for production deployments
- **Pre-built Binaries** - Available for Linux, macOS, and Windows
- **Kubernetes** - Helm charts and manifests available
- **Build from Source** - For development and customization

See our [installation guide](https://usememos.com/docs/installation) for detailed instructions.

## Contributing

We welcome contributions of all kinds! Whether you&#039;re fixing bugs, adding features, improving documentation, or helping with translations ‚Äî every contribution matters.

**Ways to contribute:**

- üêõ [Report bugs](https://github.com/usememos/memos/issues/new?template=bug_report.md)
- üí° [Suggest features](https://github.com/usememos/memos/issues/new?template=feature_request.md)
- üîß [Submit pull requests](https://github.com/usememos/memos/pulls)
- üìñ [Improve documentation](https://github.com/usememos/memos/tree/main/docs)
- üåç [Help with translations](https://github.com/usememos/memos/tree/main/web/src/locales)

## Sponsors

Love Memos? [Sponsor us on GitHub](https://github.com/sponsors/usememos) to help keep the project growing!

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=usememos/memos&amp;type=Date)](https://star-history.com/#usememos/memos&amp;Date)

## License

Memos is open-source software licensed under the [MIT License](LICENSE).

## Privacy Policy

Memos is built with privacy as a core principle. As a self-hosted application, all your data stays on your infrastructure. There is no telemetry, no tracking, and no data collection. See our [Privacy Policy](https://usememos.com/privacy) for details.

---

**[Website](https://usememos.com)** ‚Ä¢ **[Documentation](https://usememos.com/docs)** ‚Ä¢ **[Demo](https://demo.usememos.com/)** ‚Ä¢ **[Discord](https://discord.gg/tfPJa4UmAv)** ‚Ä¢ **[X/Twitter](https://x.com/usememos)**

&lt;a href=&quot;https://vercel.com/oss&quot;&gt;
  &lt;img alt=&quot;Vercel OSS Program&quot; src=&quot;https://vercel.com/oss/program-badge.svg&quot; /&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[VictoriaMetrics/VictoriaMetrics]]></title>
            <link>https://github.com/VictoriaMetrics/VictoriaMetrics</link>
            <guid>https://github.com/VictoriaMetrics/VictoriaMetrics</guid>
            <pubDate>Wed, 14 Jan 2026 00:05:21 GMT</pubDate>
            <description><![CDATA[VictoriaMetrics: fast, cost-effective monitoring solution and time series database]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/VictoriaMetrics/VictoriaMetrics">VictoriaMetrics/VictoriaMetrics</a></h1>
            <p>VictoriaMetrics: fast, cost-effective monitoring solution and time series database</p>
            <p>Language: Go</p>
            <p>Stars: 15,963</p>
            <p>Forks: 1,536</p>
            <p>Stars today: 14 stars today</p>
            <h2>README</h2><pre># VictoriaMetrics

[![Latest Release](https://img.shields.io/github/v/release/VictoriaMetrics/VictoriaMetrics?sort=semver&amp;label=&amp;filter=!*-victorialogs&amp;logo=github&amp;labelColor=gray&amp;color=gray&amp;link=https%3A%2F%2Fgithub.com%2FVictoriaMetrics%2FVictoriaMetrics%2Freleases%2Flatest)](https://github.com/VictoriaMetrics/VictoriaMetrics/releases)
![Docker Pulls](https://img.shields.io/docker/pulls/victoriametrics/victoria-metrics?label=&amp;logo=docker&amp;logoColor=white&amp;labelColor=2496ED&amp;color=2496ED&amp;link=https%3A%2F%2Fhub.docker.com%2Fr%2Fvictoriametrics%2Fvictoria-metrics)
[![Go Report](https://goreportcard.com/badge/github.com/VictoriaMetrics/VictoriaMetrics?link=https%3A%2F%2Fgoreportcard.com%2Freport%2Fgithub.com%2FVictoriaMetrics%2FVictoriaMetrics)](https://goreportcard.com/report/github.com/VictoriaMetrics/VictoriaMetrics)
[![Build Status](https://github.com/VictoriaMetrics/VictoriaMetrics/actions/workflows/build.yml/badge.svg?branch=master&amp;link=https%3A%2F%2Fgithub.com%2FVictoriaMetrics%2FVictoriaMetrics%2Factions)](https://github.com/VictoriaMetrics/VictoriaMetrics/actions/workflows/build.yml)
[![codecov](https://codecov.io/gh/VictoriaMetrics/VictoriaMetrics/branch/master/graph/badge.svg?link=https%3A%2F%2Fcodecov.io%2Fgh%2FVictoriaMetrics%2FVictoriaMetrics)](https://app.codecov.io/gh/VictoriaMetrics/VictoriaMetrics)
[![License](https://img.shields.io/github/license/VictoriaMetrics/VictoriaMetrics?labelColor=green&amp;label=&amp;link=https%3A%2F%2Fgithub.com%2FVictoriaMetrics%2FVictoriaMetrics%2Fblob%2Fmaster%2FLICENSE)](https://github.com/VictoriaMetrics/VictoriaMetrics/blob/master/LICENSE)
![Slack](https://img.shields.io/badge/Join-4A154B?logo=slack&amp;link=https%3A%2F%2Fslack.victoriametrics.com)
[![X](https://img.shields.io/twitter/follow/VictoriaMetrics?style=flat&amp;label=Follow&amp;color=black&amp;logo=x&amp;labelColor=black&amp;link=https%3A%2F%2Fx.com%2FVictoriaMetrics)](https://x.com/VictoriaMetrics/)
[![Reddit](https://img.shields.io/reddit/subreddit-subscribers/VictoriaMetrics?style=flat&amp;label=Join&amp;labelColor=red&amp;logoColor=white&amp;logo=reddit&amp;link=https%3A%2F%2Fwww.reddit.com%2Fr%2FVictoriaMetrics)](https://www.reddit.com/r/VictoriaMetrics/)

&lt;picture&gt;
  &lt;source srcset=&quot;docs/victoriametrics/logo_white.webp&quot; media=&quot;(prefers-color-scheme: dark)&quot;&gt;
  &lt;source srcset=&quot;docs/victoriametrics/logo.webp&quot; media=&quot;(prefers-color-scheme: light)&quot;&gt;
  &lt;img src=&quot;docs/victoriametrics/logo.webp&quot; width=&quot;300&quot; alt=&quot;VictoriaMetrics logo&quot;&gt;
&lt;/picture&gt;

VictoriaMetrics is a fast, cost-saving, and scalable solution for monitoring and managing time series data. It delivers high performance and reliability, making it an ideal choice for businesses of all sizes.

Here are some resources and information about VictoriaMetrics:

- Documentation: [docs.victoriametrics.com](https://docs.victoriametrics.com)
- Case studies: [Grammarly, Roblox, Wix,...](https://docs.victoriametrics.com/victoriametrics/casestudies/).
- Available: [Binary releases](https://github.com/VictoriaMetrics/VictoriaMetrics/releases/latest), docker images [Docker Hub](https://hub.docker.com/r/victoriametrics/victoria-metrics/) and [Quay](https://quay.io/repository/victoriametrics/victoria-metrics), [Source code](https://github.com/VictoriaMetrics/VictoriaMetrics)
- Deployment types: [Single-node version](https://docs.victoriametrics.com/), [Cluster version](https://docs.victoriametrics.com/victoriametrics/cluster-victoriametrics/), and [Enterprise version](https://docs.victoriametrics.com/victoriametrics/enterprise/)
- Changelog: [CHANGELOG](https://docs.victoriametrics.com/victoriametrics/changelog/), and [How to upgrade](https://docs.victoriametrics.com/victoriametrics/single-server-victoriametrics/#how-to-upgrade-victoriametrics)
- Community: [Slack](https://slack.victoriametrics.com/), [X (Twitter)](https://x.com/VictoriaMetrics), [LinkedIn](https://www.linkedin.com/company/victoriametrics/), [YouTube](https://www.youtube.com/@VictoriaMetrics)

Yes, we open-source both the single-node VictoriaMetrics and the cluster version.

## Prominent features

VictoriaMetrics is optimized for timeseries data, even when old time series are constantly replaced by new ones at a high rate, it offers a lot of features:

* **Long-term storage for Prometheus** or as a drop-in replacement for Prometheus and Graphite in Grafana.
* **Powerful stream aggregation**: Can be used as a StatsD alternative.
* **Ideal for big data**: Works well with large amounts of time series data from APM, Kubernetes, IoT sensors, connected cars, industrial telemetry, financial data and various [Enterprise workloads](https://docs.victoriametrics.com/victoriametrics/enterprise/).
* **Query language**: Supports both PromQL and the more performant MetricsQL.
* **Easy to setup**: No dependencies, single [small binary](https://medium.com/@valyala/stripping-dependency-bloat-in-victoriametrics-docker-image-983fb5912b0d), configuration through command-line flags, but the default is also fine-tuned; backup and restore with [instant snapshots](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282).
* **Global query view**: Multiple Prometheus instances or any other data sources may ingest data into VictoriaMetrics and queried via a single query.
* **Various Protocols**: Support metric scraping, ingestion and backfilling in various protocol.
    * [Prometheus exporters](https://docs.victoriametrics.com/victoriametrics/single-server-victoriametrics/#how-to-scrape-prometheus-exporters-such-as-node-exporter), [Prometheus remote write API](https://docs.victoriametrics.com/victoriametrics/integrations/prometheus/), [Prometheus exposition format](https://docs.victoriametrics.com/victoriametrics/single-server-victoriametrics/#how-to-import-data-in-prometheus-exposition-format).
    * [InfluxDB line protocol](https://docs.victoriametrics.com/victoriametrics/integrations/influxdb/) over HTTP, TCP and UDP.
    * [Graphite plaintext protocol](https://docs.victoriametrics.com/victoriametrics/integrations/graphite/#ingesting) with [tags](https://graphite.readthedocs.io/en/latest/tags.html#carbon).
    * [OpenTSDB put message](https://docs.victoriametrics.com/victoriametrics/integrations/opentsdb/#sending-data-via-telnet).
    * [HTTP OpenTSDB /api/put requests](https://docs.victoriametrics.com/victoriametrics/integrations/opentsdb/#sending-data-via-http).
    * [JSON line format](https://docs.victoriametrics.com/victoriametrics/single-server-victoriametrics/#how-to-import-data-in-json-line-format).
    * [Arbitrary CSV data](https://docs.victoriametrics.com/victoriametrics/single-server-victoriametrics/#how-to-import-csv-data).
    * [Native binary format](https://docs.victoriametrics.com/victoriametrics/single-server-victoriametrics/#how-to-import-data-in-native-format).
    * [DataDog agent or DogStatsD](https://docs.victoriametrics.com/victoriametrics/integrations/datadog/).
    * [NewRelic infrastructure agent](https://docs.victoriametrics.com/victoriametrics/integrations/newrelic/#sending-data-from-agent).
    * [OpenTelemetry metrics format](https://docs.victoriametrics.com/victoriametrics/single-server-victoriametrics/#sending-data-via-opentelemetry).
* **NFS-based storages**: Supports storing data on NFS-based storages such as Amazon EFS, Google Filestore.
* And many other features such as metrics relabeling, cardinality limiter, etc.

## Enterprise version

In addition, the Enterprise version includes extra features:

- **Anomaly detection**: Automation and simplification of your alerting rules, covering complex anomalies found in metrics data.
- **Backup automation**: Automates regular backup procedures.
- **Multiple retentions**: Reducing storage costs by specifying different retentions for different datasets.
- **Downsampling**: Reducing storage costs and increasing performance for queries over historical data.
- **Stable releases** with long-term support lines ([LTS](https://docs.victoriametrics.com/victoriametrics/lts-releases/)).
- **Comprehensive support**: First-class consulting, feature requests and technical support provided by the core VictoriaMetrics dev team.
- Many other features, which you can read about on [the Enterprise page](https://docs.victoriametrics.com/victoriametrics/enterprise/).

[Contact us](mailto:info@victoriametrics.com) if you need enterprise support for VictoriaMetrics. Or you can request a free trial license [here](https://victoriametrics.com/products/enterprise/trial/), downloaded Enterprise binaries are available at [Github Releases](https://github.com/VictoriaMetrics/VictoriaMetrics/releases/latest).

We strictly apply security measures in everything we do. VictoriaMetrics has achieved security certifications for Database Software Development and Software-Based Monitoring Services. See [Security page](https://victoriametrics.com/security/) for more details.

## Benchmarks 

Some good benchmarks VictoriaMetrics achieved:

* **Minimal memory footprint**: handling millions of unique timeseries with [10x less RAM](https://medium.com/@valyala/insert-benchmarks-with-inch-influxdb-vs-victoriametrics-e31a41ae2893) than InfluxDB, up to [7x less RAM](https://valyala.medium.com/prometheus-vs-victoriametrics-benchmark-on-node-exporter-metrics-4ca29c75590f) than Prometheus, Thanos or Cortex.
* **Highly scalable and performance** for [data ingestion](https://medium.com/@valyala/high-cardinality-tsdb-benchmarks-victoriametrics-vs-timescaledb-vs-influxdb-13e6ee64dd6b) and [querying](https://medium.com/@valyala/when-size-matters-benchmarking-victoriametrics-vs-timescale-and-influxdb-6035811952d4), [20x outperforms](https://medium.com/@valyala/insert-benchmarks-with-inch-influxdb-vs-victoriametrics-e31a41ae2893) InfluxDB and TimescaleDB.
* **High data compression**: [70x more data points](https://medium.com/@valyala/when-size-matters-benchmarking-victoriametrics-vs-timescale-and-influxdb-6035811952d4) may be stored into limited storage than TimescaleDB, [7x less storage](https://valyala.medium.com/prometheus-vs-victoriametrics-benchmark-on-node-exporter-metrics-4ca29c75590f) space is required than Prometheus, Thanos or Cortex.
* **Reducing storage costs**: [10x more effective](https://docs.victoriametrics.com/victoriametrics/casestudies/#grammarly) than Graphite according to the Grammarly case study.
* **A single-node VictoriaMetrics** can replace medium-sized clusters built with competing solutions such as Thanos, M3DB, Cortex, InfluxDB or TimescaleDB. See [VictoriaMetrics vs Thanos](https://medium.com/@valyala/comparing-thanos-to-victoriametrics-cluster-b193bea1683), [Measuring vertical scalability](https://medium.com/@valyala/measuring-vertical-scalability-for-time-series-databases-in-google-cloud-92550d78d8ae), [Remote write storage wars - PromCon 2019](https://promcon.io/2019-munich/talks/remote-write-storage-wars/).
* **Optimized for storage**: [Works well with high-latency IO](https://medium.com/@valyala/high-cardinality-tsdb-benchmarks-victoriametrics-vs-timescaledb-vs-influxdb-13e6ee64dd6b) and low IOPS (HDD and network storage in AWS, Google Cloud, Microsoft Azure, etc.).

## Community and contributions

Feel free asking any questions regarding VictoriaMetrics:

* [Slack Inviter](https://slack.victoriametrics.com/) and [Slack channel](https://victoriametrics.slack.com/)
* [X (Twitter)](https://x.com/VictoriaMetrics/)
* [Linkedin](https://www.linkedin.com/company/victoriametrics/)
* [Reddit](https://www.reddit.com/r/VictoriaMetrics/)
* [Telegram-en](https://t.me/VictoriaMetrics_en)
* [Telegram-ru](https://t.me/VictoriaMetrics_ru1)
* [Mastodon](https://mastodon.social/@victoriametrics/)

If you like VictoriaMetrics and want to contribute, then please [read these docs](https://docs.victoriametrics.com/victoriametrics/contributing/).

## VictoriaMetrics Logo

The provided [ZIP file](https://github.com/VictoriaMetrics/VictoriaMetrics/blob/master/VM_logo.zip) contains three folders with different logo orientations. Each folder includes the following file types:

* JPEG: Preview files
* PNG: Preview files with transparent background
* AI: Adobe Illustrator files

### VictoriaMetrics Logo Usage Guidelines

#### Font

* Font Used: Lato Black
* Download here: [Lato Font](https://fonts.google.com/specimen/Lato)

#### Color Palette

* Black [#000000](https://www.color-hex.com/color/000000)
* Purple [#4d0e82](https://www.color-hex.com/color/4d0e82)
* Orange [#ff2e00](https://www.color-hex.com/color/ff2e00)
* White [#ffffff](https://www.color-hex.com/color/ffffff)

### Logo Usage Rules

* Only use the Lato Black font as specified.
* Maintain sufficient clear space around the logo for visibility.
* Do not modify the spacing, alignment, or positioning of design elements.
* You may resize the logo as needed, but ensure all proportions remain intact.

Thank you for your cooperation!
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[opencost/opencost]]></title>
            <link>https://github.com/opencost/opencost</link>
            <guid>https://github.com/opencost/opencost</guid>
            <pubDate>Wed, 14 Jan 2026 00:05:20 GMT</pubDate>
            <description><![CDATA[Cost monitoring for Kubernetes workloads and cloud costs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/opencost/opencost">opencost/opencost</a></h1>
            <p>Cost monitoring for Kubernetes workloads and cloud costs</p>
            <p>Language: Go</p>
            <p>Stars: 6,254</p>
            <p>Forks: 719</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre>[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/6219/badge)](https://www.bestpractices.dev/projects/6219)
[![Gurubase](https://img.shields.io/badge/Gurubase-Ask%20OpenCost%20Guru-006BFF)](https://gurubase.io/g/opencost)

![](./opencost-header.png)

# OpenCost ‚Äî your favorite open source cost monitoring tool for Kubernetes and cloud spend

OpenCost give teams visibility into current and historical Kubernetes and cloud spend and resource allocation.
These models provide cost transparency in Kubernetes environments that support multiple applications, teams, departments, etc.
It also provides visibility into the cloud costs across multiple providers.

OpenCost was originally developed and open sourced by [Kubecost](https://kubecost.com). This project combines a [specification](/spec/) as well as a Golang implementation of these detailed requirements. The web UI is available in the [opencost/opencost-ui](http://github.com/opencost/opencost-ui) repository.

[![OpenCost UI Walkthrough](./ui/src/thumbnail.png)](https://youtu.be/lCP4Ci9Kcdg)
*OpenCost UI Walkthrough*

To see the full functionality of OpenCost you can view [OpenCost features](https://opencost.io). Here is a summary of features enabled:

- Real-time cost allocation by Kubernetes cluster, node, namespace, controller kind, controller, service, or pod
- Multi-cloud cost monitoring for all cloud services on AWS, Azure, GCP
- Dynamic on-demand k8s asset pricing enabled by integrations with AWS, Azure, and GCP billing APIs
- Supports on-prem k8s clusters with custom CSV pricing
- Allocation for in-cluster K8s resources like CPU, GPU, memory, and persistent volumes
- Easily export pricing data to Prometheus with /metrics endpoint ([learn more](https://www.opencost.io/docs/installation/prometheus))
- Carbon costs for cloud resources
- MCP support
- Support for external costs like Datadog through [OpenCost Plugins](https://github.com/opencost/opencost-plugins)
- Free and open source distribution ([Apache2 license](LICENSE))

## Getting Started

OpenCost is now installed and managed via the official Helm chart only.

Quick install on any Kubernetes 1.20+ cluster:

```bash
helm repo add opencost https://opencost.github.io/opencost-helm-chart
helm repo update
helm install opencost opencost/opencost
```

Note: The standalone Kubernetes manifest files have been removed. Please use Helm for all installations and upgrades. See the [Helm installation docs](https://www.opencost.io/docs/installation/install) for details and configuration.

&gt; **Note for sharded Prometheus users:**
&gt; If you run Prometheus in a sharded (HA) setup, set `PROMETHEUS_SERVER_ENDPOINT` to a global query endpoint (e.g., Thanos Query, Cortex, or Mimir). Pointing to a single Prometheus pod may result in incomplete or intermittent export results. See the [Prometheus integration docs](https://www.opencost.io/docs/installation/prometheus) for details.

## Usage

- [Cost APIs](https://www.opencost.io/docs/integrations/api)
- [CLI / kubectl cost](https://www.opencost.io/docs/integrations/kubectl-cost)
- [Prometheus Metrics](https://www.opencost.io/docs/integrations/prometheus)
- [User Interface](https://www.opencost.io/docs/installation/ui)

## MCP Server

The OpenCost MCP (Model Context Protocol) server provides AI agents with access to cost allocation and asset data through a standardized interface. The MCP server is **enabled by default** in all OpenCost deployments, runs on port 8081, and is **built into the Helm chart** for easy production deployment. Users have full control to disable it or configure custom ports and settings.

### Features

- **Enabled by Default**: MCP server starts automatically with OpenCost
- **Full User Control**: Easy to disable or configure port and settings
- **Allocation Queries**: Retrieve cost allocation data with filtering and aggregation
- **Asset Queries**: Access detailed asset information including nodes, disks, load balancers, and more
- **Cloud Cost Queries**: Query cloud cost data with provider, service, and region filtering
- **HTTP Transport**: Uses HTTP for reliable communication with MCP clients
- **Zero Configuration**: Works out of the box with default OpenCost deployment
- **Helm Integration**: Built into the official Helm chart for production deployments

### Quick Start

#### Using Tilt (Development)
```bash
# Clone and start OpenCost with MCP server
git clone https://github.com/opencost/opencost.git
cd opencost
tilt up
```

Tilt configuration notes (cloud costs):

OpenCost&#039;s Tilt values (`tilt-values.yaml`) include extra environment variables to enable Cloud Cost ingestion in dev:

```yaml
# tilt-values.yaml (excerpt)
opencost:
  exporter:
    extraEnv:
      CLOUD_COST_ENABLED: &quot;true&quot;
      CLOUD_COST_CONFIG_PATH: &quot;/var/cloud-integration/cloud-integration.json&quot;
```

- Set `CLOUD_COST_ENABLED` to &quot;true&quot; to turn on cloud cost ingestion.
- Point `CLOUD_COST_CONFIG_PATH` to the mounted cloud integration file used by Tilt (e.g., `/var/cloud-integration/cloud-integration.json`).
- Adjust other values in `tilt-values.yaml` as needed during development.

#### Using Helm (Production)
```bash
# Add the OpenCost Helm repository
helm repo add opencost https://opencost.github.io/opencost-helm-chart
helm repo update

# Deploy OpenCost with MCP server (enabled by default)
helm install opencost opencost/opencost

# Access MCP server via port forwarding (example)
kubectl port-forward svc/opencost 8081:8081
```

The MCP server is **enabled by default** in the Helm chart. For custom configuration:

```bash
# Deploy with MCP server disabled
helm install opencost opencost/opencost \
  --set opencost.mcp.enabled=false

# Deploy with custom MCP port
helm install opencost opencost/opencost \
  --set opencost.mcp.port=9091

# Deploy with debug logging
helm install opencost opencost/opencost \
  --set opencost.mcp.extraEnv.MCP_LOG_LEVEL=debug
```

#### Configuration Summary

| Configuration | Command | Description |
|---------------|---------|-------------|
| **Default** | `helm install opencost opencost/opencost` | MCP enabled on port 8081 |
| **Disable** | `--set opencost.mcp.enabled=false` | Completely disable MCP server |
| **Custom Port** | `--set opencost.mcp.port=9091` | Use different port |
| **Debug Mode** | `--set opencost.mcp.extraEnv.MCP_LOG_LEVEL=debug` | Enable debug logging |

### MCP Client Configuration

Configure your MCP client (e.g., Cursor) to connect to the OpenCost MCP server:

**Default configuration (port 8081):**
```json
{
  &quot;mcpServers&quot;: {
    &quot;opencost&quot;: {
      &quot;type&quot;: &quot;http&quot;,
      &quot;url&quot;: &quot;http://localhost:8081&quot;
    }
  }
}
```

**Custom port configuration:**
```json
{
  &quot;mcpServers&quot;: {
    &quot;opencost&quot;: {
      &quot;type&quot;: &quot;http&quot;,
      &quot;url&quot;: &quot;http://localhost:9091&quot;
    }
  }
}
```

**For Kubernetes deployments:**
```json
{
  &quot;mcpServers&quot;: {
    &quot;opencost&quot;: {
      &quot;type&quot;: &quot;http&quot;,
      &quot;url&quot;: &quot;http://opencost.opencost.svc.cluster.local:8081&quot;
    }
  }
}
```

**For external access (with LoadBalancer/Ingress):**
```json
{
  &quot;mcpServers&quot;: {
    &quot;opencost&quot;: {
      &quot;type&quot;: &quot;http&quot;,
      &quot;url&quot;: &quot;http://your-opencost-domain.com:8081&quot;
    }
  }
}
```

### Available MCP Tools

The MCP server provides these tools for AI agents:

#### `get_allocation_costs`
Retrieve cost allocation data with filtering and aggregation.

**Parameters:**
- `window` (required): Time window (e.g., &quot;7d&quot;, &quot;1h&quot;, &quot;30m&quot;)
- `aggregate` (optional): Aggregation properties (e.g., &quot;namespace&quot;, &quot;pod&quot;, &quot;node&quot;)
- `step` (optional): Resolution step size
- `accumulate` (optional): Whether to accumulate over time
- `share_idle` (optional): Whether to share idle costs
- `include_idle` (optional): Whether to include idle resources

#### `get_asset_costs`
Retrieve asset cost data including nodes, disks, load balancers, and more.

**Parameters:**
- `window` (required): Time window (e.g., &quot;7d&quot;, &quot;1h&quot;, &quot;30m&quot;)

#### `get_cloud_costs`
Retrieve cloud cost data with provider, service, and region filtering.

**Parameters:**
- `window` (required): Time window (e.g., &quot;7d&quot;, &quot;1h&quot;, &quot;30m&quot;)
- `aggregate` (optional): Aggregation properties (e.g., &quot;provider&quot;, &quot;service&quot;, &quot;region&quot;)
- `accumulate` (optional): Time accumulation (&quot;day&quot;, &quot;week&quot;, &quot;month&quot;)
- `provider` (optional): Filter by cloud provider (e.g., &quot;aws&quot;, &quot;gcp&quot;, &quot;azure&quot;)
- `service` (optional): Filter by service (e.g., &quot;ec2&quot;, &quot;compute&quot;, &quot;s3&quot;)
- `category` (optional): Filter by category (e.g., &quot;compute&quot;, &quot;storage&quot;, &quot;network&quot;)
- `region` (optional): Filter by region (e.g., &quot;us-west-1&quot;, &quot;us-central1&quot;)
- `accountID` (optional): Filter by account ID

### Supported Asset Types

- **Node**: Compute instances with CPU, RAM, GPU details
- **Disk**: Storage volumes with usage and cost breakdown
- **LoadBalancer**: Load balancer instances with IP and private status
- **Network**: Network-related costs and usage
- **Cloud**: Cloud service costs with credit information
- **ClusterManagement**: Kubernetes cluster management costs

### Example Usage

Once configured, AI agents can query cost data like:

```javascript
// Get cost allocation for the last 7 days
const allocation = await mcpClient.callTool(&#039;get_allocation_costs&#039;, {
  window: &#039;7d&#039;,
  aggregate: &#039;namespace,node&#039;
});

// Get asset costs for the last 24 hours
const assets = await mcpClient.callTool(&#039;get_asset_costs&#039;, {
  window: &#039;1d&#039;
});

// Get cloud costs for AWS EC2 in us-west-1
const cloudCosts = await mcpClient.callTool(&#039;get_cloud_costs&#039;, {
  window: &#039;7d&#039;,
  aggregate: &#039;service&#039;,
  provider: &#039;aws&#039;,
  service: &#039;ec2&#039;,
  accumulate: &#039;day&#039;,
  filter: &#039;regionID:&quot;us-west-1&quot;&#039;
});
```

For detailed setup instructions and advanced configuration, see the [Helm chart documentation](https://github.com/opencost/opencost-helm-chart/blob/main/charts/opencost/README.md#mcp-server).

## Contributing

We :heart: pull requests! See [`CONTRIBUTING.md`](CONTRIBUTING.md) for information on building the project from source and contributing changes.

## Community

If you need any support or have any questions on contributing to the project, you can reach us on [CNCF Slack](https://slack.cncf.io/) in the [#opencost](https://cloud-native.slack.com/archives/C03D56FPD4G) channel or attend the biweekly [OpenCost Working Group community meeting](https://bit.ly/opencost-meeting) from the [Community Calendar](https://bit.ly/opencost-calendar) to discuss OpenCost development.

## FAQ

You can view [OpenCost documentation](https://www.opencost.io/docs/FAQ) for a list of commonly asked questions.
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[publicsuffix/list]]></title>
            <link>https://github.com/publicsuffix/list</link>
            <guid>https://github.com/publicsuffix/list</guid>
            <pubDate>Wed, 14 Jan 2026 00:05:19 GMT</pubDate>
            <description><![CDATA[The Public Suffix List]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/publicsuffix/list">publicsuffix/list</a></h1>
            <p>The Public Suffix List</p>
            <p>Language: Go</p>
            <p>Stars: 2,700</p>
            <p>Forks: 1,479</p>
            <p>Stars today: 2 stars today</p>
            <h2>README</h2><pre># The Public Suffix List

A &quot;public suffix&quot; is one under which Internet users can (or historically could)
directly register names. Some examples of public suffixes are `com`, `co.uk` and
`pvt.k12.ma.us`. The Public Suffix List is a list of all known public suffixes.

See https://publicsuffix.org/ and the [Wiki](https://github.com/publicsuffix/list/wiki) link above for more information.

## Are you here to add or update something?

All submissions must conform to the [validation and acceptance factors](https://github.com/publicsuffix/list/wiki/Guidelines#validation-and-non-acceptance-factors) and provide sufficient rationale or basically be as complete as possible, and follow the [Guidelines](https://github.com/publicsuffix/list/wiki/Guidelines), especially as they relate to format and [sorting](https://github.com/publicsuffix/list/wiki/Guidelines#sort-your-submission-correctly-important).

The list is currently maintained by people who are volunteering their time towards universal acceptance and ensuring there is a bridge between the ICANN world of domain names and the crucial last mile - the world of developers and human users.

Iteration back and forth will delay PR review or inclusion. Be extremely thorough, and patient.

## Important Notices

### 2025-05-27
Were you directed here to be able to add a subdomain to your **Cloudflare** account? If so, please work directly with Cloudflare for these account limitations. The PSL is **NOT** intended as a workaround for Cloudflare&#039;s subdomain restrictions. 

Consult [Cloudflare&#039;s subdomain setup documentation](https://developers.cloudflare.com/dns/zone-setups/subdomain-setup/) or contact Cloudflare directly for subdomain setup questions. Only submit a request to the PSL if your domain truly meets our criteria outlined in [Guidelines](https://github.com/publicsuffix/list/wiki/Guidelines).

### 2024-07-26
We are sending emails asking for confirmation if certain entries are still required or need updating.

Currently, this process is purely manual and extremely low volume but if you do get an email, please respond.

Please see the [Email Communication Policy](#email-communication-policy) to see how we will often communicate these changes.

### 2023-02-20
Did [guidance from Google related to the changes that they are making to adsense subdomains](https://support.google.com/adsense/answer/12170421) bring you here? Work with Google Adsense [Help Link](https://support.google.com/adsense/gethelp) with any support questions you have. The PSL is thinly resourced, and the volunteer maintainers are unable to answer questions about Adsense changes or support Adsense.

The PSL is volunteer-resourced and is absolutely not resourced to answer questions or support changes. Guidance is in the form of self-help (READ THE [WIKI](https://github.com/publicsuffix/list/wiki)), THERE IS NO PSL CUSTOMER SERVICE RESOURCE TO ASSIST YOU. *Please work directly with Google to ensure your domain does in fact need an entry, and they should help you know what the benefits and consequences are. __IT POSSIBLE TO HARM YOUR WEBSITE COOKIES BY REQUESTING A MALFORMED PSL ENTRY__. Also, understand what propagation delays and rollback processing entail before making requests.*

### 2021-04-23
Did guidance related to an issue with Facebook or Apple bring you here? [Read this before submitting requests](https://github.com/publicsuffix/list/issues/1245) We are not approving workaround requests per the validation and acceptance standards, but do have open discussion with Facebook on the matter.

## Email Communication Policy

We tend to use the subject line tag &quot;[PSL notification]&quot; in all Public Suffix List communications. For effective spam filtering, you can create a case-insensitive filter to allow only emails with exact &quot;[PSL notification]&quot; in the subject line. If you choose to set up such a filter in your email application, please verify the filter is implemented correctly and test it thoroughly to ensure you don&#039;t accidentally miss important communications from us.

## Code of Conduct

Your participation in the Public Suffix List project should follow the [Mozilla Community Participation Guidelines](https://www.mozilla.org/en-US/about/governance/policies/participation/ &quot;Mozilla Community Participation Guidelines&quot;) as well as the [GitHub Community Participation Guidelines](https://help.github.com/en/github/site-policy/github-community-guidelines &quot;GitHub Community Participation Guidelines&quot;). Behavior that falls into the areas forbidden by either document is unwelcome and will result in further escalation.
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[DataDog/datadog-agent]]></title>
            <link>https://github.com/DataDog/datadog-agent</link>
            <guid>https://github.com/DataDog/datadog-agent</guid>
            <pubDate>Wed, 14 Jan 2026 00:05:18 GMT</pubDate>
            <description><![CDATA[Main repository for Datadog Agent]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/DataDog/datadog-agent">DataDog/datadog-agent</a></h1>
            <p>Main repository for Datadog Agent</p>
            <p>Language: Go</p>
            <p>Stars: 3,472</p>
            <p>Forks: 1,369</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre># Datadog Agent

![GitHub Release](https://img.shields.io/github/v/release/DataDog/datadog-agent?style=flat&amp;logo=datadog&amp;logoColor=%23632CA6&amp;labelColor=%23FFF&amp;color=%23632CA6)
[![Coverage status](https://codecov.io/github/DataDog/datadog-agent/coverage.svg?branch=main)](https://codecov.io/github/DataDog/datadog-agent?branch=main)
[![GoDoc](https://godoc.org/github.com/DataDog/datadog-agent?status.svg)](https://godoc.org/github.com/DataDog/datadog-agent)

This repository contains the source code of the Datadog Agent version 7 and version 6. Please refer to the [Agent user documentation](https://docs.datadoghq.com/agent/) for information about differences between Agent v5, Agent v6 and Agent v7. Additionally, we provide a list of prepackaged binaries for an easy install process [here](https://app.datadoghq.com/fleet/install-agent/latest?platform=overview).

## Documentation

The [developer docs site](https://datadoghq.dev/datadog-agent/setup/) contains information about how to develop the Datadog Agent itself.

The source of the content is located under [the docs directory](docs) and may contain pages that are not yet published.

## Contributing code

You&#039;ll find information and help on how to contribute code to this project under
[the `docs/dev` directory](docs/dev) of the present repo.

## License

The Datadog Agent user space components are licensed under the
[Apache License, Version 2.0](LICENSE). The BPF code is licensed
under the [General Public License, Version 2.0](pkg/ebpf/c/COPYING).
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[moby/moby]]></title>
            <link>https://github.com/moby/moby</link>
            <guid>https://github.com/moby/moby</guid>
            <pubDate>Wed, 14 Jan 2026 00:05:17 GMT</pubDate>
            <description><![CDATA[The Moby Project - a collaborative project for the container ecosystem to assemble container-based systems]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/moby/moby">moby/moby</a></h1>
            <p>The Moby Project - a collaborative project for the container ecosystem to assemble container-based systems</p>
            <p>Language: Go</p>
            <p>Stars: 71,345</p>
            <p>Forks: 18,877</p>
            <p>Stars today: 5 stars today</p>
            <h2>README</h2><pre>The Moby Project
================

[![PkgGoDev](https://pkg.go.dev/badge/github.com/moby/moby/v2)](https://pkg.go.dev/github.com/moby/moby/v2)
![GitHub License](https://img.shields.io/github/license/moby/moby)
[![Go Report Card](https://goreportcard.com/badge/github.com/moby/moby/v2)](https://goreportcard.com/report/github.com/moby/moby/v2)
[![OpenSSF Scorecard](https://api.scorecard.dev/projects/github.com/moby/moby/badge)](https://scorecard.dev/viewer/?uri=github.com/moby/moby)
[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/10989/badge)](https://www.bestpractices.dev/projects/10989)


![Moby Project logo](docs/static_files/moby-project-logo.png &quot;The Moby Project&quot;)

Moby is an open-source project created by Docker to enable and accelerate software containerization.

It provides a &quot;Lego set&quot; of toolkit components, the framework for assembling them into custom container-based systems, and a place for all container enthusiasts and professionals to experiment and exchange ideas.
Components include container build tools, a container registry, orchestration tools, a runtime and more, and these can be used as building blocks in conjunction with other tools and projects.

## Principles

Moby is an open project guided by strong principles, aiming to be modular, flexible and without too strong an opinion on user experience.
It is open to the community to help set its direction.

- Modular: the project includes lots of components that have well-defined functions and APIs that work together.
- Batteries included but swappable: Moby includes enough components to build fully featured container systems, but its modular architecture ensures that most of the components can be swapped by different implementations.
- Usable security: Moby provides secure defaults without compromising usability.
- Developer focused: The APIs are intended to be functional and useful to build powerful tools.
They are not necessarily intended as end user tools but as components aimed at developers.
Documentation and UX is aimed at developers not end users.

## Audience

The Moby Project is intended for engineers, integrators and enthusiasts looking to modify, hack, fix, experiment, invent and build systems based on containers.
It is not for people looking for a commercially supported system, but for people who want to work and learn with open source code.

## Relationship with Docker

The components and tools in the Moby Project are initially the open source components that Docker and the community have built for the Docker Project.
New projects can be added if they fit with the community goals. Docker is committed to using Moby as the upstream for the Docker Product.
However, other projects are also encouraged to use Moby as an upstream, and to reuse the components in diverse ways, and all these uses will be treated in the same way. External maintainers and contributors are welcomed.

The Moby project is not intended as a location for support or feature requests for Docker products, but as a place for contributors to work on open source code, fix bugs, and make the code more useful.
The releases are supported by the maintainers, community and users, on a best efforts basis only. For customers who want enterprise or commercial support, [Docker Desktop](https://www.docker.com/products/docker-desktop/) and [Mirantis Container Runtime](https://www.mirantis.com/software/mirantis-container-runtime/) are the appropriate products for these use cases.

-----

Legal
=====

*Brought to you courtesy of our legal counsel. For more context,
please see the [NOTICE](https://github.com/moby/moby/blob/master/NOTICE) document in this repo.*

Use and transfer of Moby may be subject to certain restrictions by the
United States and other governments.

It is your responsibility to ensure that your use and/or transfer does not
violate applicable laws.

For more information, please see https://www.bis.doc.gov

Licensing
=========
Moby is licensed under the Apache License, Version 2.0. See
[LICENSE](https://github.com/moby/moby/blob/master/LICENSE) for the full
license text.
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[gotenberg/gotenberg]]></title>
            <link>https://github.com/gotenberg/gotenberg</link>
            <guid>https://github.com/gotenberg/gotenberg</guid>
            <pubDate>Wed, 14 Jan 2026 00:05:16 GMT</pubDate>
            <description><![CDATA[A developer-friendly API for converting numerous document formats into PDF files, and more!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/gotenberg/gotenberg">gotenberg/gotenberg</a></h1>
            <p>A developer-friendly API for converting numerous document formats into PDF files, and more!</p>
            <p>Language: Go</p>
            <p>Stars: 10,915</p>
            <p>Forks: 722</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/8983173/130322857-185831e2-f041-46eb-a17f-0a69d066c4e5.png&quot; alt=&quot;Gotenberg Logo&quot; width=&quot;150&quot; height=&quot;150&quot; /&gt;
    &lt;h3 align=&quot;center&quot;&gt;Gotenberg&lt;/h3&gt;
    &lt;p align=&quot;center&quot;&gt;A containerized API for seamless PDF conversion&lt;/p&gt;
    &lt;p align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://hub.docker.com/r/gotenberg/gotenberg&quot;&gt;&lt;img alt=&quot;Total downloads (gotenberg/gotenberg)&quot; src=&quot;https://img.shields.io/docker/pulls/gotenberg/gotenberg&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://hub.docker.com/r/thecodingmachine/gotenberg&quot;&gt;&lt;img alt=&quot;Total downloads (thecodingmachine/gotenberg)&quot; src=&quot;https://img.shields.io/docker/pulls/thecodingmachine/gotenberg&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://github.com/gotenberg/gotenberg/actions/workflows/continuous-integration.yml&quot;&gt;&lt;img alt=&quot;Continuous Integration&quot; src=&quot;https://github.com/gotenberg/gotenberg/actions/workflows/continuous-integration.yml/badge.svg&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://pkg.go.dev/github.com/gotenberg/gotenberg/v8&quot;&gt;&lt;img alt=&quot;Go Reference&quot; src=&quot;https://pkg.go.dev/badge/github.com/gotenberg/gotenberg.svg&quot;&gt;&lt;/a&gt;
    &lt;/p&gt;
    &lt;p align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://trendshift.io/repositories/2996&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/2996&quot; alt=&quot;gotenberg%2Fgotenberg | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
    &lt;/p&gt;
    &lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://gotenberg.dev/docs/getting-started/introduction&quot;&gt;Documentation&lt;/a&gt; &amp;#183; &lt;a href=&quot;https://gotenberg.dev/docs/getting-started/installation#live-demo-&quot;&gt;Live Demo&lt;/a&gt; üî•&lt;/p&gt;
&lt;/p&gt;

---

**Gotenberg** provides a developer-friendly API to interact with powerful tools like Chromium and LibreOffice for converting
numerous document formats (HTML, Markdown, Word, Excel, etc.) into PDF files, and more!

## Quick Start

Open a terminal and run the following command:

```
docker run --rm -p 3000:3000 gotenberg/gotenberg:8
```

Alternatively, using the historic Docker repository from our sponsor [TheCodingMachine](https://www.thecodingmachine.com):

```
docker run --rm -p 3000:3000 thecodingmachine/gotenberg:8
```

The API is now available on your host at http://localhost:3000.

Head to the [documentation](https://gotenberg.dev/docs/getting-started/introduction) to learn how to interact with it üöÄ

## Sponsors

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://thecodingmachine.com&quot;&gt;
        &lt;img src=&quot;https://user-images.githubusercontent.com/8983173/130324668-9d6e7b35-53a3-49c7-a574-38190d2bd6b0.png&quot; alt=&quot;TheCodingMachine Logo&quot; width=&quot;333&quot; height=&quot;163&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://pdfme.com?utm_source=gotenberg_github&amp;utm_medium=website&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://github.com/user-attachments/assets/2a75dd40-ca18-4d34-acd5-5dd474595168&quot; alt=&quot;pdfme Logo&quot; width=&quot;333&quot; height=&quot;163&quot; /&gt;
    &lt;/a&gt;
&lt;/p&gt;

Sponsorships help maintain and improve Gotenberg - [become a sponsor](https://github.com/sponsors/gulien) ‚ù§Ô∏è

---

&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;Powered by&lt;/strong&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://jb.gg/OpenSource&quot;&gt;
    &lt;img src=&quot;https://resources.jetbrains.com/storage/products/company/brand/logos/jetbrains.svg&quot; alt=&quot;JetBrains logo&quot; width=&quot;200&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[weaviate/weaviate]]></title>
            <link>https://github.com/weaviate/weaviate</link>
            <guid>https://github.com/weaviate/weaviate</guid>
            <pubDate>Wed, 14 Jan 2026 00:05:15 GMT</pubDate>
            <description><![CDATA[Weaviate is an open-source vector database that stores both objects and vectors, allowing for the combination of vector search with structured filtering with the fault tolerance and scalability of a cloud-native database‚Äã.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/weaviate/weaviate">weaviate/weaviate</a></h1>
            <p>Weaviate is an open-source vector database that stores both objects and vectors, allowing for the combination of vector search with structured filtering with the fault tolerance and scalability of a cloud-native database‚Äã.</p>
            <p>Language: Go</p>
            <p>Stars: 15,392</p>
            <p>Forks: 1,175</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre># Weaviate &lt;img alt=&#039;Weaviate logo&#039; src=&#039;https://weaviate.io/img/site/weaviate-logo-light.png&#039; width=&#039;148&#039; align=&#039;right&#039; /&gt;

[![GitHub Repo stars](https://img.shields.io/github/stars/weaviate/weaviate?style=social)](https://github.com/weaviate/weaviate)
[![Go Reference](https://pkg.go.dev/badge/github.com/weaviate/weaviate.svg)](https://pkg.go.dev/github.com/weaviate/weaviate)
[![Build Status](https://github.com/weaviate/weaviate/actions/workflows/.github/workflows/pull_requests.yaml/badge.svg?branch=main)](https://github.com/weaviate/weaviate/actions/workflows/.github/workflows/pull_requests.yaml)
[![Go Report Card](https://goreportcard.com/badge/github.com/weaviate/weaviate)](https://goreportcard.com/report/github.com/weaviate/weaviate)
[![Coverage Status](https://codecov.io/gh/weaviate/weaviate/branch/main/graph/badge.svg)](https://codecov.io/gh/weaviate/weaviate)
[![Slack](https://img.shields.io/badge/slack--channel-blue?logo=slack)](https://weaviate.io/slack)

**Weaviate** is an open-source, cloud-native vector database that stores both objects and vectors, enabling semantic search at scale. It combines vector similarity search with keyword filtering, retrieval-augmented generation (RAG), and reranking in a single query interface. Common use cases include RAG systems, semantic and image search, recommendation engines, chatbots, and content classification.

Weaviate supports two approaches to store vectors: automatic vectorization at import using [integrated models](https://docs.weaviate.io/weaviate/model-providers) (OpenAI, Cohere, HuggingFace, and others) or direct import of [pre-computed vector embeddings](https://docs.weaviate.io/weaviate/starter-guides/custom-vectors). Production deployments benefit from built-in multi-tenancy, replication, RBAC authorization, and [many other features](#weaviate-features).

To get started quickly, have a look at one of these tutorials:

- [Quickstart - Weaviate Cloud](https://docs.weaviate.io/weaviate/quickstart)
- [Quickstart - local Docker instance](https://docs.weaviate.io/weaviate/quickstart/local)

## Installation

Weaviate offers multiple installation and deployment options:

- [Docker](https://docs.weaviate.io/deploy/installation-guides/docker-installation)
- [Kubernetes](https://docs.weaviate.io/deploy/installation-guides/k8s-installation)
- [Weaviate Cloud](https://console.weaviate.cloud)

See the [installation docs](https://docs.weaviate.io/deploy) for more deployment options, such as [AWS](https://docs.weaviate.io/deploy/installation-guides/aws-marketplace) and [GCP](https://docs.weaviate.io/deploy/installation-guides/gcp-marketplace).

## Getting started

You can easily start Weaviate and a local vector embedding model with [Docker](https://docs.docker.com/desktop/).
Create a `docker-compose.yml` file:

```yml
services:
  weaviate:
    image: cr.weaviate.io/semitechnologies/weaviate:1.32.2
    ports:
      - &quot;8080:8080&quot;
      - &quot;50051:50051&quot;
    environment:
      ENABLE_MODULES: text2vec-model2vec
      MODEL2VEC_INFERENCE_API: http://text2vec-model2vec:8080

  # A lightweight embedding model that will generate vectors from objects during import
  text2vec-model2vec:
    image: cr.weaviate.io/semitechnologies/model2vec-inference:minishlab-potion-base-32M
```

Start Weaviate and the embedding service with:

```bash
docker compose up -d
```

Install the Python client (or use another [client library](#client-libraries-and-apis)):

```bash
pip install -U weaviate-client
```

The following Python example shows how easy it is to populate a Weaviate database with data, create vector embeddings and perform semantic search:

```python
import weaviate
from weaviate.classes.config import Configure, DataType, Property

# Connect to Weaviate
client = weaviate.connect_to_local()

# Create a collection
client.collections.create(
    name=&quot;Article&quot;,
    properties=[Property(name=&quot;content&quot;, data_type=DataType.TEXT)],
    vector_config=Configure.Vectors.text2vec_model2vec(),  # Use a vectorizer to generate embeddings during import
    # vector_config=Configure.Vectors.self_provided()  # If you want to import your own pre-generated embeddings
)

# Insert objects and generate embeddings
articles = client.collections.get(&quot;Article&quot;)
articles.data.insert_many(
    [
        {&quot;content&quot;: &quot;Vector databases enable semantic search&quot;},
        {&quot;content&quot;: &quot;Machine learning models generate embeddings&quot;},
        {&quot;content&quot;: &quot;Weaviate supports hybrid search capabilities&quot;},
    ]
)

# Perform semantic search
results = articles.query.near_text(query=&quot;Search objects by meaning&quot;, limit=1)
print(results.objects[0])

client.close()
```

This example uses the `Model2Vec` vectorizer, but you can choose any other [embedding model provider](https://docs.weaviate.io/weaviate/model-providers) or [bring your own pre-generated vectors](https://docs.weaviate.io/weaviate/starter-guides/custom-vectors).

## Client libraries and APIs

Weaviate provides client libraries for several programming languages:

- [Python](https://docs.weaviate.io/weaviate/client-libraries/python)
- [JavaScript/TypeScript](https://docs.weaviate.io/weaviate/client-libraries/typescript)
- [Java](https://docs.weaviate.io/weaviate/client-libraries/java)
- [Go](https://docs.weaviate.io/weaviate/client-libraries/go)
- C# (üöß Coming soon üöß)

There are also additional [community-maintained libraries](https://docs.weaviate.io/weaviate/client-libraries/community).

Weaviate exposes [REST API](https://docs.weaviate.io/weaviate/api/rest), [gRPC API](https://docs.weaviate.io/weaviate/api/grpc), and [GraphQL API](https://docs.weaviate.io/weaviate/api/graphql) to communicate with the database server.

## Weaviate features

These features enable you to build AI-powered applications:

- **‚ö° Fast Search Performance**: Perform complex semantic [searches](https://docs.weaviate.io/weaviate/search/similarity) over billions of vectors in milliseconds. Weaviate&#039;s architecture is built in Go for speed and reliability, ensuring your AI applications are highly responsive even under heavy load. See our [ANN benchmarks](https://docs.weaviate.io/weaviate/benchmarks/ann) for more info.

- **üîå Flexible Vectorization**: Seamlessly vectorize data at import time with [integrated vectorizers](https://docs.weaviate.io/weaviate/model-providers) from OpenAI, Cohere, HuggingFace, Google, and more. Or you can import [your own vector embeddings](https://docs.weaviate.io/weaviate/starter-guides/custom-vectors).

- **üîç Advanced Hybrid &amp; Image Search**: Combine the power of semantic search with traditional [keyword (BM25) search](https://docs.weaviate.io/weaviate/search/bm25), [image search](https://docs.weaviate.io/weaviate/search/image) and [advanced filtering](https://docs.weaviate.io/weaviate/search/filters) to get the best results with a single API call.

- **ü§ñ Integrated RAG &amp; Reranking**: Go beyond simple retrieval with built-in [generative search (RAG)](https://docs.weaviate.io/weaviate/search/generative) and [reranking](https://docs.weaviate.io/weaviate/search/rerank) capabilities. Power sophisticated Q&amp;A systems, chatbots, and summarizers directly from your database without additional tooling.

- **üìà Production-Ready &amp; Scalable**: Weaviate is built for mission-critical applications. Go from rapid prototyping to production at scale with native support for [horizontal scaling](https://docs.weaviate.io/deploy/configuration/horizontal-scaling), [multi-tenancy](https://docs.weaviate.io/weaviate/manage-collections/multi-tenancy), [replication](https://docs.weaviate.io/deploy/configuration/replication), and fine-grained [role-based access control (RBAC)](https://docs.weaviate.io/weaviate/configuration/rbac).

- **üí∞ Cost-Efficient Operations**: Radically lower resource consumption and operational costs with built-in [vector compression](https://docs.weaviate.io/weaviate/configuration/compression). Vector quantization and multi-vector encoding reduce memory usage with minimal impact on search performance.

For a complete list of all functionalities, visit the [official Weaviate documentation](https://docs.weaviate.io).

## Useful resources

### Demo projects &amp; recipes

These demos are working applications that highlight some of Weaviate&#039;s capabilities. Their source code is available on GitHub.

- [Elysia](https://elysia.weaviate.io) ([GitHub](https://github.com/weaviate/elysia)): Elysia is a decision tree based agentic system which intelligently decides what tools to use, what results have been obtained, whether it should continue the process or whether its goal has been completed.
- [Verba](https://weaviate.io/blog/verba-open-source-rag-app) ([GitHub](https://github.com/weaviate/verba)): A community-driven open-source application designed to offer an end-to-end, streamlined, and user-friendly interface for Retrieval-Augmented Generation (RAG) out of the box.
- [Healthsearch](https://weaviate.io/blog/healthsearch-demo) ([GitHub](https://github.com/weaviate/healthsearch-demo)): An open-source project aimed at showcasing the potential of leveraging user-written reviews and queries to retrieve supplement products based on specific health effects.
- Awesome-Moviate ([GitHub](https://github.com/weaviate-tutorials/awesome-moviate)): A movie search and recommendation engine that allows keyword-based (BM25), semantic, and hybrid searches.

We also maintain extensive repositories of **Jupyter Notebooks** and **TypeScript code snippets** that cover how to use Weaviate features and integrations:

- [Weaviate Python Recipes](https://github.com/weaviate/recipes/)
- [Weaviate TypeScript Recipes](https://github.com/weaviate/recipes-ts/)

### Blog posts

- [What is a Vector Database](https://weaviate.io/blog/what-is-a-vector-database)
- [What is Vector Search](https://weaviate.io/blog/vector-search-explained)
- [What is Hybrid Search](https://weaviate.io/blog/hybrid-search-explained)
- [How to Choose an Embedding Model](https://weaviate.io/blog/how-to-choose-an-embedding-model)
- [What is RAG](https://weaviate.io/blog/introduction-to-rag)
- [RAG Evaluation](https://weaviate.io/blog/rag-evaluation)
- [Advanced RAG Techniques](https://weaviate.io/blog/advanced-rag)
- [What is Multimodal RAG](https://weaviate.io/blog/multimodal-rag)
- [What is Agentic RAG](https://weaviate.io/blog/what-is-agentic-rag)
- [What is Graph RAG](https://weaviate.io/blog/graph-rag)
- [Overview of Late Interaction Models](https://weaviate.io/blog/late-interaction-overview)

### Integrations

Weaviate integrates with many external services:

| Category                                                                                   | Description                                                | Integrations                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| ------------------------------------------------------------------------------------------ | ---------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **[Cloud Hyperscalers](https://docs.weaviate.io/integrations/cloud-hyperscalers)**         | Large-scale computing and storage                          | [AWS](https://docs.weaviate.io/integrations/cloud-hyperscalers/aws), [Google](https://docs.weaviate.io/integrations/cloud-hyperscalers/google)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| **[Compute Infrastructure](https://docs.weaviate.io/integrations/compute-infrastructure)** | Run and scale containerized applications                   | [Modal](https://docs.weaviate.io/integrations/compute-infrastructure/modal), [Replicate](https://docs.weaviate.io/integrations/compute-infrastructure/replicate), [Replicated](https://docs.weaviate.io/integrations/compute-infrastructure/replicated)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| **[Data Platforms](https://docs.weaviate.io/integrations/data-platforms)**                 | Data ingestion and web scraping                            | [Airbyte](https://docs.weaviate.io/integrations/data-platforms/airbyte), [Aryn](https://docs.weaviate.io/integrations/data-platforms/aryn), [Boomi](https://docs.weaviate.io/integrations/data-platforms/boomi), [Box](https://docs.weaviate.io/integrations/data-platforms/box), [Confluent](https://docs.weaviate.io/integrations/data-platforms/confluent), [Astronomer](https://docs.weaviate.io/integrations/data-platforms/astronomer), [Context Data](https://docs.weaviate.io/integrations/data-platforms/context-data), [Databricks](https://docs.weaviate.io/integrations/data-platforms/databricks), [Firecrawl](https://docs.weaviate.io/integrations/data-platforms/firecrawl), [IBM](https://docs.weaviate.io/integrations/data-platforms/ibm), [Unstructured](https://docs.weaviate.io/integrations/data-platforms/unstructured)                |
| **[LLM and Agent Frameworks](https://docs.weaviate.io/integrations/llm-agent-frameworks)** | Build agents and generative AI applications                | [Agno](https://docs.weaviate.io/integrations/llm-agent-frameworks/agno), [Composio](https://docs.weaviate.io/integrations/llm-agent-frameworks/composio), [CrewAI](https://docs.weaviate.io/integrations/llm-agent-frameworks/crewai), [DSPy](https://docs.weaviate.io/integrations/llm-agent-frameworks/dspy), [Dynamiq](https://docs.weaviate.io/integrations/llm-agent-frameworks/dynamiq), [Haystack](https://docs.weaviate.io/integrations/llm-agent-frameworks/haystack), [LangChain](https://docs.weaviate.io/integrations/llm-agent-frameworks/langchain), [LlamaIndex](https://docs.weaviate.io/integrations/llm-agent-frameworks/llamaindex), [N8n](https://docs.weaviate.io/integrations/llm-agent-frameworks/n8n), [Semantic Kernel](https://docs.weaviate.io/integrations/llm-agent-frameworks/semantic-kernel)                                   |
| **[Operations](https://docs.weaviate.io/integrations/operations)**                         | Tools for monitoring and analyzing generative AI workflows | [AIMon](https://docs.weaviate.io/integrations/operations/aimon), [Arize](https://docs.weaviate.io/integrations/operations/arize), [Cleanlab](https://docs.weaviate.io/integrations/operations/cleanlab), [Comet](https://docs.weaviate.io/integrations/operations/comet), [DeepEval](https://docs.weaviate.io/integrations/operations/deepeval), [Langtrace](https://docs.weaviate.io/integrations/operations/langtrace), [LangWatch](https://docs.weaviate.io/integrations/operations/langwatch), [Nomic](https://docs.weaviate.io/integrations/operations/nomic), [Patronus AI](https://docs.weaviate.io/integrations/operations/patronus), [Ragas](https://docs.weaviate.io/integrations/operations/ragas), [TruLens](https://docs.weaviate.io/integrations/operations/trulens), [Weights &amp; Biases](https://docs.weaviate.io/integrations/operations/wandb) |

## Contributing

We welcome and appreciate contributions! Please see our [Contributor guide](https://docs.weaviate.io/contributor-guide) for the development setup, code style guidelines, testing requirements and the pull request process.

Join our [Slack community](https://weaviate.io/slack) or [Community forum](https://forum.weaviate.io/) to discuss ideas and get help.

## License

BSD 3-Clause License. See [LICENSE](./LICENSE) for details.
</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
        <item>
            <title><![CDATA[henrygd/beszel]]></title>
            <link>https://github.com/henrygd/beszel</link>
            <guid>https://github.com/henrygd/beszel</guid>
            <pubDate>Wed, 14 Jan 2026 00:05:14 GMT</pubDate>
            <description><![CDATA[Lightweight server monitoring hub with historical data, docker stats, and alerts.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/henrygd/beszel">henrygd/beszel</a></h1>
            <p>Lightweight server monitoring hub with historical data, docker stats, and alerts.</p>
            <p>Language: Go</p>
            <p>Stars: 18,600</p>
            <p>Forks: 593</p>
            <p>Stars today: 38 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Go</category>
        </item>
    </channel>
</rss>