<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for javascript - JavaScript Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for javascript.</description>
        <lastBuildDate>Sun, 29 Jun 2025 00:05:36 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[pot-app/pot-desktop]]></title>
            <link>https://github.com/pot-app/pot-desktop</link>
            <guid>https://github.com/pot-app/pot-desktop</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:36 GMT</pubDate>
            <description><![CDATA[🌈一个跨平台的划词翻译和OCR软件 | A cross-platform software for text translation and recognition.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pot-app/pot-desktop">pot-app/pot-desktop</a></h1>
            <p>🌈一个跨平台的划词翻译和OCR软件 | A cross-platform software for text translation and recognition.</p>
            <p>Language: JavaScript</p>
            <p>Stars: 13,220</p>
            <p>Forks: 609</p>
            <p>Stars today: 82 stars today</p>
            <h2>README</h2><pre>&lt;img width=&quot;200px&quot; src=&quot;public/icon.svg&quot; align=&quot;left&quot;/&gt;

# Pot (派了个萌的翻译器)

&gt; 🌈 一个跨平台的划词翻译软件 ([QQ 频道](https://pd.qq.com/s/akns94e1r))

![License](https://img.shields.io/github/license/pot-app/pot-desktop.svg)
![Tauri](https://img.shields.io/badge/Tauri-1.6.8-blue?logo=tauri)
![JavaScript](https://img.shields.io/badge/-JavaScript-yellow?logo=javascript&amp;logoColor=white)
![Rust](https://img.shields.io/badge/-Rust-orange?logo=rust&amp;logoColor=white)
![Windows](https://img.shields.io/badge/-Windows-blue?logo=windows&amp;logoColor=white)
![MacOS](https://img.shields.io/badge/-macOS-black?&amp;logo=apple&amp;logoColor=white)
![Linux](https://img.shields.io/badge/-Linux-yellow?logo=linux&amp;logoColor=white)

&lt;br/&gt;
&lt;hr/&gt;
&lt;div align=&quot;center&quot;&gt;

&lt;h3&gt;中文 | &lt;a href=&#039;./README_EN.md&#039;&gt;English&lt;/a&gt; | &lt;a href=&#039;./README_KR.md&#039;&gt; 한글 &lt;/a&gt;&lt;/h3&gt;

&lt;table&gt;
&lt;tr&gt;
    &lt;td&gt; &lt;img src=&quot;asset/1.png&quot;&gt;
    &lt;td&gt; &lt;img src=&quot;asset/2.png&quot;&gt;
    &lt;td&gt; &lt;img src=&quot;asset/3.png&quot;&gt;
&lt;/table&gt;

# 目录

&lt;/div&gt;

-   [使用说明](#使用说明)
-   [特色功能](#特色功能)
-   [支持接口](#支持接口)
-   [插件系统](#插件系统)
-   [安装指南](#安装指南)
-   [外部调用](#外部调用)
-   [Wayland 支持](#wayland-支持)
-   [国际化](#国际化weblate)
-   [贡献者](#贡献者)
-   [感谢](#感谢)

&lt;div align=&quot;center&quot;&gt;

# 使用说明

| 划词翻译                                             | 输入翻译                                                       | 外部调用                                                             |
| ---------------------------------------------------- | -------------------------------------------------------------- | -------------------------------------------------------------------- |
| 鼠标选中需要翻译的文本，按下设置的划词翻译快捷键即可 | 按下输入翻译快捷键呼出翻译窗口，输入待翻译文本后按下 回车 翻译 | 通过被其他软件调用实现更加方便高效的功能, 详见 [外部调用](#外部调用) |
| &lt;img src=&quot;asset/eg1.gif&quot;/&gt;                           | &lt;img src=&quot;asset/eg2.gif&quot;/&gt;                                     | &lt;img src=&quot;asset/eg3.gif&quot;/&gt;                                           |

| 剪切板监听模式                                                         | 截图 OCR                                          | 截图翻译                                         |
| ---------------------------------------------------------------------- | ------------------------------------------------- | ------------------------------------------------ |
| 在任意翻译面板上点击左上角图标启动剪切板监听默认，复制文字即可完成翻译 | 按下截图 OCR 快捷键后框选需要识别区域即可完成识别 | 按下截图翻译快捷键后框选需要识别区域即可完成翻译 |
| &lt;img src=&quot;asset/eg4.gif&quot;/&gt;                                             | &lt;img src=&quot;asset/eg5.gif&quot;/&gt;                        | &lt;img src=&quot;asset/eg6.gif&quot;/&gt;                       |

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

# 特色功能

&lt;/div&gt;

-   [x] 多接口并行翻译 ([支持接口](#支持接口))
-   [x] 多接口文字识别 ([支持接口](#支持接口))
-   [x] 多接口语音合成 ([支持接口](#支持接口))
-   [x] 导出到生词本 ([支持接口](#支持接口))
-   [x] 外部调用 ([详情](#外部调用))
-   [x] 支持插件系统 ([插件系统](#插件系统))
-   [x] 支持所有 PC 平台 (Windows, macOS, Linux)
-   [x] 支持 Wayland (在 KDE、Gnome 以及 Hyprland 上测试)
-   [x] 多语言支持

&lt;div align=&quot;center&quot;&gt;

# 支持接口

&lt;/div&gt;

## 翻译

-   [x] [OpenAI](https://platform.openai.com/)
-   [x] [智谱 AI](https://www.zhipuai.cn/)
-   [x] [Gemini Pro](https://gemini.google.com/)
-   [x] [Ollama](https://www.ollama.com/) (离线)
-   [x] [阿里翻译](https://www.aliyun.com/product/ai/alimt)
-   [x] [百度翻译](https://fanyi.baidu.com/)
-   [x] [彩云小译](https://fanyi.caiyunapp.com/)
-   [x] [腾讯翻译君](https://fanyi.qq.com/)
-   [x] [腾讯交互翻译](https://transmart.qq.com/)
-   [x] [火山翻译](https://translate.volcengine.com/)
-   [x] [小牛翻译](https://niutrans.com/)
-   [x] [Google](https://translate.google.com)
-   [x] [Bing](https://learn.microsoft.com/zh-cn/azure/cognitive-services/translator/)
-   [x] [Bing 词典](https://www.bing.com/dict)
-   [x] [DeepL](https://www.deepl.com/)
-   [x] [有道翻译](https://ai.youdao.com/)
-   [x] [剑桥词典](https://dictionary.cambridge.org/)
-   [x] [Yandex](https://translate.yandex.com/)
-   [x] [Lingva](https://github.com/TheDavidDelta/lingva-translate) ([插件](https://github.com/pot-app/pot-app-translate-plugin-template))
-   [x] [Tatoeba](https://tatoeba.org/) ([插件](https://github.com/pot-app/pot-app-translate-plugin-tatoeba))
-   [x] [ECDICT](https://github.com/skywind3000/ECDICT) ([插件](https://github.com/pot-app/pot-app-translate-plugin-ecdict))

更多接口支持见 [插件系统](#插件系统)

## 文字识别

-   [x] 系统 OCR (离线)
    -   [x] [Windows.Media.OCR](https://learn.microsoft.com/en-us/uwp/api/windows.media.ocr.ocrengine?view=winrt-22621) on Windows
    -   [x] [Apple Vision Framework](https://developer.apple.com/documentation/vision/recognizing_text_in_images) on MacOS
    -   [x] [Tesseract OCR](https://github.com/tesseract-ocr) on Linux
-   [x] [Tesseract.js](https://tesseract.projectnaptha.com/) (离线)
-   [x] [百度](https://ai.baidu.com/tech/ocr/general)
-   [x] [腾讯](https://cloud.tencent.com/product/ocr-catalog)
-   [x] [火山](https://www.volcengine.com/product/OCR)
-   [x] [迅飞](https://www.xfyun.cn/services/common-ocr)
-   [x] [腾讯图片翻译](https://cloud.tencent.com/document/product/551/17232)
-   [x] [百度图片翻译](https://fanyi-api.baidu.com/product/22)
-   [x] [Simple LaTeX](https://simpletex.cn/)
-   [x] [OCRSpace](https://ocr.space/) ([插件](https://github.com/pot-app/pot-app-recognize-plugin-template))
-   [x] [Rapid](https://github.com/RapidAI/RapidOcrOnnx) (离线 [插件](https://github.com/pot-app/pot-app-recognize-plugin-rapid))
-   [x] [Paddle](https://github.com/hiroi-sora/PaddleOCR-json) (离线 [插件](https://github.com/pot-app/pot-app-recognize-plugin-paddle))

更多接口支持见 [插件系统](#插件系统)

## 语音合成

-   [x] [Lingva](https://github.com/thedaviddelta/lingva-translate)

更多接口支持见 [插件系统](#插件系统)

## 生词本

-   [x] [Anki](https://apps.ankiweb.net/)
-   [x] [欧路词典](https://dict.eudic.net/)
-   [x] [有道](https://www.youdao.com/) ([插件](https://github.com/pot-app/pot-app-collection-plugin-youdao))
-   [x] [扇贝](https://web.shanbay.com/web/main) ([插件](https://github.com/pot-app/pot-app-collection-plugin-shanbay))

更多接口支持见 [插件系统](#插件系统)

&lt;div align=&quot;center&quot;&gt;

# 插件系统

&lt;/div&gt;

软件内置接口数量有限，但是您可以通过插件系统来扩展软件的功能。

## 插件安装

你可以在 [Plugin List](https://pot-app.com/plugin.html) 查找你需要的插件，然后前往插件仓库下载插件。

pot 插件的扩展名为 `.potext`, 下载得到`.potext`文件之后， 在 偏好设置-服务设置-添加外部插件-安装外部插件 选择对应的 `.potext` 即可安装成功，添加到服务列表中即可像内置服务一样正常使用了。

### 故障排除

-   找不到指定的模块 (Windows)

    出现类似这样的报错是因为系统缺少 C++库，前往[这里](https://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist?view=msvc-170#visual-studio-2015-2017-2019-and-2022)安装即可解决问题。

-   不是有效的 Win32 应用程序 (Windows)

    出现类似这样的报错说明你没有下载对应系统或者架构的插件，前往插件仓库下载正确的插件即可解决问题。

## 插件开发

在 [Plugin List](https://pot-app.com/plugin.html) 中的 [模板](https://pot-app.com/plugin.html#%E6%A8%A1%E6%9D%BF) 章节提供了各种插件的开发模板，具体的开发文档请查看对应的模板仓库。

&lt;div align=&quot;center&quot;&gt;

# 安装指南

&lt;/div&gt;

## Windows

### 通过 Winget 安装

```powershell
winget install Pylogmon.pot
```

### 手动安装

1. 在 [Release](https://github.com/pot-app/pot-desktop/releases/latest) 页面下载最新 `exe` 安装包。

    - 64 位机器下载 `pot_{version}_x64-setup.exe`
    - 32 位机器下载 `pot_{version}_x86-setup.exe`
    - arm64 机器下载 `pot_{version}_arm64-setup.exe`

2. 双击安装包进行安装。

### 故障排除

-   启动后没有界面，点击托盘图标没有反应

    检查是否卸载/禁用了 WebView2，如果卸载/禁用了 WebView2，请手动安装 WebView2 或将其恢复。

    如果是企业版系统不方便安装或无法安装 WebView2，请尝试在 [Release](https://github.com/pot-app/pot-desktop/releases/latest) 下载内置 WebView2 的版本 `pot_{version}_{arch}_fix_webview2_runtime-setup.exe`

    若问题仍然存在请尝试使用 Windows7 兼容模式启动。

## MacOS

### 通过 Brew 安装

1. 添加我们的 tap:

```bash
brew tap pot-app/homebrew-tap
```

2. 安装 pot:

```bash
brew install --cask pot
```

3. 更新 pot

```bash
brew upgrade --cask pot
```

### 手动安装

1. 从 [Release](https://github.com/pot-app/pot-desktop/releases/latest) 页面下载最新的 `dmg` 安装包。（如果您使用的是 M1 芯片，请下载名为`pot_{version}_aarch64.dmg`的安装包，否则请下载名为`pot_{version}_x64.dmg`的安装包）
2. 双击下载的文件后将 pot 拖入 Applications 文件夹即可完成安装。

### 故障排除

-   由于开发者无法验证，“pot”无法打开。

    点击 取消 按钮，然后去 设置 -&gt; 隐私与安全性 页面，点击 仍要打开 按钮，然后在弹出窗口里点击 打开 按钮即可，以后打开 pot 就再也不会有任何弹窗告警了

    如果在 隐私与安全性 中找不到以上选项，或启动时提示文件损坏。打开 Terminal.app，并输入以下命令，然后重启 pot 即可：

    ```bash
    sudo xattr -d com.apple.quarantine /Applications/pot.app
    ```

-   如果每次打开时都遇到辅助功能权限提示，或者无法进行划词翻译，请前往设置 -&gt; 隐私与安全 -&gt; 辅助功能，移除 “pot”，并重新添加 “pot”。

## Linux

### Debian/Ubuntu

1. 从 [Release](https://github.com/pot-app/pot-desktop/releases/latest) 页面下载最新的对应架构的 `deb` 安装包。

2. 使用 `apt-get` 进行安装

    ```bash
    sudo apt-get install ./pot_{version}_amd64.deb
    ```

### Arch/Manjaro

&gt; [!WARNING]
&gt; 在最新版本的 [Webkit2Gtk](https://archlinux.org/packages/extra/x86_64/webkit2gtk) (2.42.0) 中，由于 Nvidia 专有驱动未完全实现 DMABUF，将导致无法启动和崩溃的情况发生。&lt;br&gt;
&gt; 请降级或在 `/etc/environment` （或者其他设置环境变量的地方）中加入 `WEBKIT_DISABLE_DMABUF_RENDERER=1` 环境变量关闭 DMABUF 的使用。

1. 在 [AUR](https://aur.archlinux.org/packages?O=0&amp;K=pot-translation) 查看

使用 `AUR helper` 安装：

```bash
yay -S pot-translation # 或 pot-translation-bin

# paru -S pot-translation # 或 pot-translation-bin
```

2. 如果你使用 `archlinuxcn` 源，可以直接使用 pacman 安装

```bash
sudo pacman -S pot-translation
```

### Flatpak

&gt; [!WARNING]
&gt; Flatpak 版本缺失托盘图标。

&lt;a href=&#039;https://flathub.org/apps/com.pot_app.pot&#039;&gt;
    &lt;img width=&#039;240&#039; alt=&#039;Download on Flathub&#039; src=&#039;https://flathub.org/api/badge?locale=zh-Hans&#039;/&gt;
&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;

# 外部调用

&lt;/div&gt;

Pot 提供了完整的 HTTP 接口，以便可以被其他软件调用。您可以通过向 `127.0.0.1:port` 发送 HTTP 请求来调用 pot，其中的`port`是 pot 监听的端口号，默认为`60828`,可以在软件设置中进行更改。

## API 文档:

```bash
POST &quot;/&quot; =&gt; 翻译指定文本(body为需要翻译的文本),
GET &quot;/config&quot; =&gt; 打开设置,
POST &quot;/translate&quot; =&gt; 翻译指定文本(同&quot;/&quot;),
GET &quot;/selection_translate&quot; =&gt; 划词翻译,
GET &quot;/input_translate&quot; =&gt; 输入翻译,
GET &quot;/ocr_recognize&quot; =&gt; 截图OCR,
GET &quot;/ocr_translate&quot; =&gt; 截图翻译,
GET &quot;/ocr_recognize?screenshot=false&quot; =&gt; 截图OCR(不使用软件内截图),
GET &quot;/ocr_translate?screenshot=false&quot; =&gt; 截图翻译(不使用软件内截图),
GET &quot;/ocr_recognize?screenshot=true&quot; =&gt; 截图OCR,
GET &quot;/ocr_translate?screenshot=true&quot; =&gt; 截图翻译,
```

## 示例：

-   调用划词翻译：

    如果想要调用 pot 划词翻译，只需向`127.0.0.1:port`发送请求即可。

    例如通过 curl 发送请求：

    ```bash
    curl &quot;127.0.0.1:60828/selection_translate&quot;
    ```

## 不使用软件内截图

这一功能可以让您在不使用软件内截图的情况下调用截图 OCR/截图翻译功能，这样您就可以使用您喜欢的截图工具来截图了，也可以解决在某些平台下 pot 自带的截图无法使用的问题。

### 调用流程

1. 使用其他截图工具截图
2. 将截图保存在 `$CACHE/com.pot-app.desktop/pot_screenshot_cut.png`
3. 向`127.0.0.1:port/ocr_recognize?screenshot=false`发送请求即可调用成功

&gt; `$CACHE`为系统缓存目录，例如在 Windows 上为`C:\Users\{用户名}\AppData\Local\com.pot-app.desktop\pot_screenshot_cut.png`

### 示例

在 Linux 下调用 Flameshot 进行截图 OCR:

```bash
rm ~/.cache/com.pot-app.desktop/pot_screenshot_cut.png &amp;&amp; flameshot gui -s -p ~/.cache/com.pot-app.desktop/pot_screenshot_cut.png &amp;&amp; curl &quot;127.0.0.1:60828/ocr_recognize?screenshot=false&quot;
```

## 现有用法 (快捷划词翻译)

### SnipDo (Windows)

1. 从 [Microsoft Store](https://apps.microsoft.com/store/detail/snipdo/9NPZ2TVKJVT7) 下载安装 SnipDo。
2. 从 [Release](https://github.com/pot-app/pot-desktop/releases/latest) 下载 pot 的 SnipDo 扩展 (pot.pbar)
3. 双击下载的扩展文件完成安装。
4. 选中文字，可以看到弹出的 SnipDo 工具条，点击翻译按钮即可翻译。

### PopClip (MacOS)

1. 从 [App Store](https://apps.apple.com/us/app/popclip/id445189367?mt=12) 下载安装 PopClip
2. 从 [Release](https://github.com/pot-app/pot-desktop/releases/latest) 下载 pot 的 PopClip 扩展 (pot.popclipextz)
3. 双击下载的扩展文件完成安装。
4. 在 PopClip 的扩展中启用 pot 扩展，选中文本即可点击翻译。

### Starry (Linux)

&gt; Starry 目前仍处于开发阶段，因此您只能手动编译它。

Github: [ccslykx/Starry](https://github.com/ccslykx/Starry)

&lt;div align=&quot;center&quot;&gt;

# Wayland 支持

&lt;/div&gt;

由于各大发行版对于 Wayland 的支持程度不同，所以 pot 本身没法做到特别完美的支持，这里可以提供一些常见问题的解决方案，通过合理的设置之后，pot 也可以在 Wayland 下完美运行。

## 快捷键无法使用

由于 Tauri 的快捷键方案并没有支持 Wayland，所以 pot 应用内的快捷键设置在 Wayland 下无法使用。 您可以设置系统快捷用 curl 发送请求来触发 pot，详见[外部调用](#外部调用)

## 截图无法使用

在一些纯 Wayland 桌面环境/窗口管理器(如 Hyprland)上，pot 内置的截图无法使用，这时可以通过使用其他截图工具代替，详见 [不使用软件内截图](#不使用软件内截图)

下面给出在 Hyprland 下的配置示例(通过 grim 和 slurp 实现截图)：

```conf
bind = ALT, X, exec, grim -g &quot;$(slurp)&quot; ~/.cache/com.pot-app.desktop/pot_screenshot_cut.png &amp;&amp; curl &quot;127.0.0.1:60828/ocr_recognize?screenshot=false&quot;
bind = ALT, C, exec, grim -g &quot;$(slurp)&quot; ~/.cache/com.pot-app.desktop/pot_screenshot_cut.png &amp;&amp; curl &quot;127.0.0.1:60828/ocr_translate?screenshot=false&quot;
```

其他桌面环境/窗口管理器也是类似的操作

## 划词翻译窗口跟随鼠标位置

由于目前 pot 在 Wayland 下还无法获取到正确的鼠标坐标，所以内部的实现无法工作。 对于某些桌面环境/窗口管理器，可以通过设置窗口规则来实现窗口跟随鼠标位置，这里以 Hyprland 为例：

```conf
windowrulev2 = float, class:(pot), title:(Translator|OCR|PopClip|Screenshot Translate) # Translation window floating
windowrulev2 = move cursor 0 0, class:(pot), title:(Translator|PopClip|Screenshot Translate) # Translation window follows the mouse position.
```

&lt;div align=&quot;center&quot;&gt;

# 国际化([Weblate](https://hosted.weblate.org/engage/pot-app/))

[![](https://hosted.weblate.org/widget/pot-app/pot-desktop/svg-badge.svg)](https://hosted.weblate.org/engage/pot-app/)

[![](https://hosted.weblate.org/widget/pot-app/pot-desktop/zh_Hans/multi-auto.svg)](https://hosted.weblate.org/engage/pot-app/)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

# 贡献者

&lt;/div&gt;

&lt;img src=&quot;https://github.com/pot-app/.github/blob/master/pot-desktop-contributions.svg?raw=true&quot; width=&quot;100%&quot;/&gt;

## 手动编译

### 环境要求

Node.js &gt;= 18.0.0

pnpm &gt;= 8.5.0

Rust &gt;= 1.80.0

### 开始编译

1. Clone 仓库

    ```bash
    git clone https://github.com/pot-app/pot-desktop.git
    ```

2. 安装依赖

    ```bash
    cd pot-desktop
    pnpm install
    ```

3. 安装依赖(仅 Linux 需要)

    ```bash
    sudo apt-get install -y libgtk-3-dev libwebkit2gtk-4.0-dev libayatana-appindicator3-dev librsvg2-dev patchelf libxdo-dev libxcb1 libxrandr2 libdbus-1-3
    ```

4. 开发调试

    ```bash
    pnpm tauri dev # Run the app in development mode
    ```

5. 打包构建
    ```bash
    pnpm tauri build # Build into installation package
    ```

&lt;div align=&quot;center&quot;&gt;

# 感谢

&lt;/div&gt;

-   [Bob](https://github.com/ripperhe/Bob) 灵感来源
-   [bob-plugin-openai-translator](https://github.com/yetone/bob-plugin-openai-translator) OpenAI 接口参考
-   [@uiYzzi](https://github.com/uiYzzi) 实现思路
-   [@Lichenkass](https://github.com/Lichenkass) 维护 Deepin 应用商店中的 pot
-   [Tauri](https://github.com/tauri-apps/tauri) 好用的 GUI 框架

&lt;div align=&quot;center&quot;&gt;
</pre>
          ]]></content:encoded>
            <category>JavaScript</category>
        </item>
        <item>
            <title><![CDATA[upstash/context7]]></title>
            <link>https://github.com/upstash/context7</link>
            <guid>https://github.com/upstash/context7</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:35 GMT</pubDate>
            <description><![CDATA[Context7 MCP Server -- Up-to-date code documentation for LLMs and AI code editors]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/upstash/context7">upstash/context7</a></h1>
            <p>Context7 MCP Server -- Up-to-date code documentation for LLMs and AI code editors</p>
            <p>Language: JavaScript</p>
            <p>Stars: 15,826</p>
            <p>Forks: 781</p>
            <p>Stars today: 266 stars today</p>
            <h2>README</h2><pre># Context7 MCP - Up-to-date Code Docs For Any Prompt

[![Website](https://img.shields.io/badge/Website-context7.com-blue)](https://context7.com) [![smithery badge](https://smithery.ai/badge/@upstash/context7-mcp)](https://smithery.ai/server/@upstash/context7-mcp) [&lt;img alt=&quot;Install in VS Code (npx)&quot; src=&quot;https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square&amp;label=Install%20Context7%20MCP&amp;color=0098FF&quot;&gt;](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%7B%22name%22%3A%22context7%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40upstash%2Fcontext7-mcp%40latest%22%5D%7D)

[![繁體中文](https://img.shields.io/badge/docs-繁體中文-yellow)](./docs/README.zh-TW.md) [![简体中文](https://img.shields.io/badge/docs-简体中文-yellow)](./docs/README.zh-CN.md) [![日本語](https://img.shields.io/badge/docs-日本語-b7003a)](./docs/README.ja.md) [![한국어 문서](https://img.shields.io/badge/docs-한국어-green)](./docs/README.ko.md) [![Documentación en Español](https://img.shields.io/badge/docs-Español-orange)](./docs/README.es.md) [![Documentation en Français](https://img.shields.io/badge/docs-Français-blue)](./docs/README.fr.md) [![Documentação em Português (Brasil)](&lt;https://img.shields.io/badge/docs-Português%20(Brasil)-purple&gt;)](./docs/README.pt-BR.md) [![Documentazione in italiano](https://img.shields.io/badge/docs-Italian-red)](./docs/README.it.md) [![Dokumentasi Bahasa Indonesia](https://img.shields.io/badge/docs-Bahasa%20Indonesia-pink)](./docs/README.id-ID.md) [![Dokumentation auf Deutsch](https://img.shields.io/badge/docs-Deutsch-darkgreen)](./docs/README.de.md) [![Документация на русском языке](https://img.shields.io/badge/docs-Русский-darkblue)](./docs/README.ru.md) [![Türkçe Doküman](https://img.shields.io/badge/docs-Türkçe-blue)](./docs/README.tr.md) [![Arabic Documentation](https://img.shields.io/badge/docs-Arabic-white)](./docs/README.ar.md)

## ❌ Without Context7

LLMs rely on outdated or generic information about the libraries you use. You get:

- ❌ Code examples are outdated and based on year-old training data
- ❌ Hallucinated APIs don&#039;t even exist
- ❌ Generic answers for old package versions

## ✅ With Context7

Context7 MCP pulls up-to-date, version-specific documentation and code examples straight from the source — and places them directly into your prompt.

Add `use context7` to your prompt in Cursor:

```txt
Create a basic Next.js project with app router. use context7
```

```txt
Create a script to delete the rows where the city is &quot;&quot; given PostgreSQL credentials. use context7
```

Context7 fetches up-to-date code examples and documentation right into your LLM&#039;s context.

- 1️⃣ Write your prompt naturally
- 2️⃣ Tell the LLM to `use context7`
- 3️⃣ Get working code answers

No tab-switching, no hallucinated APIs that don&#039;t exist, no outdated code generations.

## 📚 Adding Projects

Check out our [project addition guide](./docs/adding-projects.md) to learn how to add (or update) your favorite libraries to Context7.

## 🛠️ Installation

### Requirements

- Node.js &gt;= v18.0.0
- Cursor, Windsurf, Claude Desktop or another MCP Client

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Installing via Smithery&lt;/b&gt;&lt;/summary&gt;

To install Context7 MCP Server for any client automatically via [Smithery](https://smithery.ai/server/@upstash/context7-mcp):

```bash
npx -y @smithery/cli@latest install @upstash/context7-mcp --client &lt;CLIENT_NAME&gt; --key &lt;YOUR_SMITHERY_KEY&gt;
```

You can find your Smithery key in the [Smithery.ai webpage](https://smithery.ai/server/@upstash/context7-mcp).

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Install in Cursor&lt;/b&gt;&lt;/summary&gt;

Go to: `Settings` -&gt; `Cursor Settings` -&gt; `MCP` -&gt; `Add new global MCP server`

Pasting the following configuration into your Cursor `~/.cursor/mcp.json` file is the recommended approach. You may also install in a specific project by creating `.cursor/mcp.json` in your project folder. See [Cursor MCP docs](https://docs.cursor.com/context/model-context-protocol) for more info.

&gt; Since Cursor 1.0, you can click the install button below for instant one-click installation.

#### Cursor Remote Server Connection

[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=context7&amp;config=eyJ1cmwiOiJodHRwczovL21jcC5jb250ZXh0Ny5jb20vbWNwIn0%3D)

```json
{
  &quot;mcpServers&quot;: {
    &quot;context7&quot;: {
      &quot;url&quot;: &quot;https://mcp.context7.com/mcp&quot;
    }
  }
}
```

#### Cursor Local Server Connection

[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=context7&amp;config=eyJjb21tYW5kIjoibnB4IC15IEB1cHN0YXNoL2NvbnRleHQ3LW1jcCJ9)

```json
{
  &quot;mcpServers&quot;: {
    &quot;context7&quot;: {
      &quot;command&quot;: &quot;npx&quot;,
      &quot;args&quot;: [&quot;-y&quot;, &quot;@upstash/context7-mcp&quot;]
    }
  }
}
```

&lt;details&gt;
&lt;summary&gt;Alternative: Use Bun&lt;/summary&gt;

[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=context7&amp;config=eyJjb21tYW5kIjoiYnVueCAteSBAdXBzdGFzaC9jb250ZXh0Ny1tY3AifQ%3D%3D)

```json
{
  &quot;mcpServers&quot;: {
    &quot;context7&quot;: {
      &quot;command&quot;: &quot;bunx&quot;,
      &quot;args&quot;: [&quot;-y&quot;, &quot;@upstash/context7-mcp&quot;]
    }
  }
}
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Alternative: Use Deno&lt;/summary&gt;

[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=context7&amp;config=eyJjb21tYW5kIjoiZGVubyBydW4gLS1hbGxvdy1lbnYgLS1hbGxvdy1uZXQgbnBtOkB1cHN0YXNoL2NvbnRleHQ3LW1jcCJ9)

```json
{
  &quot;mcpServers&quot;: {
    &quot;context7&quot;: {
      &quot;command&quot;: &quot;deno&quot;,
      &quot;args&quot;: [&quot;run&quot;, &quot;--allow-env=NO_DEPRECATION,TRACE_DEPRECATION&quot;, &quot;--allow-net&quot;, &quot;npm:@upstash/context7-mcp&quot;]
    }
  }
}
```

&lt;/details&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Install in Windsurf&lt;/b&gt;&lt;/summary&gt;

Add this to your Windsurf MCP config file. See [Windsurf MCP docs](https://docs.windsurf.com/windsurf/mcp) for more info.

#### Windsurf Remote Server Connection

```json
{
  &quot;mcpServers&quot;: {
    &quot;context7&quot;: {
      &quot;serverUrl&quot;: &quot;https://mcp.context7.com/sse&quot;
    }
  }
}
```

#### Windsurf Local Server Connection

```json
{
  &quot;mcpServers&quot;: {
    &quot;context7&quot;: {
      &quot;command&quot;: &quot;npx&quot;,
      &quot;args&quot;: [&quot;-y&quot;, &quot;@upstash/context7-mcp&quot;]
    }
  }
}
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Install in VS Code&lt;/b&gt;&lt;/summary&gt;

[&lt;img alt=&quot;Install in VS Code (npx)&quot; src=&quot;https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square&amp;label=Install%20Context7%20MCP&amp;color=0098FF&quot;&gt;](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%7B%22name%22%3A%22context7%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40upstash%2Fcontext7-mcp%40latest%22%5D%7D)
[&lt;img alt=&quot;Install in VS Code Insiders (npx)&quot; src=&quot;https://img.shields.io/badge/VS_Code_Insiders-VS_Code_Insiders?style=flat-square&amp;label=Install%20Context7%20MCP&amp;color=24bfa5&quot;&gt;](https://insiders.vscode.dev/redirect?url=vscode-insiders%3Amcp%2Finstall%3F%7B%22name%22%3A%22context7%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40upstash%2Fcontext7-mcp%40latest%22%5D%7D)

Add this to your VS Code MCP config file. See [VS Code MCP docs](https://code.visualstudio.com/docs/copilot/chat/mcp-servers) for more info.

#### VS Code Remote Server Connection

```json
&quot;mcp&quot;: {
  &quot;servers&quot;: {
    &quot;context7&quot;: {
      &quot;type&quot;: &quot;http&quot;,
      &quot;url&quot;: &quot;https://mcp.context7.com/mcp&quot;
    }
  }
}
```

#### VS Code Local Server Connection

```json
&quot;mcp&quot;: {
  &quot;servers&quot;: {
    &quot;context7&quot;: {
      &quot;type&quot;: &quot;stdio&quot;,
      &quot;command&quot;: &quot;npx&quot;,
      &quot;args&quot;: [&quot;-y&quot;, &quot;@upstash/context7-mcp&quot;]
    }
  }
}
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Install in Visual Studio 2022&lt;/b&gt;&lt;/summary&gt;

You can configure Context7 MCP in Visual Studio 2022 by following the [Visual Studio MCP Servers documentation](https://learn.microsoft.com/visualstudio/ide/mcp-servers?view=vs-2022).

Add this to your Visual Studio MCP config file (see the [Visual Studio docs](https://learn.microsoft.com/visualstudio/ide/mcp-servers?view=vs-2022) for details):

```json
{
  &quot;mcp&quot;: {
    &quot;servers&quot;: {
      &quot;context7&quot;: {
        &quot;type&quot;: &quot;http&quot;,
        &quot;url&quot;: &quot;https://mcp.context7.com/mcp&quot;
      }
    }
  }
}
```

Or, for a local server:

```json
{
  &quot;mcp&quot;: {
    &quot;servers&quot;: {
      &quot;context7&quot;: {
        &quot;type&quot;: &quot;stdio&quot;,
        &quot;command&quot;: &quot;npx&quot;,
        &quot;args&quot;: [&quot;-y&quot;, &quot;@upstash/context7-mcp&quot;]
      }
    }
  }
}
```

For more information and troubleshooting, refer to the [Visual Studio MCP Servers documentation](https://learn.microsoft.com/visualstudio/ide/mcp-servers?view=vs-2022).
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Install in Zed&lt;/b&gt;&lt;/summary&gt;

It can be installed via [Zed Extensions](https://zed.dev/extensions?query=Context7) or you can add this to your Zed `settings.json`. See [Zed Context Server docs](https://zed.dev/docs/assistant/context-servers) for more info.

```json
{
  &quot;context_servers&quot;: {
    &quot;Context7&quot;: {
      &quot;command&quot;: {
        &quot;path&quot;: &quot;npx&quot;,
        &quot;args&quot;: [&quot;-y&quot;, &quot;@upstash/context7-mcp&quot;]
      },
      &quot;settings&quot;: {}
    }
  }
}
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Install in Gemini CLI&lt;/b&gt;&lt;/summary&gt;

See [Gemini CLI Configuration](https://github.com/google-gemini/gemini-cli/blob/main/docs/cli/configuration.md) for details.

1.  Open the Gemini CLI settings file. The location is `~/.gemini/settings.json` (where `~` is your home directory).
2.  Add the following to the `mcpServers` object in your `settings.json` file:

```json
{
  &quot;mcpServers&quot;: {
    &quot;context7&quot;: {
      &quot;command&quot;: &quot;npx&quot;,
      &quot;args&quot;: [&quot;-y&quot;, &quot;@upstash/context7-mcp&quot;]
    }
  }
}
```

If the `mcpServers` object does not exist, create it.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Install in Claude Code&lt;/b&gt;&lt;/summary&gt;

Run this command. See [Claude Code MCP docs](https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials#set-up-model-context-protocol-mcp) for more info.

#### Claude Code Remote Server Connection

```sh
claude mcp add --transport http context7 https://mcp.context7.com/mcp
```

Or using SSE transport:

```sh
claude mcp add --transport sse context7 https://mcp.context7.com/sse
```

#### Claude Code Local Server Connection

```sh
claude mcp add context7 -- npx -y @upstash/context7-mcp
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Install in Claude Desktop&lt;/b&gt;&lt;/summary&gt;

Add this to your Claude Desktop `claude_desktop_config.json` file. See [Claude Desktop MCP docs](https://modelcontextprotocol.io/quickstart/user) for more info.

```json
{
  &quot;mcpServers&quot;: {
    &quot;Context7&quot;: {
      &quot;command&quot;: &quot;npx&quot;,
      &quot;args&quot;: [&quot;-y&quot;, &quot;@upstash/context7-mcp&quot;]
    }
  }
}
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;b&gt;Install in Cline&lt;/b&gt;
&lt;/summary&gt;

You can easily install Context7 through the [Cline MCP Server Marketplace](https://cline.bot/mcp-marketplace) by following these instructions:

1. Open **Cline**.
1. Click the hamburger menu icon (☰) to enter the **MCP Servers** section.
2. Use the search bar within the **Marketplace** tab to find *Context7*.
3. Click the **Install** button.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Install in BoltAI&lt;/b&gt;&lt;/summary&gt;

Open the &quot;Settings&quot; page of the app, navigate to &quot;Plugins,&quot; and enter the following JSON:

```json
{
  &quot;mcpServers&quot;: {
    &quot;context7&quot;: {
      &quot;command&quot;: &quot;npx&quot;,
      &quot;args&quot;: [&quot;-y&quot;, &quot;@upstash/context7-mcp&quot;]
    }
  }
}
```

Once saved, enter in the chat `get-library-docs` followed by your Context7 documentation ID (e.g., `get-library-docs /nuxt/ui`). More information is available on [BoltAI&#039;s Documentation site](https://docs.boltai.com/docs/plugins/mcp-servers). For BoltAI on iOS, [see this guide](https://docs.boltai.com/docs/boltai-mobile/mcp-servers).

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Using Docker&lt;/b&gt;&lt;/summary&gt;

If you prefer to run the MCP server in a Docker container:

1. **Build the Docker Image:**

   First, create a `Dockerfile` in the project root (or anywhere you prefer):

   &lt;details&gt;
   &lt;summary&gt;Click to see Dockerfile content&lt;/summary&gt;

   ```Dockerfile
   FROM node:18-alpine

   WORKDIR /app

   # Install the latest version globally
   RUN npm install -g @upstash/context7-mcp

   # Expose default port if needed (optional, depends on MCP client interaction)
   # EXPOSE 3000

   # Default command to run the server
   CMD [&quot;context7-mcp&quot;]
   ```

   &lt;/details&gt;

   Then, build the image using a tag (e.g., `context7-mcp`). **Make sure Docker Desktop (or the Docker daemon) is running.** Run the following command in the same directory where you saved the `Dockerfile`:

   ```bash
   docker build -t context7-mcp .
   ```

2. **Configure Your MCP Client:**

   Update your MCP client&#039;s configuration to use the Docker command.

   _Example for a cline_mcp_settings.json:_

   ```json
   {
     &quot;mcpServers&quot;: {
       &quot;Сontext7&quot;: {
         &quot;autoApprove&quot;: [],
         &quot;disabled&quot;: false,
         &quot;timeout&quot;: 60,
         &quot;command&quot;: &quot;docker&quot;,
         &quot;args&quot;: [&quot;run&quot;, &quot;-i&quot;, &quot;--rm&quot;, &quot;context7-mcp&quot;],
         &quot;transportType&quot;: &quot;stdio&quot;
       }
     }
   }
   ```

   _Note: This is an example configuration. Please refer to the specific examples for your MCP client (like Cursor, VS Code, etc.) earlier in this README to adapt the structure (e.g., `mcpServers` vs `servers`). Also, ensure the image name in `args` matches the tag used during the `docker build` command._

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Install in Windows&lt;/b&gt;&lt;/summary&gt;

The configuration on Windows is slightly different compared to Linux or macOS (_`Cline` is used in the example_). The same principle applies to other editors; refer to the configuration of `command` and `args`.

```json
{
  &quot;mcpServers&quot;: {
    &quot;github.com/upstash/context7-mcp&quot;: {
      &quot;command&quot;: &quot;cmd&quot;,
      &quot;args&quot;: [&quot;/c&quot;, &quot;npx&quot;, &quot;-y&quot;, &quot;@upstash/context7-mcp@latest&quot;],
      &quot;disabled&quot;: false,
      &quot;autoApprove&quot;: []
    }
  }
}
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Install in Augment Code&lt;/b&gt;&lt;/summary&gt;

To configure Context7 MCP in Augment Code, you can use either the graphical interface or manual configuration.

### **A. Using the Augment Code UI**

1. Click the hamburger menu.
2. Select **Settings**.
3. Navigate to the **Tools** section.
4. Click the **+ Add MCP** button.
5. Enter the following command:

   ```
   npx -y @upstash/context7-mcp@latest
   ```

6. Name the MCP: **Context7**.
7. Click the **Add** button.

Once the MCP server is added, you can start using Context7&#039;s up-to-date code documentation features directly within Augment Code.

---

### **B. Manual Configuration**

1. Press Cmd/Ctrl Shift P or go to the hamburger menu in the Augment panel
2. Select Edit Settings
3. Under Advanced, click Edit in settings.json
4. Add the server configuration to the `mcpServers` array in the `augment.advanced` object

&quot;augment.advanced&quot;: {
&quot;mcpServers&quot;: [
{
&quot;name&quot;: &quot;context7&quot;,
&quot;command&quot;: &quot;npx&quot;,
&quot;args&quot;: [&quot;-y&quot;, &quot;@upstash/context7-mcp&quot;]
}
]
}

Once the MCP server is added, restart your editor. If you receive any errors, check the syntax to make sure closing brackets or commas are not missing.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Install in Roo Code&lt;/b&gt;&lt;/summary&gt;

Add this to your Roo Code MCP configuration file. See [Roo Code MCP docs](https://docs.roocode.com/features/mcp/using-mcp-in-roo) for more info.

#### Roo Code Remote Server Connection

```json
{
  &quot;mcpServers&quot;: {
    &quot;context7&quot;: {
      &quot;type&quot;: &quot;streamable-http&quot;,
      &quot;url&quot;: &quot;https://mcp.context7.com/mcp&quot;
    }
  }
}
```

#### Roo Code Local Server Connection

```json
{
  &quot;mcpServers&quot;: {
    &quot;context7&quot;: {
      &quot;command&quot;: &quot;npx&quot;,
      &quot;args&quot;: [&quot;-y&quot;, &quot;@upstash/context7-mcp&quot;]
    }
  }
}
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Install in Zencoder&lt;/b&gt;&lt;/summary&gt;

To configure Context7 MCP in Zencoder, follow these steps:

1. Go to the Zencoder menu (...)
2. From the dropdown menu, select Agent tools
3. Click on the Add custom MCP
4. Add the name and server configuration from below, and make sure to hit the Install button

```json
{
    &quot;command&quot;: &quot;npx&quot;,
    &quot;args&quot;: [
        &quot;-y&quot;,
        &quot;@upstash/context7-mcp@latest&quot;
    ]
}
```

Once the MCP server is added, you can easily continue using it.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Install in Amazon Q Developer CLI&lt;/b&gt;&lt;/summary&gt;

Add this to your Amazon Q Developer CLI configuration file. See [Amazon Q Developer CLI docs](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/command-line-mcp-configuration.html) for more details.

```json
{
  &quot;mcpServers&quot;: {
    &quot;context7&quot;: {
      &quot;command&quot;: &quot;npx&quot;,
      &quot;args&quot;: [&quot;-y&quot;, &quot;@upstash/context7-mcp@latest&quot;]
    }
  }
}
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Install in Qodo Gen&lt;/b&gt;&lt;/summary&gt;

See [Qodo Gen docs](https://docs.qodo.ai/qodo-documentation/qodo-gen/qodo-gen-chat/agentic-mode/agentic-tools-mcps) for more details.

1. Open Qodo Gen chat panel in VSCode or IntelliJ.
2. Click Connect more tools.
3. Click + Add new MCP.
4. Add the following configuration:

```json
{
  &quot;mcpServers&quot;: {
    &quot;context7&quot;: {
      &quot;url&quot;: &quot;https://mcp.context7.com/mcp&quot;
    }
  }
}
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Install in JetBrains AI Assistant&lt;/b&gt;&lt;/summary&gt;

See [JetBrains AI Assistant Documentation](https://www.jetbrains.com/help/ai-assistant/configure-an-mcp-server.html) for more details.

1. In JetBrains IDEs go to `Settings` -&gt; `Tools` -&gt; `AI Assistant` -&gt; `Model Context Protocol (MCP)`
2. Click `+ Add`.
3. Click on `Command` in the top-left corner of the dialog and select the As JSON option from the list
4. Add this configuration and click `OK`

```json
{
  &quot;mcpServers&quot;: {
    &quot;context7&quot;: {
      &quot;command&quot;: &quot;npx&quot;,
      &quot;args&quot;: [&quot;-y&quot;, &quot;@upstash/context7-mcp&quot;]
    }
  }
}
```

5. Click `Apply` to save changes.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Install in Warp&lt;/b&gt;&lt;/summary&gt;

See [Warp Model Context Protocol Documentation](https://docs.warp.dev/knowledge-and-collaboration/mcp#adding-an-mcp-server) for details.

1. Navigate `Settings` &gt; `AI` &gt; `Manage MCP servers`.
2. Add a new MCP server by clicking the `+ Add` button.
3. Paste the configuration given below:

```json
{
  &quot;Context7&quot;: {
    &quot;command&quot;: &quot;npx&quot;,
    &quot;args&quot;: [
      &quot;-y&quot;,
      &quot;@upstash/context7-mcp&quot;
    ],
    &quot;env&quot;: {},
    &quot;working_directory&quot;: null,
    &quot;start_on_launch&quot;: true
  }
}
```

4. Click `Save` to apply the changes.

&lt;/details&gt;

## 🔨 Available Tools

Context7 MCP provides the following tools that LLMs can use:

- `resolve-library-id`: Resolves a general library name into a Context7-compatible library ID.

  - `libraryName` (required): The name of the library to search for

- `get-library-docs`: Fetches documentation for a library using a Context7-compatible library ID.
  - `context7CompatibleLibraryID` (required): Exact Context7-compatible library ID (e.g., `/mongodb/docs`, `/vercel/next.js`)
  - `topic` (optional): Focus the docs on a specific topic (e.g., &quot;routing&quot;, &quot;hooks&quot;)
  - `tokens` (optional, default 10000): Max number of tokens to return. Values less than the default value of 10000 are automatically increased to 10000.

## 💻 Development

Clone the project and install dependencies:

```bash
bun i
```

Build:

```bash
bun run build
```

Run the server:

```bash
bun run dist/index.js
```

### CLI Arguments

`context7-mcp` accepts the following CLI flags:

- `--transport &lt;stdio|http|sse&gt;` – Transport to use (`stdio` by default).
- `--port &lt;number&gt;` – Port to listen on when using `http` or `sse` transport (default `3000`).

Example with http transport and port 8080:

```bash
bun run dist/index.js --transport http --port 8080
```

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Local Configuration Example&lt;/b&gt;&lt;/summary&gt;

```json
{
  &quot;mcpServers&quot;: {
    &quot;context7&quot;: {
      &quot;command&quot;: &quot;npx&quot;,
      &quot;args&quot;: [&quot;tsx&quot;, &quot;/path/to/folder/context7-mcp/src/index.ts&quot;]
    }
  }
}
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Testing with MCP Inspector&lt;/b&gt;&lt;/summary&gt;

```bash
npx -y @modelcontextprotocol/inspector npx @upstash/context7-mcp
```

&lt;/details&gt;

## 🚨 Troubleshooting

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Module Not Found Errors&lt;/b&gt;&lt;/summary&gt;

If you encounter `ERR_MODULE_NOT_FOUND`, try using `bunx` instead of `npx`:

```json
{
  &quot;mcpServers&quot;: {
    &quot;context7&quot;: {
      &quot;command&quot;: &quot;bunx&quot;,
      &quot;args&quot;: [&quot;-y&quot;, &quot;@upstash/context7-mcp&quot;]
    }
  }
}
```

This often resolves module resolution issues in environments where `npx` doesn&#039;

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>JavaScript</category>
        </item>
        <item>
            <title><![CDATA[academind/react-complete-guide-course-resources]]></title>
            <link>https://github.com/academind/react-complete-guide-course-resources</link>
            <guid>https://github.com/academind/react-complete-guide-course-resources</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:34 GMT</pubDate>
            <description><![CDATA[React - The Complete Guide Course Resources (Code, Attachments, Slides)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/academind/react-complete-guide-course-resources">academind/react-complete-guide-course-resources</a></h1>
            <p>React - The Complete Guide Course Resources (Code, Attachments, Slides)</p>
            <p>Language: JavaScript</p>
            <p>Stars: 3,236</p>
            <p>Forks: 2,489</p>
            <p>Stars today: 2 stars today</p>
            <h2>README</h2><pre># React - The Complete Guide Course Resources

This repository provides access to code files, code snapshots, slides &amp; other resources that are used or provided by the [React - The Complete Guide](https://acad.link/reactjs) course.

If you&#039;re facing any issues with the code, please post in the course Q&amp;A section.

# Repository Content

- **Code Snapshots:** All code snapshots (starting snapshots, intermediate snapshots, finished snapshots) for the various course sections can be found in the [/code](/code/) folder.
- **Lecture Attachments:** Any standalone code files or other attachments that are mentioned in course lectures (and attached to those lectures) are stored in the [/attachments](/attachments/) folder.
- **Other Resources:** Other resources (like the course slides) can be found in the [/other](/other/) folder.

The **Code Snapshots** and **Lecture Attachments** folders contain one subfolder per course section - this allows you to easily access the resources for a specific course section.

# How To Use Code Snapshots

Code snapshots are primarily provided to allow you to compare your code to mine. The snapshots are taken directly from the course recordings and therefore reflect my code you see in the videos.

Of course, you can also try running those code snapshots on your machine. You&#039;ll need to run `npm install` in the individual snapshot folders, followed by `npm run dev` to start the development server - just as shown in the course.
</pre>
          ]]></content:encoded>
            <category>JavaScript</category>
        </item>
        <item>
            <title><![CDATA[danielmiessler/Fabric]]></title>
            <link>https://github.com/danielmiessler/Fabric</link>
            <guid>https://github.com/danielmiessler/Fabric</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:33 GMT</pubDate>
            <description><![CDATA[Fabric is an open-source framework for augmenting humans using AI. It provides a modular system for solving specific problems using a crowdsourced set of AI prompts that can be used anywhere.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/danielmiessler/Fabric">danielmiessler/Fabric</a></h1>
            <p>Fabric is an open-source framework for augmenting humans using AI. It provides a modular system for solving specific problems using a crowdsourced set of AI prompts that can be used anywhere.</p>
            <p>Language: JavaScript</p>
            <p>Stars: 31,756</p>
            <p>Forks: 3,301</p>
            <p>Stars today: 34 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
Fabric is graciously supported by…

[![Github Repo Tagline](https://github.com/user-attachments/assets/96ab3d81-9b13-4df4-ba09-75dee7a5c3d2)](https://warp.dev/fabric)

&lt;img src=&quot;./images/fabric-logo-gif.gif&quot; alt=&quot;fabriclogo&quot; width=&quot;400&quot; height=&quot;400&quot;/&gt;

# `fabric`

![Static Badge](https://img.shields.io/badge/mission-human_flourishing_via_AI_augmentation-purple)
&lt;br /&gt;
![GitHub top language](https://img.shields.io/github/languages/top/danielmiessler/fabric)
![GitHub last commit](https://img.shields.io/github/last-commit/danielmiessler/fabric)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/danielmiessler/fabric)

&lt;div align=&quot;center&quot;&gt;
&lt;p class=&quot;align center&quot;&gt;
&lt;h4&gt;&lt;code&gt;fabric&lt;/code&gt; is an open-source framework for augmenting humans using AI.&lt;/h4&gt;
&lt;/p&gt;
&lt;/div&gt;

[Updates](#updates) •
[What and Why](#what-and-why) •
[Philosophy](#philosophy) •
[Installation](#installation) •
[Usage](#usage) •
[Examples](#examples) •
[Just Use the Patterns](#just-use-the-patterns) •
[Custom Patterns](#custom-patterns) •
[Helper Apps](#helper-apps) •
[Meta](#meta)

![Screenshot of fabric](images/fabric-summarize.png)

&lt;/div&gt;

## What and why

Since the start of modern AI in late 2022 we&#039;ve seen an **_extraordinary_** number of AI applications for accomplishing tasks. There are thousands of websites, chat-bots, mobile apps, and other interfaces for using all the different AI out there.

It&#039;s all really exciting and powerful, but _it&#039;s not easy to integrate this functionality into our lives._

&lt;p class=&quot;align center&quot;&gt;
&lt;h4&gt;In other words, AI doesn&#039;t have a capabilities problem—it has an &lt;em&gt;integration&lt;/em&gt; problem.&lt;/h4&gt;
&lt;/p&gt;

**Fabric was created to address this by creating and organizing the fundamental units of AI—the prompts themselves!**

Fabric organizes prompts by real-world task, allowing people to create, collect, and organize their most important AI solutions in a single place for use in their favorite tools. And if you&#039;re command-line focused, you can use Fabric itself as the interface!

## Intro videos

Keep in mind that many of these were recorded when Fabric was Python-based, so remember to use the current [install instructions](#installation) below.

- [Network Chuck](https://www.youtube.com/watch?v=UbDyjIIGaxQ)
- [David Bombal](https://www.youtube.com/watch?v=vF-MQmVxnCs)
- [My Own Intro to the Tool](https://www.youtube.com/watch?v=wPEyyigh10g)
- [More Fabric YouTube Videos](https://www.youtube.com/results?search_query=fabric+ai)

## Navigation

- [`fabric`](#fabric)
  - [What and why](#what-and-why)
  - [Intro videos](#intro-videos)
  - [Navigation](#navigation)
  - [Updates](#updates)
  - [Philosophy](#philosophy)
    - [Breaking problems into components](#breaking-problems-into-components)
    - [Too many prompts](#too-many-prompts)
  - [Installation](#installation)
    - [Get Latest Release Binaries](#get-latest-release-binaries)
      - [Windows](#windows)
      - [macOS (arm64)](#macos-arm64)
      - [macOS (amd64)](#macos-amd64)
      - [Linux (amd64)](#linux-amd64)
      - [Linux (arm64)](#linux-arm64)
    - [Using package managers](#using-package-managers)
      - [macOS (Homebrew)](#macos-homebrew)
      - [Arch Linux (AUR)](#arch-linux-aur)
    - [From Source](#from-source)
    - [Environment Variables](#environment-variables)
    - [Setup](#setup)
    - [Add aliases for all patterns](#add-aliases-for-all-patterns)
      - [Save your files in markdown using aliases](#save-your-files-in-markdown-using-aliases)
    - [Migration](#migration)
    - [Upgrading](#upgrading)
    - [Shell Completions](#shell-completions)
      - [Zsh Completion](#zsh-completion)
      - [Bash Completion](#bash-completion)
      - [Fish Completion](#fish-completion)
  - [Usage](#usage)
  - [Our approach to prompting](#our-approach-to-prompting)
  - [Examples](#examples)
  - [Just use the Patterns](#just-use-the-patterns)
    - [Prompt Strategies](#prompt-strategies)
  - [Custom Patterns](#custom-patterns)
  - [Helper Apps](#helper-apps)
    - [`to_pdf`](#to_pdf)
    - [`to_pdf` Installation](#to_pdf-installation)
    - [`code_helper`](#code_helper)
  - [pbpaste](#pbpaste)
  - [Web Interface](#web-interface)
    - [Installing](#installing)
    - [Streamlit UI](#streamlit-ui)
      - [Clipboard Support](#clipboard-support)
  - [Meta](#meta)
    - [Primary contributors](#primary-contributors)
    - [Contributors](#contributors)

&lt;br /&gt;

## Updates

&gt; [!NOTE]
&gt;
&gt;June 17, 2025
&gt;
&gt;- Fabric now supports Perplexity AI. Configure it by using `fabric -S` to add your Perplexity AI API Key,
&gt;   and then try:
&gt;
&gt;   ```bash
&gt;   fabric -m sonar-pro &quot;What is the latest world news?&quot;
&gt;   ```
&gt;
&gt;June 11, 2025
&gt;
&gt;- Fabric&#039;s YouTube transcription now needs `yt-dlp` to be installed. Make sure to install the latest
&gt; version (2025.06.09 as of this note). The YouTube API key is only needed for comments (the `--comments` flag)
&gt; and metadata extraction (the `--metadata` flag).

## Philosophy

&gt; AI isn&#039;t a thing; it&#039;s a _magnifier_ of a thing. And that thing is **human creativity**.

We believe the purpose of technology is to help humans flourish, so when we talk about AI we start with the **human** problems we want to solve.

### Breaking problems into components

Our approach is to break problems into individual pieces (see below) and then apply AI to them one at a time. See below for some examples.

&lt;img width=&quot;2078&quot; alt=&quot;augmented_challenges&quot; src=&quot;https://github.com/danielmiessler/fabric/assets/50654/31997394-85a9-40c2-879b-b347e4701f06&quot;&gt;

### Too many prompts

Prompts are good for this, but the biggest challenge I faced in 2023——which still exists today—is **the sheer number of AI prompts out there**. We all have prompts that are useful, but it&#039;s hard to discover new ones, know if they are good or not, _and manage different versions of the ones we like_.

One of `fabric`&#039;s primary features is helping people collect and integrate prompts, which we call _Patterns_, into various parts of their lives.

Fabric has Patterns for all sorts of life and work activities, including:

- Extracting the most interesting parts of YouTube videos and podcasts
- Writing an essay in your own voice with just an idea as an input
- Summarizing opaque academic papers
- Creating perfectly matched AI art prompts for a piece of writing
- Rating the quality of content to see if you want to read/watch the whole thing
- Getting summaries of long, boring content
- Explaining code to you
- Turning bad documentation into usable documentation
- Creating social media posts from any content input
- And a million more…

## Installation

To install Fabric, you can use the latest release binaries or install it from the source.

### Get Latest Release Binaries

#### Windows

`https://github.com/danielmiessler/fabric/releases/latest/download/fabric-windows-amd64.exe`

#### macOS (arm64)

`curl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-darwin-arm64 &gt; fabric &amp;&amp; chmod +x fabric &amp;&amp; ./fabric --version`

#### macOS (amd64)

`curl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-darwin-amd64 &gt; fabric &amp;&amp; chmod +x fabric &amp;&amp; ./fabric --version`

#### Linux (amd64)

`curl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-linux-amd64 &gt; fabric &amp;&amp; chmod +x fabric &amp;&amp; ./fabric --version`

#### Linux (arm64)

`curl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-linux-arm64 &gt; fabric &amp;&amp; chmod +x fabric &amp;&amp; ./fabric --version`

### Using package managers

**NOTE:** using Homebrew or the Arch Linux package managers makes `fabric` available as `fabric-ai`, so add
the following alias to your shell startup files to account for this:

```bash
alias fabric=&#039;fabric-ai&#039;
```

#### macOS (Homebrew)

`brew install fabric-ai`

#### Arch Linux (AUR)

`yay -S fabric-ai`

### From Source

To install Fabric, [make sure Go is installed](https://go.dev/doc/install), and then run the following command.

```bash
# Install Fabric directly from the repo
go install github.com/danielmiessler/fabric@latest
```

### Environment Variables

You may need to set some environment variables in your `~/.bashrc` on linux or `~/.zshrc` file on mac to be able to run the `fabric` command. Here is an example of what you can add:

For Intel based macs or linux

```bash
# Golang environment variables
export GOROOT=/usr/local/go
export GOPATH=$HOME/go

# Update PATH to include GOPATH and GOROOT binaries
export PATH=$GOPATH/bin:$GOROOT/bin:$HOME/.local/bin:$PATH
```

for Apple Silicon based macs

```bash
# Golang environment variables
export GOROOT=$(brew --prefix go)/libexec
export GOPATH=$HOME/go
export PATH=$GOPATH/bin:$GOROOT/bin:$HOME/.local/bin:$PATH
```

### Setup

Now run the following command

```bash
# Run the setup to set up your directories and keys
fabric --setup
```

If everything works you are good to go.

### Add aliases for all patterns

In order to add aliases for all your patterns and use them directly as commands ie. `summarize` instead of `fabric --pattern summarize`
You can add the following to your `.zshrc` or `.bashrc` file.

```bash
# Loop through all files in the ~/.config/fabric/patterns directory
for pattern_file in $HOME/.config/fabric/patterns/*; do
    # Get the base name of the file (i.e., remove the directory path)
    pattern_name=$(basename &quot;$pattern_file&quot;)

    # Create an alias in the form: alias pattern_name=&quot;fabric --pattern pattern_name&quot;
    alias_command=&quot;alias $pattern_name=&#039;fabric --pattern $pattern_name&#039;&quot;

    # Evaluate the alias command to add it to the current shell
    eval &quot;$alias_command&quot;
done

yt() {
    if [ &quot;$#&quot; -eq 0 ] || [ &quot;$#&quot; -gt 2 ]; then
        echo &quot;Usage: yt [-t | --timestamps] youtube-link&quot;
        echo &quot;Use the &#039;-t&#039; flag to get the transcript with timestamps.&quot;
        return 1
    fi

    transcript_flag=&quot;--transcript&quot;
    if [ &quot;$1&quot; = &quot;-t&quot; ] || [ &quot;$1&quot; = &quot;--timestamps&quot; ]; then
        transcript_flag=&quot;--transcript-with-timestamps&quot;
        shift
    fi
    local video_link=&quot;$1&quot;
    fabric -y &quot;$video_link&quot; $transcript_flag
}
```

You can add the below code for the equivalent aliases inside PowerShell by running `notepad $PROFILE` inside a PowerShell window:

```powershell
# Path to the patterns directory
$patternsPath = Join-Path $HOME &quot;.config/fabric/patterns&quot;
foreach ($patternDir in Get-ChildItem -Path $patternsPath -Directory) {
    $patternName = $patternDir.Name

    # Dynamically define a function for each pattern
    $functionDefinition = @&quot;
function $patternName {
    [CmdletBinding()]
    param(
        [Parameter(ValueFromPipeline = `$true)]
        [string] `$InputObject,

        [Parameter(ValueFromRemainingArguments = `$true)]
        [String[]] `$patternArgs
    )

    begin {
        # Initialize an array to collect pipeline input
        `$collector = @()
    }

    process {
        # Collect pipeline input objects
        if (`$InputObject) {
            `$collector += `$InputObject
        }
    }

    end {
        # Join all pipeline input into a single string, separated by newlines
        `$pipelineContent = `$collector -join &quot;`n&quot;

        # If there&#039;s pipeline input, include it in the call to fabric
        if (`$pipelineContent) {
            `$pipelineContent | fabric --pattern $patternName `$patternArgs
        } else {
            # No pipeline input; just call fabric with the additional args
            fabric --pattern $patternName `$patternArgs
        }
    }
}
&quot;@
    # Add the function to the current session
    Invoke-Expression $functionDefinition
}

# Define the &#039;yt&#039; function as well
function yt {
    [CmdletBinding()]
    param(
        [Parameter()]
        [Alias(&quot;timestamps&quot;)]
        [switch]$t,

        [Parameter(Position = 0, ValueFromPipeline = $true)]
        [string]$videoLink
    )

    begin {
        $transcriptFlag = &quot;--transcript&quot;
        if ($t) {
            $transcriptFlag = &quot;--transcript-with-timestamps&quot;
        }
    }

    process {
        if (-not $videoLink) {
            Write-Error &quot;Usage: yt [-t | --timestamps] youtube-link&quot;
            return
        }
    }

    end {
        if ($videoLink) {
            # Execute and allow output to flow through the pipeline
            fabric -y $videoLink $transcriptFlag
        }
    }
}
```

This also creates a `yt` alias that allows you to use `yt https://www.youtube.com/watch?v=4b0iet22VIk` to get transcripts, comments, and metadata.

#### Save your files in markdown using aliases

If in addition to the above aliases you would like to have the option to save the output to your favorite markdown note vault like Obsidian then instead of the above add the following to your `.zshrc` or `.bashrc` file:

```bash
# Define the base directory for Obsidian notes
obsidian_base=&quot;/path/to/obsidian&quot;

# Loop through all files in the ~/.config/fabric/patterns directory
for pattern_file in ~/.config/fabric/patterns/*; do
    # Get the base name of the file (i.e., remove the directory path)
    pattern_name=$(basename &quot;$pattern_file&quot;)

    # Remove any existing alias with the same name
    unalias &quot;$pattern_name&quot; 2&gt;/dev/null

    # Define a function dynamically for each pattern
    eval &quot;
    $pattern_name() {
        local title=\$1
        local date_stamp=\$(date +&#039;%Y-%m-%d&#039;)
        local output_path=\&quot;\$obsidian_base/\${date_stamp}-\${title}.md\&quot;

        # Check if a title was provided
        if [ -n \&quot;\$title\&quot; ]; then
            # If a title is provided, use the output path
            fabric --pattern \&quot;$pattern_name\&quot; -o \&quot;\$output_path\&quot;
        else
            # If no title is provided, use --stream
            fabric --pattern \&quot;$pattern_name\&quot; --stream
        fi
    }
    &quot;
done
```

This will allow you to use the patterns as aliases like in the above for example `summarize` instead of `fabric --pattern summarize --stream`, however if you pass in an extra argument like this `summarize &quot;my_article_title&quot;` your output will be saved in the destination that you set in `obsidian_base=&quot;/path/to/obsidian&quot;` in the following format `YYYY-MM-DD-my_article_title.md` where the date gets autogenerated for you.
You can tweak the date format by tweaking the `date_stamp` format.

### Migration

If you have the Legacy (Python) version installed and want to migrate to the Go version, here&#039;s how you do it. It&#039;s basically two steps: 1) uninstall the Python version, and 2) install the Go version.

```bash
# Uninstall Legacy Fabric
pipx uninstall fabric

# Clear any old Fabric aliases
(check your .bashrc, .zshrc, etc.)
# Install the Go version
go install github.com/danielmiessler/fabric@latest
# Run setup for the new version. Important because things have changed
fabric --setup
```

Then [set your environmental variables](#environment-variables) as shown above.

### Upgrading

The great thing about Go is that it&#039;s super easy to upgrade. Just run the same command you used to install it in the first place and you&#039;ll always get the latest version.

```bash
go install github.com/danielmiessler/fabric@latest
```

### Shell Completions

Fabric provides shell completion scripts for Zsh, Bash, and Fish
shells, making it easier to use the CLI by providing tab completion
for commands and options.

#### Zsh Completion

To enable Zsh completion:

```bash
# Copy the completion file to a directory in your $fpath
mkdir -p ~/.zsh/completions
cp completions/_fabric ~/.zsh/completions/

# Add the directory to fpath in your .zshrc before compinit
echo &#039;fpath=(~/.zsh/completions $fpath)&#039; &gt;&gt; ~/.zshrc
echo &#039;autoload -Uz compinit &amp;&amp; compinit&#039; &gt;&gt; ~/.zshrc
```

#### Bash Completion

To enable Bash completion:

```bash
# Source the completion script in your .bashrc
echo &#039;source /path/to/fabric/completions/fabric.bash&#039; &gt;&gt; ~/.bashrc

# Or copy to the system-wide bash completion directory
sudo cp completions/fabric.bash /etc/bash_completion.d/
```

#### Fish Completion

To enable Fish completion:

```bash
# Copy the completion file to the fish completions directory
mkdir -p ~/.config/fish/completions
cp completions/fabric.fish ~/.config/fish/completions/
```

## Usage

Once you have it all set up, here&#039;s how to use it.

```bash
fabric -h
```

```plaintext

Usage:
  fabric [OPTIONS]

Application Options:
  -p, --pattern=                    Choose a pattern from the available patterns
  -v, --variable=                   Values for pattern variables, e.g. -v=#role:expert -v=#points:30
  -C, --context=                    Choose a context from the available contexts
      --session=                    Choose a session from the available sessions
  -a, --attachment=                 Attachment path or URL (e.g. for OpenAI image recognition messages)
  -S, --setup                       Run setup for all reconfigurable parts of fabric
  -t, --temperature=                Set temperature (default: 0.7)
  -T, --topp=                       Set top P (default: 0.9)
  -s, --stream                      Stream
  -P, --presencepenalty=            Set presence penalty (default: 0.0)
  -r, --raw                         Use the defaults of the model without sending chat options (like temperature etc.) and use the user role instead of the system role for patterns.
  -F, --frequencypenalty=           Set frequency penalty (default: 0.0)
  -l, --listpatterns                List all patterns
  -L, --listmodels                  List all available models
  -x, --listcontexts                List all contexts
  -X, --listsessions                List all sessions
  -U, --updatepatterns              Update patterns
  -c, --copy                        Copy to clipboard
  -m, --model=                      Choose model
      --modelContextLength=         Model context length (only affects ollama)
  -o, --output=                     Output to file
      --output-session              Output the entire session (also a temporary one) to the output file
  -n, --latest=                     Number of latest patterns to list (default: 0)
  -d, --changeDefaultModel          Change default model
  -y, --youtube=                    YouTube video or play list &quot;URL&quot; to grab transcript, comments from it and send to chat or print it put to the console and store it in the output file
      --playlist                    Prefer playlist over video if both ids are present in the URL
      --transcript                  Grab transcript from YouTube video and send to chat (it is used per default).
      --transcript-with-timestamps  Grab transcript from YouTube video with timestamps and send to chat
      --comments                    Grab comments from YouTube video and send to chat
      --metadata                    Output video metadata
  -g, --language=                   Specify the Language Code for the chat, e.g. -g=en -g=zh
  -u, --scrape_url=                 Scrape website URL to markdown using Jina AI
  -q, --scrape_question=            Search question using Jina AI
  -e, --seed=                       Seed to be used for LMM generation
  -w, --wipecontext=                Wipe context
  -W, --wipesession=                Wipe session
      --printcontext=               Print context
      --printsession=               Print session
      --readability                 Convert HTML input into a clean, readable view
      --input-has-vars              Apply variables to user input
      --dry-run                     Show what would be sent to the model without actually sending it
      --serve                       Serve the Fabric Rest API
      --serveOllama                 Serve the Fabric Rest API with ollama endpoints
      --address=                    The address to bind the REST API (default: :8080)
      --api-key=                    API key used to secure server routes
      --config=                     Path to YAML config file
      --version                     Print current version
      --listextensions              Li

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>JavaScript</category>
        </item>
        <item>
            <title><![CDATA[openlayers/openlayers]]></title>
            <link>https://github.com/openlayers/openlayers</link>
            <guid>https://github.com/openlayers/openlayers</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:32 GMT</pubDate>
            <description><![CDATA[OpenLayers]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/openlayers/openlayers">openlayers/openlayers</a></h1>
            <p>OpenLayers</p>
            <p>Language: JavaScript</p>
            <p>Stars: 11,950</p>
            <p>Forks: 3,094</p>
            <p>Stars today: 5 stars today</p>
            <h2>README</h2><pre># OpenLayers

[OpenLayers](https://openlayers.org/) is a high-performance, feature-packed library for creating interactive maps on the web. It can display map tiles, vector data and markers loaded from any source on any web page. OpenLayers has been developed to further the use of geographic information of all kinds. It is completely free, Open Source JavaScript, released under the [BSD 2-Clause License](https://opensource.org/licenses/BSD-2-Clause).

## Getting Started

Install the [`ol` package](https://www.npmjs.com/package/ol):

```
npm install ol
```

Import just what you need for your application:

```js
import Map from &#039;ol/Map&#039;;
import View from &#039;ol/View&#039;;
import TileLayer from &#039;ol/layer/Tile&#039;;
import XYZ from &#039;ol/source/XYZ&#039;;

new Map({
  target: &#039;map&#039;,
  layers: [
    new TileLayer({
      source: new XYZ({
        url: &#039;https://tile.openstreetmap.org/{z}/{x}/{y}.png&#039;
      })
    })
  ],
  view: new View({
    center: [0, 0],
    zoom: 2
  })
});
```

See the following examples for more detail on bundling OpenLayers with your application:

 * Using [Vite](https://github.com/openlayers/ol-vite)
 * Using [Rollup](https://github.com/openlayers/ol-rollup)
 * Using [webpack](https://github.com/openlayers/ol-webpack)
 * Using [Parcel](https://github.com/openlayers/ol-parcel)

For more detail on quick prototyping without the use of a Node.js based setup, see the [Skypack](https://github.com/openlayers/ol-skypack) example.

## Sponsors

OpenLayers appreciates contributions of all kinds.  We especially want to thank our fiscal sponsors who contribute to ongoing project maintenance.

&lt;br&gt;

[![Pozi logo](./sponsor-logos/pozi.png)](https://pozi.com/)

&gt; Pozi helps connect communities through spatial thinking.
&gt; We love Openlayers and it forms a core part of our platform.
&gt; https://pozi.com/ https://app.pozi.com/

&lt;br&gt;

[![yey&#039;maps logo](./sponsor-logos/yeymaps.png)](https://www.yeymaps.io/)

&gt; yey&#039;maps is a scalable cloud GIS suite that is developed with the
&gt; powerful Openlayers API and the GDAL library.
&gt; https://www.yeymaps.io/

&lt;br&gt;

[![ela-compil logo](./sponsor-logos/ela-compil.png)](https://ela.pl/)

&gt; We develop leading Physical Security Information Management (PSIM) software.
&gt; OpenLayers is the core of our map engine and we love it! 
&gt; https://ela.pl/

&lt;br&gt;

[![Ubigu Oy logo](./sponsor-logos/ubigu-oy.svg)](https://www.ubigu.fi/)

&gt; We advance holistic utilization and availability of spatial information,
&gt; for a better planned, built and managed society.
&gt; https://www.ubigu.fi/

&lt;br&gt;

[![Scribble Maps logo](./sponsor-logos/scribble-maps.png)](https://www.scribblemaps.com/)

&gt; Build custom maps and identify insights across real estate, engineering, research, business, exploration, construction, manufacturing - and so much more.
&gt; https://www.scribblemaps.com/

&lt;br&gt;

See our [GitHub sponsors page](https://github.com/sponsors/openlayers) or [Open Collective](https://opencollective.com/openlayers/contribute/sponsors-214/checkout) if you too are interested in becoming a regular sponsor.

## TypeScript support

The [ol package](https://npmjs.com/package/ol) includes auto-generated TypeScript declarations as `*.d.ts` files.

## Supported Browsers

OpenLayers runs on all modern browsers (with greater than 1% global usage).  This includes Chrome, Firefox, Safari and Edge. For older browsers, polyfills ([Fastly](https://polyfill-fastly.io) or [Cloudflare](https://cdnjs.cloudflare.com/polyfill)) will likely need to be added.

## Documentation

Check out the [hosted examples](https://openlayers.org/en/latest/examples/), the [workshop](https://openlayers.org/workshop/) or the [API documentation](https://openlayers.org/en/latest/apidoc/).

## Bugs

Please use the [GitHub issue tracker](https://github.com/openlayers/openlayers/issues) for all bugs and feature requests. Before creating a new issue, do a quick search to see if the problem has been reported already.

## Contributing

Please see our guide on [contributing](CONTRIBUTING.md) if you&#039;re interested in getting involved.

## Community

- Need help? Find it on [Stack Overflow using the tag &#039;openlayers&#039;](https://stackoverflow.com/questions/tagged/openlayers)
- Follow [@openlayers](https://twitter.com/openlayers) on Twitter

![Test Status](https://github.com/openlayers/openlayers/workflows/Test/badge.svg)
</pre>
          ]]></content:encoded>
            <category>JavaScript</category>
        </item>
        <item>
            <title><![CDATA[vercel/next.js]]></title>
            <link>https://github.com/vercel/next.js</link>
            <guid>https://github.com/vercel/next.js</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:31 GMT</pubDate>
            <description><![CDATA[The React Framework]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/vercel/next.js">vercel/next.js</a></h1>
            <p>The React Framework</p>
            <p>Language: JavaScript</p>
            <p>Stars: 132,782</p>
            <p>Forks: 28,704</p>
            <p>Stars today: 33 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>JavaScript</category>
        </item>
        <item>
            <title><![CDATA[google/zx]]></title>
            <link>https://github.com/google/zx</link>
            <guid>https://github.com/google/zx</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:30 GMT</pubDate>
            <description><![CDATA[A tool for writing better scripts]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/google/zx">google/zx</a></h1>
            <p>A tool for writing better scripts</p>
            <p>Language: JavaScript</p>
            <p>Stars: 44,281</p>
            <p>Forks: 1,145</p>
            <p>Stars today: 14 stars today</p>
            <h2>README</h2><pre>&lt;h1&gt;&lt;img src=&quot;https://google.github.io/zx/img/logo.svg&quot; alt=&quot;Zx logo&quot; height=&quot;32&quot; valign=&quot;middle&quot;&gt; zx&lt;/h1&gt;

```js
#!/usr/bin/env zx

await $`cat package.json | grep name`

const branch = await $`git branch --show-current`
await $`dep deploy --branch=${branch}`

await Promise.all([
  $`sleep 1; echo 1`,
  $`sleep 2; echo 2`,
  $`sleep 3; echo 3`,
])

const name = &#039;foo bar&#039;
await $`mkdir /tmp/${name}`
```

Bash is great, but when it comes to writing more complex scripts,
many people prefer a more convenient programming language.
JavaScript is a perfect choice, but the Node.js standard library
requires additional hassle before using. The `zx` package provides
useful cross-platform wrappers around `child_process`, escapes arguments and
gives sensible defaults.

## Install

```bash
npm install zx
```
All setup options: [zx/setup](https://google.github.io/zx/setup).
See also [**zx@lite**](https://google.github.io/zx/lite).

## Usage

* [Documentation at google.github.io/zx/](https://google.github.io/zx/)
* [Code examples](https://github.com/google/zx/tree/main/examples)

## Compatibility
* Linux, macOS, or Windows
* JavaScript Runtime:
    * Node.js &gt;= 12.17.0
    * Bun &gt;= 1.0.0
    * Deno 1.x, 2.x
    * GraalVM Node.js
* Some kind of bash or PowerShell
* [Both CJS or ESM](https://google.github.io/zx/setup#hybrid) modules in [JS or TS](https://google.github.io/zx/typescript)


## License

[Apache-2.0](LICENSE)

Disclaimer: _This is not an officially supported Google product._
</pre>
          ]]></content:encoded>
            <category>JavaScript</category>
        </item>
        <item>
            <title><![CDATA[pollinations/pollinations]]></title>
            <link>https://github.com/pollinations/pollinations</link>
            <guid>https://github.com/pollinations/pollinations</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:29 GMT</pubDate>
            <description><![CDATA[Free Open-Source Image and Text Generation]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pollinations/pollinations">pollinations/pollinations</a></h1>
            <p>Free Open-Source Image and Text Generation</p>
            <p>Language: JavaScript</p>
            <p>Stars: 2,275</p>
            <p>Forks: 263</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;operations/assets/pollinations_ai_logo_text_black.png&quot; alt=&quot;Pollinations.AI Logo&quot; width=&quot;100%&quot;/&gt;
&lt;/div&gt;

## 🆕 Latest News

- **🔐 Auth Dashboard** - New and live! Visit [auth.pollinations.ai](https://auth.pollinations.ai) to manage your API tokens and referrer domains. [Learn more](APIDOCS.md#authentication-).
- **🏆 Tier System:** Seed, Flower, Nectar—higher tiers unlock more features. [Details](APIDOCS.md#tiers)
- **🔍 Special Bee:** Request Flower/Nectar upgrade. [Apply here](https://github.com/pollinations/pollinations/issues/new?template=special-bee-request.yml)
- **💲 Support Us** - You can now support us with our new **Tip Us** button. Optionally connect your Discord account to **Ko-Fi** to get premium Discord roles!
- **🎵 Audio Generation** - New text-to-speech and speech-to-text capabilities are now available! Try the `openai-audio` model - see our [API documentation](APIDOCS.md#audio-generation-api) for details.
- **🤖 AI Code Assistant** - We&#039;re excited to announce MentatBot, the autonomous AI coding assistant that implements new features directly from GitHub issues! Simply [create an issue](https://github.com/pollinations/pollinations/issues/new) describing what you&#039;d like to see, and MentatBot will analyze and implement it. [Learn more](https://mentat.ai/).
- **🖼️ GPT Image** - Introducing our new state-of-the-art text-to-image model, GPT Image, which generates high-resolution, contextually accurate visuals from any prompt. Learn more about OpenAI&#039;s latest image generation model [here](https://openai.com/index/introducing-4o-image-generation/).
---

[![Pollinations.AI Logo](https://pollinations.ai/p/3D_wireframe_blueprint_of_the_conceptual_isometric_world_of_Pollinations_AI_a_surreal_hyperrealistic_digital_garden_Elements_emerge_partially_from_all_sides?width=3000&amp;height=1000&amp;seed=-1)](https://pollinations.ai/p/3D_wireframe_blueprint_of_the_conceptual_isometric_world_of_Pollinations_AI_a_surreal_hyperrealistic_digital_garden_Elements_emerge_partially_from_all_sides?width=3000&amp;height=1000&amp;seed=-1)

## 🌟 Introduction

[Pollinations.AI](https://pollinations.ai) is an open-source gen AI startup based in Berlin, providing the most easy-to-use, free text and image generation API available. No signups or API keys required. We prioritize your privacy with zero data storage and completely anonymous usage.

## 🚀 Key Features

- 🔓 **100% Open Source**
- 🆓 **_Free to use_**
- 🔒 **Simplicity and privacy:** No logins, no keys, no data stored
- 🖼️ **Embed like any normal image or text**
- 🎵 **Audio generation:** Text-to-speech and speech-to-text capabilities
- 🌍 Free AI image and text generation APIs
- 🤝 Used by various **open-source LLMs**, **bots**, and **communities**
- 🎣 **_Easy-to-use React hooks_** ([React Hooks Examples](https://react-hooks.pollinations.ai/))
- 🤖 **Autonomous Development:** Features implemented by our MentatBot coding assistant through GitHub issues

&lt;a href=&quot;https://star-history.com/#pollinations/pollinations&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=pollinations/pollinations&amp;type=Date&amp;theme=dark&quot; width=&quot;600&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=pollinations/pollinations&amp;type=Date&quot; width=&quot;600&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=pollinations/pollinations&amp;type=Date&quot; width=&quot;600&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;

## 🚀 Getting Started

### Image Generation

1. Visit [https://pollinations.ai](https://pollinations.ai)
2. Type your description in the text box
3. Click &quot;Generate&quot; and watch the magic happen!

### Text Generation

1. Visit [https://text.pollinations.ai](https://text.pollinations.ai)
2. Start chatting with the AI

### Audio Generation

1. Use the `openai-audio` model with our API ([explore voices at OpenAI.fm](https://www.openai.fm/))
2. Generate speech from text or transcribe audio to text

### MCP Server for AI Assistants

Our MCP (Model Context Protocol) server enables AI assistants like Claude to generate images and audio directly. [Learn more](./model-context-protocol/README.md)

```bash
# Run with npx (no installation required)
npx @pollinations/model-context-protocol
```

Community alternatives like [MCPollinations](https://github.com/pinkpixel-dev/MCPollinations) are also available.

AI assistants can:
- Generate images from text descriptions
- Create text-to-speech audio with various voice options
- Play audio responses through the system speakers
- Access all Pollinations.AI models and services
- List available models, voices, and capabilities

For more advanced usage, check out our [API documentation](APIDOCS.md).

## 🖥️ How to Use

### Web Interface

Our web interface is user-friendly and doesn&#039;t require any technical knowledge. Simply visit [https://pollinations.ai](https://pollinations.ai) and start creating!

### API

Use our API directly in your browser or applications:

    https://pollinations.ai/p/conceptual_isometric_world_of_pollinations_ai_surreal_hyperrealistic_digital_garden

Replace the description with your own, and you&#039;ll get a unique image based on your words!

## 🎨 Examples

### Image Generation

Here&#039;s an example of a generated image:

[![Conceptual Isometric World](https://pollinations.ai/p/3d_wireframe_blueprint_for_the_prompt_conceptual%20isometric%20world%20of%20pollinations%20ai%20surreal%20hyperrealistic%20digital%20garden?width=2000&amp;height=500&amp;nologo=true&amp;seed=-1)](https://pollinations.ai/p/3d_wireframe_blueprint_for_the_prompt_conceptual%20isometric%20world%20of%20pollinations%20ai%20surreal%20hyperrealistic%20digital%20garden?width=2000&amp;height=500&amp;nologo=true&amp;seed=-1)

Python code to download the generated image:

    import requests

    def download_image(prompt):
        url = f&quot;https://pollinations.ai/p/{prompt}&quot;
        response = requests.get(url)
        with open(&#039;generated_image.jpg&#039;, &#039;wb&#039;) as file:
            file.write(response.content)
        print(&#039;Image downloaded!&#039;)

    download_image(&quot;conceptual_isometric_world_of_pollinations_ai_surreal_hyperrealistic_digital_garden&quot;)

### Text Generation

To generate text, use this URL:

    https://text.pollinations.ai/What%20is%20artificial%20intelligence?

### Audio Generation

To generate audio from text, use this URL:

    https://text.pollinations.ai/Welcome%20to%20Pollinations?model=openai-audio&amp;voice=nova

## 🛠️ Integration

### React Hooks

We offer React hooks for easy integration. Example usage:

    import React from &#039;react&#039;;
    import { usePollinationsImage, usePollinationsText } from &#039;@pollinations/react&#039;;
    import ReactMarkdown from &#039;react-markdown&#039;;

    const AIGeneratedContent = () =&gt; {
      const imageUrl = usePollinationsImage(&quot;Beautiful landscape of Paris with Eiffel Tower&quot;, { width: 800, height: 600, seed: 42 });
      const markdown = usePollinationsText(&quot;Write a brief travel guide for Paris, including top attractions and local cuisine in markdown&quot;, { seed: 42 });

      return (
        &lt;div&gt;
          &lt;h2&gt;AI-Generated Travel Guide&lt;/h2&gt;
          &lt;img src={imageUrl} alt=&quot;AI Generated&quot; /&gt;
          {markdown ? (
            &lt;ReactMarkdown&gt;{markdown}&lt;/ReactMarkdown&gt;
          ) : (
            &lt;p&gt;Loading markdown content...&lt;/p&gt;
          )}
        &lt;/div&gt;
      );
    };

    export default AIGeneratedContent;

Check out our [Pollinations React Hooks](./pollinations-react/README.md) for more details.

## Architecture
```mermaid
graph LR
    Q[Bots - Discord, Telegram, WhatsApp] --&gt; L1
    
    N[30+ Mobile and Web Apps] --&gt; L1
    N --&gt; L2
    
    A[pollinations.ai Web Frontend] --&gt; L1
    A --&gt; L2
    
    R[AI Agents - Qwen, Sillytavern, ...] --&gt; L1
    
    AI[AI Assistants - Claude] --&gt; MCP[MCP Server]
    MCP --&gt; L1
    
    L1[Image CDN] --&gt; CF[Cloudflare Worker with R2 Cache]
    L2[Text CDN] --&gt; C
    
    CF --&gt; B
    
    B[image-origin.pollinations.ai - AWS EC2 CPU] --&gt; F[Azure OpenAI - Prompt Enhancing]
    B --&gt; S[LlamaGuard - Safety Checker]
    F --&gt; E[Translation Service - 1 GPU VM]
    E --&gt; D[FLUX image generation model - 2-6 GPU VMs on AWS]
    
    C[text.pollinations.ai - AWS EC2 CPU] --&gt; P[karma.yt - Realtime News]
    C --&gt; SC[Scaleway API]
    C --&gt; DS[Deepseek API]
    C --&gt; G[Azure-hosted Serverless LLMs]
    C --&gt; CFM[Cloudflare AI]
    SC --&gt; MI[Mistral Models]
    SC --&gt; QW[Qwen Models]
    SC --&gt; LL[Llama Models]
    DS --&gt; DM[Deepseek Models]
    G --&gt; H[OpenAI]
    G --&gt; K[Claude]
    CFM --&gt; CFL[Llama &amp; Deepseek Models]
```

## Projects Using Pollinations.AI

&gt; **⭐ GitHub Star Counts:** Projects with GitHub repositories include star counts to help you gauge their popularity.
&gt; 
&gt; **🆕 NEW Tag:** Projects are marked with the 🆕 emoji when they are recently added. This tag is automatically removed after 15 days from the submission date or if no date is specified.
&gt; 
&lt;!-- AUTO-GENERATED-CONTENT:START --&gt;

&gt; **Note:** Some projects may be temporarily hidden from this list if they are currently broken or undergoing maintenance.

Pollinations.AI is used in various projects, including:

### Vibe Coding ✨

| Project | Description | Creator |
|---------|-------------|--------|
| Qwen-Agent ([⭐ 6.6k](https://github.com/QwenLM/Qwen-Agent)) | A framework for developing agentic LLM applications. | - |
| [Open Prompt](https://openprompt.co) ([⭐ 87](https://github.com/markojohnas/openprompt)) | A community-driven platform for creating, sharing, and discovering AI prompts for various applications. Integrated with Pollinations API for enhanced creative capabilities. | @markojohnas |
| Pollinations MCP Server ([⭐ 42](https://github.com/pollinations/model-context-protocol)) | A Model Context Protocol server that enables AI-assisted development through natural language interaction with Pollinations&#039; multimodal services. | @thomash |
| Pollinations Task Master ([⭐ 3](https://github.com/LousyBook94/pollinations-task-master)) | A task management system that uses AI to help break down and organize development tasks through natural language interaction. | @LousyBook94 |
| [AI Code Generator](https://codegen.on.websim.com/) | A websim project that generates code from description, selected programming language and other options. Integrates Pollinations because it allows for more models to choose from for potentially better results. It has modes like: Code Generator, Code Explainer, Reviewer, etc. | @Miencraft2 |
| VibeCoder | A conversational coding environment that lets you create applications by describing them in natural language. | @Aashir__Shaikh |
| [JCode Website Builder](https://jcode-ai-website-bulder.netlify.app/) | A website generator using Pollinations text API. | @rtxpower |
| [Define](https://define-i05a.onrender.com/api/docs/) ([⭐ 0](https://github.com/hasanraiyan)) | An AI-powered REST API designed to generate definitions for words or phrases, constrained to a specified target word count. It allows customization of tone, context, and language, delivering precise, context-aware definitions programmatically—ideal for developers and content creators. | @hasanraiyan |
| [WebGeniusAI](https://webgeniusai.netlify.app/) | AI tool that generates HTML websites with visuals from Pollinations. | @Aashir__Shaikh |
| [Pollinations.DIY](https://pollinations.diy) | A browser-based coding environment based on bolt.diy, featuring integrated Pollinations AI services, visual code editing, and project management tools. | @thomash |
| [NetSim](https://netsim.us.to/) | websim.ai clone that&#039;s actually good | @kennet678 |
| [Pollin-Coder](https://pollin-coder.megavault.in) | A free AI-powered website builder that lets anyone create a clean site just by describing it. It uses Pollinations AI to generate the content and layout instantly. | @r3ap3redit |
| [JustBuildThings](https://justbuildthings.com) | A natural language programming interface that lets users create web applications by simply describing what they want to build, using Pollinations AI to generate code and assets. | @buildmaster |
| [Websim](https://websim.ai/c/bXsmNE96e3op5rtUS) | A web simulation tool that integrates Pollinations.ai. | @thomash |

### Creative 🎨

| Project | Description | Creator |
|---------|-------------|--------|
| MoneyPrinterTurbo ([⭐ 32.2k](https://github.com/harry0703/MoneyPrinterTurbo)) | Simply provide a topic or keyword for a video, and it will automatically generate the video copy, video materials, video subtitles, and video background music before synthesizing a high-definition short video. Integrates Pollinations&#039; text generation service to create engaging and relevant video scripts. | @harry0703 |
| [Anime Character Generator](https://animechar.gen.ai/create) ([⭐ 250](https://github.com/animeartdevs/character-generator)) | A dedicated AI tool for generating high-quality, unique anime-style characters. Offers detailed customization of art style, character traits, clothing, and accessories, all powered by Pollinations. | @AnimeArtDevs |
| [FoldaScan](https://fs.wen.bar) ([⭐ 178](https://github.com/0010skn/WebFS-Toolkit-Local-Folder-Scan-Monitor-Versioning-AI-Prep)) | Use Natural Language to &quot;Converse&quot; with Your Codebase, Folda-Scan Smart Project Q&amp;A, powered by advanced vectorization technology, allows you to easily understand complex code, pinpoint information, and offers unprecedented convenience for AI collaboration. | @0010skn |
| [Jackey](https://jackey.ai/app) ([⭐ 120](https://github.com/creativesparks/jackey)) | Jackey is a creative AI companion that helps users generate story ideas, write scripts, and create concept art using Pollinations. It&#039;s designed for writers, game developers, and filmmakers. | Creative Sparks Ltd. |
| [Polynate](https://polynate.com) ([⭐ 78](https://github.com/polynate/platform)) | A platform for creating and sharing AI-generated art, music, and stories, with a strong community focus and Pollinations integration. | @polynate_team |
| [Memed](https://memed.io) ([⭐ 22](https://github.com/ksingh/memed)) | An AI-powered meme generator that creates humorous images based on user prompts using Pollinations API for image generation. | @k_singh |
| [Elixpo-Art](https://elixpo-art.com) ([⭐ 18](https://github.com/elixpo/art-platform)) | A digital art platform that combines AI image generation with traditional digital art tools, offering creative filters and style transfers powered by Pollinations. | @elixpo |
| [POLLIPAPER](https://github.com/Tolerable/POLLIPAPER) | A dynamic wallpaper app that uses Pollinations AI. | @intolerant0ne |
| [Elixpo Art Chrome Extension](https://chromewebstore.google.com/detail/elixpo-art-select-text-an/hcjdeknbbbllfllddkbacfgehddpnhdh) ([⭐ 8](https://github.com/Circuit-Overtime/elixpo_ai_chapter/tree/main/Elixpo%20Chrome%20%20Extension)) | It uses the pollinations image endpoint to generate an image with `boltning` as the model in 4 types of aspect ratios and themes with prompt engineering thus transforming selected texts into art smoothly with a disposable GUI in web. | Ayushman Bhatacharya |
| Pollinations.ai Image Generation (for Frame) ([⭐ 3](https://github.com/CitizenOneX/frame_pollinations)) | A Flutter application that listens for image generation prompts, requests images from Pollinations.AI, and displays them on the Frame wearable device. Users can use voice commands to generate images and save/share them using the device&#039;s sharing mechanism. | CitizenOneX |
| [Imagen](https://altkriz.github.io/imagen/) ([⭐ 3](https://github.com/altkriz/imagen)) | A beautiful web interface for generating images using Pollinations.ai API with only the &quot;flux&quot; and &quot;turbo&quot; models. | @altkriz |
| [Imagen](https://altkriz.github.io/imagen/) ([⭐ 3](https://github.com/altkriz/imagen)) | A beautiful web interface for generating images using Pollinations.ai API with only the &quot;flux&quot; and &quot;turbo&quot; models. | @altkriz |
| 🆕 [CatGPT Meme Generator 🐱](https://pollinations.github.io/catgpt/) ([⭐ 2](https://github.com/pollinations/catgpt)) | Transform your questions into sassy cat wisdom! An AI-powered meme generator that creates personalized cat comics in response to your questions. A collaboration between Pollinations.AI and Tanika Godbole, the original creator of the CatGPT comic. | @voodoohop |
| [Dreamscape AI](https://dreamscape.pinkpixel.dev) ([⭐ 2](https://github.com/pinkpixel-dev/dreamscape-ai)) | Dreamscape AI is a creative studio for generating, enhancing, and transforming images, plus conversational AI capabilities with text and voice interfaces, and a deep research tool. The entire site is almost all powered by Pollinations API aside from the image enhancement tools. It generates images, optimizes prompts and creates image titles with the text API, features lots of image styling prompts, also has chat and voice chat with chat memory, and a research tool. | @sizzlebop |
| [Match-cut video ai](https://video-gen.megavault.in) ([⭐ 2](https://github.com/iotserver24/match-cut-ai)) | This AI generates video from text in match-cut text style, uses pollinations llm to generate nearby text, and supports API integration. | @r3ap3redit |
| [Polynate](https://polynate.cloudwerx.dev/) ([⭐ 2](https://github.com/fisventurous/pollinationsai-enhancer)) | AI-powered text and audio content generation platform providing a user-friendly interface for interacting with various AI generation services from Pollinations.ai. | @fisven |
| MASala ([⭐ 1](https://github.com/Naman009/MASala)) | Multi-Agent AI That Cooks Up Recipes Just for You ~ From fridge to feast, MASALA plans it all. | @Naman009 |
| 🆕 [Image Creator](https://saepulwap.blogspot.com/p/flux-image-creator.html) | Create images with multi-language prompts, the language will be automatically translated by AI into English. | [Link](https://facebook.com/403.frobidden) |
| [Avatar GenStudio](https://astudio-dcae4.web.app) | A system for creating custom characters that uses the Pollinations API for totally free and unlimited image generation. | @nic-wq |
| 🇮🇩 [Generator AI Image 🇮🇩](https://kenthir.my.id/advanced-generator/) | Advanced AI Image Generator adalah platform inovatif yang memungkinkan Anda membuat gambar digital menakjubkan dengan kecerdasan buatan by pollinations.ai. Dengan dukungan berbagai model AI canggih seperti DALL·E 3, Stable Diffusion, dan Flux-Default. (An innovative platform that allows you to create amazing digital images with artificial intelligence powered by pollinations.ai. Supports various advanced AI models like DALL-E 3, Stable Diffusion, and Flux-Default.) | @kenthirai |
| [BlackWave](https://blackwave.studio/) | An AI image generator that creates unique images from text prompts. Fast, easy and free! | @metimol |
| [NailsGen](https://www.nailsgen.com/) | Create beautiful nail art designs with AI. Generate unique nail art designs with different styles and colors. | lipengliang2012@163.com |
| [ImageGen AI Image](https://imagegenaiimage.com/) | Generate high-quality AI images for any purpose. Features a variety of models and styles. | [Link](https://www.linkedin.com/in/narendradwivedi) |
| 🇮🇩 [RuangRiung AI Image 🇮🇩](https://ruangriung.my.id) ([⭐ 0](https://github.com/ruangriung)) | RuangRiung AI Image Generator is ideal for digital artists, designers, or anyone who wants to explore creativity with AI assistance. Available in English and Indonesian, this website combines complete functionality with an elegant and responsive design. | @ruangriung |
| [WebGeniusAI](https://webgeniusai.netlify.app/) | AI tool that generates HTML websites with visuals from Pollinations. | @logise |
| [FlowGPT](https://flowgpt.com/p/instant-image-generation-with-chatgpt-and-pollinationsai) | Generate images on-demand with ChatGPT! | - |
| [Snapgen.io](https://snapgen.io) | A free AI image generation website with a clean and professional interface, offering high-quality image generation without requiring API keys. | tharindu@tsoft-llc.com |
| [Image Gen - Uncensored Edition](https:

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>JavaScript</category>
        </item>
        <item>
            <title><![CDATA[extesy/hoverzoom]]></title>
            <link>https://github.com/extesy/hoverzoom</link>
            <guid>https://github.com/extesy/hoverzoom</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:28 GMT</pubDate>
            <description><![CDATA[Google Chrome extension for zooming images on mouse hover]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/extesy/hoverzoom">extesy/hoverzoom</a></h1>
            <p>Google Chrome extension for zooming images on mouse hover</p>
            <p>Language: JavaScript</p>
            <p>Stars: 1,415</p>
            <p>Forks: 195</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  
# Hover Zoom+

[Google Chrome](https://chrome.google.com/webstore/detail/hover-zoom%20/pccckmaobkjjboncdfnnofkonhgpceea) &lt;a href=&quot;https://chrome.google.com/webstore/detail/hover-zoom%20/pccckmaobkjjboncdfnnofkonhgpceea&quot; target=&quot;_blank&quot;&gt;&lt;img alt=&quot;Chrome Web Store&quot; src=&quot;https://img.shields.io/chrome-web-store/users/pccckmaobkjjboncdfnnofkonhgpceea?color=blue&quot;&gt;&lt;/a&gt; • [Mozilla Firefox](https://addons.mozilla.org/en-US/firefox/addon/hover-zoom-plus/) &lt;a href=&quot;https://addons.mozilla.org/en-US/firefox/addon/hover-zoom-plus/&quot; target=&quot;_blank&quot;&gt;&lt;img alt=&quot;Mozilla Add-on&quot; src=&quot;https://img.shields.io/amo/users/hover-zoom-plus&quot;&gt;&lt;/a&gt; • [Microsoft Edge](https://microsoftedge.microsoft.com/addons/detail/hover-zoom/bnibclmindjpdfiipicpdhljfblkpkml) &lt;a href=&quot;https://microsoftedge.microsoft.com/addons/detail/hover-zoom/bnibclmindjpdfiipicpdhljfblkpkml&quot; target=&quot;_blank&quot;&gt;&lt;img alt=&quot;Edge Add-on&quot; src=&quot;https://img.shields.io/badge/dynamic/json?label=users&amp;query=%24.activeInstallCount&amp;url=https%3A%2F%2Fmicrosoftedge.microsoft.com%2Faddons%2Fgetproductdetailsbycrxid%2Fbnibclmindjpdfiipicpdhljfblkpkml&quot;&gt;&lt;/a&gt;
---

Zoom images/videos on all your favorite websites (Facebook, Amazon, etc). Hover your mouse over any image on the [supported websites](https://github.com/extesy/hoverzoom/tree/master/plugins) and the extension will automatically enlarge the image to its full size, making sure that it still fits into the browser window.

This is an open-source version of the original HoverZoom extension, which was overrun by malware and deleted from the store. In this version all spyware has been removed, many bugs were fixed and new features were added. It doesn&#039;t collect any user data whatsoever.

Sometimes sites change design, and when it happens, the extension needs to be updated. Please report any issues with zooming not working by filing an issue on https://github.com/extesy/hoverzoom/issues page.

&gt; **This extension will never be sold out, and it will never compromise users&#039; privacy.&lt;br&gt;
&gt; As a proof, please see [the list](https://github.com/extesy/hoverzoom/discussions/670) of all takeover offers I have received over the last few years.**

&gt; Please help with [localizing this extension](https://crowdin.com/project/hoverzoom) to the language you are familiar with!

---

&lt;a href=&quot;https://www.star-history.com/#extesy/hoverzoom&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=extesy/hoverzoom&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=extesy/hoverzoom&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=extesy/hoverzoom&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;

---

Licensed under MIT license.
</pre>
          ]]></content:encoded>
            <category>JavaScript</category>
        </item>
        <item>
            <title><![CDATA[mendableai/firecrawl-mcp-server]]></title>
            <link>https://github.com/mendableai/firecrawl-mcp-server</link>
            <guid>https://github.com/mendableai/firecrawl-mcp-server</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:27 GMT</pubDate>
            <description><![CDATA[Official Firecrawl MCP Server - Adds powerful web scraping to Cursor, Claude and any other LLM clients.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mendableai/firecrawl-mcp-server">mendableai/firecrawl-mcp-server</a></h1>
            <p>Official Firecrawl MCP Server - Adds powerful web scraping to Cursor, Claude and any other LLM clients.</p>
            <p>Language: JavaScript</p>
            <p>Stars: 3,596</p>
            <p>Forks: 344</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre># Firecrawl MCP Server

A Model Context Protocol (MCP) server implementation that integrates with [Firecrawl](https://github.com/mendableai/firecrawl) for web scraping capabilities.

&gt; Big thanks to [@vrknetha](https://github.com/vrknetha), [@knacklabs](https://www.knacklabs.ai) for the initial implementation!


## Features

- Web scraping, crawling, and discovery
- Search and content extraction
- Deep research and batch scraping
- Automatic retries and rate limiting
- Cloud and self-hosted support
- SSE support

&gt; Play around with [our MCP Server on MCP.so&#039;s playground](https://mcp.so/playground?server=firecrawl-mcp-server) or on [Klavis AI](https://www.klavis.ai/mcp-servers).

## Installation

### Running with npx

```bash
env FIRECRAWL_API_KEY=fc-YOUR_API_KEY npx -y firecrawl-mcp
```

### Manual Installation

```bash
npm install -g firecrawl-mcp
```

### Running on Cursor

Configuring Cursor 🖥️
Note: Requires Cursor version 0.45.6+
For the most up-to-date configuration instructions, please refer to the official Cursor documentation on configuring MCP servers:
[Cursor MCP Server Configuration Guide](https://docs.cursor.com/context/model-context-protocol#configuring-mcp-servers)

To configure Firecrawl MCP in Cursor **v0.48.6**

1. Open Cursor Settings
2. Go to Features &gt; MCP Servers
3. Click &quot;+ Add new global MCP server&quot;
4. Enter the following code:
   ```json
   {
     &quot;mcpServers&quot;: {
       &quot;firecrawl-mcp&quot;: {
         &quot;command&quot;: &quot;npx&quot;,
         &quot;args&quot;: [&quot;-y&quot;, &quot;firecrawl-mcp&quot;],
         &quot;env&quot;: {
           &quot;FIRECRAWL_API_KEY&quot;: &quot;YOUR-API-KEY&quot;
         }
       }
     }
   }
   ```
   
To configure Firecrawl MCP in Cursor **v0.45.6**

1. Open Cursor Settings
2. Go to Features &gt; MCP Servers
3. Click &quot;+ Add New MCP Server&quot;
4. Enter the following:
   - Name: &quot;firecrawl-mcp&quot; (or your preferred name)
   - Type: &quot;command&quot;
   - Command: `env FIRECRAWL_API_KEY=your-api-key npx -y firecrawl-mcp`



&gt; If you are using Windows and are running into issues, try `cmd /c &quot;set FIRECRAWL_API_KEY=your-api-key &amp;&amp; npx -y firecrawl-mcp&quot;`

Replace `your-api-key` with your Firecrawl API key. If you don&#039;t have one yet, you can create an account and get it from https://www.firecrawl.dev/app/api-keys

After adding, refresh the MCP server list to see the new tools. The Composer Agent will automatically use Firecrawl MCP when appropriate, but you can explicitly request it by describing your web scraping needs. Access the Composer via Command+L (Mac), select &quot;Agent&quot; next to the submit button, and enter your query.

### Running on Windsurf

Add this to your `./codeium/windsurf/model_config.json`:

```json
{
  &quot;mcpServers&quot;: {
    &quot;mcp-server-firecrawl&quot;: {
      &quot;command&quot;: &quot;npx&quot;,
      &quot;args&quot;: [&quot;-y&quot;, &quot;firecrawl-mcp&quot;],
      &quot;env&quot;: {
        &quot;FIRECRAWL_API_KEY&quot;: &quot;YOUR_API_KEY&quot;
      }
    }
  }
}
```

### Running with SSE Local Mode

To run the server using Server-Sent Events (SSE) locally instead of the default stdio transport:

```bash
env SSE_LOCAL=true FIRECRAWL_API_KEY=fc-YOUR_API_KEY npx -y firecrawl-mcp
```

Use the url: http://localhost:3000/sse

### Installing via Smithery (Legacy)

To install Firecrawl for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@mendableai/mcp-server-firecrawl):

```bash
npx -y @smithery/cli install @mendableai/mcp-server-firecrawl --client claude
```

### Running on VS Code

For one-click installation, click one of the install buttons below...

[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-NPM-0098FF?style=flat-square&amp;logo=visualstudiocode&amp;logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=firecrawl&amp;inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%22Firecrawl%20API%20Key%22%2C%22password%22%3Atrue%7D%5D&amp;config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22firecrawl-mcp%22%5D%2C%22env%22%3A%7B%22FIRECRAWL_API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D) [![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-NPM-24bfa5?style=flat-square&amp;logo=visualstudiocode&amp;logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=firecrawl&amp;inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%22Firecrawl%20API%20Key%22%2C%22password%22%3Atrue%7D%5D&amp;config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22firecrawl-mcp%22%5D%2C%22env%22%3A%7B%22FIRECRAWL_API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D&amp;quality=insiders)

For manual installation, add the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.

```json
{
  &quot;mcp&quot;: {
    &quot;inputs&quot;: [
      {
        &quot;type&quot;: &quot;promptString&quot;,
        &quot;id&quot;: &quot;apiKey&quot;,
        &quot;description&quot;: &quot;Firecrawl API Key&quot;,
        &quot;password&quot;: true
      }
    ],
    &quot;servers&quot;: {
      &quot;firecrawl&quot;: {
        &quot;command&quot;: &quot;npx&quot;,
        &quot;args&quot;: [&quot;-y&quot;, &quot;firecrawl-mcp&quot;],
        &quot;env&quot;: {
          &quot;FIRECRAWL_API_KEY&quot;: &quot;${input:apiKey}&quot;
        }
      }
    }
  }
}
```

Optionally, you can add it to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with others:

```json
{
  &quot;inputs&quot;: [
    {
      &quot;type&quot;: &quot;promptString&quot;,
      &quot;id&quot;: &quot;apiKey&quot;,
      &quot;description&quot;: &quot;Firecrawl API Key&quot;,
      &quot;password&quot;: true
    }
  ],
  &quot;servers&quot;: {
    &quot;firecrawl&quot;: {
      &quot;command&quot;: &quot;npx&quot;,
      &quot;args&quot;: [&quot;-y&quot;, &quot;firecrawl-mcp&quot;],
      &quot;env&quot;: {
        &quot;FIRECRAWL_API_KEY&quot;: &quot;${input:apiKey}&quot;
      }
    }
  }
}
```

## Configuration

### Environment Variables

#### Required for Cloud API

- `FIRECRAWL_API_KEY`: Your Firecrawl API key
  - Required when using cloud API (default)
  - Optional when using self-hosted instance with `FIRECRAWL_API_URL`
- `FIRECRAWL_API_URL` (Optional): Custom API endpoint for self-hosted instances
  - Example: `https://firecrawl.your-domain.com`
  - If not provided, the cloud API will be used (requires API key)

#### Optional Configuration

##### Retry Configuration

- `FIRECRAWL_RETRY_MAX_ATTEMPTS`: Maximum number of retry attempts (default: 3)
- `FIRECRAWL_RETRY_INITIAL_DELAY`: Initial delay in milliseconds before first retry (default: 1000)
- `FIRECRAWL_RETRY_MAX_DELAY`: Maximum delay in milliseconds between retries (default: 10000)
- `FIRECRAWL_RETRY_BACKOFF_FACTOR`: Exponential backoff multiplier (default: 2)

##### Credit Usage Monitoring

- `FIRECRAWL_CREDIT_WARNING_THRESHOLD`: Credit usage warning threshold (default: 1000)
- `FIRECRAWL_CREDIT_CRITICAL_THRESHOLD`: Credit usage critical threshold (default: 100)

### Configuration Examples

For cloud API usage with custom retry and credit monitoring:

```bash
# Required for cloud API
export FIRECRAWL_API_KEY=your-api-key

# Optional retry configuration
export FIRECRAWL_RETRY_MAX_ATTEMPTS=5        # Increase max retry attempts
export FIRECRAWL_RETRY_INITIAL_DELAY=2000    # Start with 2s delay
export FIRECRAWL_RETRY_MAX_DELAY=30000       # Maximum 30s delay
export FIRECRAWL_RETRY_BACKOFF_FACTOR=3      # More aggressive backoff

# Optional credit monitoring
export FIRECRAWL_CREDIT_WARNING_THRESHOLD=2000    # Warning at 2000 credits
export FIRECRAWL_CREDIT_CRITICAL_THRESHOLD=500    # Critical at 500 credits
```

For self-hosted instance:

```bash
# Required for self-hosted
export FIRECRAWL_API_URL=https://firecrawl.your-domain.com

# Optional authentication for self-hosted
export FIRECRAWL_API_KEY=your-api-key  # If your instance requires auth

# Custom retry configuration
export FIRECRAWL_RETRY_MAX_ATTEMPTS=10
export FIRECRAWL_RETRY_INITIAL_DELAY=500     # Start with faster retries
```

### Usage with Claude Desktop

Add this to your `claude_desktop_config.json`:

```json
{
  &quot;mcpServers&quot;: {
    &quot;mcp-server-firecrawl&quot;: {
      &quot;command&quot;: &quot;npx&quot;,
      &quot;args&quot;: [&quot;-y&quot;, &quot;firecrawl-mcp&quot;],
      &quot;env&quot;: {
        &quot;FIRECRAWL_API_KEY&quot;: &quot;YOUR_API_KEY_HERE&quot;,

        &quot;FIRECRAWL_RETRY_MAX_ATTEMPTS&quot;: &quot;5&quot;,
        &quot;FIRECRAWL_RETRY_INITIAL_DELAY&quot;: &quot;2000&quot;,
        &quot;FIRECRAWL_RETRY_MAX_DELAY&quot;: &quot;30000&quot;,
        &quot;FIRECRAWL_RETRY_BACKOFF_FACTOR&quot;: &quot;3&quot;,

        &quot;FIRECRAWL_CREDIT_WARNING_THRESHOLD&quot;: &quot;2000&quot;,
        &quot;FIRECRAWL_CREDIT_CRITICAL_THRESHOLD&quot;: &quot;500&quot;
      }
    }
  }
}
```

### System Configuration

The server includes several configurable parameters that can be set via environment variables. Here are the default values if not configured:

```typescript
const CONFIG = {
  retry: {
    maxAttempts: 3, // Number of retry attempts for rate-limited requests
    initialDelay: 1000, // Initial delay before first retry (in milliseconds)
    maxDelay: 10000, // Maximum delay between retries (in milliseconds)
    backoffFactor: 2, // Multiplier for exponential backoff
  },
  credit: {
    warningThreshold: 1000, // Warn when credit usage reaches this level
    criticalThreshold: 100, // Critical alert when credit usage reaches this level
  },
};
```

These configurations control:

1. **Retry Behavior**

   - Automatically retries failed requests due to rate limits
   - Uses exponential backoff to avoid overwhelming the API
   - Example: With default settings, retries will be attempted at:
     - 1st retry: 1 second delay
     - 2nd retry: 2 seconds delay
     - 3rd retry: 4 seconds delay (capped at maxDelay)

2. **Credit Usage Monitoring**
   - Tracks API credit consumption for cloud API usage
   - Provides warnings at specified thresholds
   - Helps prevent unexpected service interruption
   - Example: With default settings:
     - Warning at 1000 credits remaining
     - Critical alert at 100 credits remaining

### Rate Limiting and Batch Processing

The server utilizes Firecrawl&#039;s built-in rate limiting and batch processing capabilities:

- Automatic rate limit handling with exponential backoff
- Efficient parallel processing for batch operations
- Smart request queuing and throttling
- Automatic retries for transient errors

## How to Choose a Tool

Use this guide to select the right tool for your task:

- **If you know the exact URL(s) you want:**
  - For one: use **scrape**
  - For many: use **batch_scrape**
- **If you need to discover URLs on a site:** use **map**
- **If you want to search the web for info:** use **search**
- **If you want to extract structured data:** use **extract**
- **If you want to analyze a whole site or section:** use **crawl** (with limits!)
- **If you want to do in-depth research:** use **deep_research**
- **If you want to generate LLMs.txt:** use **generate_llmstxt**

### Quick Reference Table

| Tool                | Best for                                 | Returns         |
|---------------------|------------------------------------------|-----------------|
| scrape              | Single page content                      | markdown/html   |
| batch_scrape        | Multiple known URLs                      | markdown/html[] |
| map                 | Discovering URLs on a site               | URL[]           |
| crawl               | Multi-page extraction (with limits)      | markdown/html[] |
| search              | Web search for info                      | results[]       |
| extract             | Structured data from pages               | JSON            |
| deep_research       | In-depth, multi-source research          | summary, sources|
| generate_llmstxt    | LLMs.txt for a domain                    | text            |

## Available Tools

### 1. Scrape Tool (`firecrawl_scrape`)

Scrape content from a single URL with advanced options.

**Best for:**
- Single page content extraction, when you know exactly which page contains the information.

**Not recommended for:**
- Extracting content from multiple pages (use batch_scrape for known URLs, or map + batch_scrape to discover URLs first, or crawl for full page content)
- When you&#039;re unsure which page contains the information (use search)
- When you need structured data (use extract)

**Common mistakes:**
- Using scrape for a list of URLs (use batch_scrape instead).

**Prompt Example:**
&gt; &quot;Get the content of the page at https://example.com.&quot;

**Usage Example:**
```json
{
  &quot;name&quot;: &quot;firecrawl_scrape&quot;,
  &quot;arguments&quot;: {
    &quot;url&quot;: &quot;https://example.com&quot;,
    &quot;formats&quot;: [&quot;markdown&quot;],
    &quot;onlyMainContent&quot;: true,
    &quot;waitFor&quot;: 1000,
    &quot;timeout&quot;: 30000,
    &quot;mobile&quot;: false,
    &quot;includeTags&quot;: [&quot;article&quot;, &quot;main&quot;],
    &quot;excludeTags&quot;: [&quot;nav&quot;, &quot;footer&quot;],
    &quot;skipTlsVerification&quot;: false
  }
}
```

**Returns:**
- Markdown, HTML, or other formats as specified.

### 2. Batch Scrape Tool (`firecrawl_batch_scrape`)

Scrape multiple URLs efficiently with built-in rate limiting and parallel processing.

**Best for:**
- Retrieving content from multiple pages, when you know exactly which pages to scrape.

**Not recommended for:**
- Discovering URLs (use map first if you don&#039;t know the URLs)
- Scraping a single page (use scrape)

**Common mistakes:**
- Using batch_scrape with too many URLs at once (may hit rate limits or token overflow)

**Prompt Example:**
&gt; &quot;Get the content of these three blog posts: [url1, url2, url3].&quot;

**Usage Example:**
```json
{
  &quot;name&quot;: &quot;firecrawl_batch_scrape&quot;,
  &quot;arguments&quot;: {
    &quot;urls&quot;: [&quot;https://example1.com&quot;, &quot;https://example2.com&quot;],
    &quot;options&quot;: {
      &quot;formats&quot;: [&quot;markdown&quot;],
      &quot;onlyMainContent&quot;: true
    }
  }
}
```

**Returns:**
- Response includes operation ID for status checking:

```json
{
  &quot;content&quot;: [
    {
      &quot;type&quot;: &quot;text&quot;,
      &quot;text&quot;: &quot;Batch operation queued with ID: batch_1. Use firecrawl_check_batch_status to check progress.&quot;
    }
  ],
  &quot;isError&quot;: false
}
```

### 3. Check Batch Status (`firecrawl_check_batch_status`)

Check the status of a batch operation.

```json
{
  &quot;name&quot;: &quot;firecrawl_check_batch_status&quot;,
  &quot;arguments&quot;: {
    &quot;id&quot;: &quot;batch_1&quot;
  }
}
```

### 4. Map Tool (`firecrawl_map`)

Map a website to discover all indexed URLs on the site.

**Best for:**
- Discovering URLs on a website before deciding what to scrape
- Finding specific sections of a website

**Not recommended for:**
- When you already know which specific URL you need (use scrape or batch_scrape)
- When you need the content of the pages (use scrape after mapping)

**Common mistakes:**
- Using crawl to discover URLs instead of map

**Prompt Example:**
&gt; &quot;List all URLs on example.com.&quot;

**Usage Example:**
```json
{
  &quot;name&quot;: &quot;firecrawl_map&quot;,
  &quot;arguments&quot;: {
    &quot;url&quot;: &quot;https://example.com&quot;
  }
}
```

**Returns:**
- Array of URLs found on the site

### 5. Search Tool (`firecrawl_search`)

Search the web and optionally extract content from search results.

**Best for:**
- Finding specific information across multiple websites, when you don&#039;t know which website has the information.
- When you need the most relevant content for a query

**Not recommended for:**
- When you already know which website to scrape (use scrape)
- When you need comprehensive coverage of a single website (use map or crawl)

**Common mistakes:**
- Using crawl or map for open-ended questions (use search instead)

**Usage Example:**
```json
{
  &quot;name&quot;: &quot;firecrawl_search&quot;,
  &quot;arguments&quot;: {
    &quot;query&quot;: &quot;latest AI research papers 2023&quot;,
    &quot;limit&quot;: 5,
    &quot;lang&quot;: &quot;en&quot;,
    &quot;country&quot;: &quot;us&quot;,
    &quot;scrapeOptions&quot;: {
      &quot;formats&quot;: [&quot;markdown&quot;],
      &quot;onlyMainContent&quot;: true
    }
  }
}
```

**Returns:**
- Array of search results (with optional scraped content)

**Prompt Example:**
&gt; &quot;Find the latest research papers on AI published in 2023.&quot;

### 6. Crawl Tool (`firecrawl_crawl`)

Starts an asynchronous crawl job on a website and extract content from all pages.

**Best for:**
- Extracting content from multiple related pages, when you need comprehensive coverage.

**Not recommended for:**
- Extracting content from a single page (use scrape)
- When token limits are a concern (use map + batch_scrape)
- When you need fast results (crawling can be slow)

**Warning:** Crawl responses can be very large and may exceed token limits. Limit the crawl depth and number of pages, or use map + batch_scrape for better control.

**Common mistakes:**
- Setting limit or maxDepth too high (causes token overflow)
- Using crawl for a single page (use scrape instead)

**Prompt Example:**
&gt; &quot;Get all blog posts from the first two levels of example.com/blog.&quot;

**Usage Example:**
```json
{
  &quot;name&quot;: &quot;firecrawl_crawl&quot;,
  &quot;arguments&quot;: {
    &quot;url&quot;: &quot;https://example.com/blog/*&quot;,
    &quot;maxDepth&quot;: 2,
    &quot;limit&quot;: 100,
    &quot;allowExternalLinks&quot;: false,
    &quot;deduplicateSimilarURLs&quot;: true
  }
}
```

**Returns:**
- Response includes operation ID for status checking:

```json
{
  &quot;content&quot;: [
    {
      &quot;type&quot;: &quot;text&quot;,
      &quot;text&quot;: &quot;Started crawl for: https://example.com/* with job ID: 550e8400-e29b-41d4-a716-446655440000. Use firecrawl_check_crawl_status to check progress.&quot;
    }
  ],
  &quot;isError&quot;: false
}
```

### 7. Check Crawl Status (`firecrawl_check_crawl_status`)

Check the status of a crawl job.

```json
{
  &quot;name&quot;: &quot;firecrawl_check_crawl_status&quot;,
  &quot;arguments&quot;: {
    &quot;id&quot;: &quot;550e8400-e29b-41d4-a716-446655440000&quot;
  }
}
```

**Returns:**
- Response includes the status of the crawl job:
  
### 8. Extract Tool (`firecrawl_extract`)

Extract structured information from web pages using LLM capabilities. Supports both cloud AI and self-hosted LLM extraction.

**Best for:**
- Extracting specific structured data like prices, names, details.

**Not recommended for:**
- When you need the full content of a page (use scrape)
- When you&#039;re not looking for specific structured data

**Arguments:**
- `urls`: Array of URLs to extract information from
- `prompt`: Custom prompt for the LLM extraction
- `systemPrompt`: System prompt to guide the LLM
- `schema`: JSON schema for structured data extraction
- `allowExternalLinks`: Allow extraction from external links
- `enableWebSearch`: Enable web search for additional context
- `includeSubdomains`: Include subdomains in extraction

When using a self-hosted instance, the extraction will use your configured LLM. For cloud API, it uses Firecrawl&#039;s managed LLM service.
**Prompt Example:**
&gt; &quot;Extract the product name, price, and description from these product pages.&quot;

**Usage Example:**
```json
{
  &quot;name&quot;: &quot;firecrawl_extract&quot;,
  &quot;arguments&quot;: {
    &quot;urls&quot;: [&quot;https://example.com/page1&quot;, &quot;https://example.com/page2&quot;],
    &quot;prompt&quot;: &quot;Extract product information including name, price, and description&quot;,
    &quot;systemPrompt&quot;: &quot;You are a helpful assistant that extracts product information&quot;,
    &quot;schema&quot;: {
      &quot;type&quot;: &quot;object&quot;,
      &quot;properties&quot;: {
        &quot;name&quot;: { &quot;type&quot;: &quot;string&quot; },
        &quot;price&quot;: { &quot;type&quot;: &quot;number&quot; },
        &quot;description&quot;: { &quot;type&quot;: &quot;string&quot; }
      },
      &quot;required&quot;: [&quot;name&quot;, &quot;price&quot;]
    },
    &quot;allowExternalLinks&quot;: false,
    &quot;enableWebSearch&quot;: false,
    &quot;includeSubdomains&quot;: false
  }
}
```

**Returns:**
- Extracted structured data as defined by your schema

```json
{
  &quot;content&quot;: [
    {
      &quot;type&quot;: &quot;text&quot;,
      &quot;text&quot;: {
        &quot;name&quot;: &quot;Example Product&quot;,
        &quot;price&quot;: 99.99,
        &quot;description&quot;: &quot;This is an example product description&quot;
      }
    }
  ],
  &quot;isError&quot;: false
}
```

### 9. Deep Research Tool (`firecrawl_deep_research`)

Conduct deep web research on a query using intelligent crawling, search, and LLM analysis.

**Best for:**
- Complex research questions requiring multiple sources, in-depth analysis.

**Not recommended for:**
- Simple questions that can be answered with a single search
- When you need very specific information from a known page (use scrape)
- When you need results quickly (deep research can take time)

**Arguments:**
- query (string, required): The research question or topic to explore.
- maxDepth (number, optional): Maximum recursive depth for crawling/search (default: 3).
- timeLimit (number, optional): Time limit in seconds for the research session (default: 120).
- maxUrls (number, optional): Maximum number of URLs to analyze (default: 50).

**Prompt Example:**
&gt; &quot;Research the environmental impact of electric vehicles versus gasoline vehicles.&quot;

**Usage Example:**
``

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>JavaScript</category>
        </item>
        <item>
            <title><![CDATA[open-telemetry/opentelemetry.io]]></title>
            <link>https://github.com/open-telemetry/opentelemetry.io</link>
            <guid>https://github.com/open-telemetry/opentelemetry.io</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:26 GMT</pubDate>
            <description><![CDATA[The OpenTelemetry website and documentation]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/open-telemetry/opentelemetry.io">open-telemetry/opentelemetry.io</a></h1>
            <p>The OpenTelemetry website and documentation</p>
            <p>Language: JavaScript</p>
            <p>Stars: 721</p>
            <p>Forks: 1,433</p>
            <p>Stars today: 2 stars today</p>
            <h2>README</h2><pre>&lt;!-- cSpell:ignore Chalin Ferri Benedetti Hrabusa jparsana --&gt;

# &lt;img src=&quot;https://opentelemetry.io/img/logos/opentelemetry-logo-nav.png&quot; alt=&quot;OTel logo&quot; width=&quot;45&quot;&gt; OpenTelemetry.io

This is the source repository for the [OpenTelemetry][] website, project
documentation, and blog. The site is [built][contributing.md] using [Hugo][] and
is hosted on [Netlify][].

For website analytics, see [OpenTelemetry.io Analytics][] dashboard.

## Get involved

To learn how to contribute fixes and new content to this project, read the
[Contributor&#039;s guide](https://opentelemetry.io/docs/contributing/), which
includes a style guide and useful information on the review process.

If you are new to OpenTelemetry and just get started with it, you are in a
perfect position to help us get better: the website and documentation is the
entry point for newcomers like you, so if something is unclear or something is
missing [let us know][].

### Submit a blog post

For guidance on how to write and submit a blog post, see
[Submit a blog post](https://opentelemetry.io/docs/contributing/blog/).

### Add a project to the OpenTelemetry [Registry]

For details on how to add a project to the OpenTelemetry Registry, see [Adding
to the registry][].

## Contributing

See the [Contributing](https://opentelemetry.io/docs/contributing) page in our
docs to learn how you can contribute and the
[Development Setup](https://opentelemetry.io/docs/contributing/development) page
to learn how to build the website locally.

## Found a security issue?

If you discover a security issue, read the
[Security Policy](https://github.com/open-telemetry/opentelemetry.io/security/policy)
before opening an issue.

## Meetings

We, the OTel Communications SIG, meet every two weeks on Monday at 10:00 PT.
Check out the [OpenTelemetry community calendar][] for the Zoom link and any
updates to this schedule.

Meeting notes are available as a public [Google doc][]. If you have trouble
accessing the doc, get in touch in the `#otel-comms` channel on [Slack][].

## Roles

Here is a list of community roles with current and previous members:

### Maintainers

These are the members of [@open-telemetry/docs-maintainers]:

- [Austin Parker](https://github.com/austinlparker), Honeycomb
- [Fabrizio Ferri-Benedetti](https://github.com/theletterf), Elastic
- [Patrice Chalin](https://github.com/chalin), CNCF
- [Phillip Carter](https://github.com/cartermp), Salesforce
- [Severin Neumann](https://github.com/svrnm)
- [Tiffany Hrabusa](https://github.com/tiffany76), Grafana Labs

For more information about the maintainer role, see the
[community repository](https://github.com/open-telemetry/community/blob/main/community-membership.md#maintainer).

### Approvers

These are the members of [@open-telemetry/docs-approvers]:

- [Michael Hausenblas](https://github.com/mhausenblas), Amazon
- [Ted Young](https://github.com/tedsuo), Grafana Labs

For more information about the approver role, see the
[community repository](https://github.com/open-telemetry/community/blob/main/community-membership.md#approver).

### Emeritus maintainers

- [Steve Flanders](https://github.com/flands)
- [Morgan McLean](https://github.com/mtwo)
- [jparsana](https://github.com/jparsana)

For more information about the emeritus role, see the
[community repository](https://github.com/open-telemetry/community/blob/main/guides/contributor/membership.md#emeritus-maintainerapprovertriager).

### Emeritus approvers

- [Paul Bruce](https://github.com/paulsbruce)

For more information about the emeritus role, see the
[community repository](https://github.com/open-telemetry/community/blob/main/guides/contributor/membership.md#emeritus-maintainerapprovertriager).

Learn more about roles in the [community repository][]. Thanks to [all who have
already contributed][contributors]!

## Licenses

- Documentation: [CC-BY-4.0](LICENSE)
- Code: [Apache-2.0](LICENSE-CODE)

[adding to the registry]: https://opentelemetry.io/ecosystem/registry/adding/
[let us know]:
  https://github.com/open-telemetry/opentelemetry.io/issues/new/choose
[@open-telemetry/docs-approvers]:
  https://github.com/orgs/open-telemetry/teams/docs-approvers
[@open-telemetry/docs-maintainers]:
  https://github.com/orgs/open-telemetry/teams/docs-maintainers
[community repository]:
  https://github.com/open-telemetry/community/blob/main/community-membership.md
[contributing.md]: CONTRIBUTING.md
[contributors]:
  https://github.com/open-telemetry/opentelemetry.io/graphs/contributors
[opentelemetry]: https://opentelemetry.io
[OpenTelemetry.io Analytics]: https://lookerstudio.google.com/s/jsDZ05i_YIo
[registry]: https://opentelemetry.io/ecosystem/registry/
[opentelemetry community calendar]:
  https://calendar.google.com/calendar/embed?src=google.com_b79e3e90j7bbsa2n2p5an5lf60%40group.calendar.google.com
[google doc]:
  https://docs.google.com/document/d/1wW0jLldwXN8Nptq2xmgETGbGn9eWP8fitvD5njM-xZY/edit?usp=sharing
[slack]: https://slack.cncf.io/
[hugo]: https://gohugo.io
[netlify]: https://netlify.com
</pre>
          ]]></content:encoded>
            <category>JavaScript</category>
        </item>
        <item>
            <title><![CDATA[huggingface/transformers.js]]></title>
            <link>https://github.com/huggingface/transformers.js</link>
            <guid>https://github.com/huggingface/transformers.js</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:25 GMT</pubDate>
            <description><![CDATA[State-of-the-art Machine Learning for the web. Run 🤗 Transformers directly in your browser, with no need for a server!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/huggingface/transformers.js">huggingface/transformers.js</a></h1>
            <p>State-of-the-art Machine Learning for the web. Run 🤗 Transformers directly in your browser, with no need for a server!</p>
            <p>Language: JavaScript</p>
            <p>Stars: 13,914</p>
            <p>Forks: 944</p>
            <p>Stars today: 12 stars today</p>
            <h2>README</h2><pre>

&lt;p align=&quot;center&quot;&gt;
    &lt;br/&gt;
    &lt;picture&gt; 
        &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/transformersjs-dark.svg&quot; width=&quot;500&quot; style=&quot;max-width: 100%;&quot;&gt;
        &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/transformersjs-light.svg&quot; width=&quot;500&quot; style=&quot;max-width: 100%;&quot;&gt;
        &lt;img alt=&quot;transformers.js javascript library logo&quot; src=&quot;https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/transformersjs-light.svg&quot; width=&quot;500&quot; style=&quot;max-width: 100%;&quot;&gt;
    &lt;/picture&gt;
    &lt;br/&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://www.npmjs.com/package/@huggingface/transformers&quot;&gt;&lt;img alt=&quot;NPM&quot; src=&quot;https://img.shields.io/npm/v/@huggingface/transformers&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://www.npmjs.com/package/@huggingface/transformers&quot;&gt;&lt;img alt=&quot;NPM Downloads&quot; src=&quot;https://img.shields.io/npm/dw/@huggingface/transformers&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://www.jsdelivr.com/package/npm/@huggingface/transformers&quot;&gt;&lt;img alt=&quot;jsDelivr Hits&quot; src=&quot;https://img.shields.io/jsdelivr/npm/hw/@huggingface/transformers&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/huggingface/transformers.js/blob/main/LICENSE&quot;&gt;&lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/github/license/huggingface/transformers.js?color=blue&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://huggingface.co/docs/transformers.js/index&quot;&gt;&lt;img alt=&quot;Documentation&quot; src=&quot;https://img.shields.io/website/http/huggingface.co/docs/transformers.js/index.svg?down_color=red&amp;down_message=offline&amp;up_message=online&quot;&gt;&lt;/a&gt;
&lt;/p&gt;


&lt;h3 align=&quot;center&quot;&gt;
  &lt;p&gt;State-of-the-art Machine Learning for the Web&lt;/p&gt;
&lt;/h3&gt;

Run 🤗 Transformers directly in your browser, with no need for a server!

Transformers.js is designed to be functionally equivalent to Hugging Face&#039;s [transformers](https://github.com/huggingface/transformers) python library, meaning you can run the same pretrained models using a very similar API. These models support common tasks in different modalities, such as:
  - 📝 **Natural Language Processing**: text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation.
  - 🖼️ **Computer Vision**: image classification, object detection, segmentation, and depth estimation.
  - 🗣️ **Audio**: automatic speech recognition, audio classification, and text-to-speech.
  - 🐙 **Multimodal**: embeddings, zero-shot audio classification, zero-shot image classification, and zero-shot object detection.

Transformers.js uses [ONNX Runtime](https://onnxruntime.ai/) to run models in the browser. The best part about it, is that you can easily [convert](#convert-your-models-to-onnx) your pretrained PyTorch, TensorFlow, or JAX models to ONNX using [🤗 Optimum](https://github.com/huggingface/optimum#onnx--onnx-runtime). 

For more information, check out the full [documentation](https://huggingface.co/docs/transformers.js).


## Installation


To install via [NPM](https://www.npmjs.com/package/@huggingface/transformers), run:
```bash
npm i @huggingface/transformers
```

Alternatively, you can use it in vanilla JS, without any bundler, by using a CDN or static hosting. For example, using [ES Modules](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Modules), you can import the library with:
```html
&lt;script type=&quot;module&quot;&gt;
    import { pipeline } from &#039;https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.6.0&#039;;
&lt;/script&gt;
```


## Quick tour


It&#039;s super simple to translate from existing code! Just like the python library, we support the `pipeline` API. Pipelines group together a pretrained model with preprocessing of inputs and postprocessing of outputs, making it the easiest way to run models with the library.

&lt;table&gt;
&lt;tr&gt;
&lt;th width=&quot;440px&quot; align=&quot;center&quot;&gt;&lt;b&gt;Python (original)&lt;/b&gt;&lt;/th&gt;
&lt;th width=&quot;440px&quot; align=&quot;center&quot;&gt;&lt;b&gt;Javascript (ours)&lt;/b&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;

```python
from transformers import pipeline

# Allocate a pipeline for sentiment-analysis
pipe = pipeline(&#039;sentiment-analysis&#039;)

out = pipe(&#039;I love transformers!&#039;)
# [{&#039;label&#039;: &#039;POSITIVE&#039;, &#039;score&#039;: 0.999806941}]
```

&lt;/td&gt;
&lt;td&gt;

```javascript
import { pipeline } from &#039;@huggingface/transformers&#039;;

// Allocate a pipeline for sentiment-analysis
const pipe = await pipeline(&#039;sentiment-analysis&#039;);

const out = await pipe(&#039;I love transformers!&#039;);
// [{&#039;label&#039;: &#039;POSITIVE&#039;, &#039;score&#039;: 0.999817686}]
```

&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;


You can also use a different model by specifying the model id or path as the second argument to the `pipeline` function. For example:
```javascript
// Use a different model for sentiment-analysis
const pipe = await pipeline(&#039;sentiment-analysis&#039;, &#039;Xenova/bert-base-multilingual-uncased-sentiment&#039;);
```

By default, when running in the browser, the model will be run on your CPU (via WASM). If you would like
to run the model on your GPU (via WebGPU), you can do this by setting `device: &#039;webgpu&#039;`, for example:
```javascript
// Run the model on WebGPU
const pipe = await pipeline(&#039;sentiment-analysis&#039;, &#039;Xenova/distilbert-base-uncased-finetuned-sst-2-english&#039;, {
  device: &#039;webgpu&#039;,
});
```

For more information, check out the [WebGPU guide](https://huggingface.co/docs/transformers.js/guides/webgpu).

&gt; [!WARNING]
&gt; The WebGPU API is still experimental in many browsers, so if you run into any issues,
&gt; please file a [bug report](https://github.com/huggingface/transformers.js/issues/new?title=%5BWebGPU%5D%20Error%20running%20MODEL_ID_GOES_HERE&amp;assignees=&amp;labels=bug,webgpu&amp;projects=&amp;template=1_bug-report.yml).

In resource-constrained environments, such as web browsers, it is advisable to use a quantized version of
the model to lower bandwidth and optimize performance. This can be achieved by adjusting the `dtype` option,
which allows you to select the appropriate data type for your model. While the available options may vary
depending on the specific model, typical choices include `&quot;fp32&quot;` (default for WebGPU), `&quot;fp16&quot;`, `&quot;q8&quot;`
(default for WASM), and `&quot;q4&quot;`. For more information, check out the [quantization guide](https://huggingface.co/docs/transformers.js/guides/dtypes).
```javascript
// Run the model at 4-bit quantization
const pipe = await pipeline(&#039;sentiment-analysis&#039;, &#039;Xenova/distilbert-base-uncased-finetuned-sst-2-english&#039;, {
  dtype: &#039;q4&#039;,
});
```


## Examples

Want to jump straight in? Get started with one of our sample applications/templates, which can be found [here](https://github.com/huggingface/transformers.js-examples).

| Name              | Description                      | Links                   |
|-------------------|----------------------------------|-------------------------------|
| Whisper Web       | Speech recognition w/ Whisper    | [code](https://github.com/xenova/whisper-web), [demo](https://huggingface.co/spaces/Xenova/whisper-web) |
| Doodle Dash       | Real-time sketch-recognition game | [blog](https://huggingface.co/blog/ml-web-games), [code](https://github.com/xenova/doodle-dash), [demo](https://huggingface.co/spaces/Xenova/doodle-dash) |
| Code Playground   | In-browser code completion website | [code](https://github.com/huggingface/transformers.js/tree/main/examples/code-completion/), [demo](https://huggingface.co/spaces/Xenova/ai-code-playground) |
| Semantic Image Search (client-side) | Search for images with text | [code](https://github.com/huggingface/transformers.js/tree/main/examples/semantic-image-search-client/), [demo](https://huggingface.co/spaces/Xenova/semantic-image-search-client) |
| Semantic Image Search (server-side) | Search for images with text (Supabase) | [code](https://github.com/huggingface/transformers.js/tree/main/examples/semantic-image-search/), [demo](https://huggingface.co/spaces/Xenova/semantic-image-search) |
| Vanilla JavaScript | In-browser object detection     | [video](https://scrimba.com/scrim/cKm9bDAg), [code](https://github.com/huggingface/transformers.js/tree/main/examples/vanilla-js/), [demo](https://huggingface.co/spaces/Scrimba/vanilla-js-object-detector) |
| React             | Multilingual translation website | [code](https://github.com/huggingface/transformers.js/tree/main/examples/react-translator/), [demo](https://huggingface.co/spaces/Xenova/react-translator) |
| Text to speech (client-side) | In-browser speech synthesis | [code](https://github.com/huggingface/transformers.js/tree/main/examples/text-to-speech-client/), [demo](https://huggingface.co/spaces/Xenova/text-to-speech-client) |
| Browser extension | Text classification extension    | [code](https://github.com/huggingface/transformers.js/tree/main/examples/extension/) |
| Electron          | Text classification application  | [code](https://github.com/huggingface/transformers.js/tree/main/examples/electron/)  |
| Next.js (client-side) | Sentiment analysis (in-browser inference) | [code](https://github.com/huggingface/transformers.js/tree/main/examples/next-client/), [demo](https://huggingface.co/spaces/Xenova/next-example-app) |
| Next.js (server-side) | Sentiment analysis (Node.js inference) | [code](https://github.com/huggingface/transformers.js/tree/main/examples/next-server/), [demo](https://huggingface.co/spaces/Xenova/next-server-example-app) |
| Node.js           | Sentiment analysis API           | [code](https://github.com/huggingface/transformers.js/tree/main/examples/node/)      |
| Demo site         | A collection of demos | [code](https://github.com/huggingface/transformers.js/tree/main/examples/demo-site/), [demo](https://huggingface.github.io/transformers.js/) |

Check out the Transformers.js [template](https://huggingface.co/new-space?template=static-templates%2Ftransformers.js) on Hugging Face to get started in one click!


## Custom usage



By default, Transformers.js uses [hosted pretrained models](https://huggingface.co/models?library=transformers.js) and [precompiled WASM binaries](https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.6.0/dist/), which should work out-of-the-box. You can customize this as follows:

### Settings

```javascript
import { env } from &#039;@huggingface/transformers&#039;;

// Specify a custom location for models (defaults to &#039;/models/&#039;).
env.localModelPath = &#039;/path/to/models/&#039;;

// Disable the loading of remote models from the Hugging Face Hub:
env.allowRemoteModels = false;

// Set location of .wasm files. Defaults to use a CDN.
env.backends.onnx.wasm.wasmPaths = &#039;/path/to/files/&#039;;
```

For a full list of available settings, check out the [API Reference](https://huggingface.co/docs/transformers.js/api/env).

### Convert your models to ONNX

We recommend using our [conversion script](https://github.com/huggingface/transformers.js/blob/main/scripts/convert.py) to convert your PyTorch, TensorFlow, or JAX models to ONNX in a single command. Behind the scenes, it uses [🤗 Optimum](https://huggingface.co/docs/optimum) to perform conversion and quantization of your model.

```bash
python -m scripts.convert --quantize --model_id &lt;model_name_or_path&gt;
```

For example, convert and quantize [bert-base-uncased](https://huggingface.co/bert-base-uncased) using:
```bash
python -m scripts.convert --quantize --model_id bert-base-uncased
```

This will save the following files to `./models/`:

```
bert-base-uncased/
├── config.json
├── tokenizer.json
├── tokenizer_config.json
└── onnx/
    ├── model.onnx
    └── model_quantized.onnx
```

For the full list of supported architectures, see the [Optimum documentation](https://huggingface.co/docs/optimum/main/en/exporters/onnx/overview).


## Supported tasks/models

Here is the list of all tasks and architectures currently supported by Transformers.js.
If you don&#039;t see your task/model listed here or it is not yet supported, feel free
to open up a feature request [here](https://github.com/huggingface/transformers.js/issues/new/choose).

To find compatible models on the Hub, select the &quot;transformers.js&quot; library tag in the filter menu (or visit [this link](https://huggingface.co/models?library=transformers.js)).
You can refine your search by selecting the task you&#039;re interested in (e.g., [text-classification](https://huggingface.co/models?pipeline_tag=text-classification&amp;library=transformers.js)).


### Tasks

#### Natural Language Processing

| Task                     | ID | Description | Supported? |
|--------------------------|----|-------------|------------|
| [Fill-Mask](https://huggingface.co/tasks/fill-mask)                     | `fill-mask`   | Masking some of the words in a sentence and predicting which words should replace those masks. | ✅ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.FillMaskPipeline)&lt;br&gt;[(models)](https://huggingface.co/models?pipeline_tag=fill-mask&amp;library=transformers.js) |
| [Question Answering](https://huggingface.co/tasks/question-answering)   | `question-answering`   | Retrieve the answer to a question from a given text. | ✅ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.QuestionAnsweringPipeline)&lt;br&gt;[(models)](https://huggingface.co/models?pipeline_tag=question-answering&amp;library=transformers.js) |
| [Sentence Similarity](https://huggingface.co/tasks/sentence-similarity) | `sentence-similarity`  | Determining how similar two texts are. | ✅ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.FeatureExtractionPipeline)&lt;br&gt;[(models)](https://huggingface.co/models?pipeline_tag=sentence-similarity&amp;library=transformers.js) |
| [Summarization](https://huggingface.co/tasks/summarization)             |  `summarization`  | Producing a shorter version of a document while preserving its important information. | ✅ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.SummarizationPipeline)&lt;br&gt;[(models)](https://huggingface.co/models?pipeline_tag=summarization&amp;library=transformers.js) |
| [Table Question Answering](https://huggingface.co/tasks/table-question-answering) |  `table-question-answering`  | Answering a question about information from a given table. | ❌ |
| [Text Classification](https://huggingface.co/tasks/text-classification)      | `text-classification` or `sentiment-analysis`  | Assigning a label or class to a given text. | ✅ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TextClassificationPipeline)&lt;br&gt;[(models)](https://huggingface.co/models?pipeline_tag=text-classification&amp;library=transformers.js) |
| [Text Generation](https://huggingface.co/tasks/text-generation#completion-generation-models)          | `text-generation`  | Producing new text by predicting the next word in a sequence. | ✅ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TextGenerationPipeline)&lt;br&gt;[(models)](https://huggingface.co/models?pipeline_tag=text-generation&amp;library=transformers.js) |
| [Text-to-text Generation](https://huggingface.co/tasks/text-generation#text-to-text-generation-models)  | `text2text-generation`  | Converting one text sequence into another text sequence. | ✅ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.Text2TextGenerationPipeline)&lt;br&gt;[(models)](https://huggingface.co/models?pipeline_tag=text2text-generation&amp;library=transformers.js) |
| [Token Classification](https://huggingface.co/tasks/token-classification)     | `token-classification` or `ner`  | Assigning a label to each token in a text. | ✅ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TokenClassificationPipeline)&lt;br&gt;[(models)](https://huggingface.co/models?pipeline_tag=token-classification&amp;library=transformers.js) |
| [Translation](https://huggingface.co/tasks/translation)              |  `translation`  | Converting text from one language to another. | ✅ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TranslationPipeline)&lt;br&gt;[(models)](https://huggingface.co/models?pipeline_tag=translation&amp;library=transformers.js) |
| [Zero-Shot Classification](https://huggingface.co/tasks/zero-shot-classification) | `zero-shot-classification`  | Classifying text into classes that are unseen during training.  | ✅ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ZeroShotClassificationPipeline)&lt;br&gt;[(models)](https://huggingface.co/models?pipeline_tag=zero-shot-classification&amp;library=transformers.js) |
| [Feature Extraction](https://huggingface.co/tasks/feature-extraction)         |  `feature-extraction`  | Transforming raw data into numerical features that can be processed while preserving the information in the original dataset. | ✅ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.FeatureExtractionPipeline)&lt;br&gt;[(models)](https://huggingface.co/models?pipeline_tag=feature-extraction&amp;library=transformers.js) |

#### Vision

| Task                     | ID | Description | Supported? |
|--------------------------|----|-------------|------------|
| [Background Removal](https://huggingface.co/tasks/image-segmentation#background-removal)       | `background-removal`   | Isolating the main subject of an image by removing or making the background transparent. | ✅ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.BackgroundRemovalPipeline)&lt;br&gt;[(models)](https://huggingface.co/models?other=background-removal&amp;library=transformers.js) |
| [Depth Estimation](https://huggingface.co/tasks/depth-estimation)         |  `depth-estimation`  | Predicting the depth of objects present in an image. | ✅ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.DepthEstimationPipeline)&lt;br&gt;[(models)](https://huggingface.co/models?pipeline_tag=depth-estimation&amp;library=transformers.js) |
| [Image Classification](https://huggingface.co/tasks/image-classification)                | `image-classification`   | Assigning a label or class to an entire image. | ✅ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageClassificationPipeline)&lt;br&gt;[(models)](https://huggingface.co/models?pipeline_tag=image-classification&amp;library=transformers.js) |
| [Image Segmentation](https://huggingface.co/tasks/image-segmentation)       | `image-segmentation`   | Divides an image into segments where each pixel is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation. | ✅ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageSegmentationPipeline)&lt;br&gt;[(models)](https://huggingface.co/models?pipeline_tag=image-segmentation&amp;library=transformers.js) |
| [Image-to-Image](https://huggingface.co/tasks/image-to-image)      |  `image-to-image` | Transforming a source image to match the characteristics of a target image or a target image domain. | ✅ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageToImagePipeline)&lt;br&gt;[(models)](https://huggingface.co/models?pipeline_tag=image-to-image&amp;library=transformers.js) |
| [Mask Generation](https://huggingface.co/tasks/mask-generation)            |  `mask-generation`  | Generate masks for the objects in an image. | ❌ |
| [Object Detection](https://huggingface.co/tasks/object-detection)            | `object-detection`   | Identify objects of certain defined classes within an image. | ✅ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ObjectDetectionPipeline)&lt;br&gt;[(models)](https://huggingface.co/models?pipeline_tag=object-detection&amp;library=transformers.js) |
| [Video Classification](https://huggingface.co/tasks/video-classification) |  n/a  | Assigning a label or class to an entire video. | ❌ |
| [Unconditional Image Generation](https://huggingface.co/tasks/unconditional-image-generation)      |  n/a   | Generating images with no condition in any context (like a prompt text or another image). | ❌ |
| [

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>JavaScript</category>
        </item>
        <item>
            <title><![CDATA[microsoft/monaco-editor]]></title>
            <link>https://github.com/microsoft/monaco-editor</link>
            <guid>https://github.com/microsoft/monaco-editor</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:24 GMT</pubDate>
            <description><![CDATA[A browser based code editor]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/monaco-editor">microsoft/monaco-editor</a></h1>
            <p>A browser based code editor</p>
            <p>Language: JavaScript</p>
            <p>Stars: 43,045</p>
            <p>Forks: 3,783</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre># Monaco Editor

[![Versions](https://img.shields.io/npm/v/monaco-editor)](https://www.npmjs.com/package/monaco-editor)
[![Versions](https://img.shields.io/npm/v/monaco-editor/next)](https://www.npmjs.com/package/monaco-editor)
[![Feature Requests](https://img.shields.io/github/issues/microsoft/monaco-editor/feature-request.svg)](https://github.com/microsoft/monaco-editor/issues?q=is%3Aopen+is%3Aissue+label%3Afeature-request+sort%3Areactions-%2B1-desc)
[![Bugs](https://img.shields.io/github/issues/microsoft/monaco-editor/bug.svg)](https://github.com/microsoft/monaco-editor/issues?utf8=✓&amp;q=is%3Aissue+is%3Aopen+label%3Abug)

The Monaco Editor is the fully featured code editor from [VS Code](https://github.com/microsoft/vscode). Check out the [VS Code docs](https://code.visualstudio.com/docs/editor/editingevolved) to see some of the supported features.

![image](https://user-images.githubusercontent.com/5047891/94183711-290c0780-fea3-11ea-90e3-c88ff9d21bd6.png)

## Try it out

Try out the editor and see various examples [in our interactive playground](https://microsoft.github.io/monaco-editor/playground.html).

The playground is the best way to learn about how to use the editor, which features is supports, to try out different versions and to create minimal reproducible examples for bug reports.

## Installing

```
&gt; npm install monaco-editor
```

You will get:

- inside `/esm`: ESM version of the editor (compatible with e.g. webpack)
- inside `/dev`: AMD bundled, not minified
- inside `/min`: AMD bundled, and minified
- inside `/min-maps`: source maps for `min`
- `monaco.d.ts`: this specifies the API of the editor (this is what is actually versioned, everything else is considered private and might break with any release).

It is recommended to develop against the `dev` version, and in production to use the `min` version.

## Concepts

Monaco editor is best known for being the text editor that powers VS Code. However, it&#039;s a bit more nuanced. Some basic understanding about the underlying concepts is needed to use Monaco editor effectively.

### Models

Models are at the heart of Monaco editor. It&#039;s what you interact with when managing content. A model represents a file that has been opened. This could represent a file that exists on a file system, but it doesn&#039;t have to. For example, the model holds the text content, determines the language of the content, and tracks the edit history of the content.

### URIs

Each model is identified by a URI. This is why it&#039;s not possible for two models to have the same URI. Ideally when you represent content in Monaco editor, you should think of a virtual file system that matches the files your users are editing. For example, you could use `file:///` as a base path. If a model is created without a URI, its URI will be `inmemory://model/1`. The number increases as more models are created.

### Editors

An editor is a user facing view of the model. This is what gets attached to the DOM and what your users see visually. Typical editor operations are displaying a model, managing the view state, or executing actions or commands.

### Providers

Providers provide smart editor features. For example, this includes completion and hover information. It is not the same as, but often maps to [language server protocol](https://microsoft.github.io/language-server-protocol) features.

Providers work on models. Some smart features depends on the file URI. For example, for TypeScript to resolve imports, or for JSON IntelliSense to determine which JSON schema to apply to which model. So it&#039;s important to choose proper model URIs.

### Disposables

Many Monaco related objects often implement the `.dispose()` method. This method is intended to perform cleanups when a resource is no longer needed. For example, calling `model.dispose()` will unregister it, freeing up the URI for a new model. Editors should be disposed to free up resources and remove their model listeners.

## Documentation

- Learn how to integrate the editor with these [complete samples](./samples/).
  - [Integrate the AMD version](./docs/integrate-amd.md).
  - [Integrate the ESM version](./docs/integrate-esm.md)
- Learn how to use the editor API and try out your own customizations in the [playground](https://microsoft.github.io/monaco-editor/playground.html).
- Explore the [API docs](https://microsoft.github.io/monaco-editor/docs.html) or read them straight from [`monaco.d.ts`](https://github.com/microsoft/monaco-editor/blob/gh-pages/node_modules/monaco-editor/monaco.d.ts).
- Read [this guide](https://github.com/microsoft/monaco-editor/wiki/Accessibility-Guide-for-Integrators) to ensure the editor is accessible to all your users!
- Create a Monarch tokenizer for a new programming language [in the Monarch playground](https://microsoft.github.io/monaco-editor/monarch.html).
- Ask questions on [StackOverflow](https://stackoverflow.com/questions/tagged/monaco-editor)! Search open and closed issues, there are a lot of tips in there!

## Issues

Create [issues](https://github.com/microsoft/monaco-editor/issues) in this repository for anything related to the Monaco Editor. Please search for existing issues to avoid duplicates.

## FAQ

❓ **What is the relationship between VS Code and the Monaco Editor?**

The Monaco Editor is generated straight from VS Code&#039;s sources with some shims around services the code needs to make it run in a web browser outside of its home.

❓ **What is the relationship between VS Code&#039;s version and the Monaco Editor&#039;s version?**

None. The Monaco Editor is a library and it reflects directly the source code.

❓ **I&#039;ve written an extension for VS Code, will it work on the Monaco Editor in a browser?**

No.

&gt; Note: If the extension is fully based on the [LSP](https://microsoft.github.io/language-server-protocol/) and if the language server is authored in JavaScript, then it would be possible.

❓ **Why all these web workers and why should I care?**

Language services create web workers to compute heavy stuff outside of the UI thread. They cost hardly anything in terms of resource overhead and you shouldn&#039;t worry too much about them, as long as you get them to work (see above the cross-domain case).

❓ **What is this `loader.js`? Can I use `require.js`?**

It is an AMD loader that we use in VS Code. Yes.

❓ **I see the warning &quot;Could not create web worker&quot;. What should I do?**

HTML5 does not allow pages loaded on `file://` to create web workers. Please load the editor with a web server on `http://` or `https://` schemes.

❓ **Is the editor supported in mobile browsers or mobile web app frameworks?**

No.

❓ **Why doesn&#039;t the editor support TextMate grammars?**

- Please see https://github.com/bolinfest/monaco-tm which puts together `monaco-editor`, `vscode-oniguruma` and `vscode-textmate` to get TM grammar support in the editor.

## Contributing / Local Development

We are welcoming contributions from the community!
Please see [CONTRIBUTING](./CONTRIBUTING.md) for details how you can contribute effectively, how you can run the editor from sources and how you can debug and fix issues.

## Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## License

Licensed under the [MIT](https://github.com/microsoft/monaco-editor/blob/main/LICENSE.txt) License.
</pre>
          ]]></content:encoded>
            <category>JavaScript</category>
        </item>
        <item>
            <title><![CDATA[open-webui/open-webui]]></title>
            <link>https://github.com/open-webui/open-webui</link>
            <guid>https://github.com/open-webui/open-webui</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:23 GMT</pubDate>
            <description><![CDATA[User-friendly AI Interface (Supports Ollama, OpenAI API, ...)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/open-webui/open-webui">open-webui/open-webui</a></h1>
            <p>User-friendly AI Interface (Supports Ollama, OpenAI API, ...)</p>
            <p>Language: JavaScript</p>
            <p>Stars: 100,513</p>
            <p>Forks: 13,334</p>
            <p>Stars today: 105 stars today</p>
            <h2>README</h2><pre># Open WebUI 👋

![GitHub stars](https://img.shields.io/github/stars/open-webui/open-webui?style=social)
![GitHub forks](https://img.shields.io/github/forks/open-webui/open-webui?style=social)
![GitHub watchers](https://img.shields.io/github/watchers/open-webui/open-webui?style=social)
![GitHub repo size](https://img.shields.io/github/repo-size/open-webui/open-webui)
![GitHub language count](https://img.shields.io/github/languages/count/open-webui/open-webui)
![GitHub top language](https://img.shields.io/github/languages/top/open-webui/open-webui)
![GitHub last commit](https://img.shields.io/github/last-commit/open-webui/open-webui?color=red)
[![Discord](https://img.shields.io/badge/Discord-Open_WebUI-blue?logo=discord&amp;logoColor=white)](https://discord.gg/5rJgQTnV4s)
[![](https://img.shields.io/static/v1?label=Sponsor&amp;message=%E2%9D%A4&amp;logo=GitHub&amp;color=%23fe8e86)](https://github.com/sponsors/tjbck)

**Open WebUI is an [extensible](https://docs.openwebui.com/features/plugin/), feature-rich, and user-friendly self-hosted AI platform designed to operate entirely offline.** It supports various LLM runners like **Ollama** and **OpenAI-compatible APIs**, with **built-in inference engine** for RAG, making it a **powerful AI deployment solution**.

![Open WebUI Demo](./demo.gif)

&gt; [!TIP]  
&gt; **Looking for an [Enterprise Plan](https://docs.openwebui.com/enterprise)?** – **[Speak with Our Sales Team Today!](mailto:sales@openwebui.com)**
&gt;
&gt; Get **enhanced capabilities**, including **custom theming and branding**, **Service Level Agreement (SLA) support**, **Long-Term Support (LTS) versions**, and **more!**

For more information, be sure to check out our [Open WebUI Documentation](https://docs.openwebui.com/).

## Key Features of Open WebUI ⭐

- 🚀 **Effortless Setup**: Install seamlessly using Docker or Kubernetes (kubectl, kustomize or helm) for a hassle-free experience with support for both `:ollama` and `:cuda` tagged images.

- 🤝 **Ollama/OpenAI API Integration**: Effortlessly integrate OpenAI-compatible APIs for versatile conversations alongside Ollama models. Customize the OpenAI API URL to link with **LMStudio, GroqCloud, Mistral, OpenRouter, and more**.

- 🛡️ **Granular Permissions and User Groups**: By allowing administrators to create detailed user roles and permissions, we ensure a secure user environment. This granularity not only enhances security but also allows for customized user experiences, fostering a sense of ownership and responsibility amongst users.

- 📱 **Responsive Design**: Enjoy a seamless experience across Desktop PC, Laptop, and Mobile devices.

- 📱 **Progressive Web App (PWA) for Mobile**: Enjoy a native app-like experience on your mobile device with our PWA, providing offline access on localhost and a seamless user interface.

- ✒️🔢 **Full Markdown and LaTeX Support**: Elevate your LLM experience with comprehensive Markdown and LaTeX capabilities for enriched interaction.

- 🎤📹 **Hands-Free Voice/Video Call**: Experience seamless communication with integrated hands-free voice and video call features, allowing for a more dynamic and interactive chat environment.

- 🛠️ **Model Builder**: Easily create Ollama models via the Web UI. Create and add custom characters/agents, customize chat elements, and import models effortlessly through [Open WebUI Community](https://openwebui.com/) integration.

- 🐍 **Native Python Function Calling Tool**: Enhance your LLMs with built-in code editor support in the tools workspace. Bring Your Own Function (BYOF) by simply adding your pure Python functions, enabling seamless integration with LLMs.

- 📚 **Local RAG Integration**: Dive into the future of chat interactions with groundbreaking Retrieval Augmented Generation (RAG) support. This feature seamlessly integrates document interactions into your chat experience. You can load documents directly into the chat or add files to your document library, effortlessly accessing them using the `#` command before a query.

- 🔍 **Web Search for RAG**: Perform web searches using providers like `SearXNG`, `Google PSE`, `Brave Search`, `serpstack`, `serper`, `Serply`, `DuckDuckGo`, `TavilySearch`, `SearchApi` and `Bing` and inject the results directly into your chat experience.

- 🌐 **Web Browsing Capability**: Seamlessly integrate websites into your chat experience using the `#` command followed by a URL. This feature allows you to incorporate web content directly into your conversations, enhancing the richness and depth of your interactions.

- 🎨 **Image Generation Integration**: Seamlessly incorporate image generation capabilities using options such as AUTOMATIC1111 API or ComfyUI (local), and OpenAI&#039;s DALL-E (external), enriching your chat experience with dynamic visual content.

- ⚙️ **Many Models Conversations**: Effortlessly engage with various models simultaneously, harnessing their unique strengths for optimal responses. Enhance your experience by leveraging a diverse set of models in parallel.

- 🔐 **Role-Based Access Control (RBAC)**: Ensure secure access with restricted permissions; only authorized individuals can access your Ollama, and exclusive model creation/pulling rights are reserved for administrators.

- 🌐🌍 **Multilingual Support**: Experience Open WebUI in your preferred language with our internationalization (i18n) support. Join us in expanding our supported languages! We&#039;re actively seeking contributors!

- 🧩 **Pipelines, Open WebUI Plugin Support**: Seamlessly integrate custom logic and Python libraries into Open WebUI using [Pipelines Plugin Framework](https://github.com/open-webui/pipelines). Launch your Pipelines instance, set the OpenAI URL to the Pipelines URL, and explore endless possibilities. [Examples](https://github.com/open-webui/pipelines/tree/main/examples) include **Function Calling**, User **Rate Limiting** to control access, **Usage Monitoring** with tools like Langfuse, **Live Translation with LibreTranslate** for multilingual support, **Toxic Message Filtering** and much more.

- 🌟 **Continuous Updates**: We are committed to improving Open WebUI with regular updates, fixes, and new features.

Want to learn more about Open WebUI&#039;s features? Check out our [Open WebUI documentation](https://docs.openwebui.com/features) for a comprehensive overview!

## Sponsors 🙌

#### Emerald

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;
      &lt;a href=&quot;https://n8n.io/&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://docs.openwebui.com/sponsors/logos/n8n.png&quot; alt=&quot;n8n&quot; style=&quot;width: 8rem; height: 8rem; border-radius: .75rem;&quot; /&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;a href=&quot;https://n8n.io/&quot;&gt;n8n&lt;/a&gt; • Does your interface have a backend yet?&lt;br&gt;Try &lt;a href=&quot;https://n8n.io/&quot;&gt;n8n&lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;
      &lt;a href=&quot;https://warp.dev/open-webui&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://docs.openwebui.com/sponsors/logos/warp.png&quot; alt=&quot;Warp&quot; style=&quot;width: 8rem; height: 8rem; border-radius: .75rem;&quot; /&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;a href=&quot;https://warp.dev/open-webui&quot;&gt;Warp&lt;/a&gt; • The intelligent terminal for developers
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;
      &lt;a href=&quot;https://tailscale.com/blog/self-host-a-local-ai-stack/?utm_source=OpenWebUI&amp;utm_medium=paid-ad-placement&amp;utm_campaign=OpenWebUI-Docs&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://docs.openwebui.com/sponsors/logos/tailscale.png&quot; alt=&quot;Tailscale&quot; style=&quot;width: 8rem; height: 8rem; border-radius: .75rem;&quot; /&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;a href=&quot;https://tailscale.com/blog/self-host-a-local-ai-stack/?utm_source=OpenWebUI&amp;utm_medium=paid-ad-placement&amp;utm_campaign=OpenWebUI-Docs&quot;&gt;Tailscale&lt;/a&gt; • Connect self-hosted AI to any device with Tailscale
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

---

We are incredibly grateful for the generous support of our sponsors. Their contributions help us to maintain and improve our project, ensuring we can continue to deliver quality work to our community. Thank you!

## How to Install 🚀

### Installation via Python pip 🐍

Open WebUI can be installed using pip, the Python package installer. Before proceeding, ensure you&#039;re using **Python 3.11** to avoid compatibility issues.

1. **Install Open WebUI**:
   Open your terminal and run the following command to install Open WebUI:

   ```bash
   pip install open-webui
   ```

2. **Running Open WebUI**:
   After installation, you can start Open WebUI by executing:

   ```bash
   open-webui serve
   ```

This will start the Open WebUI server, which you can access at [http://localhost:8080](http://localhost:8080)

### Quick Start with Docker 🐳

&gt; [!NOTE]  
&gt; Please note that for certain Docker environments, additional configurations might be needed. If you encounter any connection issues, our detailed guide on [Open WebUI Documentation](https://docs.openwebui.com/) is ready to assist you.

&gt; [!WARNING]
&gt; When using Docker to install Open WebUI, make sure to include the `-v open-webui:/app/backend/data` in your Docker command. This step is crucial as it ensures your database is properly mounted and prevents any loss of data.

&gt; [!TIP]  
&gt; If you wish to utilize Open WebUI with Ollama included or CUDA acceleration, we recommend utilizing our official images tagged with either `:cuda` or `:ollama`. To enable CUDA, you must install the [Nvidia CUDA container toolkit](https://docs.nvidia.com/dgx/nvidia-container-runtime-upgrade/) on your Linux/WSL system.

### Installation with Default Configuration

- **If Ollama is on your computer**, use this command:

  ```bash
  docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

- **If Ollama is on a Different Server**, use this command:

  To connect to Ollama on another server, change the `OLLAMA_BASE_URL` to the server&#039;s URL:

  ```bash
  docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

- **To run Open WebUI with Nvidia GPU support**, use this command:

  ```bash
  docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda
  ```

### Installation for OpenAI API Usage Only

- **If you&#039;re only using OpenAI API**, use this command:

  ```bash
  docker run -d -p 3000:8080 -e OPENAI_API_KEY=your_secret_key -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

### Installing Open WebUI with Bundled Ollama Support

This installation method uses a single container image that bundles Open WebUI with Ollama, allowing for a streamlined setup via a single command. Choose the appropriate command based on your hardware setup:

- **With GPU Support**:
  Utilize GPU resources by running the following command:

  ```bash
  docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
  ```

- **For CPU Only**:
  If you&#039;re not using a GPU, use this command instead:

  ```bash
  docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
  ```

Both commands facilitate a built-in, hassle-free installation of both Open WebUI and Ollama, ensuring that you can get everything up and running swiftly.

After installation, you can access Open WebUI at [http://localhost:3000](http://localhost:3000). Enjoy! 😄

### Other Installation Methods

We offer various installation alternatives, including non-Docker native installation methods, Docker Compose, Kustomize, and Helm. Visit our [Open WebUI Documentation](https://docs.openwebui.com/getting-started/) or join our [Discord community](https://discord.gg/5rJgQTnV4s) for comprehensive guidance.

Look at the [Local Development Guide](https://docs.openwebui.com/getting-started/advanced-topics/development) for instructions on setting up a local development environment.

### Troubleshooting

Encountering connection issues? Our [Open WebUI Documentation](https://docs.openwebui.com/troubleshooting/) has got you covered. For further assistance and to join our vibrant community, visit the [Open WebUI Discord](https://discord.gg/5rJgQTnV4s).

#### Open WebUI: Server Connection Error

If you&#039;re experiencing connection issues, it’s often due to the WebUI docker container not being able to reach the Ollama server at 127.0.0.1:11434 (host.docker.internal:11434) inside the container . Use the `--network=host` flag in your docker command to resolve this. Note that the port changes from 3000 to 8080, resulting in the link: `http://localhost:8080`.

**Example Docker Command**:

```bash
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

### Keeping Your Docker Installation Up-to-Date

In case you want to update your local Docker installation to the latest version, you can do it with [Watchtower](https://containrrr.dev/watchtower/):

```bash
docker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui
```

In the last part of the command, replace `open-webui` with your container name if it is different.

Check our Updating Guide available in our [Open WebUI Documentation](https://docs.openwebui.com/getting-started/updating).

### Using the Dev Branch 🌙

&gt; [!WARNING]
&gt; The `:dev` branch contains the latest unstable features and changes. Use it at your own risk as it may have bugs or incomplete features.

If you want to try out the latest bleeding-edge features and are okay with occasional instability, you can use the `:dev` tag like this:

```bash
docker run -d -p 3000:8080 -v open-webui:/app/backend/data --name open-webui --add-host=host.docker.internal:host-gateway --restart always ghcr.io/open-webui/open-webui:dev
```

### Offline Mode

If you are running Open WebUI in an offline environment, you can set the `HF_HUB_OFFLINE` environment variable to `1` to prevent attempts to download models from the internet.

```bash
export HF_HUB_OFFLINE=1
```

## What&#039;s Next? 🌟

Discover upcoming features on our roadmap in the [Open WebUI Documentation](https://docs.openwebui.com/roadmap/).

## License 📜

This project is licensed under the [Open WebUI License](LICENSE), a revised BSD-3-Clause license. You receive all the same rights as the classic BSD-3 license: you can use, modify, and distribute the software, including in proprietary and commercial products, with minimal restrictions. The only additional requirement is to preserve the &quot;Open WebUI&quot; branding, as detailed in the LICENSE file. For full terms, see the [LICENSE](LICENSE) document. 📄

## Support 💬

If you have any questions, suggestions, or need assistance, please open an issue or join our
[Open WebUI Discord community](https://discord.gg/5rJgQTnV4s) to connect with us! 🤝

## Star History

&lt;a href=&quot;https://star-history.com/#open-webui/open-webui&amp;Date&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=open-webui/open-webui&amp;type=Date&amp;theme=dark&quot; /&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=open-webui/open-webui&amp;type=Date&quot; /&gt;
    &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=open-webui/open-webui&amp;type=Date&quot; /&gt;
  &lt;/picture&gt;
&lt;/a&gt;

---

Created by [Timothy Jaeryang Baek](https://github.com/tjbck) - Let&#039;s make Open WebUI even more amazing together! 💪
</pre>
          ]]></content:encoded>
            <category>JavaScript</category>
        </item>
        <item>
            <title><![CDATA[expressjs/express]]></title>
            <link>https://github.com/expressjs/express</link>
            <guid>https://github.com/expressjs/express</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:22 GMT</pubDate>
            <description><![CDATA[Fast, unopinionated, minimalist web framework for node.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/expressjs/express">expressjs/express</a></h1>
            <p>Fast, unopinionated, minimalist web framework for node.</p>
            <p>Language: JavaScript</p>
            <p>Stars: 67,247</p>
            <p>Forks: 19,188</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>JavaScript</category>
        </item>
        <item>
            <title><![CDATA[prebid/Prebid.js]]></title>
            <link>https://github.com/prebid/Prebid.js</link>
            <guid>https://github.com/prebid/Prebid.js</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:21 GMT</pubDate>
            <description><![CDATA[Setup and manage header bidding advertising partners without writing code or confusing line items. Prebid.js is open source and free.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/prebid/Prebid.js">prebid/Prebid.js</a></h1>
            <p>Setup and manage header bidding advertising partners without writing code or confusing line items. Prebid.js is open source and free.</p>
            <p>Language: JavaScript</p>
            <p>Stars: 1,455</p>
            <p>Forks: 2,194</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre>[![Build Status](https://circleci.com/gh/prebid/Prebid.js.svg?style=svg)](https://circleci.com/gh/prebid/Prebid.js)
[![Percentage of issues still open](http://isitmaintained.com/badge/open/prebid/Prebid.js.svg)](https://isitmaintained.com/project/prebid/Prebid.js &quot;Percentage of issues still open&quot;)
[![Coverage Status](https://coveralls.io/repos/github/prebid/Prebid.js/badge.svg)](https://coveralls.io/github/prebid/Prebid.js)

# Prebid.js

&gt; A free and open source library for publishers to quickly implement header bidding.

This README is for developers who want to contribute to Prebid.js.
Additional documentation can be found at [the Prebid.js documentation homepage](https://docs.prebid.org/prebid/prebidjs.html).
Working examples can be found in [the developer docs](https://prebid.org/dev-docs/getting-started.html).

Prebid.js is open source software that is offered for free as a convenience. While it is designed to help companies address legal requirements associated with header bidding, we cannot and do not warrant that your use of Prebid.js will satisfy legal requirements. You are solely responsible for ensuring that your use of Prebid.js complies with all applicable laws.  We strongly encourage you to obtain legal advice when using Prebid.js to ensure your implementation complies with all laws where you operate.

**Table of Contents**

- [Usage](#Usage)
- [Install](#Install)
- [Build](#Build)
- [Run](#Run)
- [Contribute](#Contribute)

&lt;a name=&quot;Usage&quot;&gt;&lt;/a&gt;

## Usage (as a npm dependency)

*Note:* Requires Prebid.js v1.38.0+

Prebid.js depends on Babel and some Babel Plugins in order to run correctly in the browser.  Here are some examples for
configuring webpack to work with Prebid.js.

With Babel 7:
```javascript
// webpack.conf.js
let path = require(&#039;path&#039;);
module.exports = {
  mode: &#039;production&#039;,
  module: {
    rules: [

      // this rule can be excluded if you don&#039;t require babel-loader for your other application files
      {
        test: /\.m?js$/,
        exclude: /node_modules/,
        use: {
          loader: &#039;babel-loader&#039;,
        }
      },

      // this separate rule is required to make sure that the Prebid.js files are babel-ified.  this rule will
      // override the regular exclusion from above (for being inside node_modules).
      {
        test: /.js$/,
        include: new RegExp(`\\${path.sep}prebid\\.js`),
        use: {
          loader: &#039;babel-loader&#039;,
          // presets and plugins for Prebid.js must be manually specified separate from your other babel rule.
          // this can be accomplished by requiring prebid&#039;s .babelrc.js file (requires Babel 7 and Node v8.9.0+)
          // as of Prebid 6, babelrc.js only targets modern browsers. One can change the targets and build for
          // older browsers if they prefer, but integration tests on ie11 were removed in Prebid.js 6.0
          options: require(&#039;prebid.js/.babelrc.js&#039;)
        }
      }
    ]
  }
}
```

Or for Babel 6:
```javascript
            // you must manually install and specify the presets and plugins yourself
            options: {
              plugins: [
                &quot;transform-object-assign&quot;, // required (for IE support) and &quot;babel-plugin-transform-object-assign&quot;
                                           // must be installed as part of your package.
                require(&#039;prebid.js/plugins/pbjsGlobals.js&#039;) // required!
              ],
              presets: [
                [&quot;env&quot;, {                 // you can use other presets if you wish.
                  &quot;targets&quot;: {            // this example is using &quot;babel-presets-env&quot;, which must be installed if you
                    &quot;browsers&quot;: [         // follow this example.
                      ... // your browser targets. they should probably match the targets you&#039;re using for the rest
                          // of your application
                    ]
                  }
                }]
              ]
            }
```

Then you can use Prebid.js as any other npm dependency

```javascript
import pbjs from &#039;prebid.js&#039;;
import &#039;prebid.js/modules/rubiconBidAdapter&#039;; // imported modules will register themselves automatically with prebid
import &#039;prebid.js/modules/appnexusBidAdapter&#039;;
pbjs.processQueue();  // required to process existing pbjs.queue blocks and setup any further pbjs.queue execution

pbjs.requestBids({
  ...
})

```



&lt;a name=&quot;Install&quot;&gt;&lt;/a&gt;

## Install



    $ git clone https://github.com/prebid/Prebid.js.git
    $ cd Prebid.js
    $ npm ci

*Note:* You need to have `NodeJS` 12.16.1 or greater installed.

*Note:* In the 1.24.0 release of Prebid.js we have transitioned to using gulp 4.0 from using gulp 3.9.1.  To comply with gulp&#039;s recommended setup for 4.0, you&#039;ll need to have `gulp-cli` installed globally prior to running the general `npm ci`.  This shouldn&#039;t impact any other projects you may work on that use an earlier version of gulp in its setup.

If you have a previous version of `gulp` installed globally, you&#039;ll need to remove it before installing `gulp-cli`.  You can check if this is installed by running `gulp -v` and seeing the version that&#039;s listed in the `CLI` field of the output.  If you have the `gulp` package installed globally, it&#039;s likely the same version that you&#039;ll see in the `Local` field.  If you already have `gulp-cli` installed, it should be a lower major version (it&#039;s at version `2.0.1` at the time of the transition).

To remove the old package, you can use the command: `npm rm gulp -g`

Once setup, run the following command to globally install the `gulp-cli` package: `npm install gulp-cli -g`


&lt;a name=&quot;Build&quot;&gt;&lt;/a&gt;

## Build for Development

To build the project on your local machine we recommend, running:

    $ gulp serve-and-test --file &lt;spec_file.js&gt;

This will run testing but not linting. A web server will start at `http://localhost:9999` serving from the project root and generates the following files:

+ `./build/dev/prebid.js` - Full source code for dev and debug
+ `./build/dev/prebid.js.map` - Source map for dev and debug
+ `./build/dev/prebid-core.js`
+ `./build/dev/prebid-core.js.map`


Development may be a bit slower but if you prefer linting and additional watch files you can also still run just:

    $ gulp serve


### Build Optimization

The standard build output contains all the available modules from within the `modules` folder.  Note, however that there are bid adapters which support multiple bidders through aliases, so if you don&#039;t see a file in modules for a bid adapter, you may need to grep the repository to find the name of the module you need to include.

You might want to exclude some/most of them from the final bundle.  To make sure the build only includes the modules you want, you can specify the modules to be included with the `--modules` CLI argument.

For example, when running the serve command: `gulp serve --modules=openxBidAdapter,rubiconBidAdapter,sovrnBidAdapter`

Building with just these adapters will result in a smaller bundle which should allow your pages to load faster.

**Build standalone prebid.js**

- Clone the repo, run `npm ci`
- Then run the build:

        $ gulp build --modules=openxBidAdapter,rubiconBidAdapter,sovrnBidAdapter

Alternatively, a `.json` file can be specified that contains a list of modules you would like to include.

    $ gulp build --modules=modules.json

With `modules.json` containing the following
```json modules.json
[
  &quot;openxBidAdapter&quot;,
  &quot;rubiconBidAdapter&quot;,
  &quot;sovrnBidAdapter&quot;
]
```

**Build prebid.js using npm for bundling**

In case you&#039;d like to explicitly show that your project uses `prebid.js` and want a reproducible build, consider adding it as an `npm` dependency.

- Add `prebid.js` as a `npm` dependency of your project: `npm install prebid.js`
- Run the `prebid.js` build under the `node_modules/prebid.js/` folder

        $ gulp build --modules=path/to/your/list-of-modules.json

Most likely your custom `prebid.js` will only change when there&#039;s:

- A change in your list of modules
- A new release of `prebid.js`

Having said that, you are probably safe to check your custom bundle into your project.  You can also generate it in your build process.

**Build once, bundle multiple times**

If you need to generate multiple distinct bundles from the same Prebid version, you can reuse a single build with:

```
gulp build
gulp bundle --tag one --modules=one.json
gulp bundle --tag two --modules=two.json
```

This generates slightly larger files, but has the advantage of being much faster to run (after the initial `gulp build`). It&#039;s also the method used by [the Prebid.org download page](https://docs.prebid.org/download.html).

&lt;a name=&quot;Run&quot;&gt;&lt;/a&gt;

### Excluding particular features from the build

Since version 7.2.0, you may instruct the build to exclude code for some features - for example, if you don&#039;t need support for native ads:

```
gulp build --disable NATIVE --modules=openxBidAdapter,rubiconBidAdapter,sovrnBidAdapter # substitute your module list
```

Or, if you are consuming Prebid through npm, with the `disableFeatures` option in your Prebid rule:

```javascript
  {
    test: /.js$/,
    include: new RegExp(`\\${path.sep}prebid\\.js`),
    use: {
      loader: &#039;babel-loader&#039;,
      options: require(&#039;prebid.js/babelConfig.js&#039;)({disableFeatures: [&#039;NATIVE&#039;]})
    }
  }
```

Features that can be disabled this way are:

 - `VIDEO` - support for video bids;
 - `NATIVE` - support for native bids;
 - `UID2_CSTG` - support for UID2 client side token generation (see [Unified ID 2.0](https://docs.prebid.org/dev-docs/modules/userid-submodules/unified2.html))
 - `GREEDY` - disables the use blocking, &quot;greedy&quot; promises within Prebid (see below).    

#### Greedy promises

By default, Prebid attempts to hold control of the main thread when possible, using a [custom implementation of `Promise`](https://github.com/prebid/Prebid.js/blob/master/libraries/greedy/greedyPromise.js) that does not submit callbacks to the scheduler once the promise is resolved (running them immediately instead).
Disabling this behavior instructs Prebid to use the standard `window.Promise` instead; this has the effect of breaking up task execution, making them slower overall but giving the browser more chances to run other tasks in between, which can improve UX.         

You may also override the `Promise` constructor used by Prebid through `pbjs.Promise`, for example:

```javascript
var pbjs = pbjs || {};
pbjs.Promise = myCustomPromiseConstructor;
```

## Unminified code

You can get a version of the code that&#039;s unminified for debugging with `build-bundle-dev`:

```bash
gulp build-bundle-dev --modules=bidderA,module1,...
```

The results will be in build/dev/prebid.js.

## ES5 Output Support

For compatibility with older parsers or environments that require ES5 syntax, you can generate ES5-compatible output using the `--ES5` flag:

```bash
gulp build-bundle-dev --modules=bidderA,module1,... --ES5
```

This will:
- Transpile all code to ES5 syntax using CommonJS modules
- Target browsers: IE11+, Chrome 50+, Firefox 50+, Safari 10+
- Ensure compatibility with older JavaScript parsers

**Note:** Without the `--ES5` flag, the build will use modern ES6+ syntax by default for better performance and smaller bundle sizes.

## Test locally

To lint the code:

```bash
gulp lint
```

To lint and only show errors

```bash
gulp lint --no-lint-warnings
```

To run the unit tests:

```bash
gulp test
```

To run the unit tests for a particular file (example for pubmaticBidAdapter_spec.js):
```bash
gulp test --file &quot;test/spec/modules/pubmaticBidAdapter_spec.js&quot; --nolint
```

To generate and view the code coverage reports:

```bash
gulp test-coverage
gulp view-coverage
```

Local end-to-end testing can be done with:

```bash
gulp e2e-test --local
```

For Prebid.org members with access to BrowserStack, additional end-to-end testing can be done with:

```bash
gulp e2e-test --host=test.localhost
```

To run these tests, the following items are required:
- setup an alias of localhost in your `hosts` file (eg `127.0.0.1  test.localhost`); note - you can use any alias.  Use this alias in the command-line argument above.
- access to [BrowserStack](https://www.browserstack.com/) account.  Assign the following variables in your bash_profile:
```bash
export BROWSERSTACK_USERNAME=&#039;YourUserNameHere&#039;
export BROWSERSTACK_ACCESS_KEY=&#039;YourAccessKeyHere&#039;
```
You can get these BrowserStack values from your profile page.

For development:

```javascript
(function() {
    var d = document, pbs = d.createElement(&#039;script&#039;), pro = d.location.protocol;
    pbs.type = &#039;text/javascript&#039;;
    pbs.src = ((pro === &#039;https:&#039;) ? &#039;https&#039; : &#039;http&#039;) + &#039;./build/dev/prebid.js&#039;;
    var target = document.getElementsByTagName(&#039;head&#039;)[0];
    target.insertBefore(pbs, target.firstChild);
})();
```

For deployment:

```javascript
(function() {
    var d = document, pbs = d.createElement(&#039;script&#039;), pro = d.location.protocol;
    pbs.type = &#039;text/javascript&#039;;
    pbs.src = ((pro === &#039;https:&#039;) ? &#039;https&#039; : &#039;http&#039;) + &#039;./build/dist/prebid.js&#039;;
    var target = document.getElementsByTagName(&#039;head&#039;)[0];
    target.insertBefore(pbs, target.firstChild);
})();
```

Build and run the project locally with:

```bash
gulp serve
```

This runs `lint` and `test`, then starts a web server at `http://localhost:9999` serving from the project root.
Navigate to your example implementation to test, and if your `prebid.js` file is sourced from the `./build/dev`
directory you will have sourcemaps available in your browser&#039;s developer tools.

To run the example file, go to:

+ `http://localhost:9999/integrationExamples/gpt/hello_world.html`

As you make code changes, the bundles will be rebuilt and the page reloaded automatically.

&lt;a name=&quot;Contribute&quot;&gt;&lt;/a&gt;

## Contribute

Many SSPs, bidders, and publishers have contributed to this project. [Hundreds of bidders](https://github.com/prebid/Prebid.js/tree/master/modules) are supported by Prebid.js.

For guidelines, see [Contributing](./CONTRIBUTING.md).

Our PR review process can be found [here](https://github.com/prebid/Prebid.js/tree/master/PR_REVIEW.md).

### Add a Bidder Adapter

To add a bidder adapter module, see the instructions in [How to add a bidder adapter](https://docs.prebid.org/dev-docs/bidder-adaptor.html).

### Code Quality

Code quality is defined by `.eslintrc` and errors are reported in the terminal.

If you are contributing code, you should [configure your editor](http://eslint.org/docs/user-guide/integrations#editors) with the provided `.eslintrc` settings.

### Unit Testing with Karma

        $ gulp test --watch --browsers=chrome

This will run tests and keep the Karma test browser open. If your `prebid.js` file is sourced from the `./build/dev` directory you will also have sourcemaps available when using your browser&#039;s developer tools.

+ To access the Karma debug page, go to `http://localhost:9876/debug.html`

+ For test results, see the console

+ To set breakpoints in source code, see the developer tools

Detailed code coverage reporting can be generated explicitly with

        $ gulp test --coverage

The results will be in

        ./build/coverage

*Note*: Starting in June 2016, all pull requests to Prebid.js need to include tests with greater than 80% code coverage before they can be merged.  For more information, see [#421](https://github.com/prebid/Prebid.js/issues/421).

For instructions on writing tests for Prebid.js, see [Testing Prebid.js](https://prebid.org/dev-docs/testing-prebid.html).

### Supported Browsers

Prebid.js is supported on IE11 and modern browsers until 5.x. 6.x+ transpiles to target &gt;0.25%; not Opera Mini; not IE11.

### Governance
Review our governance model [here](https://github.com/prebid/Prebid.js/tree/master/governance.md).
### END
</pre>
          ]]></content:encoded>
            <category>JavaScript</category>
        </item>
    </channel>
</rss>